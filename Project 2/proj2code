{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------LINEAR REGRESSION--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HUMAN-OBSERVED DATA\n",
    "human_raw = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "human_differentpairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "human_samepairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "# GSC DATA\n",
    "GSC_raw = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/GSC-Features.csv\")\n",
    "GSC_differentpairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\")\n",
    "GSC_samepairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/same_pairs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning the human data sets in order to make training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking features of images for the human observed dataset(diff pairs)\n",
    "human_differentpairs['feature_id_A'] = 'NaN'\n",
    "human_differentpairs['feature_id_B'] = 'NaN'\n",
    "for i in human_differentpairs.index[0:1300]:\n",
    "    a=human_raw[human_raw[\"img_id\"] == human_differentpairs.at[i,\"img_id_A\"]]\n",
    "    b=human_raw[human_raw[\"img_id\"] == human_differentpairs.at[i,\"img_id_B\"]]\n",
    "    human_differentpairs.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    human_differentpairs.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Taking a subset of the data as it is very big\n",
    "human_df1=human_differentpairs[0:1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "1         [0, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "2          [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "3          [0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "4         [0, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "5         [0, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "6          [1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "7         [1, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "8         [0, 0, 0, -3, 0, 0, 0, 2, 0]\n",
       "9         [-1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "10       [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "11         [1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "12        [0, 0, 0, 0, 0, 0, -1, 0, 0]\n",
       "13        [1, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "14        [1, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "15        [-1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "16       [-1, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "17        [0, 0, 0, 0, 0, 0, -1, 1, 0]\n",
       "18        [1, 0, 0, -3, 0, 0, 0, 2, 0]\n",
       "19       [0, 0, 0, -3, 0, 0, -1, 0, 0]\n",
       "20        [0, 0, 0, -1, 0, 0, 0, 0, 0]\n",
       "21       [1, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "22        [-1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "23        [0, 0, 0, 0, 0, 0, 0, -2, 0]\n",
       "24        [1, 0, 0, 0, 0, 0, -1, 0, 0]\n",
       "25       [-1, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "26       [-1, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "27        [0, 0, 0, 0, 0, 0, -1, 2, 0]\n",
       "28       [0, 0, 0, -3, 0, 0, -1, 1, 0]\n",
       "29       [0, 0, 0, 0, 0, 0, -1, -1, 0]\n",
       "                     ...              \n",
       "1270       [0, 0, 0, 2, 2, 0, 0, 1, 0]\n",
       "1271      [1, 0, 0, 1, 0, 0, -1, 2, 0]\n",
       "1272    [-1, -3, 0, 3, 0, 0, -1, 1, 0]\n",
       "1273    [-1, 0, 0, 3, 0, 1, -1, -1, 0]\n",
       "1274      [-1, 0, 0, 2, 0, 1, 0, 1, 0]\n",
       "1275       [2, 1, 0, 3, 0, 0, 0, 0, 0]\n",
       "1276     [0, 0, 0, 2, 0, -1, 0, -1, 0]\n",
       "1277      [0, 0, 1, 3, 0, 0, -1, 0, 0]\n",
       "1278      [1, 0, 0, 0, 0, 0, -3, 2, 0]\n",
       "1279     [-1, -1, 0, 0, 0, 0, 0, 2, 0]\n",
       "1280     [0, -1, 0, 0, 0, 0, 0, -2, 0]\n",
       "1281     [1, 0, 0, 1, 0, 0, -1, -1, 0]\n",
       "1282     [1, -3, 0, 0, 0, 0, 0, -2, 0]\n",
       "1283    [-1, -3, 0, 0, 0, 0, -1, 0, 0]\n",
       "1284      [0, 0, 0, 2, 0, 0, -3, 0, 0]\n",
       "1285      [1, 0, 0, 2, 0, -1, 0, 1, 0]\n",
       "1286      [-1, 0, 0, 0, 2, 0, 0, 2, 0]\n",
       "1287      [1, 1, 0, 0, 0, 0, 0, -2, 0]\n",
       "1288     [-1, 1, 0, 0, 0, 0, -1, 0, 0]\n",
       "1289     [-1, 0, 0, 3, 0, 0, -2, 2, 0]\n",
       "1290       [2, 0, 0, 3, 0, 0, 0, 1, 1]\n",
       "1291      [1, -1, 0, 2, 0, 0, 0, 0, 0]\n",
       "1292     [-1, 0, 0, 3, 0, 0, -3, 2, 0]\n",
       "1293     [-1, -3, 0, 2, 0, 0, 0, 0, 0]\n",
       "1294     [0, 0, 0, 3, 0, 0, -3, -2, 0]\n",
       "1295    [-1, 0, 0, 3, 0, -1, -1, 1, 0]\n",
       "1296     [-1, 0, 0, 0, 0, 1, 0, -2, 0]\n",
       "1297      [0, -3, 0, 3, 0, 1, 0, 1, 0]\n",
       "1298     [-1, 0, 0, 0, 0, 0, -2, 1, 0]\n",
       "1299    [-1, -3, 0, 3, 0, 0, 0, -2, 0]\n",
       "Name: feature_id_AsB, Length: 1300, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for human observed dataset(diff pairs)\n",
    "a = np.array(human_df1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "human_df1['feature_id_AnB'] = np.hstack((a[:, 0], a[:, 1])).tolist()\n",
    "human_df1['feature_id_AnB']\n",
    "# Subtracting the features of images for human observed dataset(diff pairs)\n",
    "human_df1['feature_id_AsB'] = np.subtract(a[:, 0], a[:, 1]).tolist()\n",
    "human_df1['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking features of images for the human observed dataset(same pairs)\n",
    "human_samepairs['feature_id_A'] = 'NaN'\n",
    "human_samepairs['feature_id_B'] = 'NaN'\n",
    "for i in human_samepairs.index:\n",
    "    a=human_raw[human_raw[\"img_id\"] == human_samepairs.at[i,\"img_id_A\"]]\n",
    "    b=human_raw[human_raw[\"img_id\"] == human_samepairs.at[i,\"img_id_B\"]]\n",
    "    human_samepairs.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    human_samepairs.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [-1, -1, 0, 0, 0, 0, -3, 2, 0]\n",
       "1           [0, 0, 1, -3, 0, 0, -1, -1, 0]\n",
       "2            [1, 0, 0, -1, 0, -1, 0, 1, 0]\n",
       "3              [1, 0, 0, 3, 0, 0, 0, 0, 0]\n",
       "4              [0, 0, 0, 3, 0, 0, 0, 2, 0]\n",
       "5             [-1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "6             [0, 0, 0, 3, 0, 0, 0, -2, 0]\n",
       "7              [1, 0, 0, 3, 0, 0, 0, 0, 0]\n",
       "8              [1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "9            [-1, 0, 1, 3, 0, 0, -1, 0, 0]\n",
       "10           [-1, 0, 0, 3, 0, 0, 0, -2, 0]\n",
       "11            [0, 0, 0, 0, 0, 0, 0, -3, 0]\n",
       "12           [1, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "13          [-1, 0, 0, 0, 0, 0, -1, -2, 0]\n",
       "14           [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "15            [1, 0, 0, -3, 0, 0, 1, 1, 0]\n",
       "16            [-2, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "17           [-2, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "18           [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "19          [-1, -3, 0, 0, 0, 0, 0, -3, 0]\n",
       "20        [-1, -3, 0, -3, 0, -1, 1, -2, 0]\n",
       "21          [-1, 0, 0, 0, 0, 0, -1, -2, 0]\n",
       "22         [-1, -3, 0, 3, 0, 0, -1, -1, 0]\n",
       "23            [0, -3, 0, 3, 0, 0, 0, 1, 0]\n",
       "24           [1, 0, 0, -2, 0, 0, 0, -1, 0]\n",
       "25          [-1, 1, 0, 1, 0, 0, -1, -2, 0]\n",
       "26          [-2, 1, 0, 3, 0, 0, -1, -1, 0]\n",
       "27           [0, 0, 1, 0, 0, 0, -3, -2, 0]\n",
       "28            [1, 0, 0, 0, 0, 0, -1, 2, 0]\n",
       "29           [-1, 0, 0, -3, 0, 0, 0, 3, 0]\n",
       "                      ...                 \n",
       "761           [0, -1, 1, 0, 2, 1, 2, 2, 1]\n",
       "762           [3, 2, 0, 4, 2, 1, -1, 1, 1]\n",
       "763       [-2, -3, -1, 0, 0, -1, 0, 1, -1]\n",
       "764      [1, 0, -1, -2, -2, -1, -1, 2, -1]\n",
       "765          [-1, 1, 0, -1, 2, 0, 2, 3, 1]\n",
       "766            [0, 3, 0, 0, 2, 0, 0, 4, 1]\n",
       "767           [1, 2, 0, 1, 0, 0, -2, 1, 0]\n",
       "768        [-1, 2, -2, -1, 2, 0, 3, -3, 0]\n",
       "769         [0, -2, 1, -2, 1, 1, -3, 0, 0]\n",
       "770         [-1, 0, -2, -1, 0, 1, 1, 2, 0]\n",
       "771           [1, 3, 1, 3, 0, 1, -1, 4, 0]\n",
       "772          [3, 4, -1, -1, 2, 0, 0, 1, 1]\n",
       "773           [1, 1, 1, 3, 2, 0, 2, -1, 0]\n",
       "774           [0, 2, 2, 3, 2, 0, -2, 0, 0]\n",
       "775           [0, 2, 1, 0, 2, 0, -3, 0, 0]\n",
       "776         [0, 0, -1, -3, 0, 0, -1, 0, 0]\n",
       "777    [-1, -1, -1, -2, 0, -1, -1, -1, -1]\n",
       "778          [1, -3, 1, 0, 2, 0, 3, -2, 0]\n",
       "779          [1, -1, 0, -2, 3, 1, 1, 2, 1]\n",
       "780       [1, -3, 0, 2, 0, -1, -1, -1, -1]\n",
       "781        [1, 1, 0, 0, -2, -2, 0, -1, -1]\n",
       "782         [0, 4, 0, -2, -2, -1, 1, 0, 0]\n",
       "783         [2, 1, 0, -1, 0, 0, -2, 0, -1]\n",
       "784           [2, 0, 1, 1, 0, 2, -2, 2, 1]\n",
       "785        [-3, 0, -1, -1, -1, 0, 0, 0, 0]\n",
       "786        [0, -4, -2, -1, 0, -2, 0, 0, 0]\n",
       "787         [3, 3, 0, -2, -2, 0, -1, 1, 0]\n",
       "788           [0, 1, 0, 0, 0, -1, 0, 1, 0]\n",
       "789       [-1, 4, -1, -1, -3, 0, 0, -1, 0]\n",
       "790           [0, 4, 2, 3, 2, -1, 2, 1, 1]\n",
       "Name: feature_id_AsB, Length: 791, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for human observed dataset(same pairs)\n",
    "b = np.array(human_samepairs[['feature_id_A','feature_id_B']].values.tolist())\n",
    "human_samepairs['feature_id_AnB'] = np.hstack((b[:, 0], b[:, 1])).tolist()\n",
    "human_samepairs['feature_id_AnB']\n",
    "# Subtracting the features of images for human observed dataset(same pairs)\n",
    "human_samepairs['feature_id_AsB'] = np.subtract(b[:, 0], b[:, 1]).tolist()\n",
    "human_samepairs['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1517a</td>\n",
       "      <td>1517b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, -2, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0539c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -3, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1508b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 1, 1, 0, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, -1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1494b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 1, 1, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, -1, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1344a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 3, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, -1, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1391b</td>\n",
       "      <td>1391a</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 0, 2, 2, 1, 4, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 2, 1, 4, 2, 3, 4, 1, 3, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, -1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0540a</td>\n",
       "      <td>0540c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 3, 1, 1, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 0, 4, 0, 2, 0, 3, 2]</td>\n",
       "      <td>[3, 3, 1, 1, 2, 3, 0, 3, 2, 2, 1, 0, 4, 0, 2, ...</td>\n",
       "      <td>[1, 2, 1, -3, 2, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0323a</td>\n",
       "      <td>0323b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 2, 4, 2, 2, 3, 2, 2]</td>\n",
       "      <td>[3, 0, 0, 1, 0, 3, 1, 1, 1]</td>\n",
       "      <td>[3, 4, 2, 4, 2, 2, 3, 2, 2, 3, 0, 0, 1, 0, 3, ...</td>\n",
       "      <td>[0, 4, 2, 3, 2, -1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1426b</td>\n",
       "      <td>1426a</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 1, 0, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 1, 0, 4, 2, 2, 0, 1, 1, 2, 2, ...</td>\n",
       "      <td>[0, 1, 0, -1, 0, -1, -1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1448b</td>\n",
       "      <td>1448c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 1, 3, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[1, 2, 1, 3, 2, 2, 1, 1, 2, 3, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-2, -2, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0375c</td>\n",
       "      <td>0375b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 4, 2, 2, 2, 1, 1]</td>\n",
       "      <td>[1, 3, 0, 1, 0, 2, 0, 2, 1]</td>\n",
       "      <td>[2, 4, 1, 4, 2, 2, 2, 1, 1, 1, 3, 0, 1, 0, 2, ...</td>\n",
       "      <td>[1, 1, 1, 3, 2, 0, 2, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1289c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, -1, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0460b</td>\n",
       "      <td>0460c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 4, 0, 0, 2, 3, 3, 3, 2]</td>\n",
       "      <td>[2, 4, 2, 1, 2, 2, 2, 1, 2]</td>\n",
       "      <td>[1, 4, 0, 0, 2, 3, 3, 3, 2, 2, 4, 2, 1, 2, 2, ...</td>\n",
       "      <td>[-1, 0, -2, -1, 0, 1, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0555c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 0, 1, 1, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 0, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, -1, 0, 0, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>1212a</td>\n",
       "      <td>1212c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 2, 2, 1]</td>\n",
       "      <td>[2, 0, 1, 0, 0, 2, 2, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 2, 2, 1, 2, 0, 1, 0, 0, 2, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1234a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, -3, 0, -3, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0550a</td>\n",
       "      <td>0550b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 0, 1, 0, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 0, 2, 3, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 4, 0, 3, 0, 0, 0, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1491a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1497b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 2, 0, -1, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0405c</td>\n",
       "      <td>0405a</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 1, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[3, 0, 1, 0, 2, 3, 1, 2, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 2, 2, 1, 2, 1, 3, 0, 1, 0, 2, 3, ...</td>\n",
       "      <td>[-2, 0, 0, 1, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1365c</td>\n",
       "      <td>1365b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 1, 1, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[0, 4, 1, 3, 2, 2, 1, 0, 2]</td>\n",
       "      <td>[2, 2, 1, 1, 2, 2, 1, 2, 2, 0, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[2, -2, 0, -2, 0, 0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1417c</td>\n",
       "      <td>1417a</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[1, 1, 0, 0, 2, 3, 1, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 3, ...</td>\n",
       "      <td>[0, 0, 1, 3, 0, -1, 0, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1411b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 1, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[-1, 0, 0, -3, 0, -1, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0328b</td>\n",
       "      <td>0328a</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 2, 1, 1, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 0, 2, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[0, 2, 1, 1, 2, 3, 0, 3, 2, 3, 1, 0, 2, 2, 3, ...</td>\n",
       "      <td>[-3, 1, 1, -1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0316b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 2, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 0, -2, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1517a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -1, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1424c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, -1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0411c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 1, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 1, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>1267c</td>\n",
       "      <td>1267a</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 0, 2, 2, 0, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 2, 2, 1]</td>\n",
       "      <td>[1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, -2, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1382c</td>\n",
       "      <td>1382b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 2, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 2, ...</td>\n",
       "      <td>[0, 1, 0, 0, 2, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1131a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 1, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[1, 0, 0, -1, 0, 1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1307a</td>\n",
       "      <td>1307b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 1, 1]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 3, 1, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 1, 1, 3, 4, 1, 0, 2, 3, ...</td>\n",
       "      <td>[-2, -3, 0, 3, 0, -1, -1, 0, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1289b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, -3, 0, -3, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1370b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 0, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1320a</td>\n",
       "      <td>1320c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[2, 2, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 3, 2, 2, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 3, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0552a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1329c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, -3, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1378c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0301b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2, 3, ...</td>\n",
       "      <td>[1, 0, 0, -2, 0, -1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1516a</td>\n",
       "      <td>1516b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 0, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 0, 1, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[0, -1, 0, -1, 0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0546c</td>\n",
       "      <td>0546b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 3, 2, 1, 3, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 1, 3, 0, 1, 2]</td>\n",
       "      <td>[2, 0, 1, 3, 2, 1, 3, 1, 2, 3, 1, 1, 3, 1, 3, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 1, -2, 3, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0496c</td>\n",
       "      <td>0496a</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 3, 1, 2, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2, 3, 4, 1, 0, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, 3, 0, -1, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1563a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[0, 1, 1, 3, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[2, 0, 0, -3, 0, -1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0575c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, -1, 0, -1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1547a</td>\n",
       "      <td>1547c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 1, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 3, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 4, 1, 1, 2, 1, 0, 2, 2, 2, 0, 1, 3, 2, 3, ...</td>\n",
       "      <td>[0, 4, 0, -2, 0, -2, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0592b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 2, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 1, 2, 2, ...</td>\n",
       "      <td>[1, -1, 0, -1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0477b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, 2, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1554a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 2, 2, 1, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 2, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, -2, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1394a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 1, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 0, -1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0593a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, -3, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1268a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, ...</td>\n",
       "      <td>[0, 0, 0, 3, 2, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1562b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 0, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 0, 0, 2, ...</td>\n",
       "      <td>[1, -1, 0, 0, 2, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0316b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 2, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 0, -2, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0432a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 0, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 0, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, 0, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1237c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 0, 2, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, ...</td>\n",
       "      <td>[0, 0, 1, -2, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0472a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[-1, 0, 0, -3, 0, -1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1248b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 1, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, -1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1417c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, -3, 0, 0, -1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1417c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, -3, 0, 0, -1, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1547a</td>\n",
       "      <td>1547b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 1, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 4, 1, 1, 2, 1, 0, 2, 2, 2, 0, 1, 1, 2, 1, ...</td>\n",
       "      <td>[0, 4, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2091 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     img_id_A img_id_B  target                 feature_id_A  \\\n",
       "24      1517a    1517b       1  [2, 1, 1, 1, 2, 2, 0, 2, 2]   \n",
       "178     0359a    0539c       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "487     0359a    1508b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "771     0577a    1494b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "161     0359a    1344a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "441     1391b    1391a       1  [3, 4, 1, 0, 2, 2, 1, 4, 2]   \n",
       "760     0540a    0540c       1  [3, 3, 1, 1, 2, 3, 0, 3, 2]   \n",
       "790     0323a    0323b       1  [3, 4, 2, 4, 2, 2, 3, 2, 2]   \n",
       "170     1426b    1426a       1  [2, 1, 1, 0, 2, 1, 0, 4, 2]   \n",
       "364     1448b    1448c       1  [1, 2, 1, 3, 2, 2, 1, 1, 2]   \n",
       "773     0375c    0375b       1  [2, 4, 1, 4, 2, 2, 2, 1, 1]   \n",
       "1112    1120a    1289c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "770     0460b    0460c       1  [1, 4, 0, 0, 2, 3, 3, 3, 2]   \n",
       "965     0577a    0555c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "675     1212a    1212c       1  [2, 1, 1, 0, 0, 2, 2, 2, 1]   \n",
       "738     0577a    1234a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "267     0550a    0550b       1  [3, 4, 1, 3, 2, 2, 0, 0, 2]   \n",
       "1063    1120a    1491a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "972     0577a    1497b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "633     0405c    0405a       1  [1, 0, 1, 1, 2, 2, 1, 2, 1]   \n",
       "403     1365c    1365b       1  [2, 2, 1, 1, 2, 2, 1, 2, 2]   \n",
       "256     1417c    1417a       1  [1, 1, 1, 3, 2, 2, 1, 2, 1]   \n",
       "345     0359a    1411b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "716     0328b    0328a       1  [0, 2, 1, 1, 2, 3, 0, 3, 2]   \n",
       "342     0359a    0316b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "519     0577a    1517a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1245    1120a    1424c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "189     0359a    0411c       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "420     1267c    1267a       1  [1, 0, 1, 0, 2, 2, 0, 1, 1]   \n",
       "126     1382c    1382b       1  [2, 2, 1, 0, 2, 2, 0, 1, 2]   \n",
       "...       ...      ...     ...                          ...   \n",
       "262     0359a    1131a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "143     1307a    1307b       1  [1, 1, 1, 3, 2, 2, 0, 1, 1]   \n",
       "614     0577a    1289b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "144     0359a    1370b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "163     1320a    1320c       1  [1, 1, 1, 3, 2, 1, 0, 3, 2]   \n",
       "120     0359a    0552a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "932     0577a    1329c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1083    1120a    1378c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "968     0577a    0301b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "193     1516a    1516b       1  [2, 0, 1, 0, 2, 2, 1, 1, 2]   \n",
       "673     0546c    0546b       1  [2, 0, 1, 3, 2, 1, 3, 1, 2]   \n",
       "232     0496c    0496a       1  [3, 4, 1, 3, 2, 2, 0, 1, 2]   \n",
       "394     0359a    1563a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "270     0359a    0575c       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "448     1547a    1547c       1  [2, 4, 1, 1, 2, 1, 0, 2, 2]   \n",
       "295     0359a    0592b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "1204    1120a    0477b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "821     0577a    1554a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "694     0577a    1394a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "795     0577a    0593a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1250    1120a    1268a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "476     0359a    1562b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "839     0577a    0316b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1288    1120a    0432a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "499     0359a    1237c       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "253     0359a    0472a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "582     0577a    1248b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "219     0359a    1417c       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "718     0577a    1417c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "447     1547a    1547b       1  [2, 4, 1, 1, 2, 1, 0, 2, 2]   \n",
       "\n",
       "                     feature_id_B  \\\n",
       "24    [1, 1, 1, 3, 2, 2, 0, 3, 2]   \n",
       "178   [2, 1, 1, 0, 2, 2, 3, 0, 2]   \n",
       "487   [1, 1, 1, 0, 2, 1, 1, 0, 1]   \n",
       "771   [2, 1, 1, 0, 2, 1, 1, 4, 2]   \n",
       "161   [1, 1, 1, 0, 2, 3, 0, 3, 2]   \n",
       "441   [3, 4, 1, 3, 2, 3, 0, 3, 2]   \n",
       "760   [2, 1, 0, 4, 0, 2, 0, 3, 2]   \n",
       "790   [3, 0, 0, 1, 0, 3, 1, 1, 1]   \n",
       "170   [2, 0, 1, 1, 2, 2, 1, 2, 1]   \n",
       "364   [3, 4, 1, 3, 2, 2, 1, 1, 2]   \n",
       "773   [1, 3, 0, 1, 0, 2, 0, 2, 1]   \n",
       "1112  [1, 1, 1, 3, 2, 2, 1, 4, 2]   \n",
       "770   [2, 4, 2, 1, 2, 2, 2, 1, 2]   \n",
       "965   [3, 0, 1, 1, 2, 2, 0, 4, 2]   \n",
       "675   [2, 0, 1, 0, 0, 2, 2, 2, 1]   \n",
       "738   [3, 4, 1, 3, 2, 2, 0, 3, 2]   \n",
       "267   [3, 0, 1, 0, 2, 2, 0, 4, 2]   \n",
       "1063  [3, 1, 1, 0, 2, 2, 1, 3, 2]   \n",
       "972   [2, 1, 1, 0, 0, 2, 1, 2, 1]   \n",
       "633   [3, 0, 1, 0, 2, 3, 1, 2, 1]   \n",
       "403   [0, 4, 1, 3, 2, 2, 1, 0, 2]   \n",
       "256   [1, 1, 0, 0, 2, 3, 1, 3, 2]   \n",
       "345   [3, 1, 1, 3, 2, 3, 1, 1, 2]   \n",
       "716   [3, 1, 0, 2, 2, 3, 0, 2, 2]   \n",
       "342   [2, 1, 1, 3, 2, 2, 2, 4, 2]   \n",
       "519   [2, 1, 1, 1, 2, 2, 0, 2, 2]   \n",
       "1245  [1, 1, 1, 3, 2, 2, 1, 1, 1]   \n",
       "189   [2, 1, 1, 3, 2, 1, 0, 4, 2]   \n",
       "420   [2, 1, 1, 0, 2, 2, 2, 2, 1]   \n",
       "126   [2, 1, 1, 0, 0, 2, 0, 0, 2]   \n",
       "...                           ...   \n",
       "262   [1, 1, 1, 1, 2, 1, 0, 0, 2]   \n",
       "143   [3, 4, 1, 0, 2, 3, 1, 1, 2]   \n",
       "614   [1, 4, 1, 3, 2, 2, 0, 2, 2]   \n",
       "144   [1, 0, 1, 0, 2, 2, 0, 3, 2]   \n",
       "163   [2, 2, 1, 0, 2, 2, 0, 3, 2]   \n",
       "120   [2, 1, 1, 0, 2, 2, 1, 2, 1]   \n",
       "932   [0, 1, 1, 0, 2, 2, 3, 0, 2]   \n",
       "1083  [1, 1, 1, 3, 2, 1, 0, 2, 2]   \n",
       "968   [1, 1, 1, 2, 2, 3, 0, 0, 2]   \n",
       "193   [2, 1, 1, 1, 2, 1, 0, 1, 2]   \n",
       "673   [3, 1, 1, 3, 1, 3, 0, 1, 2]   \n",
       "232   [3, 4, 1, 0, 2, 3, 1, 2, 2]   \n",
       "394   [0, 1, 1, 3, 2, 3, 0, 0, 2]   \n",
       "270   [2, 1, 1, 1, 2, 3, 0, 0, 2]   \n",
       "448   [2, 0, 1, 3, 2, 3, 0, 0, 2]   \n",
       "295   [1, 2, 1, 1, 2, 2, 0, 2, 2]   \n",
       "1204  [2, 1, 1, 1, 2, 3, 0, 2, 2]   \n",
       "821   [2, 1, 1, 2, 2, 1, 0, 1, 2]   \n",
       "694   [2, 1, 1, 3, 2, 2, 1, 1, 1]   \n",
       "795   [3, 1, 1, 0, 2, 2, 3, 0, 2]   \n",
       "1250  [2, 1, 1, 0, 0, 2, 0, 4, 2]   \n",
       "476   [1, 2, 1, 0, 0, 2, 0, 2, 2]   \n",
       "839   [2, 1, 1, 3, 2, 2, 2, 4, 2]   \n",
       "1288  [3, 0, 1, 3, 2, 2, 1, 2, 2]   \n",
       "499   [2, 1, 0, 2, 2, 2, 0, 1, 2]   \n",
       "253   [3, 1, 1, 3, 2, 3, 0, 0, 2]   \n",
       "582   [3, 1, 1, 1, 2, 2, 0, 0, 2]   \n",
       "219   [1, 1, 1, 3, 2, 2, 1, 2, 1]   \n",
       "718   [1, 1, 1, 3, 2, 2, 1, 2, 1]   \n",
       "447   [2, 0, 1, 1, 2, 1, 0, 2, 2]   \n",
       "\n",
       "                                         feature_id_AnB  \\\n",
       "24    [2, 1, 1, 1, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "178   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "487   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 1, ...   \n",
       "771   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 1, ...   \n",
       "161   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 3, ...   \n",
       "441   [3, 4, 1, 0, 2, 2, 1, 4, 2, 3, 4, 1, 3, 2, 3, ...   \n",
       "760   [3, 3, 1, 1, 2, 3, 0, 3, 2, 2, 1, 0, 4, 0, 2, ...   \n",
       "790   [3, 4, 2, 4, 2, 2, 3, 2, 2, 3, 0, 0, 1, 0, 3, ...   \n",
       "170   [2, 1, 1, 0, 2, 1, 0, 4, 2, 2, 0, 1, 1, 2, 2, ...   \n",
       "364   [1, 2, 1, 3, 2, 2, 1, 1, 2, 3, 4, 1, 3, 2, 2, ...   \n",
       "773   [2, 4, 1, 4, 2, 2, 2, 1, 1, 1, 3, 0, 1, 0, 2, ...   \n",
       "1112  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "770   [1, 4, 0, 0, 2, 3, 3, 3, 2, 2, 4, 2, 1, 2, 2, ...   \n",
       "965   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 0, 1, 1, 2, 2, ...   \n",
       "675   [2, 1, 1, 0, 0, 2, 2, 2, 1, 2, 0, 1, 0, 0, 2, ...   \n",
       "738   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 4, 1, 3, 2, 2, ...   \n",
       "267   [3, 4, 1, 3, 2, 2, 0, 0, 2, 3, 0, 1, 0, 2, 2, ...   \n",
       "1063  [2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "972   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 2, ...   \n",
       "633   [1, 0, 1, 1, 2, 2, 1, 2, 1, 3, 0, 1, 0, 2, 3, ...   \n",
       "403   [2, 2, 1, 1, 2, 2, 1, 2, 2, 0, 4, 1, 3, 2, 2, ...   \n",
       "256   [1, 1, 1, 3, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 3, ...   \n",
       "345   [2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...   \n",
       "716   [0, 2, 1, 1, 2, 3, 0, 3, 2, 3, 1, 0, 2, 2, 3, ...   \n",
       "342   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 2, ...   \n",
       "519   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, ...   \n",
       "1245  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "189   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 1, ...   \n",
       "420   [1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, ...   \n",
       "126   [2, 2, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 0, 2, ...   \n",
       "...                                                 ...   \n",
       "262   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, ...   \n",
       "143   [1, 1, 1, 3, 2, 2, 0, 1, 1, 3, 4, 1, 0, 2, 3, ...   \n",
       "614   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "144   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 2, ...   \n",
       "163   [1, 1, 1, 3, 2, 1, 0, 3, 2, 2, 2, 1, 0, 2, 2, ...   \n",
       "120   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "932   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...   \n",
       "1083  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 1, ...   \n",
       "968   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2, 3, ...   \n",
       "193   [2, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, ...   \n",
       "673   [2, 0, 1, 3, 2, 1, 3, 1, 2, 3, 1, 1, 3, 1, 3, ...   \n",
       "232   [3, 4, 1, 3, 2, 2, 0, 1, 2, 3, 4, 1, 0, 2, 3, ...   \n",
       "394   [2, 1, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 3, 2, 3, ...   \n",
       "270   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 3, ...   \n",
       "448   [2, 4, 1, 1, 2, 1, 0, 2, 2, 2, 0, 1, 3, 2, 3, ...   \n",
       "295   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 1, 2, 2, ...   \n",
       "1204  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 3, ...   \n",
       "821   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 2, 2, 1, ...   \n",
       "694   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...   \n",
       "795   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "1250  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, ...   \n",
       "476   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 0, 0, 2, ...   \n",
       "839   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...   \n",
       "1288  [2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 0, 1, 3, 2, 2, ...   \n",
       "499   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, ...   \n",
       "253   [2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...   \n",
       "582   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 1, 2, 2, ...   \n",
       "219   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "718   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "447   [2, 4, 1, 1, 2, 1, 0, 2, 2, 2, 0, 1, 1, 2, 1, ...   \n",
       "\n",
       "                        feature_id_AsB  \n",
       "24       [1, 0, 0, -2, 0, 0, 0, -1, 0]  \n",
       "178       [0, 0, 0, 0, 0, 0, -3, 2, 0]  \n",
       "487       [1, 0, 0, 0, 0, 1, -1, 2, 1]  \n",
       "771      [0, 0, 0, 0, 0, 1, -1, -3, 0]  \n",
       "161      [1, 0, 0, 0, 0, -1, 0, -1, 0]  \n",
       "441      [0, 0, 0, -3, 0, -1, 1, 1, 0]  \n",
       "760       [1, 2, 1, -3, 2, 1, 0, 0, 0]  \n",
       "790       [0, 4, 2, 3, 2, -1, 2, 1, 1]  \n",
       "170     [0, 1, 0, -1, 0, -1, -1, 2, 1]  \n",
       "364      [-2, -2, 0, 0, 0, 0, 0, 0, 0]  \n",
       "773       [1, 1, 1, 3, 2, 0, 2, -1, 0]  \n",
       "1112     [1, 0, 0, 0, 0, 0, -1, -2, 0]  \n",
       "770     [-1, 0, -2, -1, 0, 1, 1, 2, 0]  \n",
       "965     [-1, 1, 0, -1, 0, 0, 0, -3, 0]  \n",
       "675        [0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "738    [-1, -3, 0, -3, 0, 0, 0, -2, 0]  \n",
       "267       [0, 4, 0, 3, 0, 0, 0, -4, 0]  \n",
       "1063    [-1, 0, 0, 3, 0, 0, -1, -1, 0]  \n",
       "972      [0, 0, 0, 0, 2, 0, -1, -1, 1]  \n",
       "633      [-2, 0, 0, 1, 0, -1, 0, 0, 0]  \n",
       "403      [2, -2, 0, -2, 0, 0, 0, 2, 0]  \n",
       "256     [0, 0, 1, 3, 0, -1, 0, -1, -1]  \n",
       "345    [-1, 0, 0, -3, 0, -1, -1, 1, 0]  \n",
       "716      [-3, 1, 1, -1, 0, 0, 0, 1, 0]  \n",
       "342     [0, 0, 0, -3, 0, 0, -2, -2, 0]  \n",
       "519      [0, 0, 0, -1, 0, 0, 0, -1, 0]  \n",
       "1245      [1, 0, 0, 0, 0, 0, -1, 1, 1]  \n",
       "189      [0, 0, 0, -3, 0, 1, 0, -2, 0]  \n",
       "420    [-1, -1, 0, 0, 0, 0, -2, -1, 0]  \n",
       "126        [0, 1, 0, 0, 2, 0, 0, 1, 0]  \n",
       "...                                ...  \n",
       "262       [1, 0, 0, -1, 0, 1, 0, 2, 0]  \n",
       "143   [-2, -3, 0, 3, 0, -1, -1, 0, -1]  \n",
       "614     [1, -3, 0, -3, 0, 0, 0, -1, 0]  \n",
       "144       [1, 1, 0, 0, 0, 0, 0, -1, 0]  \n",
       "163     [-1, -1, 0, 3, 0, -1, 0, 0, 0]  \n",
       "120       [0, 0, 0, 0, 0, 0, -1, 0, 1]  \n",
       "932       [2, 0, 0, 0, 0, 0, -3, 1, 0]  \n",
       "1083       [1, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "968      [1, 0, 0, -2, 0, -1, 0, 1, 0]  \n",
       "193      [0, -1, 0, -1, 0, 1, 1, 0, 0]  \n",
       "673     [-1, -1, 0, 0, 1, -2, 3, 0, 0]  \n",
       "232     [0, 0, 0, 3, 0, -1, -1, -1, 0]  \n",
       "394      [2, 0, 0, -3, 0, -1, 0, 2, 0]  \n",
       "270      [0, 0, 0, -1, 0, -1, 0, 2, 0]  \n",
       "448      [0, 4, 0, -2, 0, -2, 0, 2, 0]  \n",
       "295      [1, -1, 0, -1, 0, 0, 0, 0, 0]  \n",
       "1204      [0, 0, 0, 2, 0, -1, 0, 0, 0]  \n",
       "821       [0, 0, 0, -2, 0, 1, 0, 0, 0]  \n",
       "694      [0, 0, 0, -3, 0, 0, -1, 0, 1]  \n",
       "795      [-1, 0, 0, 0, 0, 0, -3, 1, 0]  \n",
       "1250      [0, 0, 0, 3, 2, 0, 0, -2, 0]  \n",
       "476       [1, -1, 0, 0, 2, 0, 0, 0, 0]  \n",
       "839     [0, 0, 0, -3, 0, 0, -2, -3, 0]  \n",
       "1288     [-1, 1, 0, 0, 0, 0, -1, 0, 0]  \n",
       "499       [0, 0, 1, -2, 0, 0, 0, 1, 0]  \n",
       "253     [-1, 0, 0, -3, 0, -1, 0, 2, 0]  \n",
       "582      [-1, 0, 0, -1, 0, 0, 0, 1, 0]  \n",
       "219      [1, 0, 0, -3, 0, 0, -1, 0, 1]  \n",
       "718     [1, 0, 0, -3, 0, 0, -1, -1, 1]  \n",
       "447        [0, 4, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[2091 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling to avoid biasing\n",
    "human_df = human_df1.append(human_samepairs)\n",
    "human_df = shuffle(human_df)\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking features of images for the GSC observed dataset(diff pairs)\n",
    "GSC_differentpairs['feature_id_A'] = 'NaN'\n",
    "GSC_differentpairs['feature_id_B'] = 'NaN'\n",
    "for i in GSC_differentpairs.index[0:72000]:\n",
    "    a=GSC_raw[GSC_raw[\"img_id\"] == GSC_differentpairs.at[i,\"img_id_A\"]]\n",
    "    b=GSC_raw[GSC_raw[\"img_id\"] == GSC_differentpairs.at[i,\"img_id_B\"]]\n",
    "    GSC_differentpairs.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    GSC_differentpairs.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a subset as dataset is very big\n",
    "GSC_differentpairs1=GSC_differentpairs[0:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5        [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "6        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "8        [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "9        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "10       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "11       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "12       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13       [-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...\n",
       "14       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "15       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...\n",
       "16       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "18       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "19       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "20       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "21       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "22       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "23       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "24       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "25       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "26       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "27       [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "28       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "29       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "39970    [0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...\n",
       "39971    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39972    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39973    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39974    [1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...\n",
       "39975    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39976    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39977    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39978    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39979    [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39980    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39981    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39982    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39983    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39984    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39985    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39986    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39987    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39988    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39989    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39990    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39991    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39992    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39993    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39994    [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...\n",
       "39995    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39996    [0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...\n",
       "39997    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39998    [1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...\n",
       "39999    [1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...\n",
       "Name: feature_id_AsB, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for GSC dataset(diff pairs)\n",
    "c = np.array(GSC_differentpairs1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "GSC_differentpairs1['feature_id_AnB'] = np.hstack((c[:, 0], c[:, 1])).tolist()\n",
    "GSC_differentpairs1['feature_id_AnB']\n",
    "# Subtracting the features of images for GSC dataset(diff pairs)\n",
    "GSC_differentpairs1['feature_id_AsB'] = np.subtract(c[:, 0], c[:, 1]).tolist()\n",
    "GSC_differentpairs1['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004aa_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39970</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39972</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39974</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39975</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39976</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39977</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39978</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39979</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39980</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39981</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39982</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39983</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39986</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0140b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39987</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39993</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num8.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0142a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0142a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             img_id_A         img_id_B  target  \\\n",
       "0      0001a_num1.png   0002a_num1.png       0   \n",
       "1      0001a_num1.png   0002a_num2.png       0   \n",
       "2      0001a_num1.png   0002a_num3.png       0   \n",
       "3      0001a_num1.png   0002a_num4.png       0   \n",
       "4      0001a_num1.png   0002b_num1.png       0   \n",
       "5      0001a_num1.png   0002b_num2.png       0   \n",
       "6      0001a_num1.png   0002b_num3.png       0   \n",
       "7      0001a_num1.png   0002b_num4.png       0   \n",
       "8      0001a_num1.png   0002b_num5.png       0   \n",
       "9      0001a_num1.png   0002b_num6.png       0   \n",
       "10     0001a_num1.png   0002c_num1.png       0   \n",
       "11     0001a_num1.png   0002c_num2.png       0   \n",
       "12     0001a_num1.png   0002c_num3.png       0   \n",
       "13     0001a_num1.png   0002c_num4.png       0   \n",
       "14     0001a_num1.png   0003a_num1.png       0   \n",
       "15     0001a_num1.png   0003a_num2.png       0   \n",
       "16     0001a_num1.png   0003a_num3.png       0   \n",
       "17     0001a_num1.png   0003b_num1.png       0   \n",
       "18     0001a_num1.png   0003b_num2.png       0   \n",
       "19     0001a_num1.png   0003b_num4.png       0   \n",
       "20     0001a_num1.png   0003c_num1.png       0   \n",
       "21     0001a_num1.png   0003c_num2.png       0   \n",
       "22     0001a_num1.png   0003c_num3.png       0   \n",
       "23     0001a_num1.png   0004a_num2.png       0   \n",
       "24     0001a_num1.png   0004a_num3.png       0   \n",
       "25     0001a_num1.png   0004a_num5.png       0   \n",
       "26     0001a_num1.png   0004a_num6.png       0   \n",
       "27     0001a_num1.png   0004a_num7.png       0   \n",
       "28     0001a_num1.png  0004aa_num1.png       0   \n",
       "29     0001a_num1.png   0004b_num1.png       0   \n",
       "...               ...              ...     ...   \n",
       "39970  0083a_num1.png   0138a_num2.png       0   \n",
       "39971  0083a_num1.png   0138a_num3.png       0   \n",
       "39972  0083a_num1.png   0138a_num4.png       0   \n",
       "39973  0083a_num1.png   0138b_num1.png       0   \n",
       "39974  0083a_num1.png   0138b_num2.png       0   \n",
       "39975  0083a_num1.png   0138b_num3.png       0   \n",
       "39976  0083a_num1.png   0138b_num4.png       0   \n",
       "39977  0083a_num1.png   0138c_num1.png       0   \n",
       "39978  0083a_num1.png   0138c_num2.png       0   \n",
       "39979  0083a_num1.png   0138c_num4.png       0   \n",
       "39980  0083a_num1.png   0139a_num1.png       0   \n",
       "39981  0083a_num1.png   0139a_num4.png       0   \n",
       "39982  0083a_num1.png   0139b_num1.png       0   \n",
       "39983  0083a_num1.png   0139c_num1.png       0   \n",
       "39984  0083a_num1.png   0139c_num2.png       0   \n",
       "39985  0083a_num1.png   0139c_num4.png       0   \n",
       "39986  0083a_num1.png   0140b_num1.png       0   \n",
       "39987  0083a_num1.png   0141a_num2.png       0   \n",
       "39988  0083a_num1.png   0141a_num3.png       0   \n",
       "39989  0083a_num1.png   0141b_num1.png       0   \n",
       "39990  0083a_num1.png   0141b_num2.png       0   \n",
       "39991  0083a_num1.png   0141b_num5.png       0   \n",
       "39992  0083a_num1.png   0141b_num7.png       0   \n",
       "39993  0083a_num1.png   0141c_num1.png       0   \n",
       "39994  0083a_num1.png   0141c_num2.png       0   \n",
       "39995  0083a_num1.png   0141c_num5.png       0   \n",
       "39996  0083a_num1.png   0141c_num7.png       0   \n",
       "39997  0083a_num1.png   0141c_num8.png       0   \n",
       "39998  0083a_num1.png   0142a_num1.png       0   \n",
       "39999  0083a_num1.png   0142a_num2.png       0   \n",
       "\n",
       "                                            feature_id_A  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            feature_id_B  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AnB  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AsB  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5      [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8      [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "13     [-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...  \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "27     [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "39970  [0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...  \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39974  [1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39976  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39978  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39979  [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39987  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39988  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39989  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39991  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39993  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39994  [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...  \n",
       "39995  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39996  [0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "39997  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39998  [1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...  \n",
       "39999  [1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[40000 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a subset as its very big\n",
    "df_GSC1=GSC_differentpairs1[0:40000]\n",
    "df_GSC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking features of images for the GSC observed dataset(same pairs)\n",
    "GSC_samepairs['feature_id_A'] = 'NaN'\n",
    "GSC_samepairs['feature_id_B'] = 'NaN'\n",
    "for i in GSC_samepairs.index:\n",
    "    a=GSC_raw[GSC_raw[\"img_id\"] == GSC_samepairs.at[i,\"img_id_A\"]]\n",
    "    b=GSC_raw[GSC_raw[\"img_id\"] == GSC_samepairs.at[i,\"img_id_B\"]]\n",
    "    GSC_samepairs.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    GSC_samepairs.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001a_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001a_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001a_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0001c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001a_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001a_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001a_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0001a_num2.png</td>\n",
       "      <td>0001c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0001a_num3.png</td>\n",
       "      <td>0001a_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0001a_num3.png</td>\n",
       "      <td>0001a_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0001a_num3.png</td>\n",
       "      <td>0001b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71501</th>\n",
       "      <td>1566a_num3.png</td>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71502</th>\n",
       "      <td>1566a_num3.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71503</th>\n",
       "      <td>1566b_num1.png</td>\n",
       "      <td>1566b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71504</th>\n",
       "      <td>1566b_num1.png</td>\n",
       "      <td>1566b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71505</th>\n",
       "      <td>1566b_num1.png</td>\n",
       "      <td>1566b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71506</th>\n",
       "      <td>1566b_num1.png</td>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71507</th>\n",
       "      <td>1566b_num1.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71508</th>\n",
       "      <td>1566b_num2.png</td>\n",
       "      <td>1566b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71509</th>\n",
       "      <td>1566b_num2.png</td>\n",
       "      <td>1566b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71510</th>\n",
       "      <td>1566b_num2.png</td>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71511</th>\n",
       "      <td>1566b_num2.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71512</th>\n",
       "      <td>1566b_num3.png</td>\n",
       "      <td>1566b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71513</th>\n",
       "      <td>1566b_num3.png</td>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71514</th>\n",
       "      <td>1566b_num3.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71515</th>\n",
       "      <td>1566b_num4.png</td>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71516</th>\n",
       "      <td>1566b_num4.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71517</th>\n",
       "      <td>1566c_num1.png</td>\n",
       "      <td>1566c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71518</th>\n",
       "      <td>1567b_num1.png</td>\n",
       "      <td>1567b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71519</th>\n",
       "      <td>1567b_num1.png</td>\n",
       "      <td>1567c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71520</th>\n",
       "      <td>1567b_num3.png</td>\n",
       "      <td>1567c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71521</th>\n",
       "      <td>1568a_num1.png</td>\n",
       "      <td>1568a_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71522</th>\n",
       "      <td>1568a_num1.png</td>\n",
       "      <td>1568a_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71523</th>\n",
       "      <td>1568a_num1.png</td>\n",
       "      <td>1568c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71524</th>\n",
       "      <td>1568a_num1.png</td>\n",
       "      <td>1568c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71525</th>\n",
       "      <td>1568a_num2.png</td>\n",
       "      <td>1568a_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71526</th>\n",
       "      <td>1568a_num2.png</td>\n",
       "      <td>1568c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71527</th>\n",
       "      <td>1568a_num2.png</td>\n",
       "      <td>1568c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71528</th>\n",
       "      <td>1568a_num3.png</td>\n",
       "      <td>1568c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71529</th>\n",
       "      <td>1568a_num3.png</td>\n",
       "      <td>1568c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71530</th>\n",
       "      <td>1568c_num1.png</td>\n",
       "      <td>1568c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71531 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             img_id_A        img_id_B  target  \\\n",
       "0      0001a_num1.png  0001a_num2.png       1   \n",
       "1      0001a_num1.png  0001a_num3.png       1   \n",
       "2      0001a_num1.png  0001a_num4.png       1   \n",
       "3      0001a_num1.png  0001a_num5.png       1   \n",
       "4      0001a_num1.png  0001b_num1.png       1   \n",
       "5      0001a_num1.png  0001b_num2.png       1   \n",
       "6      0001a_num1.png  0001b_num3.png       1   \n",
       "7      0001a_num1.png  0001b_num4.png       1   \n",
       "8      0001a_num1.png  0001b_num5.png       1   \n",
       "9      0001a_num1.png  0001c_num1.png       1   \n",
       "10     0001a_num1.png  0001c_num2.png       1   \n",
       "11     0001a_num1.png  0001c_num3.png       1   \n",
       "12     0001a_num1.png  0001c_num4.png       1   \n",
       "13     0001a_num1.png  0001c_num5.png       1   \n",
       "14     0001a_num2.png  0001a_num3.png       1   \n",
       "15     0001a_num2.png  0001a_num4.png       1   \n",
       "16     0001a_num2.png  0001a_num5.png       1   \n",
       "17     0001a_num2.png  0001b_num1.png       1   \n",
       "18     0001a_num2.png  0001b_num2.png       1   \n",
       "19     0001a_num2.png  0001b_num3.png       1   \n",
       "20     0001a_num2.png  0001b_num4.png       1   \n",
       "21     0001a_num2.png  0001b_num5.png       1   \n",
       "22     0001a_num2.png  0001c_num1.png       1   \n",
       "23     0001a_num2.png  0001c_num2.png       1   \n",
       "24     0001a_num2.png  0001c_num3.png       1   \n",
       "25     0001a_num2.png  0001c_num4.png       1   \n",
       "26     0001a_num2.png  0001c_num5.png       1   \n",
       "27     0001a_num3.png  0001a_num4.png       1   \n",
       "28     0001a_num3.png  0001a_num5.png       1   \n",
       "29     0001a_num3.png  0001b_num1.png       1   \n",
       "...               ...             ...     ...   \n",
       "71501  1566a_num3.png  1566c_num1.png       1   \n",
       "71502  1566a_num3.png  1566c_num2.png       1   \n",
       "71503  1566b_num1.png  1566b_num2.png       1   \n",
       "71504  1566b_num1.png  1566b_num3.png       1   \n",
       "71505  1566b_num1.png  1566b_num4.png       1   \n",
       "71506  1566b_num1.png  1566c_num1.png       1   \n",
       "71507  1566b_num1.png  1566c_num2.png       1   \n",
       "71508  1566b_num2.png  1566b_num3.png       1   \n",
       "71509  1566b_num2.png  1566b_num4.png       1   \n",
       "71510  1566b_num2.png  1566c_num1.png       1   \n",
       "71511  1566b_num2.png  1566c_num2.png       1   \n",
       "71512  1566b_num3.png  1566b_num4.png       1   \n",
       "71513  1566b_num3.png  1566c_num1.png       1   \n",
       "71514  1566b_num3.png  1566c_num2.png       1   \n",
       "71515  1566b_num4.png  1566c_num1.png       1   \n",
       "71516  1566b_num4.png  1566c_num2.png       1   \n",
       "71517  1566c_num1.png  1566c_num2.png       1   \n",
       "71518  1567b_num1.png  1567b_num3.png       1   \n",
       "71519  1567b_num1.png  1567c_num1.png       1   \n",
       "71520  1567b_num3.png  1567c_num1.png       1   \n",
       "71521  1568a_num1.png  1568a_num2.png       1   \n",
       "71522  1568a_num1.png  1568a_num3.png       1   \n",
       "71523  1568a_num1.png  1568c_num1.png       1   \n",
       "71524  1568a_num1.png  1568c_num2.png       1   \n",
       "71525  1568a_num2.png  1568a_num3.png       1   \n",
       "71526  1568a_num2.png  1568c_num1.png       1   \n",
       "71527  1568a_num2.png  1568c_num2.png       1   \n",
       "71528  1568a_num3.png  1568c_num1.png       1   \n",
       "71529  1568a_num3.png  1568c_num2.png       1   \n",
       "71530  1568c_num1.png  1568c_num2.png       1   \n",
       "\n",
       "                                            feature_id_A  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "71501  [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71502  [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71503  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "71504  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "71505  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "71506  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "71507  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "71508  [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "71509  [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "71510  [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "71511  [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "71512  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "71513  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "71514  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "71515  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71516  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71517  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71518  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "71519  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "71520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "71521  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71522  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71523  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71524  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71525  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71526  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71527  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71528  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71529  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "71530  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            feature_id_B  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "71501  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71502  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71503  [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "71504  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71505  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71506  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71507  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71508  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71509  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71510  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71511  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71512  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71513  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71514  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71515  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71516  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71517  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "71518  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "71519  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71521  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71522  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71523  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71524  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "71525  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71526  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71527  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "71528  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "71529  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "71530  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...  \n",
       "\n",
       "[71531 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSC_samepairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "6        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "8        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "9        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "10       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "11       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "12       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "14       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "15       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "16       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "18       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "19       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "20       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "21       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "22       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "23       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "24       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "25       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "26       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "27       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "28       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "29       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "71501    [-1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "71502    [0, 0, 1, 1, 1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0...\n",
       "71503    [0, 0, 1, 0, -1, 0, -1, -1, 1, 0, -1, 0, 0, 0,...\n",
       "71504    [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0,...\n",
       "71505    [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "71506    [-1, 1, 1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0...\n",
       "71507    [0, 0, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0...\n",
       "71508    [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, -1, 0, 0,...\n",
       "71509    [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, ...\n",
       "71510    [-1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,...\n",
       "71511    [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 0,...\n",
       "71512    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "71513    [-1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0...\n",
       "71514    [0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...\n",
       "71515    [-1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "71516    [0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, ...\n",
       "71517    [1, -1, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, -1, 0, ...\n",
       "71518    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1,...\n",
       "71519    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "71520    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "71521    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71522    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71523    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71524    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...\n",
       "71525    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71526    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71527    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...\n",
       "71528    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "71529    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...\n",
       "71530    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...\n",
       "Name: feature_id_AsB, Length: 71531, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for GSC dataset(same pairs)\n",
    "d = np.array(GSC_samepairs[['feature_id_A','feature_id_B']].values.tolist())\n",
    "GSC_samepairs['feature_id_AnB'] = np.hstack((d[:, 0], d[:, 1])).tolist()\n",
    "GSC_samepairs['feature_id_AnB']\n",
    "# Subtracting the features of images for GSC dataset(same pairs)\n",
    "GSC_samepairs['feature_id_AsB'] = np.subtract(d[:, 0], d[:, 1]).tolist()\n",
    "GSC_samepairs['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>0375b_num1.png</td>\n",
       "      <td>0375c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>0022a_num1.png</td>\n",
       "      <td>0056c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>0012a_num1.png</td>\n",
       "      <td>0062b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>0009a_num1.png</td>\n",
       "      <td>0038a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65840</th>\n",
       "      <td>1413a_num1.png</td>\n",
       "      <td>1413b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24574</th>\n",
       "      <td>0052a_num1.png</td>\n",
       "      <td>0061c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>0157a_num4.png</td>\n",
       "      <td>0157c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20389</th>\n",
       "      <td>0041a_num1.png</td>\n",
       "      <td>0087a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26218</th>\n",
       "      <td>0578b_num6.png</td>\n",
       "      <td>0578c_num6.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57033</th>\n",
       "      <td>1203b_num2.png</td>\n",
       "      <td>1203c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41656</th>\n",
       "      <td>0902c_num1.png</td>\n",
       "      <td>0902c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46020</th>\n",
       "      <td>0987a_num2.png</td>\n",
       "      <td>0987c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37759</th>\n",
       "      <td>0078a_num2.png</td>\n",
       "      <td>0110a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20036</th>\n",
       "      <td>0041a_num1.png</td>\n",
       "      <td>0047c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>0321b_num2.png</td>\n",
       "      <td>0321c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57950</th>\n",
       "      <td>1228a_num1.png</td>\n",
       "      <td>1228b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15597</th>\n",
       "      <td>0032b_num2.png</td>\n",
       "      <td>0047b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>0169b_num5.png</td>\n",
       "      <td>0169c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28668</th>\n",
       "      <td>0060a_num1.png</td>\n",
       "      <td>0080a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10791</th>\n",
       "      <td>0022a_num1.png</td>\n",
       "      <td>0059c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>0197c_num2.png</td>\n",
       "      <td>0197c_num7.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46117</th>\n",
       "      <td>0989c_num3.png</td>\n",
       "      <td>0989c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38579</th>\n",
       "      <td>0081c_num1.png</td>\n",
       "      <td>0090b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24645</th>\n",
       "      <td>0052a_num1.png</td>\n",
       "      <td>0068c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21435</th>\n",
       "      <td>0043a_num1.png</td>\n",
       "      <td>0092c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36735</th>\n",
       "      <td>0076a_num1.png</td>\n",
       "      <td>0103c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69577</th>\n",
       "      <td>1515a_num1.png</td>\n",
       "      <td>1515c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11044</th>\n",
       "      <td>0023a_num1.png</td>\n",
       "      <td>0028b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>0019a_num1.png</td>\n",
       "      <td>0063b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43385</th>\n",
       "      <td>0941a_num2.png</td>\n",
       "      <td>0941c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6510</th>\n",
       "      <td>0014aa_num1.png</td>\n",
       "      <td>0016a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42954</th>\n",
       "      <td>0931c_num3.png</td>\n",
       "      <td>0931c_num7.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15120</th>\n",
       "      <td>0337b_num2.png</td>\n",
       "      <td>0337c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62240</th>\n",
       "      <td>1323b_num5.png</td>\n",
       "      <td>1323c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60989</th>\n",
       "      <td>1296a_num2.png</td>\n",
       "      <td>1296b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25616</th>\n",
       "      <td>0054a_num2.png</td>\n",
       "      <td>0067c_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33520</th>\n",
       "      <td>0070a_num1.png</td>\n",
       "      <td>0072c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52312</th>\n",
       "      <td>1119a_num2.png</td>\n",
       "      <td>1119b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40647</th>\n",
       "      <td>0884a_num3.png</td>\n",
       "      <td>0884b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58897</th>\n",
       "      <td>1247c_num1.png</td>\n",
       "      <td>1247c_num6.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19085</th>\n",
       "      <td>0039a_num1.png</td>\n",
       "      <td>0050a_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66245</th>\n",
       "      <td>1421a_num3.png</td>\n",
       "      <td>1421a_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34885</th>\n",
       "      <td>0072a_num1.png</td>\n",
       "      <td>0121c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32713</th>\n",
       "      <td>0717bb_num2.png</td>\n",
       "      <td>0717c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0004a_num2.png</td>\n",
       "      <td>0004a_num7.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59400</th>\n",
       "      <td>1259b_num1.png</td>\n",
       "      <td>1259b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7549</th>\n",
       "      <td>0016a_num2.png</td>\n",
       "      <td>0021c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34946</th>\n",
       "      <td>0072a_num1.png</td>\n",
       "      <td>0127a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>0108aa_num1.png</td>\n",
       "      <td>0108bb_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24185</th>\n",
       "      <td>0527c_num5.png</td>\n",
       "      <td>0527c_num9.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54837</th>\n",
       "      <td>1165a_num1.png</td>\n",
       "      <td>1165b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37777</th>\n",
       "      <td>0825c_num1.png</td>\n",
       "      <td>0825c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7256</th>\n",
       "      <td>0155a_num3.png</td>\n",
       "      <td>0155c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>0100a_num2.png</td>\n",
       "      <td>0100c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 1, 0, -1, -1, 0, 0, 1, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10593</th>\n",
       "      <td>0220a_num2.png</td>\n",
       "      <td>0220c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35891</th>\n",
       "      <td>0074a_num1.png</td>\n",
       "      <td>0122c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30312</th>\n",
       "      <td>0063a_num2.png</td>\n",
       "      <td>0097b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50503</th>\n",
       "      <td>1085a_num1.png</td>\n",
       "      <td>1085c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38932</th>\n",
       "      <td>0081c_num1.png</td>\n",
       "      <td>0132b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37747</th>\n",
       "      <td>0078a_num2.png</td>\n",
       "      <td>0107c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111531 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              img_id_A         img_id_B  target  \\\n",
       "16596   0375b_num1.png   0375c_num3.png       1   \n",
       "10764   0022a_num1.png   0056c_num1.png       0   \n",
       "5902    0012a_num1.png   0062b_num3.png       0   \n",
       "4224    0009a_num1.png   0038a_num4.png       0   \n",
       "65840   1413a_num1.png   1413b_num1.png       1   \n",
       "24574   0052a_num1.png   0061c_num3.png       0   \n",
       "7546    0157a_num4.png   0157c_num1.png       1   \n",
       "20389   0041a_num1.png   0087a_num2.png       0   \n",
       "26218   0578b_num6.png   0578c_num6.png       1   \n",
       "57033   1203b_num2.png   1203c_num4.png       1   \n",
       "41656   0902c_num1.png   0902c_num3.png       1   \n",
       "46020   0987a_num2.png   0987c_num4.png       1   \n",
       "37759   0078a_num2.png   0110a_num2.png       0   \n",
       "20036   0041a_num1.png   0047c_num1.png       0   \n",
       "14396   0321b_num2.png   0321c_num1.png       1   \n",
       "57950   1228a_num1.png   1228b_num3.png       1   \n",
       "15597   0032b_num2.png   0047b_num4.png       0   \n",
       "8061    0169b_num5.png   0169c_num1.png       1   \n",
       "28668   0060a_num1.png   0080a_num3.png       0   \n",
       "10791   0022a_num1.png   0059c_num4.png       0   \n",
       "9680    0197c_num2.png   0197c_num7.png       1   \n",
       "46117   0989c_num3.png   0989c_num5.png       1   \n",
       "38579   0081c_num1.png   0090b_num1.png       0   \n",
       "24645   0052a_num1.png   0068c_num4.png       0   \n",
       "21435   0043a_num1.png   0092c_num2.png       0   \n",
       "36735   0076a_num1.png   0103c_num2.png       0   \n",
       "69577   1515a_num1.png   1515c_num1.png       1   \n",
       "11044   0023a_num1.png   0028b_num2.png       0   \n",
       "9348    0019a_num1.png   0063b_num4.png       0   \n",
       "43385   0941a_num2.png   0941c_num3.png       1   \n",
       "...                ...              ...     ...   \n",
       "6510   0014aa_num1.png   0016a_num4.png       0   \n",
       "42954   0931c_num3.png   0931c_num7.png       1   \n",
       "15120   0337b_num2.png   0337c_num5.png       1   \n",
       "62240   1323b_num5.png   1323c_num4.png       1   \n",
       "60989   1296a_num2.png   1296b_num2.png       1   \n",
       "25616   0054a_num2.png   0067c_num5.png       0   \n",
       "33520   0070a_num1.png   0072c_num4.png       0   \n",
       "52312   1119a_num2.png   1119b_num5.png       1   \n",
       "40647   0884a_num3.png   0884b_num4.png       1   \n",
       "58897   1247c_num1.png   1247c_num6.png       1   \n",
       "19085   0039a_num1.png   0050a_num6.png       0   \n",
       "66245   1421a_num3.png   1421a_num4.png       1   \n",
       "34885   0072a_num1.png   0121c_num4.png       0   \n",
       "32713  0717bb_num2.png   0717c_num2.png       1   \n",
       "235     0004a_num2.png   0004a_num7.png       1   \n",
       "59400   1259b_num1.png   1259b_num2.png       1   \n",
       "7549    0016a_num2.png   0021c_num3.png       0   \n",
       "34946   0072a_num1.png   0127a_num1.png       0   \n",
       "4901   0108aa_num1.png  0108bb_num1.png       1   \n",
       "24185   0527c_num5.png   0527c_num9.png       1   \n",
       "54837   1165a_num1.png   1165b_num1.png       1   \n",
       "37777   0825c_num1.png   0825c_num3.png       1   \n",
       "7256    0155a_num3.png   0155c_num5.png       1   \n",
       "4590    0100a_num2.png   0100c_num5.png       1   \n",
       "10593   0220a_num2.png   0220c_num1.png       1   \n",
       "35891   0074a_num1.png   0122c_num2.png       0   \n",
       "30312   0063a_num2.png   0097b_num4.png       0   \n",
       "50503   1085a_num1.png   1085c_num4.png       1   \n",
       "38932   0081c_num1.png   0132b_num5.png       0   \n",
       "37747   0078a_num2.png   0107c_num3.png       0   \n",
       "\n",
       "                                            feature_id_A  \\\n",
       "16596  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "10764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5902   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4224   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "65840  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24574  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "7546   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20389  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26218  [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "57033  [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "41656  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46020  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37759  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20036  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "57950  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "15597  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8061   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28668  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9680   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46117  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38579  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24645  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "21435  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "36735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "69577  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9348   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43385  [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "6510   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "42954  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "15120  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62240  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60989  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25616  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "33520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "52312  [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40647  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "58897  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19085  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "66245  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34885  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32713  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "235    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "59400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7549   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4901   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24185  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "54837  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37777  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "7256   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4590   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "10593  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "35891  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30312  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50503  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38932  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37747  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            feature_id_B  \\\n",
       "16596  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "10764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5902   [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "4224   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "65840  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24574  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7546   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20389  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26218  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "57033  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41656  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46020  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37759  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "20036  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "57950  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15597  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8061   [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "28668  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9680   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46117  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "38579  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24645  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21435  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "36735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "69577  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9348   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43385  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "6510   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "42954  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "15120  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62240  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60989  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25616  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "33520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "52312  [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40647  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58897  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19085  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "66245  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34885  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32713  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "235    [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "59400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7549   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "34946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4901   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24185  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "54837  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37777  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7256   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4590   [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10593  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "35891  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30312  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "50503  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38932  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37747  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "\n",
       "                                          feature_id_AnB  \\\n",
       "16596  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "10764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5902   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4224   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "65840  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24574  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "7546   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20389  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26218  [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "57033  [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "41656  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46020  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37759  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20036  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "57950  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "15597  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8061   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28668  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9680   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "46117  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38579  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24645  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "21435  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "36735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "69577  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9348   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "43385  [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "6510   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "42954  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "15120  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62240  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60989  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25616  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "33520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "52312  [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40647  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "58897  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19085  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "66245  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34885  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32713  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "235    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "59400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7549   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "34946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4901   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24185  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "54837  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37777  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "7256   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4590   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "10593  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "35891  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30312  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50503  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38932  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37747  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AsB  \n",
       "16596  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1,...  \n",
       "10764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5902   [0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, ...  \n",
       "4224   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "65840  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24574  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "7546   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20389  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "26218  [1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1...  \n",
       "57033  [0, 0, 1, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0,...  \n",
       "41656  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "46020  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "37759  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, -...  \n",
       "20036  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "57950  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...  \n",
       "15597  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8061   [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...  \n",
       "28668  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9680   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "46117  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0,...  \n",
       "38579  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24645  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "21435  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "36735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...  \n",
       "69577  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "11044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9348   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "43385  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "6510   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "42954  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "15120  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "62240  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "60989  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25616  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...  \n",
       "33520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "52312  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "40647  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "58897  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19085  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "66245  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "34885  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "32713  [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "235    [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "59400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7549   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "34946  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4901   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24185  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...  \n",
       "54837  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "37777  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "7256   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4590   [-1, -1, 0, 0, 1, 0, -1, -1, 0, 0, 1, 0, 0, 0,...  \n",
       "10593  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 1,...  \n",
       "35891  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "30312  [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...  \n",
       "50503  [0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "38932  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "37747  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, -...  \n",
       "\n",
       "[111531 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling to avoid biasing towards different pairs\n",
    "df_GSC = df_GSC1.append(GSC_samepairs)\n",
    "df_GSC = shuffle(df_GSC)\n",
    "df_GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the functions, hyperparameters, training, testing, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "M = 10\n",
    "M_list_iterations = list(range(1,31))\n",
    "C_Lambda = 0.03\n",
    "C_Lambda_list_iterations = np.linspace(0.01,0.15,num = 12)\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "scaling_list_iterations = np.linspace(100,350,num = 15)\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are defined so as to make the code less complicated while executing the model.\n",
    "# The GetTargetVector function is used to import the csv file containing the target values.\n",
    "def GetTargetVector(filePath):\n",
    "    t = []\n",
    "    with open(filePath, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  \n",
    "            t.append(int(row[0]))\n",
    "    #print(\"Raw Training Generated..\")\n",
    "    return t\n",
    "\n",
    "# The GenerateRawData function is used to import the csv file containing the feature values.\n",
    "def GenerateRawData(filePath, IsSynthetic):    \n",
    "    dataMatrix = [] \n",
    "    with open(filePath, 'rU') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(float(column))\n",
    "            dataMatrix.append(dataRow)   \n",
    "    \n",
    "    if IsSynthetic == False :\n",
    "        dataMatrix = np.delete(dataMatrix, [5,6,7,8,9], axis=1)\n",
    "    dataMatrix = np.transpose(dataMatrix)     \n",
    "    #print (\"Data Matrix Generated..\")\n",
    "    return dataMatrix\n",
    "\n",
    "# The GenerateTrainingTarget function, to classify 80% the target data set as the training target set.\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "# The GenerateTrainingDataMatrix function, to classify 80% the raw feature data set as the training feature set.\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "# The GenerateValData function, to classify 10% the raw feature data set as the validation feature set.\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "# The GenerateValTargetVector function, to classify 10% the target data set as the validation target set.\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "# The GenerateBigSigma function is used to generate the variance of the basis function\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]+0.2\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "# The GetErms function has the formula of Erms defined in it, so it just has to be called in the model to get\n",
    "# the Erms values.\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "def GetAccuracy(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Testing sets for Human Observered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1673,)\n",
      "(18, 1673)\n",
      "(209,)\n",
      "(18, 209)\n",
      "(209,)\n",
      "(18, 209)\n",
      "(1673,)\n",
      "(9, 1673)\n",
      "(209,)\n",
      "(9, 209)\n",
      "(209,)\n",
      "(9, 209)\n"
     ]
    }
   ],
   "source": [
    "#Data Partitioning of Human Dataset\n",
    "split_human_train = int(len(human_df)*0.8)\n",
    "\n",
    "#Raw data\n",
    "h_targx = human_df['target'].values.tolist()\n",
    "h_targy = np.array(h_targx)\n",
    "h_targ = h_targy.T\n",
    "h_feat_s1 = human_df['feature_id_AsB'].values.tolist()\n",
    "h_feat_s2 = np.array(h_feat_s1)\n",
    "h_feat_s = h_feat_s2.T\n",
    "h_feat_c1 = human_df['feature_id_AnB'].values.tolist()\n",
    "h_feat_c2 = np.array(h_feat_c1)\n",
    "h_feat_c = h_feat_c2.T\n",
    "\n",
    "### CONCATENATION ###\n",
    "#Training sets\n",
    "h_tr_targ = np.array(GenerateTrainingTarget(h_targ,80))\n",
    "h_tr_feat_c   = GenerateTrainingDataMatrix(h_feat_c,80)\n",
    "print(h_tr_targ.shape)\n",
    "print(h_tr_feat_c.shape)\n",
    "h_tr_targ\n",
    "\n",
    "#Validation sets\n",
    "h_val_targ = np.array(GenerateValTargetVector(h_targ,ValidationPercent, (len(h_tr_targ))))\n",
    "h_val_feat_c    = GenerateValData(h_feat_c,ValidationPercent, (len(h_tr_targ)))\n",
    "print(h_val_targ.shape)\n",
    "print(h_val_feat_c.shape)\n",
    "\n",
    "#Testing sets\n",
    "h_test_targ = np.array(GenerateValTargetVector(h_targ,TestPercent, (len(h_tr_targ)+len(h_val_targ))))\n",
    "h_test_feat_c = GenerateValData(h_feat_c,TestPercent, (len(h_tr_targ)+len(h_val_targ)))\n",
    "print(h_val_targ.shape)\n",
    "print(h_val_feat_c.shape)\n",
    "\n",
    "\n",
    "\n",
    "### SUBTRACTION ###\n",
    "#Training sets\n",
    "h_tr_targ = np.array(GenerateTrainingTarget(h_targ,TrainingPercent))\n",
    "h_tr_feat_s   = GenerateTrainingDataMatrix(h_feat_s,TrainingPercent)\n",
    "print(h_tr_targ.shape)\n",
    "print(h_tr_feat_s.shape)\n",
    "h_tr_targ\n",
    "\n",
    "#Validation sets\n",
    "h_val_targ = np.array(GenerateValTargetVector(h_targ,ValidationPercent, (len(h_tr_targ))))\n",
    "h_val_feat_s    = GenerateValData(h_feat_s,ValidationPercent, (len(h_tr_targ)))\n",
    "print(h_val_targ.shape)\n",
    "print(h_val_feat_s.shape)\n",
    "\n",
    "#Testing sets\n",
    "h_test_targ = np.array(GenerateValTargetVector(h_targ,TestPercent, (len(h_tr_targ)+len(h_val_targ))))\n",
    "h_test_feat_s = GenerateValData(h_feat_s,TestPercent, (len(h_tr_targ)+len(h_val_targ)))\n",
    "print(h_val_targ.shape)\n",
    "print(h_val_feat_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89225,)\n",
      "(1024, 89225)\n",
      "(11153,)\n",
      "(1024, 11153)\n",
      "(11153,)\n",
      "(1024, 11153)\n",
      "(89225,)\n",
      "(512, 89225)\n",
      "(11153,)\n",
      "(512, 11153)\n",
      "(11153,)\n",
      "(512, 11153)\n"
     ]
    }
   ],
   "source": [
    "# Data Partitioning of GSC Dataset\n",
    "split_GSC_train = int(len(df_GSC)*0.8)\n",
    "\n",
    "#Raw data\n",
    "G_targx = df_GSC['target'].values.tolist()\n",
    "G_targy = np.array(G_targx)\n",
    "G_targ = G_targy.T\n",
    "G_feat_s1 = df_GSC['feature_id_AsB'].values.tolist()\n",
    "G_feat_s2 = np.array(G_feat_s1)\n",
    "G_feat_s = G_feat_s2.T\n",
    "G_feat_c1 = df_GSC['feature_id_AnB'].values.tolist()\n",
    "G_feat_c2 = np.array(G_feat_c1)\n",
    "G_feat_c= G_feat_c2.T\n",
    "\n",
    "#Training set for con\n",
    "G_tr_targ = np.array(GenerateTrainingTarget(G_targ,TrainingPercent))\n",
    "G_tr_feat_c   = GenerateTrainingDataMatrix(G_feat_c,TrainingPercent)\n",
    "print(G_tr_targ.shape)\n",
    "print(G_tr_feat_c.shape)\n",
    "G_tr_targ\n",
    "\n",
    "#Val set for con\n",
    "G_val_targ = np.array(GenerateValTargetVector(G_targ,ValidationPercent, (len(G_tr_targ))))\n",
    "G_val_feat_c    = GenerateValData(G_feat_c,ValidationPercent, (len(G_tr_targ)))\n",
    "print(G_val_targ.shape)\n",
    "print(G_val_feat_c.shape)\n",
    "\n",
    "#Testing set for con\n",
    "G_test_targ = np.array(GenerateValTargetVector(G_targ,TestPercent, (len(G_tr_targ)+len(G_val_targ))))\n",
    "G_test_feat_c = GenerateValData(G_feat_c,TestPercent, (len(G_tr_targ)+len(G_val_targ)))\n",
    "print(G_val_targ.shape)\n",
    "print(G_val_feat_c.shape)\n",
    "\n",
    "#Training set for sub\n",
    "G_tr_targ = np.array(GenerateTrainingTarget(G_targ,TrainingPercent))\n",
    "G_tr_feat_s   = GenerateTrainingDataMatrix(G_feat_s,TrainingPercent)\n",
    "print(G_tr_targ.shape)\n",
    "print(G_tr_feat_s.shape)\n",
    "G_tr_targ\n",
    "\n",
    "#Val set for sub\n",
    "G_val_targ = np.array(GenerateValTargetVector(G_targ,ValidationPercent, (len(G_tr_targ))))\n",
    "G_val_feat_s    = GenerateValData(G_feat_s,ValidationPercent, (len(G_tr_targ)))\n",
    "print(G_val_targ.shape)\n",
    "print(G_val_feat_s.shape)\n",
    "\n",
    "#Testing set for sub\n",
    "G_test_targ = np.array(GenerateValTargetVector(G_targ,TestPercent, (len(G_tr_targ)+len(G_val_targ))))\n",
    "G_test_feat_s = GenerateValData(G_feat_s,TestPercent, (len(G_tr_targ)+len(G_val_targ)))\n",
    "print(G_val_targ.shape)\n",
    "print(G_val_feat_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUMAN-CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Closed form\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(h_tr_feat_c))\n",
    "Muhcon = kmeans.cluster_centers_\n",
    "\n",
    "BigSigmahcon     = GenerateBigSigma(h_feat_c, Muhcon, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHIhcon = GetPhiMatrix(h_feat_c, Muhcon, BigSigmahcon, TrainingPercent)\n",
    "Whcon            = GetWeightsClosedForm(TRAINING_PHIhcon,h_tr_targ,(C_Lambda)) \n",
    "TEST_PHIhcon     = GetPhiMatrix(h_test_feat_c, Muhcon, BigSigmahcon, 100) \n",
    "VAL_PHIhcon      = GetPhiMatrix(h_val_feat_c, Muhcon, BigSigmahcon, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 18)\n",
      "(18, 18)\n",
      "(1673, 10)\n",
      "(10,)\n",
      "(209, 10)\n",
      "(208, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Muhcon.shape)\n",
    "print(BigSigmahcon.shape)\n",
    "print(TRAINING_PHIhcon.shape)\n",
    "print(Whcon.shape)\n",
    "print(VAL_PHIhcon.shape)\n",
    "print(TEST_PHIhcon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowhcon        = np.dot(220, Whcon)\n",
    "Lahcon           = 2\n",
    "learningRate_loophcon = np.linspace(0.01,0.05,num = 6)\n",
    "L_Erms_Valhcon   = []\n",
    "L_Erms_TRhcon    = []\n",
    "L_Erms_Testhcon  = []\n",
    "W_Mathcon        = []\n",
    "L_Erms_Val_newhcon = []\n",
    "Accuracy_testhcon = []\n",
    "\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(learningRate_loophcon)):\n",
    "    for i in range(0,400):\n",
    "        Delta_E_D     = -np.dot((h_tr_targ[i] - np.dot(np.transpose(W_Nowhcon),TRAINING_PHIhcon[i])),TRAINING_PHIhcon[i])\n",
    "        La_Delta_E_W  = np.dot(Lahcon,W_Nowhcon)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate_loophcon[j],Delta_E)\n",
    "        W_T_Next      = W_Nowhcon + Delta_W\n",
    "        W_Nowhcon     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIhcon,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,h_tr_targ)\n",
    "        L_Erms_TRhcon.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIhcon,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,h_val_targ)\n",
    "        L_Erms_Valhcon.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIhcon,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,h_test_targ)\n",
    "        L_Erms_Testhcon.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,h_test_targ)\n",
    "        Accuracy_testhcon.append(float(Accuracy_test))\n",
    "        \n",
    "    L_Erms_Val_newhcon.append(L_Erms_Valhcon[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 75.0%\n",
      "E_rms Training   = 0.43225\n",
      "E_rms Validation = 0.4318\n",
      "E_rms Testing    = 0.41683\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testhcon),2))+\"%\")  \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRhcon),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valhcon),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testhcon),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of Learning Rate\n",
    "plt.title('Validation ERMS Vs Varying Values of Learning Rate')\n",
    "plt.plot(learningRate_loophcon,L_Erms_Val_newhcon,'ys-', label='Validation ERMS')\n",
    "plt.axis([min(learningRate_loophcon), max(learningRate_loophcon), min(L_Erms_Val_newhcon)-0.1, max(L_Erms_Val_newhcon)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('Varying_eta.pdf', bbox_inches='tight')\n",
    "#plt.savefig('Varying_eta.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowhcon        = np.dot(220, Whcon)\n",
    "Lahcon           = 2\n",
    "learningRatehcon = 0.01\n",
    "La_loophcon = np.linspace(1,10,num = 5)\n",
    "L_Erms_Valhcon   = []\n",
    "L_Erms_TRhcon    = []\n",
    "L_Erms_Testhcon  = []\n",
    "W_Mathcon        = []\n",
    "L_Erms_Val_newhcon = []\n",
    "Accuracy_testhcon = []\n",
    "\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(La_loophcon)):\n",
    "    for i in range(0,400):\n",
    "        Delta_E_D     = -np.dot((h_tr_targ[i] - np.dot(np.transpose(W_Nowhcon),TRAINING_PHIhcon[i])),TRAINING_PHIhcon[i])\n",
    "        La_Delta_E_W  = np.dot(La_loophcon[j],W_Nowhcon)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRatehcon,Delta_E)\n",
    "        W_T_Next      = W_Nowhcon + Delta_W\n",
    "        W_Nowhcon     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIhcon,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,h_tr_targ)\n",
    "        L_Erms_TRhcon.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIhcon,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,h_val_targ)\n",
    "        L_Erms_Valhcon.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIhcon,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,h_test_targ)\n",
    "        L_Erms_Testhcon.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,h_test_targ)\n",
    "        Accuracy_testhcon.append(float(Accuracy_test))\n",
    "        \n",
    "    L_Erms_Val_newhcon.append(L_Erms_Valhcon[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 75.0%\n",
      "E_rms Training   = 0.43212\n",
      "E_rms Validation = 0.43174\n",
      "E_rms Testing    = 0.417\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testhcon),2))+\"%\")  \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRhcon),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valhcon),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testhcon),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8XGXZ//HPN2madEn3tHRPCt1YuhErylasYEEoizzSWpYCUkARQfT5gRuIoOLDrigCskmh8oBARaQiUhWfok2hFNo03Ze0pZTupWuS6/fHOQkn08lkJs10kvR6v17zypztPtdMZs517vucuW+ZGc4551yysjIdgHPOuebFE4dzzrmUeOJwzjmXEk8czjnnUuKJwznnXEo8cTjnnEuJJ44MkVQoySS1Cqf/LOmSZNZtwL6+K+mRA4nXpZ+kByX9INNxxJI0WdKbmY6jmgKPSdos6T8ZjOMWSU+lsP7jkm5LZ0wHiyeOBpI0Q9KtceafLemDVA/yZna6mT3RCHGNkVQeU/ZPzOyrB1p2nH1NllQpaUfMo1e4fIWkXeG8D8IvTvvI9o+HCXF8TLn3hvMnh9OtJd0lqTwsa7mke+qIaaGky+LM/6akkhRe28QwfsXMbyXpQ0lnJltWsszsKjP7cWOWKSlP0hZJn4uz7B5JzzXm/g6SE4BTgT5mNjp2YVNLdC2RJ46Gexy4KPbAAlwETDWzioMfUkbMMrP2MY+1keVnmVl7YAQwErgpZvtFQE1NK0y4/wUsjaxzE1AMjAbygVOAd+qI5wng4jjzLwqXJesFoBNwcsz8cYABr6ZQVvVZ8kH/vpnZbuD3xLwnkrKBiaT2njQV/YEVZvZxpgM5VHniaLgXgS7AidUzJHUGzgSeDKe/KOkdSdskrZZ0S12FSZop6avh82xJd0r6SNIy4Isx614qqVTSdknLJF0Zzm8H/BnoFT37j61SSxovaX54JjpT0tDIshWSvi1pnqStkn4vKe9A3ywz+wCYQZBAov4IHB++dxAcmOcBH0TW+RTwgpmttcAKM3uyjl39DjhBUv/IaxoKDAOeCacnh+/b9rD2MilOvLuBZ9k/CV1MeGIgqbOklyVtCJtNXpbUJ7LfmZJul/QvYCdwg6Q50cIk3SDpxfB5TVNGdc0xXP6hpHWSLo1s11XSH8PP1mxJtyU4y34C+JKktpF5XyD4/v85LO9GSUvD92SBpHPjFaQ4zabRz244fVn4+dysoGbeP5yvsJbzYfjZmifp6Dr200vSdEmbJC2RdEU4/3LgEeAz4ef7R3W85rjq+u6Ey6rf8/+OvOfnSDpD0qIwlu/GFJkXfke2S3pb0vBIeSPDedsl/R7IiyxL+Nlp6jxxNJCZ7WL/A8uXgYVm9m44/XG4vBPBwf9qSeckUfwVBAloJMGZ9vkxyz8Ml3cALgXukTQqPAM7HVhbx9k/kgYRHECvAwqAV4A/Smod8zrGAUUEB9zJScScUPilOB1YErNoNzAdmBBOX0yYeCPeAr4l6WuSjpH2q+XVMLNy4A2CGka1i4FXzOyjMLneD5xuZvnAZ4G5dRT3BHC+pDbha+gInBWJLwt4jOAMuB+wC/hlTBkXAVMIakr3A0XRRA1cSJDs4jkM6Aj0Bi4HHogk2AcIPl+HEdTY4l4fAzCz/wPWAefFxPV0pGa8lOAkqCPwI+ApST3rKrMu4ef7u+G+CoB/EiZs4DTgJGAQwXfiAmBjHUU9A5QDvQg+/z+RNNbMfgtcxSc13ZtTDDHudyey/DCCA3xv4IfAwwT/o2MJ3p8fShoQWf9s4H8JTiKfBl6UlBN+n14k+N92Cdf5UmS7ZD47TZeZ+aOBD4K21q1Am3D6X8D1Cda/F7gnfF5I0OTRKpyeCXw1fP434KrIdqdF141T7ovAN8PnY4DymOW3AE+Fz38APBtZlgWsAcaE0yuACyPLfw48WMd+JwMVwJbIY2lk+QpgB7A9jP91oFNk+ePAbeH7OIvgoLUeaAO8CUwO18sGvh6+v3uAtcAlCd7nC4GyyOtbBZwbTrcL4/xS9f+tnv/xYuAr4fMrgHcTrDsC2ByZngncGrPOr4Hbw+dHAZuB3Oj7Efk/7or+zwkOeseF78c+YHBk2W3Amwli+z7wl/B5B4Ia0MgE688Fzo78n9+M97mN89n9M3B5zOdrJ8EB8nMETZPHAVkJ9t0XqATyI/N+CjweG0+Cz2Wdy+v57uwCssPp/PC1fjqy/hzgnMj36q2Y17qOIMGcFH5OFVn+f9X/3/o+O0394TWOA2BmbwIbgLPDs5BPEZx1ACDp05LeCKujWwnOlLolUXQvYHVkemV0oaTTJb0VVp23AGckWW512TXlmVlVuK/ekXWizUQ7gfbU7S0z6xR5HB6z/BwLzuzHAEPixRm+jwUEB7eXLajNRZdXmtkDZnY8wZnq7cCjMWfuUX8Aeko6LtxvW+BPYVkfE5zpXgWsk/QnSUMSvL4n+aRWWes6iaS2kn4jaaWkbcA/gE4Krh9Ui/4fCbf/Slhruoggie+pY98brfa1sur/RQHQKqbs2P3Eex2nSOpNcAa/xMxqrhNJuljSXAXNl1uAo0n+MxXVH7gvUs4mQEBvM/sbwVn1A8B6SQ9J6hCnjF7AJjPbHpm3ktqf0QZJ4ruz0cwqw+fVn8P1keW7qP19qHnfw+9SdS2pF7DGwqwQeQ3VcSTz2WmyPHEcuOoDy0UEZ3TRD9nTBM0wfc2sI/AgwZeoPusIzrqq9at+IikXeB64E+hhZp0Impuqy62vu+O1BF/u6vIU7mtNEnE1mJn9neCM+s46VnkKuIH9m6liy9llZg8QnKkfWcc6O4Hn+OT/Ms3M9kaWzzCzU4GewEKC5oi6PAmMlfQZgjPlpyPLbgAGE5yRdiA4y4Ta/+Na/w8zewvYS3BW+hXqbqZKZANBTS/aJt63jnWr97uKoNloEsF7UvM+h9cgHgauAbqGn6n3if9Zrb4gHb1ecljk+WrgypiTiTYWNJdhZveb2bEEta1BwHfi7GMt0EVSfmRePw7wM5rEd6chat53BTc/9CGIfx3QO6ZZtV/keTKfnSbLE8eBexL4PEEzRuwdKvkEZ067JY0mOFAk41ngWkl9wjbtGyPLWgO5hAcPSacTNGVVWw90Ddvj6yr7i5LGSsoh+ADvIahGp9u9wKmSYi+QQ9D+fyrBmVctkq4LL1y2UXA77CUE721dd1ZB8L+4gKBJKlpL6KHg5oB2BK97B0GzSFxmtpKg2ewZ4DULLvJXyyc4A90iqQuQbHv7kwRn3hVhbSsl4RnxH4BbwjPXIcS/kyzWEwTJ4XhgamR+O4IEtwGCC8gENY54+95AcAC/UMFNHJcB0Vrmg8BNko4Ky+oo6b/C558Ka+E5BAloN3HeezNbTfB5/KmC24mHEVzjmRq7bgIKt615UP93pyGOlXSegpsFriP4TL1F0PRaQfA9biXpPIK7Aqs19LPTJHjiOEBmtoLgQ96OoHYR9TXgVknbCS60PZtksQ8T3IH0LvA2wUGien/bgWvDsjYTJKPpkeULCQ5yy8Lmgl4x8ZYRXAP4BfARwcXes6Jn5Cmqvrsl+vhUvBXDg86TBNdZYpdtMrPXY6r21XYBdxE0oX1EcL3jS2a2LEFc/yC4/rTGzGZH5mcRJMu1BM0oJxP8nxJ5gqCWFlsbupfgesxHBAeLZG/R/R3BgbkhtY1q1xBcE/ogLOcZgoNWIs8BnYHXzWxd9UwzW0Dw/s4iOPE4huB6Ul2uIKgpbCSoOdScdJjZC8AdwLSwCeZ9gpsiILi28jDB53ZluH1dNdCJBNdT1hLcGn2zmb1Wz+uL+izB5yb2Ued3p4FeIjhB2UxQkzvPzPaF36fzCK63bA7X+UNku4Z+dpoExf+eOufSJbxL60NglJktbqQy7wAOM7M6765yrrF4jcO5g+9qYPaBJA1JQyQNU2A0QVPOC40WoXMJNKjvI+dcw0haQXABNJnf8ySST9A81Yug9nIXQbOJc2nnTVXOOedS4k1VzjnnUtJimqq6detmhYWFmQ7DOeealTlz5nxkZgWpbNNiEkdhYSElJUn3mu2ccw6QtLL+tWrzpirnnHMp8cThnHMuJZ44nHPOpaTFXONwzjWuffv2UV5ezu7duzMdimsEeXl59OnTh5ycnAMuyxOHcy6u8vJy8vPzKSwsJMHYWa4ZMDM2btxIeXk5RUVFB1yeN1U55+LavXs3Xbt29aTRAkiia9eujVZ7TGvikDROUpmCMYNvjLO8XzjQ0TsKxh8+I5xfKGlXOLDMXEkPpjNO51x8njRajsb8X6atqSocyeoBgjEWyoHZkqaHXThX+z7BCGi/lnQkwaAqheGypWYWb9wG55xzGZTOGsdoguEpl4V9008jGNg9ygj66IdgbIG1aYzHOdeMjBkzhhkzZtSad++99/K1ryUePqV9+2Bk17Vr13L++efXWXZ9Pxi+99572blzZ830GWecwZYtW5IJPaFbbrmF3r17M2LEiJrHli1bmDlzJh07dmTkyJEMGTKEb3/72zXbPP7440ji9ddfr5n3wgsvIInnnnsOgJdffpmRI0cyfPhwjjzySH7zm98ccKx1SWfi6E3tcZDL2X/M4FsIRhIrJ6htfCOyrChswvq7pBPj7UDSFEklkko2bNjQiKE751K1fup6ZhXOYmbWTGYVzmL91PX1b5TAxIkTmTZtWq1506ZNY+LEiUlt36tXr5qDakPEJo5XXnmFTp06Nbi8qOuvv565c+fWPKrLPfHEE3nnnXd45513ePnll/nXvz4ZT+uYY47hmWeeqZmeNm0aw4cPB4I74KZMmcIf//hH3n33Xd555x3GjBnTKLHGk87EEa9BLbYr3onA42bWh2DQ+N+F4/auA/qZ2UjgW8DT8Qa1N7OHzKzYzIoLClLqasU514jWT11P2ZQy9qzcAwZ7Vu6hbErZASWP888/n5dffpk9e4KBDVesWMHatWs54YQT2LFjB2PHjmXUqFEcc8wxvPTS/j3Kr1ixgqOPDkbA3bVrFxMmTGDYsGFccMEF7Nq1q2a9q6++muLiYo466ihuvjkYwfX+++9n7dq1nHLKKZxyyilA0K3RRx99BMDdd9/N0UcfzdFHH829995bs7+hQ4dyxRVXcNRRR3HaaafV2k8q2rRpw4gRI1iz5pNh1k888UT+85//sG/fPnbs2MGSJUsYMSJozd++fTsVFRV07doVgNzcXAYPHtygfScjnbfjlhMZyJ1PBnGPuhwYB2Bms8JxgbuZ2YeEw2Ca2RxJSwkGtvfOqJzLgMXXLWbH3B11Lt/21jZsT+3zwqqdVSy8fCFrH47fAt1+RHsG3juwzjK7du3K6NGjefXVVzn77LOZNm0aF1xwAZLIy8vjhRdeoEOHDnz00Uccd9xxjB8/vs4LwL/+9a9p27Yt8+bNY968eYwaNapm2e23306XLl2orKxk7NixzJs3j2uvvZa7776bN954g27dutUqa86cOTz22GP8+9//xsz49Kc/zcknn0znzp1ZvHgxzzzzDA8//DBf/vKXef7557nwwgv3i+eee+7hqaeeAqBz58688cYbtZZv3ryZxYsXc9JJJ9XMk8TnP/95ZsyYwdatWxk/fjzLly8HoEuXLowfP57+/fszduxYzjzzTCZOnEhWVnrqBumsccwGBkoqktQamMD+4/uuAsYCSBoK5AEbJBWEF9eRNAAYCCQaX9o5l0GxSaO++cmKNldFm6nMjO9+97sMGzaMz3/+86xZs4b16+uu3fzjH/+oOYAPGzaMYcOG1Sx79tlnGTVqFCNHjmT+/PksWLCgrmIAePPNNzn33HNp164d7du357zzzuOf//wnAEVFRTW1gGOPPZYVK1bELSPaVBVNGv/85z8ZNmwYhx12GGeeeSaHHXZYre0mTJjAtGnT4jbZPfLII7z++uuMHj2aO++8k8suuyzh6zgQaatxmFmFpGuAGUA28KiZzZd0K1BiZtOBG4CHJV1P0Iw12cxM0knArZIqgErgKjPblK5YnXOJJaoZAMwqnBU0U8XI7Z/LyJkjG7zfc845h29961u8/fbb7Nq1q6amMHXqVDZs2MCcOXPIycmhsLCw3t8oxKuNLF++nDvvvJPZs2fTuXNnJk+eXG85iQa/y83NrXmenZ2dclPViSeeyMsvv8yiRYs44YQTOPfcc2sSEcDo0aN5//33adOmDYMGDdpv+2OOOYZjjjmGiy66iKKiIh5//PGU9p+stP6Ow8xeMbNBZna4md0ezvthmDQwswVmdryZDTezEWb2l3D+82Z2VDh/lJn9MZ1xOucOzIDbB5DVtvbhJKttFgNuH3BA5bZv354xY8Zw2WWX1TrD3rp1K927dycnJ4c33niDlSsT9wx+0kknMXXqVADef/995s2bB8C2bdto164dHTt2ZP369fz5z3+u2SY/P5/t27fHLevFF19k586dfPzxx7zwwguceGLc+3cabNCgQdx0003ccccd+y376U9/yk9+8pNa83bs2MHMmTNrpufOnUv//v0bNaYo73LEOXfAekzqAcCy7y1jz6o95PbLZcDtA2rmH4iJEydy3nnn1brDatKkSZx11lkUFxczYsQIhgwZkrCMq6++mksvvZRhw4YxYsQIRo8eDcDw4cMZOXIkRx11FAMGDOD444+v2WbKlCmcfvrp9OzZs1Zz0qhRo5g8eXJNGV/96lcZOXJknc1S8USvcQC8+OKL+61z1VVXceedd9Zcx6h2+umn77eumfHzn/+cK6+8kjZt2tCuXbu01TagBY05XlxcbD6Qk3ONp7S0lKFDh2Y6DNeI4v1PJc0xs+JUyvG+qpxzzqXEE4dzzrmUeOJwztWppTRlu8b9X3ricM7FlZeXx8aNGz15tADV43Hk5eU1Snl+V5VzLq4+ffpQXl6O9wPXMlSPANgYPHE45+LKyclplNHiXMvjTVXOOedS4onDOedcSjxxOOecS4knDueccynxxOGccy4lnjicc86lxBOHc865lHjicM45l5K0Jg5J4ySVSVoi6cY4y/tJekPSO5LmSTojsuymcLsySV9IZ5zOOeeSl7Zfjodjhj8AnAqUA7MlTTez6IC+3weeNbNfSzoSeAUoDJ9PAI4CegF/lTTIzCrTFa9zzrnkpLPGMRpYYmbLzGwvMA04O2YdAzqEzzsCa8PnZwPTzGyPmS0HloTlOeecy7B0Jo7ewOrIdHk4L+oW4EJJ5QS1jW+ksC2SpkgqkVTiHbE559zBkc7EoTjzYvtnngg8bmZ9gDOA30nKSnJbzOwhMys2s+KCgoIDDtg551z90tk7bjnQNzLdh0+aoqpdDowDMLNZkvKAbklu65xzLgPSWeOYDQyUVCSpNcHF7ukx66wCxgJIGgrkARvC9SZIypVUBAwE/pPGWJ1zziUpbTUOM6uQdA0wA8gGHjWz+ZJuBUrMbDpwA/CwpOsJmqImWzDc2HxJzwILgArg635HlXPONQ1qKcNCFhcXW0lJSabDcM65ZkXSHDMrTmUb/+W4c865lHjicM45lxJPHM4551LiicM551xKPHE455xLSYtJHNvnbGdW4SzWT12f6VCcc65FazGJA2DPyj2UTSnz5OGcc2nUohIHQNXOKpZ9b1mmw3DOuRarxSUOgD2r9mQ6BOeca7FaZOLI7Zeb6RCcc67FapGJo8dFPTIdgnPOtVgtKnHk9s0lp3sO659cz74t+zIdjnPOtUgtJnHkH5vPZ1Z9hmOmH8OeNXtY/PXFmQ7JOedapBaTOKp1+HQHCm8u5MOnP2T9035brnPONbYWlzgA+t3Ujw6f7cCiry1i98rdmQ7HOedalBaZOLJaZTH0qaFQBaUXl2KVLWPMEeecawrSmjgkjZNUJmmJpBvjLL9H0tzwsUjSlsiyysiy2CFn69WmqA0DfzmQrf/Yyqr/WXWgL8U551wobUPHSsoGHgBOBcqB2ZKmm9mC6nXM7PrI+t8ARkaK2GVmIw4khh4X9WDjyxtZ8YMVdDm1C/nH5h9Icc4550hvjWM0sMTMlpnZXmAacHaC9ScCzzRmAJIY9OAgcnrksGDSAip3+rDlzjl3oNKZOHoDqyPT5eG8/UjqDxQBf4vMzpNUIuktSefUsd2UcJ2SDRs2xA0ip0sOQ58cyq5Fu1h6w9IGvRDnnHOfSGfiUJx5dV2lngA8Z2bRKkG/cAD1rwD3Sjp8v8LMHjKzYjMrLigoqDOQzp/rTN8b+rL2wbV89MePUngJzjnnYqUzcZQDfSPTfYC1daw7gZhmKjNbG/5dBsyk9vWPlBXdVkT7Ee0pu7yMvev3HkhRzjl3SEtn4pgNDJRUJKk1QXLY7+4oSYOBzsCsyLzOknLD592A44EFsdumIis3i6FTh1K5vZKFly3EzG/Rdc65hkhb4jCzCuAaYAZQCjxrZvMl3SppfGTVicA0q30kHwqUSHoXeAP4WfRurIZqd2Q7BvzPADa9som1v6qr8uOccy4RtZQz7+LiYispKal3PTPjvS++x5Y3tnDsnGNpd2S7gxCdc841TZLmhNeTk9YifzmeiCQGPzqY7PbZlE4qpWpPVaZDcs65ZuWQSxwAuYflMvjRweyYu4PlP1ie6XCcc65ZOSQTB0C3s7rR88qerL5zNZvf2JzpcJxzrtk4ZBMHwBF3HUGbgW1YePFC9m32gZ+ccy4ZdSYOSVdIGhg+l6THJG2TNE/SqIMXYvpkt8vmyKePZO8He1l09SK/Rdc555KQqMbxTWBF+HwiMIygW5BvAfelN6yDJ//YfApvLWTD7zew/ikf+Mk55+qTKHFUmFl1+82ZwJNmttHM/gq0qHtY+/13Pzqe2JHFX1/MruW7Mh2Oc841aYkSR5WknpLygLHAXyPL2qQ3rINL2WLo74aCoPSiUqoq/BZd55yrS6LE8UOghKC5arqZzQeQdDKwLP2hHVx5/fMY9KtBbPvXNlb9zAd+cs65utQ5kJOZvRx2d55vZtH7VUuAC9IeWQb0mNSDjX/ayIpbVtDltC50GN0h0yE551yTU2fikHRe5Hm8Vf6QjoAybeCvBrL1X1spnVTKse8cS6v2aRsk0TnnmqVER8XngLnhA2qPr2G00MSR0ykY+GnuKXNZev1SBj88ONMhOedck5IocXyJoElqGPAS8IyZLTkoUWVYp5M70e//9WPVz1bR5YwuFJxb9yBRzjl3qKnz4riZvWBmE4CTgaXAXZLeDC+Ot3iFPyqk/aj2lF1Rxp61ezIdjnPONRnJdDmyG9gKbCP4/UZeWiNqIrJaBwM/Ve2sYuGlC7Eq/1W5c85B4i5HTpH0EDAHOAW4z8xGmtmMgxZdhrUb0o7D7z6czX/ZzJpfrMl0OM451yQkqnG8DowG3gRygYsl3V/9SKZwSeMklUlaIunGOMvvkTQ3fCyStCWy7BJJi8PHJSm+rkbT68pedD2rK0v/31J2vLcjU2E451yTkeji+KUHUrCkbOAB4FSgHJgtaXp0CFgzuz6y/jeAkeHzLsDNQDHBHVxzwm0Pev/nkhj8yGBmHzOb0kmljPrPKLLzsg92GM4512Qk+gHgE3UtC38YWJ/RwBIzWxZuMw04G6hr7PCJBMkC4AvAa2a2Kdz2NWAc8EwS+210rbu3ZshjQ3jvi++x/HvLOeKuIzIRhnPONQkJL45L+oyk8yV1D6eHSXqaoPmqPr2B1ZHp8nBevP30J+h592+pbCtpiqQSSSUbNmxIIqSG63pGV3p9vRfld5ez6a+b0rov55xryhJdHP8f4FGC33P8SdLNwGvAv4GBSZQd7+fmdd2aNAF4zswqU9nWzB4ys2IzKy4oSP9vLQ7/+eG0HdqWhZcsZN9GH/jJOXdoSlTj+CIw0swmAqcBNwInmNl9ZrY7ibLLgb6R6T7A2jrWnUDtZqhUtj1osttmM3TqUPZt2EfZlWU+8JNz7pCUKHHsqk4Q4UXpMjNbnELZs4GBkooktSZIDtNjV5I0GOgMzIrMngGcJqmzpM4EiatJ3AacPzKfotuK+Oj5j/jg8Q8yHY5zzh10ie6qOlxS9EBfGJ02s/GJCjazCknXEBzws4FHzWy+pFuBEjOrLmsiMM0ip+9mtknSjwmSD8Ct1RfKm4K+N/Rl0583seTaJXQ6qRNtDm9Rw5M451xCqqu5pb6uRczs72mJqIGKi4utpKTkoO1v9+rdlAwroe2Qtoz45wiyWiXzI3znnGtaJM0xs+JUtklU43jHzLbVsaN+KUXWAuX1zWPQg4NYMGEBK29bSdEtRZkOyTnnDopEp8kzq59Iej1m2YtpiaaZ6X5Bd3pc1IOVP17J1llbMx2Oc84dFIkSR/SW2C4Jlh3SBv5yIHn98ii9sJSK7RWZDsc559IuUeKwOp7Hmz5kterQiqFPDWX3it0sufaQGK7EOXeIS3SNo7ukbxHULqqfE077yEYRHY/vSP/v9mflbSvp8sUudD+/e6ZDcs65tElU43gYyAfaR55XTz+S/tCal/4/7E/+6HwWTVnE7vJkfh/pnHPNU6JODn90MANp7rJyshj61FBKRpawcPJChv9lOMryS0HOuZbHf3zQiNoObMsR9x7Blte3UH5veabDcc65tPDE0ch6Xt6Tbud0Y9lNy9jxrg/85JxreTxxNDJJDHp4EDldc1gwaQGVuyrr38g555qRRHdVASApl6Br9cLo+mZ2a/rCat5adwsGfpo3bh7LblzGwPuS6YXeOeeah2RqHC8RjNxXAXwcebgEunyhC72v7c2a+9ewaUaT6Z/ROecOWL01DqCPmY1LeyQt0ICfDWDz65tZOHkhxfOKaV3QOtMhOefcAUumxvF/ko5JeyQtUHabbI58+kj2bdpH2RU+8JNzrmVIJnGcAMyRVCZpnqT3JM1Ld2AtRfth7Rnw0wFsfGkj6x5Zl+lwnHPugCXTVHV6QwuXNA64j2Agp0fM7Gdx1vkycAtB/1fvmtlXwvmVwHvhaqvqGziqKetzXZ9g4KfrltDp5E60HdQ20yE551yD1VvjMLOVQCfgrPDRKZyXkKRs4AGCxHMkMFHSkTHrDARuAo43s6OA6yKLd5nZiPDRbJMGgLLEkMeHkJWXRemFpVTtq8p0SM4512D1Jg5J3wSmAt3Dx1OSvpFE2aMMnT/5AAAakklEQVSBJWa2zMz2AtMI7s6KugJ4IBzTHDP7MJXgm5Pc3rkMfmgw22dvZ8WPVmQ6HOeca7BkrnFcDnzazH5oZj8EjiM44NenN7A6Ml0ezosaBAyS9C9Jb4VNW9XyJJWE88+JtwNJU8J1SjZs2JBESJlV8KUCDrv0MFb9dBVb3tyS6XCcc65BkkkcAqI/f64kuYGc4q0Te1tRK2AgMAaYCDwiqVO4rF84Du5XgHslHb5fYWYPmVmxmRUXFDSPnt6PuO8I8orCgZ+2+sBPzrnmJ5nE8Rjwb0m3SLoFeAv4bRLblQN9I9N9gLVx1nnJzPaZ2XKgjCCRYGZrw7/LCIaxHZnEPpu8VvnBwE97yvew+JrFmQ7HOedSlszF8buBS4FNwGbgUjO7N4myZwMDJRVJag1MAKbHrPMicAqApG4ETVfLJHUOuzqpnn88sCC5l9T0dTyuI4U/KGT9U+tZP219psNxzrmU1Hk7rqQOZrZNUhdgRfioXtbFzBL2o2FmFZKuAWYQ3I77qJnNl3QrUGJm08Nlp0laQNAE9h0z2yjps8BvJFURJLefmVmLSRwA/b7Xj00zNrH46sV0PL4jeX3zMh2Sc84lRXX9mlnSy2Z2pqTl1L42IcDMbMDBCDBZxcXFVlJSkukwUrJr2S5KhpeQX5zP8L8OR9k+8JNz7uCSNCe8npy0OpuqzOzM8G+RmQ2IPIqaWtJortoMaMMRvziCLTO3sPqu1fVv4JxzTUAyv+N4PZl5rmEOu+QwCs4vYPn3l7P97e2ZDsc55+pVZ+KQlBde3+gWXqzuEj4KgV4HK8CWThKDHhxETkEOpZNKqdzpAz8555q2RDWOK4E5wJDwb/XjJYKuRFwjyemaw5AnhrBz4U6WfmdppsNxzrmEEl3juM/MioBvR65tFJnZcDP75UGM8ZDQ5fNd6POtPqz91Vo2/mljpsNxzrk61ds7rpn9QtLRBB0V5kXmP5nOwA5FA34ygM1/3czCyxbyqfc+RevuPvCTc67pSebi+M3AL8LHKcDPgWbdW21TlZWbxZFPH0nF1goWXrbQB35yzjVJyXQ5cj4wFvjAzC4FhgO5aY3qENbuqHYc/vPD2fSnTax9MLaHFuecy7xkEscuM6sCKiR1AD4E/HccadT7G73p/IXOLL1hKR8v/DjT4TjnXC3JJI6SsMfahwnuqnob+E9aozrESWLIY0PIbpdN6aRSqvb6wE/OuaYjmU4Ov2ZmW8zsQeBU4JKwycqlUW7PXAY/Mpgdb+9g+Q+XZzoc55yrkaiTw1GJlpnZ2+kJyVXrdnY3el7Rk9U/X02XcV3oPKZzpkNyzrmEt+PeFf7NA4qBdwk6OBwG/Bs4Ib2hOYAj7gn6slp48UKK5xWT0ykn0yE55w5xiX4AeIqZnQKsBEaFI+0dSzCg0pKDFeChLrtdNkOnDmXvur0s/poP/OScy7xkLo4PMbP3qifM7H1gRPpCcrE6fKoDhbcU8uEzH7J+qg/85JzLrGQSR6mkRySNkXSypIeB0nQH5mrrd2M/OhzfgUVfW8SuFbsyHY5z7hCWTOK4FJgPfBO4jmAI16TuqpI0TlKZpCWSbqxjnS9LWiBpvqSnI/MvkbQ4fFySzP5aMmWLob8bCgYLL16IVfqvyp1zmVHnCIAHXLCUDSwiuIW3nGAM8onRIWAlDQSeBT5nZpsldTezD8Pu3EsILsobwe9HjjWzzXXtrzmOANgQH/zuAxZevJCi24vo/93+mQ7HOdfMNeoIgJKeDf++J2le7COJskcDS8xsmZntBaYBZ8escwXwQHVCMLMPw/lfAF4zs03hsteAcam8sJaqx4U9KLiggBU3r2BbybZMh+OcOwQluh33m+HfMxtYdm8gOh5qOfDpmHUGAUj6F5AN3GJmr9axbe/YHUiaAkwB6NevXwPDbF4kMejXg9j2f9sonVRK8dvFZLfLznRYzrlDSKLbcdeFf1fGeyRRtuIVGzPdChgIjAEmAo+E3Zsksy1m9lB4m3BxQUFBEiG1DDmdcxjy5BB2Ld7Fkm/5ndHOuYMrUVPVdknb4jy2S0qmjaQc6BuZ7gPEdvdaDrxkZvvMbDlQRpBIktn2kNZ5TGf6fqcv6x5ax0cvfZTpcJxzh5BENY58M+sQ55FvZh2SKHs2MFBSkaTWwARgesw6LxKM8YGkbgRNV8uAGcBp4VjnnYHTwnkuoujHRbQf2Z6yr5ax54M9mQ7HOXeISOZ2XAAkdZfUr/pR3/pmVgFcQ3DALwWeNbP5km6VVD0Q1Axgo6QFwBvAd8xso5ltAn5MkHxmA7eG81xEVusshk4dSuWOSsouLfOBn5xzB0W9t+OGB/m7gF4EY3H0B0rN7Kj0h5e8Q+V23HjW/GoNi7++mCPuP4I+3+iT6XCcc81Io96OG/Fj4DhgkZkVEYwG+K8GxOfSpNfVvejyxS4s/c5SPp7vAz8559IrmcSxz8w2AlmSsszsDbyvqiZFEkN+O4RWHVqxYNICqvb4wE/OufRJJnFskdQe+AcwVdJ9QEV6w3Kpat2jNYMfHczH737M8u/7wE/OufRJJnGcDewCrgdeBZYCZ6UzKNcw3c7sRq+re7H6rtVs/ludvbM459wBSfQ7jl9K+qyZfWxmlWZWYWZPmNn9YdOVa4IOv/Nw2g5uS+nFpezbtC/T4TjnWqBENY7FwF2SVki6Q5Jf12gGstsGAz/tW7+PRVct8lt0nXONLtEPAO8zs88AJwObgMcklUr6oaRBBy1Cl7L8UfkU/riQDf+7gfVP+sBPzrnGVe81jrBvqjvMbCTwFeBcfCCnJq/fd/rR8aSOLL5mMbuW+cBPzrnGU2/ikJQj6SxJU4E/E4yx8aW0R+YOSM3AT9lQemEpVRV+i65zrnEkujh+qqRHCTocnAK8AhxuZheY2YsHK0DXcHn98oIu2GdtY9VPVmU6HOdcC5GoxvFdYBYw1MzOMrOpZuY/S25mekzsQfdJ3Vlx6wq2vrU10+E451qARBfHTzGzh71zweZv0AODyO2TS+mFpVRs999uOucOTNK947rmq1XHVgz93VB2L9/Nkut84Cfn3IHxxHGI6HRiJ/rd2I8PHv2ADX/YkOlwnHMZtn7qemYVzmIQg45NdVtPHIeQwlsKyS/Op+yKMvas8YGfnDtUrZ+6nrIpZexZ2bDjQKtGjsc1YVk5wcBPJSNLWHjpQoa9OgxlxRve3TnXFJgZVburqNpZReWuSqp2VdU8Knd+Ml25q5KqnZHncdaJrrft39uwvQ3vVSKtiUPSOOA+IBt4xMx+FrN8MvA/wJpw1i/N7JFwWSXwXjh/lZmNxx2wtoPacsQ9R7DoykWU319O3+v61r+Rc03E+qnrWfa9ZexZtYfcfrkMuH0APSb1OGj7NzOq9lTFPRgne9CutV49CaFqd8N/f5WVl0VWm08e2W2yyWobPD+QpAFpTBySsoEHgFMJfgsyW9J0M1sQs+rvzeyaOEXsMjPvHysNel7Rk42vbGTZjcvoPLYz7Y9pn+mQnKtXdfNK1c7gYLpn5Z5guqKKgnMKGv2gXdd6NPCYq9ba7wCe3SabrDZZtOrUiuxe2XUe6KvXy2qTRXbbJNbLy0rYmjCrcFaDm6kgvTWO0cASM1sGIGkaQRftsYnDHWSSGPzwYEqGlVD6lVJGzR5Fdl52psNyLZSZBQfijyuDx45Kqj6uqnlePb/q46pa09XPq+dv/b+t+50pV+2somxyGWWUpRyXWgUH8qy2tQ/MWW2yyM7PJqd7Tu2Dcbz1IgfxhOu1yUbZTadZeMDtA2ol4VSlM3H0BlZHpsuBT8dZ70uSTiLoyuR6M6veJk9SCcGgUT+L92t1SVMIftVOv379GjP2Fq91QWsGPzaY905/j+U3LeeIe47IdEguw6r2ViV9EI87PyYRRBNESmfp2ZDdPpvsdtmf/G2XnbB55fA7D0/57Dyr1aF7b1B1896y7y2Dlalvr3R1uy3pv4AvmNlXw+mLgNFm9o3IOl2BHWa2R9JVwJfN7HPhsl5mtlbSAOBvwFgzW1rX/oqLi62kpCQtr6UlW3ztYtb8Yg3DZgyjy2ldMh1Oi9PYbfJWZXUf3BOczScz3/aldizIapdVc1CvPsBntcuqdbBPan5MglBrIe1/dl5X80pu/1w+s+IzDX5PD3WS5phZcSrbpLPGUQ5Er7z2AdZGV4gZEOph4I7IsrXh32WSZgIjCUYfdI1owB0D2Pz6ZhZOXkjxvGJad2ud6ZBaBDNj3WPrWHLNkqBdnKBNfuHlC9k6ayv5I/NTbqqp/LiypqxkKVdxD9atD2u934E83kG8rvlZbRK3oadDvOaVrLZZDLh9wEGNw6U3ccwGBkoqIrhragJBt+w1JPU0s3Xh5HjC7toldQZ2hjWRbsDxwM/TGOshK7tNMPDT26PfZtGURRz1/FFxz/YOFdV3zVRuraRiWwUVWyuo3Fb5yd9487ZWULGtomab6vWIc4y3PcbaB9bWnplF3IN1q06tyO2T+8nBugFn8y2pOSbavJKpu6pcIG2Jw8wqJF0DzCC4HfdRM5sv6VagxMymA9dKGk9wHWMTMDncfCjwG0lVBD9S/Fmcu7FcI8kfkU/RT4pY9p1lfPDoB/S8vGemQ2qQqn1VdR/ckz3gb61IqskmKy+L7A7ZtOrYquZv6yNak90xm1Ydgnmrbq+jR2LBccuPq0kQWblZh3SyTkWPST08UTQBabvGcbD5NY4DY1XGu6e+y5Y3t9C6W2v2rtt70M7orMqo3N7wA371/GSacdRKtQ7urTq2Cp6H86KJILtD3fOyWtd/Ju9t8q45aGrXOFwzoizR9ZyubPnbFvau3Qt8cp88EDd5mBlVO6sO+Ay/cntlEgGy34E8pyCHvMPz4h/cqxNCzLysvIN3du9t8q6l8sThapTfVb7fvKqdVZRNKWP9M+v3O+BXbKuAJI75We2y9ju4t+7dus6De7QGULOsXXaz6x7F2+RdS+WJw9XYsyr+L0mrdlaxd91eWnVsRV5RXt3NO3HmZednt6gLtKnyNnnXEnnicDVy++XW2SZfPCelJlDnXAt26J4Kuv0MuH0AWW1rfyS8Td45F8sTh6vRY1IPBj80mNz+uaCgpjH4ocHe1OKcq8Wbqlwt3ibvnKuP1zicc86lxBOHc865lHjicM45lxJPHM4551LiicM551xKPHE455xLiScO55xzKfHE4ZxzLiWeOJxzzqUkrYlD0jhJZZKWSLoxzvLJkjZImhs+vhpZdomkxeHjknTG6ZxzLnlp63JEUjbwAHAqUA7MljQ9zhCwvzeza2K27QLcDBQDBswJt92crnidc84lJ501jtHAEjNbZmZ7gWnA2Ulu+wXgNTPbFCaL14BxaYrTOedcCtKZOHoDqyPT5eG8WF+SNE/Sc5L6prKtpCmSSiSVbNiwobHids45l0A6E0e8cT4tZvqPQKGZDQP+CjyRwraY2UNmVmxmxQUFBQcUrHPOueSkM3GUA30j032AtdEVzGyjmVUPOfcwcGyy2zrnnMuMdCaO2cBASUWSWgMTgOnRFST1jEyOB0rD5zOA0yR1ltQZOC2c55xzLsPSdleVmVVIuobggJ8NPGpm8yXdCpSY2XTgWknjgQpgEzA53HaTpB8TJB+AW81sU7pidc45lzyZ7XfpoFkqLi62kpKSTIfhnHPNiqQ5Zlacyjb+y3HnnHMp8cThnHMuJZ44nHPOpcQTh3POuZR44nDOOZcSTxzOOedS4onDOedcSjxxOOecS4knDueccynxxOGccy4lnjicc86lxBOHc865lHjicM45lxJPHM4551LiicM551xK0po4JI2TVCZpiaQbE6x3viSTVBxOF0raJWlu+HgwnXE655xLXtpGAJSUDTwAnEowhvhsSdPNbEHMevnAtcC/Y4pYamYj0hWfc865hklnjWM0sMTMlpnZXmAacHac9X4M/BzYncZYnHPONZJ0Jo7ewOrIdHk4r4akkUBfM3s5zvZFkt6R9HdJJ8bbgaQpkkoklWzYsKHRAnfOOVe3dCYOxZlXM8C5pCzgHuCGOOutA/qZ2UjgW8DTkjrsV5jZQ2ZWbGbFBQUFjRS2c865RNKZOMqBvpHpPsDayHQ+cDQwU9IK4DhguqRiM9tjZhsBzGwOsBQYlMZYnXPOJSmdiWM2MFBSkaTWwARgevVCM9tqZt3MrNDMCoG3gPFmViKpILy4jqQBwEBgWRpjdc45l6S03VVlZhWSrgFmANnAo2Y2X9KtQImZTU+w+UnArZIqgErgKjPblK5YnXPOJU9mVv9azUBxcbGVlJRkOgznnGtWJM0xs+JUtvFfjjvnnEuJJw7nnHMp8cThnHMuJS3mGoek7UBZpuOIoxvwUaaDiOExJcdjSl5TjMtjSs5gM8tPZYO03VWVAWWpXuA5GCSVNLW4PKbkeEzJa4pxeUzJkZTyXUXeVOWccy4lnjicc86lpCUljocyHUAdmmJcHlNyPKbkNcW4PKbkpBxTi7k47pxz7uBoSTUO55xzB4EnDueccylp9olD0qOSPpT0fqZjqSapr6Q3JJVKmi/pm00gpjxJ/5H0bhjTjzIdUzVJ2eGgXfEG9MoISSskvReOed8kOkGT1EnSc5IWhp+tz2Q4nsHh+1P92CbpukzGFMZ1ffgZf1/SM5LymkBM3wzjmZ/J9yje8VJSF0mvSVoc/u1cXznNPnEAjwPjMh1EjArgBjMbSjDOyNclHZnhmPYAnzOz4cAIYJyk4zIcU7VvAqWZDiKOU8xsRBO67/4+4FUzGwIMJ8PvmZmVhe/PCOBYYCfwQiZjktQbuBYoNrOjCXrmnpDhmI4GriAYTns4cKakgRkK53H2P17eCLxuZgOB18PphJp94jCzfwBNqst1M1tnZm+Hz7cTfMF7J94q7TGZme0IJ3PCR8bvjJDUB/gi8EimY2nKwhEwTwJ+C2Bme81sS2ajqmUssNTMVmY6EIIfNreR1ApoS+0B5DJhKPCWme00swrg78C5mQikjuPl2cAT4fMngHPqK6fZJ46mTlIhMBL4d2YjqWkSmgt8CLxmZhmPCbgX+G+gKtOBxDDgL5LmSJqS6WCAAcAG4LGwWe8RSe0yHVTEBOCZTAdhZmuAO4FVBENQbzWzv2Q2Kt4HTpLUVVJb4Axqj46aaT3MbB0EJ71A9/o28MSRRpLaA88D15nZtkzHY2aVYbNCH2B0WIXOGElnAh+GwwM3Nceb2SjgdIKmxpMyHE8rYBTwazMbCXxMEk0KB0M4wud44H+bQCydCc6gi4BeQDtJF2YyJjMrBe4AXgNeBd4laM5utjxxpImkHIKkMdXM/pDpeKLCJo6ZZP7a0PHA+HDM+WnA5yQ9ldmQAma2Nvz7IUG7/ejMRkQ5UB6pJT5HkEiagtOBt81sfaYDAT4PLDezDWa2D/gD8NkMx4SZ/dbMRpnZSQRNRYszHVPEekk9AcK/H9a3gSeONJAkgrboUjO7O9PxAITjuHcKn7ch+IItzGRMZnaTmfUJx5yfAPzNzDJ6dgggqZ2k/OrnwGkEzQ0ZY2YfAKslDQ5njQUWZDCkqIk0gWaq0CrgOEltw+/hWJrAjReSuod/+wHn0XTeL4DpwCXh80uAl+rboNn3jivpGWAM0E1SOXCzmf02s1FxPHAR8F54TQHgu2b2SgZj6gk8ISmb4IThWTNrMre/NjE9gBeC4w6tgKfN7NXMhgTAN4CpYdPQMuDSDMdD2GZ/KnBlpmMBMLN/S3oOeJugOegdmkY3H89L6grsA75uZpszEUS84yXwM+BZSZcTJN7/qrcc73LEOedcKrypyjnnXEo8cTjnnEuJJw7nnHMp8cThnHMuJZ44nHPOpcQTh2t2JM2U9IWYeddJ+lU92+1ItDxdwh5a50m6Pmb+LZK+3cj7mizpl0ms1+j7doeOZv87DndIeobgB4MzIvMmAN/JTDh1k3QY8Fkz65/pWJxrLF7jcM3RcwRdU+dCTUeSvYA3JbWX9Lqkt8PxNM6O3VjSmOjYH5J+KWly+PxYSX8POzecEemK4VpJC8Kaw7Q4ZeZJeizc5zuSTgkX/QXoHo5XcWIyL07Si+H+50c7WJS0Q9Id4bK/Shod1r6WSRofKaKvpFcllUm6ObL998J5fwUGR+ZfIWm2grFang9/1OdcnbzG4ZodM9so6T8EfW29RFDb+L2ZmaTdwLlmtk1SN+AtSdMtiV+6hv2L/QI428w2SLoAuB24jKBDwSIz21PddUuMr4exHSNpCEHPuoMIOv97OexcMlmXmdmmsGuY2ZKeN7ONQDtgppn9P0kvALcR/Gr7SILusKeH248GjiYYH2O2pD8R9PY7gaCn5lYEv6yu7lzyD2b2cPge3AZcHr4PzsXlicM1V9XNVdWJ47JwvoCfhL3ZVhGMg9ID+CCJMgcTHHBfC7sbySbomhtgHkF3Hy8CL8bZ9gTCg62ZLZS0EhgENKRX5GslVY/X0BcYCGwE9hL0rgrwHrDHzPZJeg8ojGz/WphokPSHMDaAF8xsZzh/emT9o8OE0QloT+0mQOf244nDNVcvAndLGgW0qR44C5gEFADHhgfVFUDs0KEV1G6mrV4uYL6ZxRuS9YsEAymNB34g6ahwUB4i2x4wSWMIOqD8jJntlDQzEt++SM2pimBUR8ysSsGgRdVia1cWxldXretx4BwzezdsshtzYK/CtXR+jcM1S+FohjOBR6nd02hHgjE+9oXXGeJdlF4JHCkpV1JHgh5UAcqAAoVjeUvKkXSUpCygr5m9QTDoVPWZedQ/CJIWYRNVv7C8VHUENodJYwjB0MOpOlXBONJtCEZz+1cY37mS2ijo+fesyPr5wLqwqW5SA/bnDjFe43DN2TME4y1Ex5SeCvxRUgkwlzhdx5vZaknPEjQ/LSboQRUz2yvpfOD+MKG0IhihcBHwVDhPwD1xhm39FfBg2GxUAUwOr4fU9xq+L+m6yPThwFWS5hEknrfqKyCON4HfAUcQ9OxbAiDp9wTvyUrgn5H1f0AwQuVKgiaw/Abs0x1CvHdc55xzKfGmKueccynxxOGccy4lnjicc86lxBOHc865lHjicM45lxJPHM4551LiicM551xK/j+HtyUDxxaYAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of lambda\n",
    "plt.title('Validation ERMS Vs Varying Values of Lambda')\n",
    "plt.plot(La_loophcon,L_Erms_Val_newhcon,'mo-', label='Validation ERMS')\n",
    "plt.axis([min(La_loophcon), max(La_loophcon), min(L_Erms_Val_newhcon)-0.1, max(L_Erms_Val_newhcon)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Lambda\")\n",
    "l = plt.legend()\n",
    "plt.savefig('Varying_lambda.pdf', bbox_inches='tight')\n",
    "plt.savefig('Varying_lambda.png', bbox_inches='tight')\n",
    "# Observation: With increase in lambda values, ERMS is roughly constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUMAN-SUBTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Closed form\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(h_tr_feat_s))\n",
    "Mu_hsub = kmeans.cluster_centers_\n",
    "\n",
    "BigSigma_hsub     = GenerateBigSigma(h_feat_s, Mu_hsub, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHI_hsub = GetPhiMatrix(h_feat_s, Mu_hsub, BigSigma_hsub, TrainingPercent)\n",
    "W_hsub            = GetWeightsClosedForm(TRAINING_PHI_hsub,h_tr_targ,(C_Lambda)) \n",
    "TEST_PHI_hsub     = GetPhiMatrix(h_test_feat_s, Mu_hsub, BigSigma_hsub, 100) \n",
    "VAL_PHI_hsub      = GetPhiMatrix(h_val_feat_s, Mu_hsub, BigSigma_hsub, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 9)\n",
      "(9, 9)\n",
      "(1673, 10)\n",
      "(10,)\n",
      "(209, 10)\n",
      "(208, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Mu_hsub.shape)\n",
    "print(BigSigma_hsub.shape)\n",
    "print(TRAINING_PHI_hsub.shape)\n",
    "print(W_hsub.shape)\n",
    "print(VAL_PHI_hsub.shape)\n",
    "print(TEST_PHI_hsub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now_hsub        = np.dot(220, W_hsub)\n",
    "La_hsub           = 2\n",
    "learningRate_loop_hsub = np.linspace(0.01,0.05,num = 6)\n",
    "L_Erms_Val_hsub   = []\n",
    "L_Erms_TR_hsub    = []\n",
    "L_Erms_Test_hsub  = []\n",
    "W_Mat_hsub        = []\n",
    "L_Erms_Val_new_hsub = []\n",
    "Accuracy_test_hsub = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(learningRate_loop_hsub)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((h_tr_targ[i] - np.dot(np.transpose(W_Now_hsub),TRAINING_PHI_hsub[i])),TRAINING_PHI_hsub[i])\n",
    "        La_Delta_E_W  = np.dot(La_hsub,W_Now_hsub)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate_loop_hsub[j],Delta_E)\n",
    "        W_T_Next      = W_Now_hsub + Delta_W\n",
    "        W_Now_hsub     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI_hsub,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,h_tr_targ)\n",
    "        L_Erms_TR_hsub.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI_hsub,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,h_val_targ)\n",
    "        L_Erms_Val_hsub.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI_hsub,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,h_test_targ)\n",
    "        L_Erms_Test_hsub.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,h_test_targ)\n",
    "        Accuracy_test_hsub.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_new_hsub.append(L_Erms_Val_hsub[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 73.08%\n",
      "E_rms Training   = 0.47953\n",
      "E_rms Validation = 0.47515\n",
      "E_rms Testing    = 0.46144\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_test_hsub),2))+\"%\")   \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR_hsub),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val_hsub),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test_hsub),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of Learning Rate\n",
    "plt.title('Validation ERMS Vs Varying Values of Learning Rate')\n",
    "plt.plot(learningRate_loop_hsub,L_Erms_Val_new_hsub,'cd-', label='Validation ERMS')\n",
    "plt.axis([min(learningRate_loop_hsub), max(learningRate_loop_hsub), min(L_Erms_Val_new_hsub)-0.1, max(L_Erms_Val_new_hsub)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('Varying_eta.pdf', bbox_inches='tight')\n",
    "#plt.savefig('Varying_eta.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now_hsub        = np.dot(220, W_hsub)\n",
    "La_loop_hsub = np.linspace(1,10,num = 10)\n",
    "learningRate_hsub = 0.01\n",
    "L_Erms_Val_hsub   = []\n",
    "L_Erms_TR_hsub    = []\n",
    "L_Erms_Test_hsub  = []\n",
    "W_Mat_hsub        = []\n",
    "L_Erms_Val_new_hsub = []\n",
    "Accuracy_test_hsub = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(La_loop_hsub)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((h_tr_targ[i] - np.dot(np.transpose(W_Now_hsub),TRAINING_PHI_hsub[i])),TRAINING_PHI_hsub[i])\n",
    "        La_Delta_E_W  = np.dot(La_loop_hsub[j],W_Now_hsub)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate_hsub,Delta_E)\n",
    "        W_T_Next      = W_Now_hsub + Delta_W\n",
    "        W_Now_hsub     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI_hsub,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,h_tr_targ)\n",
    "        L_Erms_TR_hsub.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI_hsub,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,h_val_targ)\n",
    "        L_Erms_Val_hsub.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHI_hsub,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,h_test_targ)\n",
    "        L_Erms_Test_hsub.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,h_test_targ)\n",
    "        Accuracy_test_hsub.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_new_hsub.append(L_Erms_Val_hsub[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 73.08%\n",
      "E_rms Training   = 0.47948\n",
      "E_rms Validation = 0.47524\n",
      "E_rms Testing    = 0.4613\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_test_hsub),2))+\"%\")   \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR_hsub),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val_hsub),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test_hsub),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of lambda\n",
    "plt.title('Validation ERMS Vs Varying Values of Lambda')\n",
    "plt.plot(La_loop_hsub,L_Erms_Val_new_hsub,'md-', label='Validation ERMS')\n",
    "plt.axis([min(La_loop_hsub), max(La_loop_hsub), min(L_Erms_Val_new_hsub)-0.1, max(L_Erms_Val_new_hsub)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Lambda\")\n",
    "l = plt.legend()\n",
    "# Observation: With increase in lambda values, ERMS is roughly constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC-CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Closed form\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(G_tr_feat_c))\n",
    "Mugcon = kmeans.cluster_centers_\n",
    "\n",
    "BigSigmagcon     = GenerateBigSigma(G_feat_c, Mugcon, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHIgcon = GetPhiMatrix(G_feat_c, Mugcon, BigSigmagcon, TrainingPercent)\n",
    "Wgcon            = GetWeightsClosedForm(TRAINING_PHIgcon,G_tr_targ,(C_Lambda)) \n",
    "TEST_PHIgcon     = GetPhiMatrix(G_test_feat_c, Mugcon, BigSigmagcon, 100) \n",
    "VAL_PHIgcon      = GetPhiMatrix(G_val_feat_c, Mugcon, BigSigmagcon, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1024)\n",
      "(1024, 1024)\n",
      "(89225, 10)\n",
      "(10,)\n",
      "(11153, 10)\n",
      "(11152, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Mugcon.shape)\n",
    "print(BigSigmagcon.shape)\n",
    "print(TRAINING_PHIgcon.shape)\n",
    "print(Wgcon.shape)\n",
    "print(VAL_PHIgcon.shape)\n",
    "print(TEST_PHIgcon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowgcon        = np.dot(220, Wgcon)\n",
    "Lagcon           = 2\n",
    "learningRate_loopgcon = np.linspace(0.01,0.05,num = 3)\n",
    "L_Erms_Valgcon   = []\n",
    "L_Erms_TRgcon    = []\n",
    "L_Erms_Testgcon  = []\n",
    "W_Matgcon        = []\n",
    "L_Erms_Val_newgcon = []\n",
    "Accuracy_testgcon = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(learningRate_loopgcon)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((G_tr_targ[i] - np.dot(np.transpose(W_Nowgcon),TRAINING_PHIgcon[i])),TRAINING_PHIgcon[i])\n",
    "        La_Delta_E_W  = np.dot(Lagcon,W_Nowgcon)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate_loopgcon[j],Delta_E)\n",
    "        W_T_Next      = W_Nowgcon + Delta_W\n",
    "        W_Nowgcon     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIgcon,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,G_tr_targ)\n",
    "        L_Erms_TRgcon.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIgcon,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,G_val_targ)\n",
    "        L_Erms_Valgcon.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIgcon,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,G_test_targ)\n",
    "        L_Erms_Testgcon.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,G_test_targ)\n",
    "        Accuracy_testgcon.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_newgcon.append(L_Erms_Valgcon[-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 63.29%\n",
      "E_rms Training   = 0.50833\n",
      "E_rms Validation = 0.50197\n",
      "E_rms Testing    = 0.5122\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testgcon),2))+\"%\") \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRgcon),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valgcon),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testgcon),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of Learning Rate\n",
    "plt.title('Plot of Validation ERMS with Varying Values of Learning Rate')\n",
    "plt.plot(learningRate_loopgcon,L_Erms_Val_newgcon,'go-', label='Validation ERMS')\n",
    "plt.axis([min(learningRate_loopgcon), max(learningRate_loopgcon), min(L_Erms_Val_newgcon)-0.1, max(L_Erms_Val_newgcon)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('Varying_eta.pdf', bbox_inches='tight')\n",
    "#plt.savefig('Varying_eta.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowgcon        = np.dot(220, Wgcon)\n",
    "La_loopgcon = np.linspace(1,10,num = 3)\n",
    "learningRategcon = 0.01\n",
    "L_Erms_Valgcon   = []\n",
    "L_Erms_TRgcon    = []\n",
    "L_Erms_Testgcon  = []\n",
    "W_Matgcon        = []\n",
    "L_Erms_Val_newgcon = []\n",
    "Accuracy_testgcon = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(La_loopgcon)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((G_tr_targ[i] - np.dot(np.transpose(W_Nowgcon),TRAINING_PHIgcon[i])),TRAINING_PHIgcon[i])\n",
    "        La_Delta_E_W  = np.dot(La_loopgcon[j],W_Nowgcon)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRategcon,Delta_E)\n",
    "        W_T_Next      = W_Nowgcon + Delta_W\n",
    "        W_Nowgcon     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIgcon,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,G_tr_targ)\n",
    "        L_Erms_TRgcon.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIgcon,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,G_val_targ)\n",
    "        L_Erms_Valgcon.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIgcon,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,G_test_targ)\n",
    "        L_Erms_Testgcon.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,G_test_targ)\n",
    "        Accuracy_testgcon.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_newgcon.append(L_Erms_Valgcon[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 53.34%\n",
      "E_rms Training   = 0.65682\n",
      "E_rms Validation = 0.65086\n",
      "E_rms Testing    = 0.66083\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testgcon),2))+\"%\") \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRgcon),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valgcon),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testgcon),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of lambda\n",
    "plt.title('Plot of Validation ERMS with Varying Values of Lambda')\n",
    "plt.plot(La_loopgcon,L_Erms_Val_newgcon,'go-', label='Validation ERMS')\n",
    "plt.axis([min(La_loopgcon), max(La_loopgcon), min(L_Erms_Val_newgcon)-0.1, max(L_Erms_Val_newgcon)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Lambda\")\n",
    "l = plt.legend()\n",
    "plt.savefig('Varying_lambda.pdf', bbox_inches='tight')\n",
    "plt.savefig('Varying_lambda.png', bbox_inches='tight')\n",
    "# Observation: With increase in lambda values, ERMS is roughly constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC-SUBTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Closed form\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(G_tr_feat_s))\n",
    "Mugsub = kmeans.cluster_centers_\n",
    "\n",
    "BigSigmagsub     = GenerateBigSigma(G_feat_s, Mugsub, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHIgsub = GetPhiMatrix(G_feat_s, Mugsub, BigSigmagsub, TrainingPercent)\n",
    "Wgsub            = GetWeightsClosedForm(TRAINING_PHIgsub,G_tr_targ,(C_Lambda)) \n",
    "TEST_PHIgsub     = GetPhiMatrix(G_test_feat_s, Mugsub, BigSigmagsub, 100) \n",
    "VAL_PHIgsub      = GetPhiMatrix(G_val_feat_s, Mugsub, BigSigmagsub, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512)\n",
      "(512, 512)\n",
      "(89225, 10)\n",
      "(10,)\n",
      "(11153, 10)\n",
      "(11152, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Mugsub.shape)\n",
    "print(BigSigmagsub.shape)\n",
    "print(TRAINING_PHIgsub.shape)\n",
    "print(Wgsub.shape)\n",
    "print(VAL_PHIgsub.shape)\n",
    "print(TEST_PHIgsub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowgsub        = np.dot(220, Wgsub)\n",
    "Lagsub           = 2\n",
    "learningRate_loopgsub = np.linspace(0.01,0.05,num = 3)\n",
    "L_Erms_Valgsub   = []\n",
    "L_Erms_TRgsub    = []\n",
    "L_Erms_Testgsub  = []\n",
    "W_Matgsub        = []\n",
    "L_Erms_Val_newgsub = []\n",
    "Accuracy_testgsub = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(learningRate_loopgsub)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((G_tr_targ[i] - np.dot(np.transpose(W_Nowgsub),TRAINING_PHIgsub[i])),TRAINING_PHIgsub[i])\n",
    "        La_Delta_E_W  = np.dot(Lagsub,W_Nowgsub)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate_loopgsub[j],Delta_E)\n",
    "        W_T_Next      = W_Nowgsub + Delta_W\n",
    "        W_Nowgsub     = W_T_Next\n",
    "    \n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIgsub,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,G_tr_targ)\n",
    "        L_Erms_TRgsub.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIgsub,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,G_val_targ)\n",
    "        L_Erms_Valgsub.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIgsub,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,G_test_targ)\n",
    "        L_Erms_Testgsub.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,G_test_targ)\n",
    "        Accuracy_testgsub.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_newgsub.append(L_Erms_Valgsub[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 68.62%\n",
      "E_rms Training   = 0.45687\n",
      "E_rms Validation = 0.45766\n",
      "E_rms Testing    = 0.45943\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testgsub),2))+\"%\") \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRgsub),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valgsub),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testgsub),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of Learning Rate\n",
    "plt.title('Plot of Validation ERMS with Varying Values of Learning Rate')\n",
    "plt.plot(learningRate_loopgsub,L_Erms_Val_newgsub,'mo-', label='Validation ERMS')\n",
    "plt.axis([min(learningRate_loopgsub), max(learningRate_loopgsub), min(L_Erms_Val_newgsub)-0.1, max(L_Erms_Val_newgsub)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('Varying_eta.pdf', bbox_inches='tight')\n",
    "#plt.savefig('Varying_eta.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Nowgsub        = np.dot(220, Wgsub)\n",
    "La_loopgsub = np.linspace(1,10,num = 3)\n",
    "learningRategsub = 0.01\n",
    "L_Erms_Valgsub   = []\n",
    "L_Erms_TRgsub    = []\n",
    "L_Erms_Testgsub  = []\n",
    "W_Matgsub        = []\n",
    "L_Erms_Val_newgsub = []\n",
    "Accuracy_testgsub = []\n",
    "# This loop is used as iterations to update the new weight values\n",
    "for j in range(len(La_loopgsub)):\n",
    "    for i in range(0,400):\n",
    "    \n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((G_tr_targ[i] - np.dot(np.transpose(W_Nowgsub),TRAINING_PHIgsub[i])),TRAINING_PHIgsub[i])\n",
    "        La_Delta_E_W  = np.dot(La_loopgsub[j],W_Nowgsub)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRategsub,Delta_E)\n",
    "        W_T_Next      = W_Nowgsub + Delta_W\n",
    "        W_Nowgsub     = W_T_Next\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHIgsub,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,G_tr_targ)\n",
    "        L_Erms_TRgsub.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHIgsub,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,G_val_targ)\n",
    "        L_Erms_Valgsub.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        TEST_OUT      = GetValTest(TEST_PHIgsub,W_T_Next) \n",
    "        Erms_Test = GetErms(TEST_OUT,G_test_targ)\n",
    "        L_Erms_Testgsub.append(float(Erms_Test.split(',')[1]))\n",
    "        Accuracy_test = GetAccuracy(TEST_OUT,G_test_targ)\n",
    "        Accuracy_testgsub.append(float(Accuracy_test))\n",
    "    \n",
    "    L_Erms_Val_newgsub.append(L_Erms_Valgsub[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 57.35%\n",
      "E_rms Training   = 0.56251\n",
      "E_rms Validation = 0.56197\n",
      "E_rms Testing    = 0.56269\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy = \" + str(np.around(max(Accuracy_testgsub),2))+\"%\") \n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TRgsub),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Valgsub),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Testgsub),5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block generates plot of ERMS with varying values of lambda\n",
    "plt.title('Plot of Validation ERMS with Varying Values of Lambda')\n",
    "plt.plot(La_loopgsub,L_Erms_Val_newgsub,'go-', label='Validation ERMS')\n",
    "plt.axis([min(La_loopgsub), max(La_loopgsub), min(L_Erms_Val_newgsub)-0.1, max(L_Erms_Val_newgsub)+0.1])\n",
    "plt.ylabel('Validation ERMS')\n",
    "plt.xlabel(\"Values of Lambda\")\n",
    "l = plt.legend()\n",
    "plt.savefig('Varying_lambda.pdf', bbox_inches='tight')\n",
    "plt.savefig('Varying_lambda.png', bbox_inches='tight')\n",
    "# Observation: With increase in lambda values, ERMS is roughly constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------- NEURAL NETWORKS --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data(Human Observed, GSC, same and different pairs)\n",
    "df_main_hum = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "df_diff_hum = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "df_same_hum = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "df_main_GSC = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/GSC-Features.csv\")\n",
    "df_diff_GSC = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\")\n",
    "df_same_GSC = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/same_pairs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Human Observed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features of Image Id A and Image ID B for human observed dataset with different pairs\n",
    "df_diff_hum['feature_id_A'] = 'NaN'\n",
    "df_diff_hum['feature_id_B'] = 'NaN'\n",
    "for i in df_diff_hum.index[0:1300]:\n",
    "    a=df_main_hum[df_main_hum[\"img_id\"] == df_diff_hum.at[i,\"img_id_A\"]]\n",
    "    b=df_main_hum[df_main_hum[\"img_id\"] == df_diff_hum.at[i,\"img_id_B\"]]\n",
    "    df_diff_hum.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    df_diff_hum.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])\n",
    "df_human1=df_diff_hum[0:1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "1         [0, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "2          [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "3          [0, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "4         [0, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "5         [0, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "6          [1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "7         [1, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "8         [0, 0, 0, -3, 0, 0, 0, 2, 0]\n",
       "9         [-1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "10       [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "11         [1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "12        [0, 0, 0, 0, 0, 0, -1, 0, 0]\n",
       "13        [1, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "14        [1, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "15        [-1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "16       [-1, 0, 0, -3, 0, 0, 0, 0, 0]\n",
       "17        [0, 0, 0, 0, 0, 0, -1, 1, 0]\n",
       "18        [1, 0, 0, -3, 0, 0, 0, 2, 0]\n",
       "19       [0, 0, 0, -3, 0, 0, -1, 0, 0]\n",
       "20        [0, 0, 0, -1, 0, 0, 0, 0, 0]\n",
       "21       [1, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "22        [-1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "23        [0, 0, 0, 0, 0, 0, 0, -2, 0]\n",
       "24        [1, 0, 0, 0, 0, 0, -1, 0, 0]\n",
       "25       [-1, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "26       [-1, 0, 0, 0, 0, 0, 0, -1, 0]\n",
       "27        [0, 0, 0, 0, 0, 0, -1, 2, 0]\n",
       "28       [0, 0, 0, -3, 0, 0, -1, 1, 0]\n",
       "29       [0, 0, 0, 0, 0, 0, -1, -1, 0]\n",
       "                     ...              \n",
       "1270       [0, 0, 0, 2, 2, 0, 0, 1, 0]\n",
       "1271      [1, 0, 0, 1, 0, 0, -1, 2, 0]\n",
       "1272    [-1, -3, 0, 3, 0, 0, -1, 1, 0]\n",
       "1273    [-1, 0, 0, 3, 0, 1, -1, -1, 0]\n",
       "1274      [-1, 0, 0, 2, 0, 1, 0, 1, 0]\n",
       "1275       [2, 1, 0, 3, 0, 0, 0, 0, 0]\n",
       "1276     [0, 0, 0, 2, 0, -1, 0, -1, 0]\n",
       "1277      [0, 0, 1, 3, 0, 0, -1, 0, 0]\n",
       "1278      [1, 0, 0, 0, 0, 0, -3, 2, 0]\n",
       "1279     [-1, -1, 0, 0, 0, 0, 0, 2, 0]\n",
       "1280     [0, -1, 0, 0, 0, 0, 0, -2, 0]\n",
       "1281     [1, 0, 0, 1, 0, 0, -1, -1, 0]\n",
       "1282     [1, -3, 0, 0, 0, 0, 0, -2, 0]\n",
       "1283    [-1, -3, 0, 0, 0, 0, -1, 0, 0]\n",
       "1284      [0, 0, 0, 2, 0, 0, -3, 0, 0]\n",
       "1285      [1, 0, 0, 2, 0, -1, 0, 1, 0]\n",
       "1286      [-1, 0, 0, 0, 2, 0, 0, 2, 0]\n",
       "1287      [1, 1, 0, 0, 0, 0, 0, -2, 0]\n",
       "1288     [-1, 1, 0, 0, 0, 0, -1, 0, 0]\n",
       "1289     [-1, 0, 0, 3, 0, 0, -2, 2, 0]\n",
       "1290       [2, 0, 0, 3, 0, 0, 0, 1, 1]\n",
       "1291      [1, -1, 0, 2, 0, 0, 0, 0, 0]\n",
       "1292     [-1, 0, 0, 3, 0, 0, -3, 2, 0]\n",
       "1293     [-1, -3, 0, 2, 0, 0, 0, 0, 0]\n",
       "1294     [0, 0, 0, 3, 0, 0, -3, -2, 0]\n",
       "1295    [-1, 0, 0, 3, 0, -1, -1, 1, 0]\n",
       "1296     [-1, 0, 0, 0, 0, 1, 0, -2, 0]\n",
       "1297      [0, -3, 0, 3, 0, 1, 0, 1, 0]\n",
       "1298     [-1, 0, 0, 0, 0, 0, -2, 1, 0]\n",
       "1299    [-1, -3, 0, 3, 0, 0, 0, -2, 0]\n",
       "Name: feature_id_AsB, Length: 1300, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating and Subtracting the features of Image Id A and Image ID B for human observed dataset with different pairs\n",
    "a = np.array(df_human1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "df_human1['feature_id_AnB'] = np.hstack((a[:, 0], a[:, 1])).tolist()\n",
    "df_human1['feature_id_AnB']\n",
    "df_human1['feature_id_AsB'] = np.subtract(a[:, 0], a[:, 1]).tolist()\n",
    "df_human1['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features of Image Id A and Image ID B for human observed dataset with same pairs\n",
    "df_same_hum['feature_id_A'] = 'NaN'\n",
    "df_same_hum['feature_id_B'] = 'NaN'\n",
    "for i in df_same_hum.index:\n",
    "    a=df_main_hum[df_main_hum[\"img_id\"] == df_same_hum.at[i,\"img_id_A\"]]\n",
    "    b=df_main_hum[df_main_hum[\"img_id\"] == df_same_hum.at[i,\"img_id_B\"]]\n",
    "    df_same_hum.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    df_same_hum.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [-1, -1, 0, 0, 0, 0, -3, 2, 0]\n",
       "1           [0, 0, 1, -3, 0, 0, -1, -1, 0]\n",
       "2            [1, 0, 0, -1, 0, -1, 0, 1, 0]\n",
       "3              [1, 0, 0, 3, 0, 0, 0, 0, 0]\n",
       "4              [0, 0, 0, 3, 0, 0, 0, 2, 0]\n",
       "5             [-1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "6             [0, 0, 0, 3, 0, 0, 0, -2, 0]\n",
       "7              [1, 0, 0, 3, 0, 0, 0, 0, 0]\n",
       "8              [1, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "9            [-1, 0, 1, 3, 0, 0, -1, 0, 0]\n",
       "10           [-1, 0, 0, 3, 0, 0, 0, -2, 0]\n",
       "11            [0, 0, 0, 0, 0, 0, 0, -3, 0]\n",
       "12           [1, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "13          [-1, 0, 0, 0, 0, 0, -1, -2, 0]\n",
       "14           [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "15            [1, 0, 0, -3, 0, 0, 1, 1, 0]\n",
       "16            [-2, 0, 0, 0, 0, 0, 0, 2, 0]\n",
       "17           [-2, 0, 0, -3, 0, 0, 0, 1, 0]\n",
       "18           [0, 0, 0, -3, 0, 0, 0, -1, 0]\n",
       "19          [-1, -3, 0, 0, 0, 0, 0, -3, 0]\n",
       "20        [-1, -3, 0, -3, 0, -1, 1, -2, 0]\n",
       "21          [-1, 0, 0, 0, 0, 0, -1, -2, 0]\n",
       "22         [-1, -3, 0, 3, 0, 0, -1, -1, 0]\n",
       "23            [0, -3, 0, 3, 0, 0, 0, 1, 0]\n",
       "24           [1, 0, 0, -2, 0, 0, 0, -1, 0]\n",
       "25          [-1, 1, 0, 1, 0, 0, -1, -2, 0]\n",
       "26          [-2, 1, 0, 3, 0, 0, -1, -1, 0]\n",
       "27           [0, 0, 1, 0, 0, 0, -3, -2, 0]\n",
       "28            [1, 0, 0, 0, 0, 0, -1, 2, 0]\n",
       "29           [-1, 0, 0, -3, 0, 0, 0, 3, 0]\n",
       "                      ...                 \n",
       "761           [0, -1, 1, 0, 2, 1, 2, 2, 1]\n",
       "762           [3, 2, 0, 4, 2, 1, -1, 1, 1]\n",
       "763       [-2, -3, -1, 0, 0, -1, 0, 1, -1]\n",
       "764      [1, 0, -1, -2, -2, -1, -1, 2, -1]\n",
       "765          [-1, 1, 0, -1, 2, 0, 2, 3, 1]\n",
       "766            [0, 3, 0, 0, 2, 0, 0, 4, 1]\n",
       "767           [1, 2, 0, 1, 0, 0, -2, 1, 0]\n",
       "768        [-1, 2, -2, -1, 2, 0, 3, -3, 0]\n",
       "769         [0, -2, 1, -2, 1, 1, -3, 0, 0]\n",
       "770         [-1, 0, -2, -1, 0, 1, 1, 2, 0]\n",
       "771           [1, 3, 1, 3, 0, 1, -1, 4, 0]\n",
       "772          [3, 4, -1, -1, 2, 0, 0, 1, 1]\n",
       "773           [1, 1, 1, 3, 2, 0, 2, -1, 0]\n",
       "774           [0, 2, 2, 3, 2, 0, -2, 0, 0]\n",
       "775           [0, 2, 1, 0, 2, 0, -3, 0, 0]\n",
       "776         [0, 0, -1, -3, 0, 0, -1, 0, 0]\n",
       "777    [-1, -1, -1, -2, 0, -1, -1, -1, -1]\n",
       "778          [1, -3, 1, 0, 2, 0, 3, -2, 0]\n",
       "779          [1, -1, 0, -2, 3, 1, 1, 2, 1]\n",
       "780       [1, -3, 0, 2, 0, -1, -1, -1, -1]\n",
       "781        [1, 1, 0, 0, -2, -2, 0, -1, -1]\n",
       "782         [0, 4, 0, -2, -2, -1, 1, 0, 0]\n",
       "783         [2, 1, 0, -1, 0, 0, -2, 0, -1]\n",
       "784           [2, 0, 1, 1, 0, 2, -2, 2, 1]\n",
       "785        [-3, 0, -1, -1, -1, 0, 0, 0, 0]\n",
       "786        [0, -4, -2, -1, 0, -2, 0, 0, 0]\n",
       "787         [3, 3, 0, -2, -2, 0, -1, 1, 0]\n",
       "788           [0, 1, 0, 0, 0, -1, 0, 1, 0]\n",
       "789       [-1, 4, -1, -1, -3, 0, 0, -1, 0]\n",
       "790           [0, 4, 2, 3, 2, -1, 2, 1, 1]\n",
       "Name: feature_id_AsB, Length: 791, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating and Subtracting the features of Image Id A and Image ID B for human observed dataset with same pairs\n",
    "b = np.array(df_same_hum[['feature_id_A','feature_id_B']].values.tolist())\n",
    "df_same_hum['feature_id_AnB'] = np.hstack((b[:, 0], b[:, 1])).tolist()\n",
    "df_same_hum['feature_id_AnB']\n",
    "df_same_hum['feature_id_AsB'] = np.subtract(b[:, 0], b[:, 1]).tolist()\n",
    "df_same_hum['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1320a</td>\n",
       "      <td>1320b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 1, 0, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 3, 2, 3, 1, 1, 0, 2, 1, ...</td>\n",
       "      <td>[-2, 0, 0, 3, 0, 0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0321a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 1, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 1, -1, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1497b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 0, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 2, 0, -1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1374b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 2, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, -2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1498a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[0, 1, 1, 1, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, -1, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1490c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, -3, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1444a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 3, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, -1, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0419a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 1, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, -1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1362a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, -3, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0496c</td>\n",
       "      <td>0496e</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 0, 1, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 3, ...</td>\n",
       "      <td>[1, 3, 1, 2, 0, -1, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1483a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, -1, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1402a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, -1, 0, 1, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1164c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[-1, 0, 0, 0, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1463c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, -1, 0, 3, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0351c</td>\n",
       "      <td>0351b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 3, 2, 1, 3, 2]</td>\n",
       "      <td>[1, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 1, 1, 3, 2, ...</td>\n",
       "      <td>[0, -1, 0, -1, -1, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0579c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 0, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1382a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 1, 0, 3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1565c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 2, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -2, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0552b</td>\n",
       "      <td>0552c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 3, 2, 2, 0, 0, 2, 1, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, -2, 0, 3, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1386b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1497a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 4, 1, 0, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 4, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, -3, 0, 0, 0, 0, -1, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1127c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 4, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, -3, 0, 0, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1442a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 2, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1442a</td>\n",
       "      <td>1442c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 2, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 2, 0, 2, 1]</td>\n",
       "      <td>[3, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 0, 0, 2, ...</td>\n",
       "      <td>[2, 1, 0, 0, 2, 0, 0, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1296c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 0, 2, 1, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 0, 2, ...</td>\n",
       "      <td>[1, 0, 0, -3, 2, 0, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1197a</td>\n",
       "      <td>1197b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 4, 1, 3, 0, 2, 1, 0, 2]</td>\n",
       "      <td>[0, 0, 1, 4, 2, 3, 0, 4, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 0, 2, 1, 0, 2, 0, 0, 1, 4, 2, 3, ...</td>\n",
       "      <td>[1, 4, 0, -1, -2, -1, 1, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0363c</td>\n",
       "      <td>0363b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 1, 0, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 3, 0, 2, 1, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, -1, 0, 1, 3, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0496d</td>\n",
       "      <td>0496e</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 1, 1]</td>\n",
       "      <td>[2, 1, 0, 1, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 0, 1, 2, 3, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, -1, 0, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>0474b</td>\n",
       "      <td>0474c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 3, 3, 3, 2]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 2, 3, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 3, 3, 3, 2, 3, 4, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -3, 0, 3, 0, 1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1236c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 3, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 3, ...</td>\n",
       "      <td>[1, 0, 0, 2, 0, -1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>0969a</td>\n",
       "      <td>0969b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 1, 1, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[2, 0, 1, 1, 2, 1, 1, 2, 1]</td>\n",
       "      <td>[2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2, 1, ...</td>\n",
       "      <td>[0, 2, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1408c</td>\n",
       "      <td>1408b</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 0, 2, 1]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 3, 0, 2, 1]</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 3, ...</td>\n",
       "      <td>[-1, 0, 1, -1, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1131b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 0, 1, 1, 2, 2, ...</td>\n",
       "      <td>[0, 1, 0, 2, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1540b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0302a</td>\n",
       "      <td>0302c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 1, 0, 2, 1, 1, 1]</td>\n",
       "      <td>[3, 1, 0, 2, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 0, 2, 1, 1, 1, 3, 1, 0, 2, 2, 3, ...</td>\n",
       "      <td>[-1, -1, 1, -1, -2, -1, 1, -2, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1229c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 0, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 0, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 1, 3, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1129a</td>\n",
       "      <td>1129b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 3, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 1, 4, 1]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 3, 1, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 2, -3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1543a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 3, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1413b</td>\n",
       "      <td>1413c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, -3, 0, -3, 0, -1, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1446a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0367b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 3, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>1234b</td>\n",
       "      <td>1234c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 0, 1, 1, 2, 3, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 0, 2, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[3, 0, 1, 1, 2, 3, 0, 4, 2, 2, 1, 0, 2, 2, 3, ...</td>\n",
       "      <td>[1, -1, 1, -1, 0, 0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1335c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 2, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 0, -2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1284b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 0, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 0, 2, ...</td>\n",
       "      <td>[1, 0, 0, 0, 2, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0584c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1445b</td>\n",
       "      <td>1445c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 2, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, 2, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0336a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 2, 1, 3, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, -1, 0, -3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1274a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, -1, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0539a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 4, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, -3, 0, 0, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1475a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 0, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 0, 0, 2, 2, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0552c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, -1, 0, 0, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0359a</td>\n",
       "      <td>1243a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 0, 1, 3, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, 1, 0, -3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1134b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 4, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-1, -3, 0, -1, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1375b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, -3, 0, -3, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1320a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 2, 1, ...</td>\n",
       "      <td>[1, 0, 0, -3, 0, 1, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1408c</td>\n",
       "      <td>1408a</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 0, 2, 1]</td>\n",
       "      <td>[3, 1, 0, 3, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 0, 2, 0, 2, 1, 3, 1, 0, 3, 2, 1, ...</td>\n",
       "      <td>[-3, 0, 1, -3, -2, 1, 0, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0577a</td>\n",
       "      <td>1362c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 0, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, 0, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>1036b</td>\n",
       "      <td>1036c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 0, 2, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 4, 1, 0, 0, 2, 2, 0, 2]</td>\n",
       "      <td>[2, 2, 0, 2, 2, 2, 0, 0, 2, 1, 4, 1, 0, 0, 2, ...</td>\n",
       "      <td>[1, -2, -1, 2, 2, 0, -2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1531b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 1, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1386a</td>\n",
       "      <td>1386b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 3, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-2, 0, 0, 0, 0, 0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2091 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     img_id_A img_id_B  target                 feature_id_A  \\\n",
       "162     1320a    1320b       1  [1, 1, 1, 3, 2, 1, 0, 3, 2]   \n",
       "903     0577a    0321a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "475     0359a    1497b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "443     0359a    1374b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "273     0359a    1498a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "565     0577a    1490c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "871     0577a    1444a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "674     0577a    0419a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "531     0577a    1362a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "233     0496c    0496e       1  [3, 4, 1, 3, 2, 2, 0, 1, 2]   \n",
       "1147    1120a    1483a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "717     0577a    1402a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1179    1120a    1164c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "1145    1120a    1463c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "229     0351c    0351b       1  [1, 0, 1, 0, 2, 2, 1, 2, 2]   \n",
       "679     0577a    0579c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1213    1120a    1382a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "618     0577a    1565c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "142     0552b    0552c       1  [2, 0, 1, 3, 2, 2, 0, 0, 2]   \n",
       "15      0359a    1386b       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "971     0577a    1497a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "560     0577a    1127c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "210     0359a    1442a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "246     1442a    1442c       1  [3, 2, 1, 0, 2, 2, 0, 1, 2]   \n",
       "855     0577a    1296c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "641     1197a    1197b       1  [1, 4, 1, 3, 0, 2, 1, 0, 2]   \n",
       "277     0363c    0363b       1  [1, 1, 1, 0, 2, 2, 3, 0, 2]   \n",
       "235     0496d    0496e       1  [2, 1, 1, 1, 2, 2, 0, 1, 1]   \n",
       "562     0474b    0474c       1  [2, 1, 1, 3, 2, 3, 3, 3, 2]   \n",
       "1285    1120a    1236c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "...       ...      ...     ...                          ...   \n",
       "631     0969a    0969b       1  [2, 2, 1, 1, 2, 2, 1, 2, 1]   \n",
       "619     1408c    1408b       1  [0, 1, 1, 0, 0, 2, 0, 2, 1]   \n",
       "1259    1120a    1131b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "1041    1120a    1540b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "732     0302a    0302c       1  [2, 0, 1, 1, 0, 2, 1, 1, 1]   \n",
       "1277    1120a    1229c       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "309     1129a    1129b       1  [3, 1, 1, 0, 2, 2, 3, 1, 2]   \n",
       "1051    1120a    1543a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "274     1413b    1413c       1  [0, 1, 1, 0, 2, 1, 0, 2, 2]   \n",
       "1030    1120a    1446a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "1020    1120a    0367b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "694     1234b    1234c       1  [3, 0, 1, 1, 2, 3, 0, 4, 2]   \n",
       "967     0577a    1335c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1201    1120a    1284b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "557     0577a    0584c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "130     1445b    1445c       1  [2, 1, 1, 2, 2, 2, 0, 0, 2]   \n",
       "365     0359a    0336a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "561     0577a    1274a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "1172    1120a    0539a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "340     0359a    1475a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "621     0577a    0552c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "312     0359a    1243a       0  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "796     0577a    1134b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "684     0577a    1375b       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "638     0577a    1320a       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "618     1408c    1408a       1  [0, 1, 1, 0, 0, 2, 0, 2, 1]   \n",
       "624     0577a    1362c       0  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "715     1036b    1036c       1  [2, 2, 0, 2, 2, 2, 0, 0, 2]   \n",
       "1209    1120a    1531b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "16      1386a    1386b       1  [1, 1, 1, 0, 2, 2, 0, 3, 2]   \n",
       "\n",
       "                     feature_id_B  \\\n",
       "162   [3, 1, 1, 0, 2, 1, 0, 0, 2]   \n",
       "903   [2, 1, 1, 3, 2, 1, 1, 2, 1]   \n",
       "475   [2, 1, 1, 0, 0, 2, 1, 2, 1]   \n",
       "443   [1, 1, 1, 0, 2, 2, 2, 2, 1]   \n",
       "273   [0, 1, 1, 1, 2, 2, 1, 2, 2]   \n",
       "565   [0, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "871   [0, 1, 1, 0, 2, 3, 0, 3, 2]   \n",
       "674   [0, 1, 1, 1, 2, 2, 0, 1, 2]   \n",
       "531   [3, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "233   [2, 1, 0, 1, 2, 3, 0, 2, 2]   \n",
       "1147  [2, 1, 1, 3, 2, 3, 0, 3, 2]   \n",
       "717   [2, 1, 1, 1, 2, 1, 0, 3, 2]   \n",
       "1179  [3, 1, 1, 3, 2, 3, 0, 2, 2]   \n",
       "1145  [1, 2, 1, 0, 2, 2, 0, 1, 2]   \n",
       "229   [1, 1, 1, 1, 3, 2, 1, 3, 2]   \n",
       "679   [2, 1, 1, 0, 2, 2, 1, 0, 1]   \n",
       "1213  [2, 0, 1, 0, 2, 2, 1, 3, 2]   \n",
       "618   [2, 1, 1, 0, 2, 2, 2, 2, 2]   \n",
       "142   [1, 2, 1, 0, 2, 2, 0, 2, 2]   \n",
       "15    [3, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "971   [0, 4, 1, 0, 2, 2, 1, 3, 2]   \n",
       "560   [2, 4, 1, 0, 2, 2, 0, 2, 2]   \n",
       "210   [3, 2, 1, 0, 2, 2, 0, 1, 2]   \n",
       "246   [1, 1, 1, 0, 0, 2, 0, 2, 1]   \n",
       "855   [1, 1, 1, 3, 0, 2, 1, 0, 2]   \n",
       "641   [0, 0, 1, 4, 2, 3, 0, 4, 2]   \n",
       "277   [1, 1, 1, 1, 2, 1, 0, 1, 1]   \n",
       "235   [2, 1, 0, 1, 2, 3, 0, 2, 2]   \n",
       "562   [3, 4, 1, 0, 2, 2, 3, 1, 2]   \n",
       "1285  [1, 1, 1, 1, 2, 3, 0, 1, 2]   \n",
       "...                           ...   \n",
       "631   [2, 0, 1, 1, 2, 1, 1, 2, 1]   \n",
       "619   [1, 1, 0, 1, 0, 3, 0, 2, 1]   \n",
       "1259  [2, 0, 1, 1, 2, 2, 0, 3, 2]   \n",
       "1041  [1, 1, 1, 0, 2, 2, 1, 3, 2]   \n",
       "732   [3, 1, 0, 2, 2, 3, 0, 3, 2]   \n",
       "1277  [2, 1, 0, 0, 2, 2, 1, 2, 2]   \n",
       "309   [3, 1, 1, 0, 2, 2, 1, 4, 1]   \n",
       "1051  [3, 1, 1, 0, 2, 2, 0, 4, 2]   \n",
       "274   [1, 4, 1, 3, 2, 2, 1, 2, 2]   \n",
       "1030  [1, 1, 1, 3, 2, 2, 1, 2, 2]   \n",
       "1020  [1, 1, 1, 0, 2, 2, 1, 2, 2]   \n",
       "694   [2, 1, 0, 2, 2, 3, 0, 2, 2]   \n",
       "967   [2, 1, 1, 3, 2, 2, 2, 1, 1]   \n",
       "1201  [1, 1, 1, 3, 0, 2, 0, 1, 2]   \n",
       "557   [0, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "130   [2, 1, 1, 0, 2, 3, 0, 0, 2]   \n",
       "365   [1, 2, 1, 3, 2, 2, 1, 3, 2]   \n",
       "561   [1, 1, 1, 1, 2, 2, 0, 3, 2]   \n",
       "1172  [2, 4, 1, 3, 2, 2, 1, 2, 2]   \n",
       "340   [1, 1, 0, 0, 2, 2, 1, 2, 2]   \n",
       "621   [1, 2, 1, 0, 2, 2, 0, 2, 2]   \n",
       "312   [1, 0, 1, 3, 2, 2, 1, 3, 2]   \n",
       "796   [3, 4, 1, 1, 2, 2, 0, 2, 2]   \n",
       "684   [1, 4, 1, 3, 2, 2, 0, 3, 2]   \n",
       "638   [1, 1, 1, 3, 2, 1, 0, 3, 2]   \n",
       "618   [3, 1, 0, 3, 2, 1, 0, 3, 2]   \n",
       "624   [3, 0, 1, 0, 2, 2, 0, 2, 2]   \n",
       "715   [1, 4, 1, 0, 0, 2, 2, 0, 2]   \n",
       "1209  [2, 1, 1, 3, 2, 1, 1, 3, 2]   \n",
       "16    [3, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "\n",
       "                                         feature_id_AnB  \\\n",
       "162   [1, 1, 1, 3, 2, 1, 0, 3, 2, 3, 1, 1, 0, 2, 1, ...   \n",
       "903   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 1, ...   \n",
       "475   [2, 1, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, ...   \n",
       "443   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...   \n",
       "273   [2, 1, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, ...   \n",
       "565   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 3, 2, 2, ...   \n",
       "871   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 3, ...   \n",
       "674   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 1, 2, 2, ...   \n",
       "531   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 3, 2, 2, ...   \n",
       "233   [3, 4, 1, 3, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 3, ...   \n",
       "1147  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 3, ...   \n",
       "717   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, ...   \n",
       "1179  [2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 3, 2, 3, ...   \n",
       "1145  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 2, 1, 0, 2, 2, ...   \n",
       "229   [1, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 1, 1, 3, 2, ...   \n",
       "679   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "1213  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 0, 1, 0, 2, 2, ...   \n",
       "618   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "142   [2, 0, 1, 3, 2, 2, 0, 0, 2, 1, 2, 1, 0, 2, 2, ...   \n",
       "15    [2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "971   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 4, 1, 0, 2, 2, ...   \n",
       "560   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 4, 1, 0, 2, 2, ...   \n",
       "210   [2, 1, 1, 0, 2, 2, 0, 2, 2, 3, 2, 1, 0, 2, 2, ...   \n",
       "246   [3, 2, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 0, 0, 2, ...   \n",
       "855   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 0, 2, ...   \n",
       "641   [1, 4, 1, 3, 0, 2, 1, 0, 2, 0, 0, 1, 4, 2, 3, ...   \n",
       "277   [1, 1, 1, 0, 2, 2, 3, 0, 2, 1, 1, 1, 1, 2, 1, ...   \n",
       "235   [2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 0, 1, 2, 3, ...   \n",
       "562   [2, 1, 1, 3, 2, 3, 3, 3, 2, 3, 4, 1, 0, 2, 2, ...   \n",
       "1285  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 3, ...   \n",
       "...                                                 ...   \n",
       "631   [2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2, 1, ...   \n",
       "619   [0, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 3, ...   \n",
       "1259  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 0, 1, 1, 2, 2, ...   \n",
       "1041  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...   \n",
       "732   [2, 0, 1, 1, 0, 2, 1, 1, 1, 3, 1, 0, 2, 2, 3, ...   \n",
       "1277  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 0, 0, 2, 2, ...   \n",
       "309   [3, 1, 1, 0, 2, 2, 3, 1, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "1051  [2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "274   [0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "1030  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "1020  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...   \n",
       "694   [3, 0, 1, 1, 2, 3, 0, 4, 2, 2, 1, 0, 2, 2, 3, ...   \n",
       "967   [2, 1, 1, 0, 2, 2, 0, 1, 2, 2, 1, 1, 3, 2, 2, ...   \n",
       "1201  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 0, 2, ...   \n",
       "557   [2, 1, 1, 0, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...   \n",
       "130   [2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 3, ...   \n",
       "365   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 3, 2, 2, ...   \n",
       "561   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2, ...   \n",
       "1172  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 4, 1, 3, 2, 2, ...   \n",
       "340   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 1, 0, 0, 2, 2, ...   \n",
       "621   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 2, 1, 0, 2, 2, ...   \n",
       "312   [2, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 3, 2, 2, ...   \n",
       "796   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 4, 1, 1, 2, 2, ...   \n",
       "684   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "638   [2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 1, 1, 3, 2, 1, ...   \n",
       "618   [0, 1, 1, 0, 0, 2, 0, 2, 1, 3, 1, 0, 3, 2, 1, ...   \n",
       "624   [2, 1, 1, 0, 2, 2, 0, 1, 2, 3, 0, 1, 0, 2, 2, ...   \n",
       "715   [2, 2, 0, 2, 2, 2, 0, 0, 2, 1, 4, 1, 0, 0, 2, ...   \n",
       "1209  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 1, ...   \n",
       "16    [1, 1, 1, 0, 2, 2, 0, 3, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "\n",
       "                          feature_id_AsB  \n",
       "162         [-2, 0, 0, 3, 0, 0, 0, 3, 0]  \n",
       "903       [0, 0, 0, -3, 0, 1, -1, -1, 1]  \n",
       "475         [0, 0, 0, 0, 2, 0, -1, 0, 1]  \n",
       "443         [1, 0, 0, 0, 0, 0, -2, 0, 1]  \n",
       "273        [2, 0, 0, -1, 0, 0, -1, 0, 0]  \n",
       "565        [2, 0, 0, -3, 0, 0, 0, -1, 0]  \n",
       "871        [2, 0, 0, 0, 0, -1, 0, -2, 0]  \n",
       "674         [2, 0, 0, -1, 0, 0, 0, 0, 0]  \n",
       "531        [-1, 0, 0, -3, 0, 0, 0, 1, 0]  \n",
       "233        [1, 3, 1, 2, 0, -1, 0, -1, 0]  \n",
       "1147       [0, 0, 0, 0, 0, -1, 0, -1, 0]  \n",
       "717        [0, 0, 0, -1, 0, 1, 0, -2, 0]  \n",
       "1179       [-1, 0, 0, 0, 0, -1, 0, 0, 0]  \n",
       "1145        [1, -1, 0, 3, 0, 0, 0, 1, 0]  \n",
       "229      [0, -1, 0, -1, -1, 0, 0, -1, 0]  \n",
       "679         [0, 0, 0, 0, 0, 0, -1, 1, 1]  \n",
       "1213       [0, 1, 0, 3, 0, 0, -1, -1, 0]  \n",
       "618        [0, 0, 0, 0, 0, 0, -2, -1, 0]  \n",
       "142        [1, -2, 0, 3, 0, 0, 0, -2, 0]  \n",
       "15          [-1, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "971       [2, -3, 0, 0, 0, 0, -1, -2, 0]  \n",
       "560        [0, -3, 0, 0, 0, 0, 0, -1, 0]  \n",
       "210        [-1, -1, 0, 0, 0, 0, 0, 1, 0]  \n",
       "246         [2, 1, 0, 0, 2, 0, 0, -1, 1]  \n",
       "855        [1, 0, 0, -3, 2, 0, -1, 1, 0]  \n",
       "641      [1, 4, 0, -1, -2, -1, 1, -4, 0]  \n",
       "277        [0, 0, 0, -1, 0, 1, 3, -1, 1]  \n",
       "235       [0, 0, 1, 0, 0, -1, 0, -1, -1]  \n",
       "562        [-1, -3, 0, 3, 0, 1, 0, 2, 0]  \n",
       "1285        [1, 0, 0, 2, 0, -1, 0, 1, 0]  \n",
       "...                                  ...  \n",
       "631          [0, 2, 0, 0, 0, 1, 0, 0, 0]  \n",
       "619       [-1, 0, 1, -1, 0, -1, 0, 0, 0]  \n",
       "1259        [0, 1, 0, 2, 0, 0, 0, -1, 0]  \n",
       "1041       [1, 0, 0, 3, 0, 0, -1, -1, 0]  \n",
       "732   [-1, -1, 1, -1, -2, -1, 1, -2, -1]  \n",
       "1277        [0, 0, 1, 3, 0, 0, -1, 0, 0]  \n",
       "309         [0, 0, 0, 0, 0, 0, 2, -3, 1]  \n",
       "1051       [-1, 0, 0, 3, 0, 0, 0, -2, 0]  \n",
       "274     [-1, -3, 0, -3, 0, -1, -1, 0, 0]  \n",
       "1030        [1, 0, 0, 0, 0, 0, -1, 0, 0]  \n",
       "1020        [1, 0, 0, 3, 0, 0, -1, 0, 0]  \n",
       "694        [1, -1, 1, -1, 0, 0, 0, 2, 0]  \n",
       "967        [0, 0, 0, -3, 0, 0, -2, 0, 1]  \n",
       "1201         [1, 0, 0, 0, 2, 0, 0, 1, 0]  \n",
       "557          [2, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "130         [0, 0, 0, 2, 0, -1, 0, 0, 0]  \n",
       "365      [1, -1, 0, -3, 0, 0, -1, -1, 0]  \n",
       "561        [1, 0, 0, -1, 0, 0, 0, -2, 0]  \n",
       "1172       [0, -3, 0, 0, 0, 0, -1, 0, 0]  \n",
       "340         [1, 0, 1, 0, 0, 0, -1, 0, 0]  \n",
       "621        [1, -1, 0, 0, 0, 0, 0, -1, 0]  \n",
       "312       [1, 1, 0, -3, 0, 0, -1, -1, 0]  \n",
       "796      [-1, -3, 0, -1, 0, 0, 0, -1, 0]  \n",
       "684       [1, -3, 0, -3, 0, 0, 0, -2, 0]  \n",
       "638        [1, 0, 0, -3, 0, 1, 0, -2, 0]  \n",
       "618     [-3, 0, 1, -3, -2, 1, 0, -1, -1]  \n",
       "624        [-1, 1, 0, 0, 0, 0, 0, -1, 0]  \n",
       "715       [1, -2, -1, 2, 2, 0, -2, 0, 0]  \n",
       "1209       [0, 0, 0, 0, 0, 1, -1, -1, 0]  \n",
       "16          [-2, 0, 0, 0, 0, 0, 0, 2, 0]  \n",
       "\n",
       "[2091 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mixing both same and different pairs for human observerd dataset\n",
    "df_human = df_human1.append(df_same_hum)\n",
    "df_human = shuffle(df_human)\n",
    "df_human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing GSC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features of Image Id A and Image ID B for GSC dataset with different pairs\n",
    "df_diff_GSC['feature_id_A'] = 'NaN'\n",
    "df_diff_GSC['feature_id_B'] = 'NaN'\n",
    "for i in df_diff_GSC.index[0:72000]:\n",
    "    a=df_main_GSC[df_main_GSC[\"img_id\"] == df_diff_GSC.at[i,\"img_id_A\"]]\n",
    "    b=df_main_GSC[df_main_GSC[\"img_id\"] == df_diff_GSC.at[i,\"img_id_B\"]]\n",
    "    df_diff_GSC.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    df_diff_GSC.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_GSC1=df_diff_GSC[0:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "5        [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "6        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "8        [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "9        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "10       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "11       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "12       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "13       [-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...\n",
       "14       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "15       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...\n",
       "16       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "17       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "18       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "19       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "20       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "21       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "22       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "23       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "24       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "25       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "26       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "27       [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "28       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "29       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "                               ...                        \n",
       "39970    [0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...\n",
       "39971    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39972    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39973    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39974    [1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...\n",
       "39975    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39976    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39977    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39978    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39979    [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39980    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39981    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39982    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39983    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39984    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39985    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39986    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39987    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39988    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39989    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39990    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39991    [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39992    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "39993    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39994    [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...\n",
       "39995    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39996    [0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...\n",
       "39997    [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "39998    [1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...\n",
       "39999    [1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...\n",
       "Name: feature_id_AsB, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating and Subtracting the features of Image Id A and Image ID B for GSC dataset with different pairs\n",
    "c = np.array(df_diff_GSC1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "df_diff_GSC1['feature_id_AnB'] = np.hstack((c[:, 0], c[:, 1])).tolist()\n",
    "df_diff_GSC1['feature_id_AnB']\n",
    "df_diff_GSC1['feature_id_AsB'] = np.subtract(c[:, 0], c[:, 1]).tolist()\n",
    "df_diff_GSC1['feature_id_AsB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002b_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0002c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0003c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004a_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004aa_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0001a_num1.png</td>\n",
       "      <td>0004b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39970</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39972</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39974</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39975</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39976</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39977</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39978</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39979</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0138c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39980</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39981</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39982</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39983</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0139c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39986</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0140b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39987</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141b_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39993</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0141c_num8.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0142a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0083a_num1.png</td>\n",
       "      <td>0142a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             img_id_A         img_id_B  target  \\\n",
       "0      0001a_num1.png   0002a_num1.png       0   \n",
       "1      0001a_num1.png   0002a_num2.png       0   \n",
       "2      0001a_num1.png   0002a_num3.png       0   \n",
       "3      0001a_num1.png   0002a_num4.png       0   \n",
       "4      0001a_num1.png   0002b_num1.png       0   \n",
       "5      0001a_num1.png   0002b_num2.png       0   \n",
       "6      0001a_num1.png   0002b_num3.png       0   \n",
       "7      0001a_num1.png   0002b_num4.png       0   \n",
       "8      0001a_num1.png   0002b_num5.png       0   \n",
       "9      0001a_num1.png   0002b_num6.png       0   \n",
       "10     0001a_num1.png   0002c_num1.png       0   \n",
       "11     0001a_num1.png   0002c_num2.png       0   \n",
       "12     0001a_num1.png   0002c_num3.png       0   \n",
       "13     0001a_num1.png   0002c_num4.png       0   \n",
       "14     0001a_num1.png   0003a_num1.png       0   \n",
       "15     0001a_num1.png   0003a_num2.png       0   \n",
       "16     0001a_num1.png   0003a_num3.png       0   \n",
       "17     0001a_num1.png   0003b_num1.png       0   \n",
       "18     0001a_num1.png   0003b_num2.png       0   \n",
       "19     0001a_num1.png   0003b_num4.png       0   \n",
       "20     0001a_num1.png   0003c_num1.png       0   \n",
       "21     0001a_num1.png   0003c_num2.png       0   \n",
       "22     0001a_num1.png   0003c_num3.png       0   \n",
       "23     0001a_num1.png   0004a_num2.png       0   \n",
       "24     0001a_num1.png   0004a_num3.png       0   \n",
       "25     0001a_num1.png   0004a_num5.png       0   \n",
       "26     0001a_num1.png   0004a_num6.png       0   \n",
       "27     0001a_num1.png   0004a_num7.png       0   \n",
       "28     0001a_num1.png  0004aa_num1.png       0   \n",
       "29     0001a_num1.png   0004b_num1.png       0   \n",
       "...               ...              ...     ...   \n",
       "39970  0083a_num1.png   0138a_num2.png       0   \n",
       "39971  0083a_num1.png   0138a_num3.png       0   \n",
       "39972  0083a_num1.png   0138a_num4.png       0   \n",
       "39973  0083a_num1.png   0138b_num1.png       0   \n",
       "39974  0083a_num1.png   0138b_num2.png       0   \n",
       "39975  0083a_num1.png   0138b_num3.png       0   \n",
       "39976  0083a_num1.png   0138b_num4.png       0   \n",
       "39977  0083a_num1.png   0138c_num1.png       0   \n",
       "39978  0083a_num1.png   0138c_num2.png       0   \n",
       "39979  0083a_num1.png   0138c_num4.png       0   \n",
       "39980  0083a_num1.png   0139a_num1.png       0   \n",
       "39981  0083a_num1.png   0139a_num4.png       0   \n",
       "39982  0083a_num1.png   0139b_num1.png       0   \n",
       "39983  0083a_num1.png   0139c_num1.png       0   \n",
       "39984  0083a_num1.png   0139c_num2.png       0   \n",
       "39985  0083a_num1.png   0139c_num4.png       0   \n",
       "39986  0083a_num1.png   0140b_num1.png       0   \n",
       "39987  0083a_num1.png   0141a_num2.png       0   \n",
       "39988  0083a_num1.png   0141a_num3.png       0   \n",
       "39989  0083a_num1.png   0141b_num1.png       0   \n",
       "39990  0083a_num1.png   0141b_num2.png       0   \n",
       "39991  0083a_num1.png   0141b_num5.png       0   \n",
       "39992  0083a_num1.png   0141b_num7.png       0   \n",
       "39993  0083a_num1.png   0141c_num1.png       0   \n",
       "39994  0083a_num1.png   0141c_num2.png       0   \n",
       "39995  0083a_num1.png   0141c_num5.png       0   \n",
       "39996  0083a_num1.png   0141c_num7.png       0   \n",
       "39997  0083a_num1.png   0141c_num8.png       0   \n",
       "39998  0083a_num1.png   0142a_num1.png       0   \n",
       "39999  0083a_num1.png   0142a_num2.png       0   \n",
       "\n",
       "                                            feature_id_A  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            feature_id_B  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AnB  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "39970  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39974  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39976  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39978  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39979  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39987  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39988  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39989  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39991  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39993  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39994  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39995  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39996  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39997  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39998  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39999  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AsB  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5      [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "6      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8      [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "9      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "10     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "11     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "13     [-1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...  \n",
       "14     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "15     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "16     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "18     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "22     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "23     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "26     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "27     [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "28     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "39970  [0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0...  \n",
       "39971  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39972  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39973  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39974  [1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "39975  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39976  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39977  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39978  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39979  [0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39980  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39981  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39982  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39983  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39984  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39985  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39986  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39987  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39988  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39989  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39990  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39991  [1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39992  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "39993  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39994  [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...  \n",
       "39995  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39996  [0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "39997  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "39998  [1, 1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...  \n",
       "39999  [1, 1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[40000 rows x 7 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GSC1=df_diff_GSC1[0:40000]\n",
    "df_GSC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Features of Image Id A and Image ID B for GSC dataset with same pairs\n",
    "df_same_GSC['feature_id_A'] = 'NaN'\n",
    "df_same_GSC['feature_id_B'] = 'NaN'\n",
    "for i in df_same_GSC.index[0:72000]:\n",
    "    a=df_main_GSC[df_main_GSC[\"img_id\"] == df_same_GSC.at[i,\"img_id_A\"]]\n",
    "    b=df_main_GSC[df_main_GSC[\"img_id\"] == df_same_GSC.at[i,\"img_id_B\"]]\n",
    "    df_same_GSC.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    df_same_GSC.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating and Subtracting the features of Image Id A and Image ID B for GSC dataset with same pairs\n",
    "d = np.array(df_same_GSC[['feature_id_A','feature_id_B']].values.tolist())\n",
    "df_same_GSC['feature_id_AnB'] = np.hstack((d[:, 0], d[:, 1])).tolist()\n",
    "df_same_GSC['feature_id_AsB'] = np.subtract(d[:, 0], d[:, 1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AnB</th>\n",
       "      <th>feature_id_AsB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66736</th>\n",
       "      <td>1430a_num1.png</td>\n",
       "      <td>1430b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19852</th>\n",
       "      <td>0040a_num3.png</td>\n",
       "      <td>0082a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63123</th>\n",
       "      <td>1343a_num5.png</td>\n",
       "      <td>1343b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21849</th>\n",
       "      <td>0485a_num3.png</td>\n",
       "      <td>0485c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>0025a_num5.png</td>\n",
       "      <td>0025c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37290</th>\n",
       "      <td>0077a_num1.png</td>\n",
       "      <td>0113c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>0085c_num1.png</td>\n",
       "      <td>0085c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>1067c_num2.png</td>\n",
       "      <td>1067c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8821</th>\n",
       "      <td>0018a_num1.png</td>\n",
       "      <td>0057b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>0022a_num1.png</td>\n",
       "      <td>0023a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68536</th>\n",
       "      <td>1482a_num1.png</td>\n",
       "      <td>1482c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>0007a_num2.png</td>\n",
       "      <td>0013b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16918</th>\n",
       "      <td>0034a_num1.png</td>\n",
       "      <td>0084c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23038</th>\n",
       "      <td>0513a_num1.png</td>\n",
       "      <td>0513c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20089</th>\n",
       "      <td>0454a_num1.png</td>\n",
       "      <td>0454c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24044</th>\n",
       "      <td>0050a_num1.png</td>\n",
       "      <td>0056b_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58687</th>\n",
       "      <td>1242b_num3.png</td>\n",
       "      <td>1242c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69239</th>\n",
       "      <td>1503a_num3.png</td>\n",
       "      <td>1503a_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64628</th>\n",
       "      <td>1377b_num2.png</td>\n",
       "      <td>1377c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25483</th>\n",
       "      <td>0053a_num1.png</td>\n",
       "      <td>0110a_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54526</th>\n",
       "      <td>1156a_num1.png</td>\n",
       "      <td>1156a_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58724</th>\n",
       "      <td>1243c_num3.png</td>\n",
       "      <td>1243cc_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14901</th>\n",
       "      <td>0030a_num1.png</td>\n",
       "      <td>0080b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50146</th>\n",
       "      <td>1073b_num5.png</td>\n",
       "      <td>1073c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63106</th>\n",
       "      <td>1343a_num3.png</td>\n",
       "      <td>1343b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31107</th>\n",
       "      <td>0688b_num4.png</td>\n",
       "      <td>0688b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>0011a_num1.png</td>\n",
       "      <td>0017c_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7416</th>\n",
       "      <td>0156a_num5.png</td>\n",
       "      <td>0156b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27069</th>\n",
       "      <td>0599a_num3.png</td>\n",
       "      <td>0599c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50136</th>\n",
       "      <td>1073b_num3.png</td>\n",
       "      <td>1073c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33622</th>\n",
       "      <td>0070a_num1.png</td>\n",
       "      <td>0086b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32544</th>\n",
       "      <td>0715a_num1.png</td>\n",
       "      <td>0715a_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13090</th>\n",
       "      <td>0290a_num3.png</td>\n",
       "      <td>0290c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62914</th>\n",
       "      <td>1339b_num2.png</td>\n",
       "      <td>1339c_num6.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, ...</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40064</th>\n",
       "      <td>0869b_num2.png</td>\n",
       "      <td>0869c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12421</th>\n",
       "      <td>0268a_num1.png</td>\n",
       "      <td>0268b_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20064</th>\n",
       "      <td>0041a_num1.png</td>\n",
       "      <td>0049c_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44743</th>\n",
       "      <td>0961a_num4.png</td>\n",
       "      <td>0961cc_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>0253c_num2.png</td>\n",
       "      <td>0253c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42455</th>\n",
       "      <td>0917b_num2.png</td>\n",
       "      <td>0917c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>0006a_num1.png</td>\n",
       "      <td>0059a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>0894a_num4.png</td>\n",
       "      <td>0894c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15730</th>\n",
       "      <td>0032b_num2.png</td>\n",
       "      <td>0062a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40976</th>\n",
       "      <td>0891a_num3.png</td>\n",
       "      <td>0891c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64709</th>\n",
       "      <td>1380a_num1.png</td>\n",
       "      <td>1380b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51396</th>\n",
       "      <td>1105b_num1.png</td>\n",
       "      <td>1105c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60956</th>\n",
       "      <td>1295a_num3.png</td>\n",
       "      <td>1295c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8853</th>\n",
       "      <td>0187a_num3.png</td>\n",
       "      <td>0187b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26212</th>\n",
       "      <td>0578b_num5.png</td>\n",
       "      <td>0578c_num6.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41458</th>\n",
       "      <td>0899b_num4.png</td>\n",
       "      <td>0899c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62818</th>\n",
       "      <td>1336c_num1.png</td>\n",
       "      <td>1336c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53805</th>\n",
       "      <td>1146a_num1.png</td>\n",
       "      <td>1146b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30866</th>\n",
       "      <td>0064a_num1.png</td>\n",
       "      <td>0104c_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18967</th>\n",
       "      <td>0038a_num1.png</td>\n",
       "      <td>0091c_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>0023a_num1.png</td>\n",
       "      <td>0065c_num8.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>0087b_num2.png</td>\n",
       "      <td>0087b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47539</th>\n",
       "      <td>1016aa_num1.png</td>\n",
       "      <td>1016aa_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64828</th>\n",
       "      <td>1385a_num2.png</td>\n",
       "      <td>1385b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>0012a_num1.png</td>\n",
       "      <td>0027a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51473</th>\n",
       "      <td>1109a_num1.png</td>\n",
       "      <td>1109c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111531 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              img_id_A         img_id_B  target  \\\n",
       "66736   1430a_num1.png   1430b_num3.png       1   \n",
       "19852   0040a_num3.png   0082a_num1.png       0   \n",
       "63123   1343a_num5.png   1343b_num3.png       1   \n",
       "21849   0485a_num3.png   0485c_num3.png       1   \n",
       "1450    0025a_num5.png   0025c_num1.png       1   \n",
       "37290   0077a_num1.png   0113c_num2.png       0   \n",
       "3910    0085c_num1.png   0085c_num4.png       1   \n",
       "49988   1067c_num2.png   1067c_num4.png       1   \n",
       "8821    0018a_num1.png   0057b_num4.png       0   \n",
       "10500   0022a_num1.png   0023a_num1.png       0   \n",
       "68536   1482a_num1.png   1482c_num2.png       1   \n",
       "3060    0007a_num2.png   0013b_num4.png       0   \n",
       "16918   0034a_num1.png   0084c_num1.png       0   \n",
       "23038   0513a_num1.png   0513c_num3.png       1   \n",
       "20089   0454a_num1.png   0454c_num4.png       1   \n",
       "24044   0050a_num1.png   0056b_num6.png       0   \n",
       "58687   1242b_num3.png   1242c_num4.png       1   \n",
       "69239   1503a_num3.png   1503a_num4.png       1   \n",
       "64628   1377b_num2.png   1377c_num1.png       1   \n",
       "25483   0053a_num1.png   0110a_num5.png       0   \n",
       "54526   1156a_num1.png   1156a_num2.png       1   \n",
       "58724   1243c_num3.png  1243cc_num1.png       1   \n",
       "14901   0030a_num1.png   0080b_num3.png       0   \n",
       "50146   1073b_num5.png   1073c_num1.png       1   \n",
       "63106   1343a_num3.png   1343b_num3.png       1   \n",
       "31107   0688b_num4.png   0688b_num5.png       1   \n",
       "5058    0011a_num1.png   0017c_num4.png       0   \n",
       "7416    0156a_num5.png   0156b_num5.png       1   \n",
       "27069   0599a_num3.png   0599c_num4.png       1   \n",
       "50136   1073b_num3.png   1073c_num2.png       1   \n",
       "...                ...              ...     ...   \n",
       "33622   0070a_num1.png   0086b_num4.png       0   \n",
       "32544   0715a_num1.png   0715a_num2.png       1   \n",
       "13090   0290a_num3.png   0290c_num3.png       1   \n",
       "62914   1339b_num2.png   1339c_num6.png       1   \n",
       "40064   0869b_num2.png   0869c_num2.png       1   \n",
       "12421   0268a_num1.png   0268b_num1.png       1   \n",
       "20064   0041a_num1.png   0049c_num5.png       0   \n",
       "44743   0961a_num4.png  0961cc_num2.png       1   \n",
       "11834   0253c_num2.png   0253c_num4.png       1   \n",
       "42455   0917b_num2.png   0917c_num3.png       1   \n",
       "2940    0006a_num1.png   0059a_num1.png       0   \n",
       "41183   0894a_num4.png   0894c_num1.png       1   \n",
       "15730   0032b_num2.png   0062a_num2.png       0   \n",
       "40976   0891a_num3.png   0891c_num4.png       1   \n",
       "64709   1380a_num1.png   1380b_num4.png       1   \n",
       "51396   1105b_num1.png   1105c_num1.png       1   \n",
       "60956   1295a_num3.png   1295c_num1.png       1   \n",
       "8853    0187a_num3.png   0187b_num3.png       1   \n",
       "26212   0578b_num5.png   0578c_num6.png       1   \n",
       "41458   0899b_num4.png   0899c_num5.png       1   \n",
       "62818   1336c_num1.png   1336c_num4.png       1   \n",
       "53805   1146a_num1.png   1146b_num3.png       1   \n",
       "30866   0064a_num1.png   0104c_num3.png       0   \n",
       "18967   0038a_num1.png   0091c_num6.png       0   \n",
       "11333   0023a_num1.png   0065c_num8.png       0   \n",
       "3980    0087b_num2.png   0087b_num3.png       1   \n",
       "47539  1016aa_num1.png  1016aa_num2.png       1   \n",
       "64828   1385a_num2.png   1385b_num4.png       1   \n",
       "5632    0012a_num1.png   0027a_num2.png       0   \n",
       "51473   1109a_num1.png   1109c_num1.png       1   \n",
       "\n",
       "                                            feature_id_A  \\\n",
       "66736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19852  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "63123  [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...   \n",
       "21849  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "1450   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37290  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3910   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49988  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8821   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "68536  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3060   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16918  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23038  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20089  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "58687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "69239  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "64628  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "54526  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58724  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14901  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "50146  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "63106  [1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "31107  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5058   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7416   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27069  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50136  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "33622  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32544  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "13090  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62914  [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "40064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11834  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "42455  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2940   [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41183  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "15730  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40976  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60956  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8853   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26212  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41458  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62818  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53805  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30866  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18967  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11333  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3980   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "47539  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "5632   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51473  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            feature_id_B  \\\n",
       "66736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19852  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "63123  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "21849  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1450   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37290  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3910   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "49988  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8821   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "10500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "68536  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3060   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "16918  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23038  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20089  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "69239  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, ...   \n",
       "64628  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "54526  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58724  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "14901  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50146  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "63106  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "31107  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5058   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7416   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27069  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50136  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "33622  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32544  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "13090  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62914  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, ...   \n",
       "40064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11834  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "42455  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "2940   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41183  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "15730  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "40976  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60956  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8853   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26212  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "41458  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62818  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53805  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "30866  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18967  [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11333  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3980   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "47539  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5632   [0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "51473  [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AnB  \\\n",
       "66736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "19852  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "63123  [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...   \n",
       "21849  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "1450   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37290  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3910   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "49988  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8821   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "68536  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3060   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16918  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "23038  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20089  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "24044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "58687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "69239  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "64628  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "54526  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "58724  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14901  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "50146  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "63106  [1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "31107  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5058   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7416   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27069  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50136  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "33622  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "32544  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "13090  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62914  [1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "40064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "12421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11834  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "42455  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2940   [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41183  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "15730  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "40976  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "60956  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8853   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "26212  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "41458  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "62818  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "53805  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30866  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "18967  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "11333  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3980   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "47539  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "64828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "5632   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51473  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          feature_id_AsB  \n",
       "66736  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "19852  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63123  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, ...  \n",
       "21849  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "1450   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "37290  [1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, ...  \n",
       "3910   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...  \n",
       "49988  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8821   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...  \n",
       "10500  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "68536  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3060   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...  \n",
       "16918  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "23038  [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "20089  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "24044  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "58687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0,...  \n",
       "69239  [0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1...  \n",
       "64628  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25483  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "54526  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "58724  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1...  \n",
       "14901  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "50146  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "63106  [0, 0, -1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,...  \n",
       "31107  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5058   [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "7416   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "27069  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "50136  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...  \n",
       "...                                                  ...  \n",
       "33622  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "32544  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "13090  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "62914  [1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, -1, -1...  \n",
       "40064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20064  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "44743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "11834  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "42455  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1...  \n",
       "2940   [0, 1, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "41183  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...  \n",
       "15730  [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -...  \n",
       "40976  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "64709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "51396  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "60956  [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "8853   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "26212  [0, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, -...  \n",
       "41458  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "62818  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "53805  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "30866  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "18967  [0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "11333  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3980   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "47539  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "64828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "5632   [0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0...  \n",
       "51473  [-1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[111531 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mixing both same and different pairs for GSC dataset\n",
    "df_GSC = df_GSC1.append(df_same_GSC)\n",
    "df_GSC = shuffle(df_GSC)\n",
    "df_GSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training and Testing Datasets for GSC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Partitioning of GSC Dataset\n",
    "split_GSC_train = int(len(df_GSC)*0.9)\n",
    "\n",
    "# Training set\n",
    "train_GSC = df_GSC[:split_GSC_train]\n",
    "\n",
    "# Testing Set\n",
    "test_GSC = df_GSC[:-split_GSC_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training and Testing Datasets for Human Observed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Partitioning of Human Dataset\n",
    "split_human_train = int(len(df_human)*0.9)\n",
    "\n",
    "# Training set\n",
    "train_human = df_human[:split_human_train]\n",
    "\n",
    "# Testing Set\n",
    "test_human = df_human[:-split_human_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Neural Networks\n",
    "## Human Observed Dataset with Feature Concatentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 18)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing datasets for Human Features with Feature Concatentation\n",
    "x_train_human1 = train_human.iloc[:,5].values.tolist()\n",
    "x_train_human_con = np.array(x_train_human1)\n",
    "y_train_human1 = train_human.iloc[:,2].values.tolist()\n",
    "y_train_human_con = np.array(y_train_human1)\n",
    "x_test_human1 = test_human.iloc[:,5].values.tolist()\n",
    "x_test_human_con = np.array(x_test_human1)\n",
    "y_test_human1 = test_human.iloc[:,2].values.tolist()\n",
    "y_test_human_con = np.array(y_test_human1)\n",
    "x_test_human_con.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 774us/step - loss: 0.6102 - acc: 0.6661 - val_loss: 0.5688 - val_acc: 0.6931\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.5450 - acc: 0.7435 - val_loss: 0.5058 - val_acc: 0.7831\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.4924 - acc: 0.7985 - val_loss: 0.4565 - val_acc: 0.8519\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4561 - acc: 0.8257 - val_loss: 0.4149 - val_acc: 0.8677\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.4231 - acc: 0.8434 - val_loss: 0.3829 - val_acc: 0.8730\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.3970 - acc: 0.8587 - val_loss: 0.3501 - val_acc: 0.8889\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.3743 - acc: 0.8712 - val_loss: 0.3306 - val_acc: 0.9048\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.3560 - acc: 0.8788 - val_loss: 0.3091 - val_acc: 0.9101\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.3405 - acc: 0.8848 - val_loss: 0.2957 - val_acc: 0.8995\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.3272 - acc: 0.8883 - val_loss: 0.2829 - val_acc: 0.8995\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.3142 - acc: 0.8877 - val_loss: 0.2668 - val_acc: 0.9048\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3013 - acc: 0.8983 - val_loss: 0.2569 - val_acc: 0.9048\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2891 - acc: 0.9031 - val_loss: 0.2462 - val_acc: 0.9153\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2796 - acc: 0.9048 - val_loss: 0.2378 - val_acc: 0.9206\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2698 - acc: 0.9108 - val_loss: 0.2282 - val_acc: 0.9259\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2595 - acc: 0.9155 - val_loss: 0.2352 - val_acc: 0.9471\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2522 - acc: 0.9161 - val_loss: 0.2095 - val_acc: 0.9365\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2415 - acc: 0.9173 - val_loss: 0.2056 - val_acc: 0.9471\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2323 - acc: 0.9243 - val_loss: 0.1929 - val_acc: 0.9471\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2235 - acc: 0.9273 - val_loss: 0.1843 - val_acc: 0.9365\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2145 - acc: 0.9320 - val_loss: 0.1761 - val_acc: 0.9471\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2069 - acc: 0.9356 - val_loss: 0.1721 - val_acc: 0.9683\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1977 - acc: 0.9368 - val_loss: 0.1700 - val_acc: 0.9683\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1899 - acc: 0.9379 - val_loss: 0.1538 - val_acc: 0.9577\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1836 - acc: 0.9450 - val_loss: 0.1488 - val_acc: 0.9577\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1765 - acc: 0.9439 - val_loss: 0.1505 - val_acc: 0.9524\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1713 - acc: 0.9492 - val_loss: 0.1378 - val_acc: 0.9788\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1630 - acc: 0.9527 - val_loss: 0.1362 - val_acc: 0.9683\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1592 - acc: 0.9509 - val_loss: 0.1295 - val_acc: 0.9788\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1548 - acc: 0.9551 - val_loss: 0.1270 - val_acc: 0.9788\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1490 - acc: 0.9557 - val_loss: 0.1284 - val_acc: 0.9788\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1445 - acc: 0.9580 - val_loss: 0.1201 - val_acc: 0.9788\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1362 - acc: 0.9580 - val_loss: 0.1208 - val_acc: 0.9788\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1356 - acc: 0.9586 - val_loss: 0.1184 - val_acc: 0.9788\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1311 - acc: 0.9622 - val_loss: 0.1127 - val_acc: 0.9788\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1281 - acc: 0.9622 - val_loss: 0.1147 - val_acc: 0.9788\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1236 - acc: 0.9657 - val_loss: 0.1099 - val_acc: 0.9788\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1219 - acc: 0.9628 - val_loss: 0.1077 - val_acc: 0.9841\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1184 - acc: 0.9657 - val_loss: 0.1394 - val_acc: 0.9735\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1166 - acc: 0.9693 - val_loss: 0.1043 - val_acc: 0.9841\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1113 - acc: 0.9657 - val_loss: 0.1020 - val_acc: 0.9788\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1100 - acc: 0.9693 - val_loss: 0.0991 - val_acc: 0.9841\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1081 - acc: 0.9687 - val_loss: 0.0969 - val_acc: 0.9841\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1063 - acc: 0.9687 - val_loss: 0.0975 - val_acc: 0.9841\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1019 - acc: 0.9663 - val_loss: 0.1028 - val_acc: 0.9841\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.1011 - acc: 0.9693 - val_loss: 0.1020 - val_acc: 0.9841\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0985 - acc: 0.9710 - val_loss: 0.0978 - val_acc: 0.9841\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0975 - acc: 0.9734 - val_loss: 0.0930 - val_acc: 0.9841\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0933 - acc: 0.9734 - val_loss: 0.0950 - val_acc: 0.9841\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0941 - acc: 0.9728 - val_loss: 0.0902 - val_acc: 0.9841\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0908 - acc: 0.9752 - val_loss: 0.0858 - val_acc: 0.9841\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0888 - acc: 0.9740 - val_loss: 0.0912 - val_acc: 0.9841\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.0874 - acc: 0.9746 - val_loss: 0.0865 - val_acc: 0.9841\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0864 - acc: 0.9758 - val_loss: 0.0852 - val_acc: 0.9841\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0859 - acc: 0.9740 - val_loss: 0.0850 - val_acc: 0.9841\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0847 - acc: 0.9775 - val_loss: 0.0839 - val_acc: 0.9841\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0836 - acc: 0.9752 - val_loss: 0.0816 - val_acc: 0.9841\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0800 - acc: 0.9758 - val_loss: 0.0846 - val_acc: 0.9841\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0799 - acc: 0.9793 - val_loss: 0.0813 - val_acc: 0.9841\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0782 - acc: 0.9781 - val_loss: 0.0801 - val_acc: 0.9841\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0783 - acc: 0.9770 - val_loss: 0.0802 - val_acc: 0.9841\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0775 - acc: 0.9775 - val_loss: 0.0856 - val_acc: 0.9894\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0742 - acc: 0.9817 - val_loss: 0.0730 - val_acc: 0.9841\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0742 - acc: 0.9764 - val_loss: 0.0787 - val_acc: 0.9841\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0729 - acc: 0.9805 - val_loss: 0.0742 - val_acc: 0.9841\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0716 - acc: 0.9799 - val_loss: 0.0751 - val_acc: 0.9841\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0698 - acc: 0.9781 - val_loss: 0.0734 - val_acc: 0.9841\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0687 - acc: 0.9817 - val_loss: 0.0714 - val_acc: 0.9894\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0682 - acc: 0.9805 - val_loss: 0.0770 - val_acc: 0.9894\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0654 - acc: 0.9817 - val_loss: 0.0753 - val_acc: 0.9841\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0677 - acc: 0.9793 - val_loss: 0.0695 - val_acc: 0.9841\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0656 - acc: 0.9811 - val_loss: 0.0708 - val_acc: 0.9894\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0657 - acc: 0.9823 - val_loss: 0.0717 - val_acc: 0.9894\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0641 - acc: 0.9829 - val_loss: 0.0665 - val_acc: 0.9894\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0644 - acc: 0.9829 - val_loss: 0.0699 - val_acc: 0.9841\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0609 - acc: 0.9840 - val_loss: 0.0681 - val_acc: 0.9894\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0609 - acc: 0.9829 - val_loss: 0.0881 - val_acc: 0.9894\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0611 - acc: 0.9835 - val_loss: 0.0665 - val_acc: 0.9894\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0591 - acc: 0.9846 - val_loss: 0.0642 - val_acc: 0.9894\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0597 - acc: 0.9846 - val_loss: 0.0657 - val_acc: 0.9894\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0598 - acc: 0.9840 - val_loss: 0.0676 - val_acc: 0.9894\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0586 - acc: 0.9840 - val_loss: 0.0645 - val_acc: 0.9894\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0573 - acc: 0.9858 - val_loss: 0.0646 - val_acc: 0.9894\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0560 - acc: 0.9858 - val_loss: 0.0620 - val_acc: 0.9894\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0548 - acc: 0.9840 - val_loss: 0.0618 - val_acc: 0.9894\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0566 - acc: 0.9829 - val_loss: 0.0633 - val_acc: 0.9894\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0510 - acc: 0.9876 - val_loss: 0.0621 - val_acc: 0.9894\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0622 - val_acc: 0.9894\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0515 - acc: 0.9846 - val_loss: 0.0775 - val_acc: 0.9947\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0525 - acc: 0.9852 - val_loss: 0.0640 - val_acc: 0.9894\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0521 - acc: 0.9852 - val_loss: 0.0605 - val_acc: 0.9894\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0519 - acc: 0.9858 - val_loss: 0.0613 - val_acc: 0.9894\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0495 - acc: 0.9858 - val_loss: 0.0610 - val_acc: 0.9947\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0492 - acc: 0.9870 - val_loss: 0.0652 - val_acc: 0.9894\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0487 - acc: 0.9852 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0489 - acc: 0.9870 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0456 - acc: 0.9864 - val_loss: 0.0603 - val_acc: 0.9947\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0478 - acc: 0.9870 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0483 - acc: 0.9870 - val_loss: 0.0585 - val_acc: 0.9947\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0480 - acc: 0.9864 - val_loss: 0.0609 - val_acc: 0.9947\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0474 - acc: 0.9870 - val_loss: 0.0565 - val_acc: 0.9947\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0453 - acc: 0.9870 - val_loss: 0.0584 - val_acc: 0.9947\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0455 - acc: 0.9882 - val_loss: 0.0620 - val_acc: 0.9947\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0451 - acc: 0.9864 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0448 - acc: 0.9876 - val_loss: 0.0565 - val_acc: 0.9947\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0424 - acc: 0.9876 - val_loss: 0.0557 - val_acc: 0.9947\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0431 - acc: 0.9864 - val_loss: 0.0591 - val_acc: 0.9947\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0409 - acc: 0.9882 - val_loss: 0.0601 - val_acc: 0.9894\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0422 - acc: 0.9888 - val_loss: 0.0631 - val_acc: 0.9894\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0422 - acc: 0.9876 - val_loss: 0.0584 - val_acc: 0.9947\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0404 - acc: 0.9882 - val_loss: 0.0590 - val_acc: 0.9947\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.9894 - val_loss: 0.0540 - val_acc: 0.9947\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0412 - acc: 0.9894 - val_loss: 0.0544 - val_acc: 0.9947\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0399 - acc: 0.9900 - val_loss: 0.0573 - val_acc: 0.9947\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0388 - acc: 0.9894 - val_loss: 0.0560 - val_acc: 0.9947\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0374 - acc: 0.9894 - val_loss: 0.0555 - val_acc: 0.9947\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0361 - acc: 0.9911 - val_loss: 0.0532 - val_acc: 0.9947\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0368 - acc: 0.9882 - val_loss: 0.0636 - val_acc: 0.9947\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0357 - acc: 0.9905 - val_loss: 0.0530 - val_acc: 0.9947\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0370 - acc: 0.9888 - val_loss: 0.0586 - val_acc: 0.9947\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0351 - acc: 0.9929 - val_loss: 0.0556 - val_acc: 0.9947\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0353 - acc: 0.9911 - val_loss: 0.0706 - val_acc: 0.9894\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0357 - acc: 0.9905 - val_loss: 0.0556 - val_acc: 0.9947\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0333 - acc: 0.9900 - val_loss: 0.0585 - val_acc: 0.9947\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0352 - acc: 0.9911 - val_loss: 0.0539 - val_acc: 0.9947\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0346 - acc: 0.9923 - val_loss: 0.0535 - val_acc: 0.9947\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0319 - acc: 0.9911 - val_loss: 0.0603 - val_acc: 0.9894\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0328 - acc: 0.9911 - val_loss: 0.0560 - val_acc: 0.9894\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0328 - acc: 0.9935 - val_loss: 0.0545 - val_acc: 0.9947\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0332 - acc: 0.9935 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0318 - acc: 0.9929 - val_loss: 0.0561 - val_acc: 0.9894\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0323 - acc: 0.9923 - val_loss: 0.0540 - val_acc: 0.9947\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0313 - acc: 0.9929 - val_loss: 0.0540 - val_acc: 0.9947\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0287 - acc: 0.9929 - val_loss: 0.0538 - val_acc: 0.9947\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0299 - acc: 0.9923 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0290 - acc: 0.9923 - val_loss: 0.0588 - val_acc: 0.9894\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0300 - acc: 0.9935 - val_loss: 0.0996 - val_acc: 0.9894\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0584 - val_acc: 0.9894\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.0563 - val_acc: 0.9947\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.0568 - val_acc: 0.9894\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0293 - acc: 0.9929 - val_loss: 0.0575 - val_acc: 0.9894\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0550 - val_acc: 0.9947\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0277 - acc: 0.9929 - val_loss: 0.0648 - val_acc: 0.9894\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0557 - val_acc: 0.9894\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0600 - val_acc: 0.9894\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0277 - acc: 0.9929 - val_loss: 0.0527 - val_acc: 0.9947\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0281 - acc: 0.9935 - val_loss: 0.0557 - val_acc: 0.9947\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0575 - val_acc: 0.9947\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0256 - acc: 0.9929 - val_loss: 0.0579 - val_acc: 0.9894\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0271 - acc: 0.9941 - val_loss: 0.0729 - val_acc: 0.9894\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0254 - acc: 0.9923 - val_loss: 0.0539 - val_acc: 0.9947\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0608 - val_acc: 0.9894\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0268 - acc: 0.9935 - val_loss: 0.0529 - val_acc: 0.9947\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0249 - acc: 0.9935 - val_loss: 0.0552 - val_acc: 0.9894\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0537 - val_acc: 0.9894\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0558 - val_acc: 0.9947\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0252 - acc: 0.9935 - val_loss: 0.0520 - val_acc: 0.9947\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0250 - acc: 0.9929 - val_loss: 0.0529 - val_acc: 0.9947\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0253 - acc: 0.9935 - val_loss: 0.0558 - val_acc: 0.9894\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0509 - val_acc: 0.9947\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0580 - val_acc: 0.9947\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0239 - acc: 0.9941 - val_loss: 0.0561 - val_acc: 0.9947\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0531 - val_acc: 0.9947\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0233 - acc: 0.9935 - val_loss: 0.0599 - val_acc: 0.9894\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 0.0561 - val_acc: 0.9894\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0225 - acc: 0.9935 - val_loss: 0.0576 - val_acc: 0.9894\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0246 - acc: 0.9923 - val_loss: 0.0531 - val_acc: 0.9947\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 0.0511 - val_acc: 0.9947\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0231 - acc: 0.9941 - val_loss: 0.0535 - val_acc: 0.9947\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0531 - val_acc: 0.9947\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0537 - val_acc: 0.9947\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0222 - acc: 0.9935 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0212 - acc: 0.9941 - val_loss: 0.0512 - val_acc: 0.9947\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0561 - val_acc: 0.9947\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0549 - val_acc: 0.9947\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0587 - val_acc: 0.9894\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0536 - val_acc: 0.9947\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.0574 - val_acc: 0.9947\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0224 - acc: 0.9935 - val_loss: 0.0553 - val_acc: 0.9947\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0544 - val_acc: 0.9947\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0553 - val_acc: 0.9947\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0513 - val_acc: 0.9947\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0552 - val_acc: 0.9947\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0213 - acc: 0.9935 - val_loss: 0.0527 - val_acc: 0.9947\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0681 - val_acc: 0.9894\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0700 - val_acc: 0.9947\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.0500 - val_acc: 0.9947\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0576 - val_acc: 0.9947\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0569 - val_acc: 0.9894\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0612 - val_acc: 0.9894\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0570 - val_acc: 0.9947\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.0610 - val_acc: 0.9894\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9941 - val_loss: 0.0601 - val_acc: 0.9894\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0573 - val_acc: 0.9947\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0198 - acc: 0.9935 - val_loss: 0.0601 - val_acc: 0.9947\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0199 - acc: 0.9965 - val_loss: 0.0570 - val_acc: 0.9947\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0589 - val_acc: 0.9947\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0532 - val_acc: 0.9947\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0584 - val_acc: 0.9894\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0185 - acc: 0.9953 - val_loss: 0.0562 - val_acc: 0.9947\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0586 - val_acc: 0.9947\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0527 - val_acc: 0.9947\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0518 - val_acc: 0.9947\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0568 - val_acc: 0.9947\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0685 - val_acc: 0.9947\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0578 - val_acc: 0.9947\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0555 - val_acc: 0.9947\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0560 - val_acc: 0.9947\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0521 - val_acc: 0.9947\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0539 - val_acc: 0.9947\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0550 - val_acc: 0.9947\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0569 - val_acc: 0.9947\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0526 - val_acc: 0.9947\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0537 - val_acc: 0.9947\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0590 - val_acc: 0.9947\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0569 - val_acc: 0.9947\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0181 - acc: 0.9947 - val_loss: 0.0495 - val_acc: 0.9947\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0568 - val_acc: 0.9947\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0521 - val_acc: 0.9947\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0561 - val_acc: 0.9947\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0496 - val_acc: 0.9947\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0600 - val_acc: 0.9947\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0533 - val_acc: 0.9947\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0581 - val_acc: 0.9947\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0162 - acc: 0.9970 - val_loss: 0.0630 - val_acc: 0.9947\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 0.0574 - val_acc: 0.9947\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0610 - val_acc: 0.9947\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0558 - val_acc: 0.9947\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0161 - acc: 0.9953 - val_loss: 0.0577 - val_acc: 0.9947\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0164 - acc: 0.9953 - val_loss: 0.0548 - val_acc: 0.9947\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 0.0573 - val_acc: 0.9947\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0584 - val_acc: 0.9947\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0632 - val_acc: 0.9947\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0564 - val_acc: 0.9947\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0148 - acc: 0.9970 - val_loss: 0.0547 - val_acc: 0.9947\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0588 - val_acc: 0.9947\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0579 - val_acc: 0.9947\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0539 - val_acc: 0.9947\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0627 - val_acc: 0.9947\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0595 - val_acc: 0.9947\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0156 - acc: 0.9970 - val_loss: 0.0601 - val_acc: 0.9947\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0590 - val_acc: 0.9947\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0139 - acc: 0.9959 - val_loss: 0.0552 - val_acc: 0.9947\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0148 - acc: 0.9965 - val_loss: 0.0551 - val_acc: 0.9947\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0596 - val_acc: 0.9947\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0532 - val_acc: 0.9947\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0609 - val_acc: 0.9947\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0575 - val_acc: 0.9947\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0601 - val_acc: 0.9947\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0575 - val_acc: 0.9947\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0570 - val_acc: 0.9947\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0142 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9947\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0562 - val_acc: 0.9947\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0630 - val_acc: 0.9947\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0566 - val_acc: 0.9947\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0546 - val_acc: 0.9947\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0525 - val_acc: 0.9947\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0627 - val_acc: 0.9947\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0608 - val_acc: 0.9947\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0608 - val_acc: 0.9947\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0145 - acc: 0.9959 - val_loss: 0.0602 - val_acc: 0.9947\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0607 - val_acc: 0.9947\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0142 - acc: 0.9965 - val_loss: 0.0571 - val_acc: 0.9947\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0537 - val_acc: 0.9947\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0594 - val_acc: 0.9947\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0642 - val_acc: 0.9947\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0616 - val_acc: 0.9947\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9976 - val_loss: 0.0579 - val_acc: 0.9947\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0678 - val_acc: 0.9947\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0142 - acc: 0.9947 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0648 - val_acc: 0.9947\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0136 - acc: 0.9976 - val_loss: 0.0610 - val_acc: 0.9947\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0658 - val_acc: 0.9947\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0141 - acc: 0.9965 - val_loss: 0.0573 - val_acc: 0.9947\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0615 - val_acc: 0.9947\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0649 - val_acc: 0.9947\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 0.0701 - val_acc: 0.9947\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0618 - val_acc: 0.9947\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0639 - val_acc: 0.9947\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0578 - val_acc: 0.9947\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0584 - val_acc: 0.9947\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0572 - val_acc: 0.9947\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0148 - acc: 0.9970 - val_loss: 0.0580 - val_acc: 0.9947\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0138 - acc: 0.9976 - val_loss: 0.0658 - val_acc: 0.9947\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0140 - acc: 0.9970 - val_loss: 0.0644 - val_acc: 0.9947\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0143 - acc: 0.9976 - val_loss: 0.0649 - val_acc: 0.9947\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0142 - acc: 0.9965 - val_loss: 0.0594 - val_acc: 0.9947\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0127 - acc: 0.9976 - val_loss: 0.0714 - val_acc: 0.9947\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0591 - val_acc: 0.9947\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0714 - val_acc: 0.9947\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0165 - acc: 0.9941 - val_loss: 0.0615 - val_acc: 0.9947\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0598 - val_acc: 0.9947\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0136 - acc: 0.9976 - val_loss: 0.0679 - val_acc: 0.9947\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.0590 - val_acc: 0.9947\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0585 - val_acc: 0.9947\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0593 - val_acc: 0.9947\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0584 - val_acc: 0.9947\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0583 - val_acc: 0.9947\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0141 - acc: 0.9976 - val_loss: 0.0595 - val_acc: 0.9947\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0609 - val_acc: 0.9947\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0623 - val_acc: 0.9947\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0579 - val_acc: 0.9947\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0128 - acc: 0.9976 - val_loss: 0.0599 - val_acc: 0.9947\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.0670 - val_acc: 0.9947\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0563 - val_acc: 0.9947\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0582 - val_acc: 0.9947\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0139 - acc: 0.9976 - val_loss: 0.0586 - val_acc: 0.9947\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0628 - val_acc: 0.9947\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0574 - val_acc: 0.9947\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.0557 - val_acc: 0.9947\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0132 - acc: 0.9970 - val_loss: 0.0555 - val_acc: 0.9947\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0627 - val_acc: 0.9947\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0130 - acc: 0.9976 - val_loss: 0.0551 - val_acc: 0.9947\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.0578 - val_acc: 0.9947\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0579 - val_acc: 0.9947\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0145 - acc: 0.9976 - val_loss: 0.0641 - val_acc: 0.9947\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0131 - acc: 0.9976 - val_loss: 0.0671 - val_acc: 0.9947\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0542 - val_acc: 0.9947\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0630 - val_acc: 0.9947\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0132 - acc: 0.9976 - val_loss: 0.0667 - val_acc: 0.9947\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0708 - val_acc: 0.9947\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0149 - acc: 0.9970 - val_loss: 0.0596 - val_acc: 0.9947\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0607 - val_acc: 0.9947\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0671 - val_acc: 0.9947\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.0618 - val_acc: 0.9947\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.0687 - val_acc: 0.9947\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.0611 - val_acc: 0.9947\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0700 - val_acc: 0.9841\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0143 - acc: 0.9959 - val_loss: 0.0690 - val_acc: 0.9947\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0614 - val_acc: 0.9947\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0620 - val_acc: 0.9947\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0679 - val_acc: 0.9947\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0679 - val_acc: 0.9947\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0139 - acc: 0.9976 - val_loss: 0.0707 - val_acc: 0.9947\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0640 - val_acc: 0.9947\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.0610 - val_acc: 0.9947\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0665 - val_acc: 0.9947\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0691 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0617 - val_acc: 0.9947\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0639 - val_acc: 0.9947\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0707 - val_acc: 0.9947\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.0606 - val_acc: 0.9947\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0132 - acc: 0.9970 - val_loss: 0.0659 - val_acc: 0.9947\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.0563 - val_acc: 0.9947\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0699 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0618 - val_acc: 0.9947\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0131 - acc: 0.9976 - val_loss: 0.0593 - val_acc: 0.9947\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0697 - val_acc: 0.9947\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0650 - val_acc: 0.9947\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.0685 - val_acc: 0.9947\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0662 - val_acc: 0.9947\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0677 - val_acc: 0.9947\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.0726 - val_acc: 0.9947\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0130 - acc: 0.9976 - val_loss: 0.0698 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0589 - val_acc: 0.9947\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0127 - acc: 0.9976 - val_loss: 0.0752 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0716 - val_acc: 0.9947\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.1443 - val_acc: 0.9630\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0737 - val_acc: 0.9947\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0658 - val_acc: 0.9947\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0664 - val_acc: 0.9947\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0620 - val_acc: 0.9947\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0132 - acc: 0.9970 - val_loss: 0.0721 - val_acc: 0.9947\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0694 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 0.0731 - val_acc: 0.9947\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0683 - val_acc: 0.9947\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0623 - val_acc: 0.9947\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0619 - val_acc: 0.9947\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0138 - acc: 0.9976 - val_loss: 0.0737 - val_acc: 0.9947\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0763 - val_acc: 0.9947\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0711 - val_acc: 0.9947\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0685 - val_acc: 0.9947\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0132 - acc: 0.9970 - val_loss: 0.0670 - val_acc: 0.9947\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0125 - acc: 0.9970 - val_loss: 0.0698 - val_acc: 0.9947\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0734 - val_acc: 0.9947\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0782 - val_acc: 0.9841\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0135 - acc: 0.9976 - val_loss: 0.0734 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0683 - val_acc: 0.9947\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0136 - acc: 0.9965 - val_loss: 0.0696 - val_acc: 0.9947\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0704 - val_acc: 0.9947\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0644 - val_acc: 0.9947\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0748 - val_acc: 0.9947\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0140 - acc: 0.9976 - val_loss: 0.0697 - val_acc: 0.9947\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0131 - acc: 0.9976 - val_loss: 0.0717 - val_acc: 0.9947\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0665 - val_acc: 0.9947\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0735 - val_acc: 0.9947\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.0680 - val_acc: 0.9947\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0742 - val_acc: 0.9947\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0717 - val_acc: 0.9947\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0630 - val_acc: 0.9947\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0751 - val_acc: 0.9947\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0142 - acc: 0.9976 - val_loss: 0.0648 - val_acc: 0.9947\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0630 - val_acc: 0.9947\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0693 - val_acc: 0.9947\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0663 - val_acc: 0.9947\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 0.0646 - val_acc: 0.9947\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0693 - val_acc: 0.9947\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0638 - val_acc: 0.9947\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0644 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0761 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0657 - val_acc: 0.9947\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0629 - val_acc: 0.9947\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0640 - val_acc: 0.9947\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0688 - val_acc: 0.9947\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0750 - val_acc: 0.9947\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0708 - val_acc: 0.9947\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0688 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0700 - val_acc: 0.9947\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9982 - val_loss: 0.0599 - val_acc: 0.9947\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0678 - val_acc: 0.9947\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0649 - val_acc: 0.9947\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0679 - val_acc: 0.9947\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0707 - val_acc: 0.9947\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0693 - val_acc: 0.9947\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0748 - val_acc: 0.9947\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0681 - val_acc: 0.9947\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0132 - acc: 0.9970 - val_loss: 0.0701 - val_acc: 0.9947\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0650 - val_acc: 0.9947\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0682 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0729 - val_acc: 0.9947\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0784 - val_acc: 0.9947\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0128 - acc: 0.9976 - val_loss: 0.0709 - val_acc: 0.9947\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0724 - val_acc: 0.9947\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0717 - val_acc: 0.9947\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0725 - val_acc: 0.9947\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0686 - val_acc: 0.9947\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0678 - val_acc: 0.9894\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0751 - val_acc: 0.9947\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.0639 - val_acc: 0.9947\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0762 - val_acc: 0.9947\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0714 - val_acc: 0.9947\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0734 - val_acc: 0.9947\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0721 - val_acc: 0.9947\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0724 - val_acc: 0.9947\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0100 - acc: 0.9982 - val_loss: 0.0653 - val_acc: 0.9894\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0747 - val_acc: 0.9947\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0781 - val_acc: 0.9947\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0796 - val_acc: 0.9947\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0792 - val_acc: 0.9947\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0709 - val_acc: 0.9947\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0777 - val_acc: 0.9947\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0745 - val_acc: 0.9947\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0727 - val_acc: 0.9947\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0688 - val_acc: 0.9947\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0134 - acc: 0.9959 - val_loss: 0.0778 - val_acc: 0.9947\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0728 - val_acc: 0.9947\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0121 - acc: 0.9976 - val_loss: 0.0750 - val_acc: 0.9947\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0768 - val_acc: 0.9947\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0785 - val_acc: 0.9947\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0797 - val_acc: 0.9947\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0757 - val_acc: 0.9947\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0678 - val_acc: 0.9947\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0707 - val_acc: 0.9947\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0727 - val_acc: 0.9947\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0730 - val_acc: 0.9894\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0722 - val_acc: 0.9947\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0718 - val_acc: 0.9894\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0835 - val_acc: 0.9947\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0128 - acc: 0.9976 - val_loss: 0.0748 - val_acc: 0.9894\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0777 - val_acc: 0.9947\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0738 - val_acc: 0.9894\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0803 - val_acc: 0.9947\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0822 - val_acc: 0.9947\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0720 - val_acc: 0.9947\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0820 - val_acc: 0.9947\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0763 - val_acc: 0.9947\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0816 - val_acc: 0.9947\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0721 - val_acc: 0.9947\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0729 - val_acc: 0.9947\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0773 - val_acc: 0.9947\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0121 - acc: 0.9976 - val_loss: 0.0710 - val_acc: 0.9947\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0700 - val_acc: 0.9947\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0751 - val_acc: 0.9947\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0785 - val_acc: 0.9947\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0799 - val_acc: 0.9947\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0804 - val_acc: 0.9947\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0747 - val_acc: 0.9947\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0794 - val_acc: 0.9947\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0796 - val_acc: 0.9947\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0743 - val_acc: 0.9947\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0746 - val_acc: 0.9947\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0794 - val_acc: 0.9947\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0768 - val_acc: 0.9947\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0836 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0795 - val_acc: 0.9947\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0765 - val_acc: 0.9947\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0780 - val_acc: 0.9947\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0841 - val_acc: 0.9947\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0773 - val_acc: 0.9947\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0777 - val_acc: 0.9947\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0823 - val_acc: 0.9894\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0709 - val_acc: 0.9894\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.0749 - val_acc: 0.9947\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0779 - val_acc: 0.9947\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0788 - val_acc: 0.9947\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0815 - val_acc: 0.9947\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0754 - val_acc: 0.9947\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0817 - val_acc: 0.9947\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0811 - val_acc: 0.9947\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0794 - val_acc: 0.9947\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0110 - acc: 0.9982 - val_loss: 0.0829 - val_acc: 0.9947\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0851 - val_acc: 0.9947\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0811 - val_acc: 0.9947\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0791 - val_acc: 0.9947\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0787 - val_acc: 0.9947\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0819 - val_acc: 0.9947\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0785 - val_acc: 0.9947\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0852 - val_acc: 0.9947\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0836 - val_acc: 0.9947\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0806 - val_acc: 0.9947\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0741 - val_acc: 0.9947\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0748 - val_acc: 0.9947\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0809 - val_acc: 0.9947\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0781 - val_acc: 0.9947\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.0836 - val_acc: 0.9947\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0837 - val_acc: 0.9947\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0746 - val_acc: 0.9947\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0773 - val_acc: 0.9947\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0825 - val_acc: 0.9947\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0829 - val_acc: 0.9947\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0852 - val_acc: 0.9947\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0849 - val_acc: 0.9947\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0819 - val_acc: 0.9947\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0828 - val_acc: 0.9947\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0799 - val_acc: 0.9947\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0836 - val_acc: 0.9894\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0825 - val_acc: 0.9947\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.0828 - val_acc: 0.9947\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0809 - val_acc: 0.9947\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0808 - val_acc: 0.9947\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0763 - val_acc: 0.9947\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0843 - val_acc: 0.9947\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0847 - val_acc: 0.9947\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0841 - val_acc: 0.9947\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0842 - val_acc: 0.9947\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0978 - val_acc: 0.9841\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0798 - val_acc: 0.9947\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0823 - val_acc: 0.9947\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0847 - val_acc: 0.9947\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0817 - val_acc: 0.9947\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 102us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0845 - val_acc: 0.9947\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0833 - val_acc: 0.9947\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0850 - val_acc: 0.9947\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0780 - val_acc: 0.9947\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0837 - val_acc: 0.9947\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0856 - val_acc: 0.9947\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0810 - val_acc: 0.9947\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0850 - val_acc: 0.9947\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0129 - acc: 0.9965 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.0815 - val_acc: 0.9947\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0828 - val_acc: 0.9947\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0845 - val_acc: 0.9947\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0834 - val_acc: 0.9947\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0839 - val_acc: 0.9947\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0936 - val_acc: 0.9947\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0101 - acc: 0.9982 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0099 - acc: 0.9982 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0911 - val_acc: 0.9947\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0973 - val_acc: 0.9894\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0121 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0938 - val_acc: 0.9894\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0106 - acc: 0.9959 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0898 - val_acc: 0.9894\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0910 - val_acc: 0.9894\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0895 - val_acc: 0.9947\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.0909 - val_acc: 0.9947\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0892 - val_acc: 0.9947\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0893 - val_acc: 0.9947\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0931 - val_acc: 0.9947\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0913 - val_acc: 0.9947\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0901 - val_acc: 0.9947\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0892 - val_acc: 0.9947\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9959 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0909 - val_acc: 0.9947\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0893 - val_acc: 0.9947\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0906 - val_acc: 0.9947\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0898 - val_acc: 0.9947\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0885 - val_acc: 0.9947\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0113 - acc: 0.9959 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0105 - acc: 0.9953 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0902 - val_acc: 0.9947\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0909 - val_acc: 0.9947\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0092 - acc: 0.9959 - val_loss: 0.0949 - val_acc: 0.9894\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0938 - val_acc: 0.9894\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0905 - val_acc: 0.9947\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0929 - val_acc: 0.9947\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0886 - val_acc: 0.9947\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0105 - acc: 0.9959 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0094 - acc: 0.9982 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0123 - acc: 0.9965 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0120 - acc: 0.9959 - val_loss: 0.0911 - val_acc: 0.9947\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0977 - val_acc: 0.9894\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1028 - val_acc: 0.9894\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0910 - val_acc: 0.9947\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0917 - val_acc: 0.9894\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9953 - val_loss: 0.0899 - val_acc: 0.9947\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0953 - val_acc: 0.9894\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1761 - val_acc: 0.9471\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0886 - val_acc: 0.9947\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0080 - acc: 0.9965 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0110 - acc: 0.9959 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0914 - val_acc: 0.9947\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0937 - val_acc: 0.9894\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0981 - val_acc: 0.9894\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0936 - val_acc: 0.9894\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0099 - acc: 0.9959 - val_loss: 0.0936 - val_acc: 0.9894\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0107 - acc: 0.9982 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0955 - val_acc: 0.9894\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0938 - val_acc: 0.9894\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0912 - val_acc: 0.9894\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0903 - val_acc: 0.9947\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0951 - val_acc: 0.9894\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0898 - val_acc: 0.9947\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.1120 - val_acc: 0.9841\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0086 - acc: 0.9988 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.1332 - val_acc: 0.9735\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0958 - val_acc: 0.9894\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0096 - acc: 0.9959 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0886 - val_acc: 0.9947\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.1039 - val_acc: 0.9894\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0093 - acc: 0.9965 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0884 - val_acc: 0.9947\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0090 - acc: 0.9965 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0082 - acc: 0.9965 - val_loss: 0.0926 - val_acc: 0.9894\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1047 - val_acc: 0.9841\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0932 - val_acc: 0.9894\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.1022 - val_acc: 0.9788\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0894 - val_acc: 0.9894\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0093 - acc: 0.9959 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.0899 - val_acc: 0.9894\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1023 - val_acc: 0.9894\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0931 - val_acc: 0.9894\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0884 - val_acc: 0.9947\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0099 - acc: 0.9959 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0958 - val_acc: 0.9894\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0946 - val_acc: 0.9894\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.1129 - val_acc: 0.9841\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0104 - acc: 0.9959 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1049 - val_acc: 0.9841\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0889 - val_acc: 0.9947\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0884 - val_acc: 0.9947\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0897 - val_acc: 0.9947\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0898 - val_acc: 0.9947\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0938 - val_acc: 0.9894\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0912 - val_acc: 0.9894\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0914 - val_acc: 0.9947\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0075 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0902 - val_acc: 0.9947\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0906 - val_acc: 0.9894\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0886 - val_acc: 0.9947\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0971 - val_acc: 0.9894\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.1007 - val_acc: 0.9841\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0902 - val_acc: 0.9947\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9965 - val_loss: 0.0904 - val_acc: 0.9947\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0092 - acc: 0.9953 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0108 - acc: 0.9953 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0890 - val_acc: 0.9947\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0891 - val_acc: 0.9947\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0940 - val_acc: 0.9894\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0095 - acc: 0.9953 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.1654 - val_acc: 0.9524\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0102 - acc: 0.9953 - val_loss: 0.0889 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0908 - val_acc: 0.9947\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0889 - val_acc: 0.9947\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.1054 - val_acc: 0.9894\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0073 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0889 - val_acc: 0.9947\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.0930 - val_acc: 0.9894\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0911 - val_acc: 0.9894\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0904 - val_acc: 0.9947\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.1185 - val_acc: 0.9735\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0898 - val_acc: 0.9947\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0110 - acc: 0.9959 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.1079 - val_acc: 0.9841\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0961 - val_acc: 0.9841\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0981 - val_acc: 0.9894\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0885 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0916 - val_acc: 0.9894\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.0906 - val_acc: 0.9947\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0958 - val_acc: 0.9894\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0088 - acc: 0.9959 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0913 - val_acc: 0.9947\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0884 - val_acc: 0.9947\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0089 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0891 - val_acc: 0.9947\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0938 - val_acc: 0.9894\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0082 - acc: 0.9965 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0877 - val_acc: 0.9947\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0114 - acc: 0.9959 - val_loss: 0.0876 - val_acc: 0.9947\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0896 - val_acc: 0.9947\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0937 - val_acc: 0.9894\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0924 - val_acc: 0.9894\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.1385 - val_acc: 0.9683\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0101 - acc: 0.9953 - val_loss: 0.0980 - val_acc: 0.9894\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0906 - val_acc: 0.9894\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0885 - val_acc: 0.9947\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0103 - acc: 0.9959 - val_loss: 0.1413 - val_acc: 0.9683\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0072 - acc: 0.9965 - val_loss: 0.0884 - val_acc: 0.9947\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0056 - acc: 0.9976 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0897 - val_acc: 0.9947\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947   \n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0905 - val_acc: 0.9947\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0977 - val_acc: 0.9894\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0950 - val_acc: 0.9894\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0902 - val_acc: 0.9894\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0915 - val_acc: 0.9894\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0089 - acc: 0.9965 - val_loss: 0.0998 - val_acc: 0.9894\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0990 - val_acc: 0.9894\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.1022 - val_acc: 0.9894\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.1112 - val_acc: 0.9841\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.0941 - val_acc: 0.9947\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0112 - acc: 0.9959 - val_loss: 0.0922 - val_acc: 0.9894\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1057 - val_acc: 0.9894\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0885 - val_acc: 0.9947\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0914 - val_acc: 0.9894\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0886 - val_acc: 0.9947\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0937 - val_acc: 0.9894\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0072 - acc: 0.9965 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0103 - acc: 0.9959 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0088 - acc: 0.9959 - val_loss: 0.0871 - val_acc: 0.9947\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 0s 271us/step - loss: 0.6691 - acc: 0.6152 - val_loss: 0.6539 - val_acc: 0.6032\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.6438 - acc: 0.6259 - val_loss: 0.6397 - val_acc: 0.6032\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.6308 - acc: 0.6294 - val_loss: 0.6219 - val_acc: 0.6243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.6146 - acc: 0.6602 - val_loss: 0.5982 - val_acc: 0.7302\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.5942 - acc: 0.7240 - val_loss: 0.5706 - val_acc: 0.7989\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.5697 - acc: 0.7636 - val_loss: 0.5373 - val_acc: 0.8148\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5425 - acc: 0.7902 - val_loss: 0.5067 - val_acc: 0.8413\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5144 - acc: 0.8132 - val_loss: 0.4729 - val_acc: 0.8360\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4866 - acc: 0.8257 - val_loss: 0.4444 - val_acc: 0.8360\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.4623 - acc: 0.8304 - val_loss: 0.4195 - val_acc: 0.8413\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.4401 - acc: 0.8416 - val_loss: 0.3969 - val_acc: 0.8624\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4199 - acc: 0.8499 - val_loss: 0.3768 - val_acc: 0.8783\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4034 - acc: 0.8582 - val_loss: 0.3600 - val_acc: 0.8836\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3881 - acc: 0.8582 - val_loss: 0.3448 - val_acc: 0.8889\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3750 - acc: 0.8664 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3630 - acc: 0.8717 - val_loss: 0.3215 - val_acc: 0.8889\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3515 - acc: 0.8753 - val_loss: 0.3103 - val_acc: 0.8889\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3434 - acc: 0.8783 - val_loss: 0.3040 - val_acc: 0.8889\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3354 - acc: 0.8812 - val_loss: 0.2953 - val_acc: 0.8942\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3279 - acc: 0.8877 - val_loss: 0.2880 - val_acc: 0.9048\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3211 - acc: 0.8918 - val_loss: 0.2833 - val_acc: 0.8995\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3150 - acc: 0.8918 - val_loss: 0.2827 - val_acc: 0.9101\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3092 - acc: 0.8972 - val_loss: 0.2747 - val_acc: 0.9048\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3030 - acc: 0.8972 - val_loss: 0.2686 - val_acc: 0.9048\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2979 - acc: 0.8989 - val_loss: 0.2642 - val_acc: 0.9101\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2930 - acc: 0.8989 - val_loss: 0.2621 - val_acc: 0.9101\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2874 - acc: 0.9037 - val_loss: 0.2554 - val_acc: 0.9101\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2830 - acc: 0.9025 - val_loss: 0.2508 - val_acc: 0.9101\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2778 - acc: 0.9048 - val_loss: 0.2475 - val_acc: 0.9153\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2731 - acc: 0.9066 - val_loss: 0.2448 - val_acc: 0.9259\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2670 - acc: 0.9102 - val_loss: 0.2367 - val_acc: 0.9153\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2616 - acc: 0.9137 - val_loss: 0.2323 - val_acc: 0.9153\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2571 - acc: 0.9155 - val_loss: 0.2265 - val_acc: 0.9206\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2504 - acc: 0.9190 - val_loss: 0.2202 - val_acc: 0.9259\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2457 - acc: 0.9208 - val_loss: 0.2153 - val_acc: 0.9312\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2405 - acc: 0.9232 - val_loss: 0.2101 - val_acc: 0.9259\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2346 - acc: 0.9243 - val_loss: 0.2127 - val_acc: 0.9365\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2298 - acc: 0.9273 - val_loss: 0.1977 - val_acc: 0.9365\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2228 - acc: 0.9314 - val_loss: 0.1920 - val_acc: 0.9418\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2188 - acc: 0.9303 - val_loss: 0.1860 - val_acc: 0.9418\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2124 - acc: 0.9350 - val_loss: 0.1790 - val_acc: 0.9418\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2083 - acc: 0.9350 - val_loss: 0.1742 - val_acc: 0.9524\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2022 - acc: 0.9385 - val_loss: 0.1661 - val_acc: 0.9471\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1966 - acc: 0.9421 - val_loss: 0.1646 - val_acc: 0.9630\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1924 - acc: 0.9444 - val_loss: 0.1547 - val_acc: 0.9524\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1882 - acc: 0.9439 - val_loss: 0.1459 - val_acc: 0.9471\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1820 - acc: 0.9450 - val_loss: 0.1402 - val_acc: 0.9630\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1788 - acc: 0.9468 - val_loss: 0.1398 - val_acc: 0.9735\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1737 - acc: 0.9486 - val_loss: 0.1301 - val_acc: 0.9735\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1694 - acc: 0.9498 - val_loss: 0.1240 - val_acc: 0.9788\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1648 - acc: 0.9533 - val_loss: 0.1207 - val_acc: 0.9735\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1605 - acc: 0.9539 - val_loss: 0.1142 - val_acc: 0.9788\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1555 - acc: 0.9551 - val_loss: 0.1109 - val_acc: 0.9683\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1524 - acc: 0.9545 - val_loss: 0.1235 - val_acc: 0.9788\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1486 - acc: 0.9574 - val_loss: 0.1062 - val_acc: 0.9788\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1456 - acc: 0.9580 - val_loss: 0.1066 - val_acc: 0.9788\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1414 - acc: 0.9586 - val_loss: 0.1001 - val_acc: 0.9788\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1376 - acc: 0.9598 - val_loss: 0.0976 - val_acc: 0.9788\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1340 - acc: 0.9598 - val_loss: 0.0989 - val_acc: 0.9788\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1310 - acc: 0.9628 - val_loss: 0.0932 - val_acc: 0.9788\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1279 - acc: 0.9622 - val_loss: 0.1000 - val_acc: 0.9788\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1244 - acc: 0.9657 - val_loss: 0.0900 - val_acc: 0.9788\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1214 - acc: 0.9651 - val_loss: 0.0914 - val_acc: 0.9788\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1189 - acc: 0.9675 - val_loss: 0.0851 - val_acc: 0.9788\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1154 - acc: 0.9657 - val_loss: 0.0825 - val_acc: 0.9788\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1132 - acc: 0.9675 - val_loss: 0.0843 - val_acc: 0.9788\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1095 - acc: 0.9699 - val_loss: 0.0820 - val_acc: 0.9788\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1073 - acc: 0.9722 - val_loss: 0.0872 - val_acc: 0.9841\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1045 - acc: 0.9734 - val_loss: 0.0797 - val_acc: 0.9841\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1018 - acc: 0.9728 - val_loss: 0.0778 - val_acc: 0.9841\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0998 - acc: 0.9734 - val_loss: 0.0730 - val_acc: 0.9841\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0975 - acc: 0.9758 - val_loss: 0.0720 - val_acc: 0.9841\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0957 - acc: 0.9781 - val_loss: 0.0701 - val_acc: 0.9841\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0932 - acc: 0.9775 - val_loss: 0.0684 - val_acc: 0.9841\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0919 - acc: 0.9781 - val_loss: 0.0673 - val_acc: 0.9841\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0889 - acc: 0.9787 - val_loss: 0.0709 - val_acc: 0.9841\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0879 - acc: 0.9787 - val_loss: 0.0640 - val_acc: 0.9841\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0860 - acc: 0.9787 - val_loss: 0.0615 - val_acc: 0.9841\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0834 - acc: 0.9811 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0824 - acc: 0.9793 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0804 - acc: 0.9811 - val_loss: 0.0585 - val_acc: 0.9841\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0794 - acc: 0.9829 - val_loss: 0.0543 - val_acc: 0.9841\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0768 - acc: 0.9817 - val_loss: 0.0545 - val_acc: 0.9841\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0765 - acc: 0.9817 - val_loss: 0.0533 - val_acc: 0.9841\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0749 - acc: 0.9840 - val_loss: 0.0508 - val_acc: 0.9841\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0726 - acc: 0.9835 - val_loss: 0.0748 - val_acc: 0.9947\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0724 - acc: 0.9858 - val_loss: 0.0477 - val_acc: 0.9841\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0706 - acc: 0.9846 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0692 - acc: 0.9846 - val_loss: 0.0511 - val_acc: 0.9841\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0687 - acc: 0.9840 - val_loss: 0.0469 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0670 - acc: 0.9852 - val_loss: 0.0468 - val_acc: 0.9947\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0663 - acc: 0.9846 - val_loss: 0.0514 - val_acc: 0.9947\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0661 - acc: 0.9852 - val_loss: 0.0443 - val_acc: 0.9841\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0651 - acc: 0.9858 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0637 - acc: 0.9870 - val_loss: 0.0432 - val_acc: 0.9841\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0633 - acc: 0.9852 - val_loss: 0.0444 - val_acc: 0.9841\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0621 - acc: 0.9870 - val_loss: 0.0427 - val_acc: 0.9841\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0617 - acc: 0.9864 - val_loss: 0.0414 - val_acc: 0.9841\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0607 - acc: 0.9864 - val_loss: 0.0409 - val_acc: 0.9894\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0597 - acc: 0.9870 - val_loss: 0.0439 - val_acc: 0.9841\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0598 - acc: 0.9870 - val_loss: 0.0429 - val_acc: 0.9841\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0583 - acc: 0.9870 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0580 - acc: 0.9864 - val_loss: 0.0396 - val_acc: 0.9841\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0575 - acc: 0.9876 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0563 - acc: 0.9870 - val_loss: 0.0401 - val_acc: 0.9894\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0550 - acc: 0.9882 - val_loss: 0.0440 - val_acc: 0.9841\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0561 - acc: 0.9882 - val_loss: 0.0381 - val_acc: 0.9894\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0540 - acc: 0.9888 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0538 - acc: 0.9894 - val_loss: 0.0376 - val_acc: 0.9894\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0533 - acc: 0.9888 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0529 - acc: 0.9894 - val_loss: 0.0393 - val_acc: 0.9894\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0536 - acc: 0.9888 - val_loss: 0.0402 - val_acc: 0.9841\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0518 - acc: 0.9894 - val_loss: 0.0352 - val_acc: 0.9894\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0518 - acc: 0.9894 - val_loss: 0.0365 - val_acc: 0.9894\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0507 - acc: 0.9882 - val_loss: 0.0369 - val_acc: 0.9947\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0503 - acc: 0.9894 - val_loss: 0.0352 - val_acc: 0.9894\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0493 - acc: 0.9894 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0499 - acc: 0.9894 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0489 - acc: 0.9900 - val_loss: 0.0381 - val_acc: 0.9841\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0489 - acc: 0.9894 - val_loss: 0.0365 - val_acc: 0.9894\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0480 - acc: 0.9894 - val_loss: 0.0361 - val_acc: 0.9841\n",
      "Epoch 122/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0476 - acc: 0.9900 - val_loss: 0.0385 - val_acc: 0.9841\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0468 - acc: 0.9888 - val_loss: 0.0323 - val_acc: 0.9894\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0469 - acc: 0.9894 - val_loss: 0.0390 - val_acc: 0.9841\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0478 - acc: 0.9888 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0454 - acc: 0.9900 - val_loss: 0.0378 - val_acc: 0.9841\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0464 - acc: 0.9894 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0448 - acc: 0.9900 - val_loss: 0.0353 - val_acc: 0.9841\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0448 - acc: 0.9894 - val_loss: 0.0361 - val_acc: 0.9841\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0443 - acc: 0.9894 - val_loss: 0.0342 - val_acc: 0.9841\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0438 - acc: 0.9900 - val_loss: 0.0334 - val_acc: 0.9841\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0445 - acc: 0.9900 - val_loss: 0.0357 - val_acc: 0.9841\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0349 - val_acc: 0.9841\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0427 - acc: 0.9905 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0428 - acc: 0.9905 - val_loss: 0.0333 - val_acc: 0.9841\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0413 - acc: 0.9905 - val_loss: 0.0322 - val_acc: 0.9894\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0415 - acc: 0.9911 - val_loss: 0.0347 - val_acc: 0.9841\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0413 - acc: 0.9911 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 0.9911 - val_loss: 0.0364 - val_acc: 0.9841\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0413 - acc: 0.9888 - val_loss: 0.0321 - val_acc: 0.9841\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0408 - acc: 0.9917 - val_loss: 0.0346 - val_acc: 0.9841\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0401 - acc: 0.9911 - val_loss: 0.0364 - val_acc: 0.9841\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.9911 - val_loss: 0.0353 - val_acc: 0.9841\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.0360 - val_acc: 0.9841\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0388 - acc: 0.9917 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0308 - val_acc: 0.9841\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0376 - acc: 0.9929 - val_loss: 0.0426 - val_acc: 0.9841\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0389 - acc: 0.9917 - val_loss: 0.0350 - val_acc: 0.9841\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0381 - acc: 0.9905 - val_loss: 0.0357 - val_acc: 0.9841\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0316 - val_acc: 0.9841\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0371 - acc: 0.9929 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0374 - acc: 0.9923 - val_loss: 0.0388 - val_acc: 0.9841\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9929 - val_loss: 0.0458 - val_acc: 0.9841\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0491 - val_acc: 0.9841\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0371 - acc: 0.9917 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0288 - val_acc: 0.9841\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0351 - acc: 0.9929 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0361 - acc: 0.9929 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0347 - acc: 0.9929 - val_loss: 0.0352 - val_acc: 0.9841\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0354 - acc: 0.9929 - val_loss: 0.0344 - val_acc: 0.9841\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0340 - acc: 0.9929 - val_loss: 0.0293 - val_acc: 0.9841\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0338 - acc: 0.9929 - val_loss: 0.0280 - val_acc: 0.9894\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0351 - acc: 0.9923 - val_loss: 0.0312 - val_acc: 0.9841\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0344 - acc: 0.9923 - val_loss: 0.0303 - val_acc: 0.9841\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0336 - acc: 0.9929 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0344 - acc: 0.9929 - val_loss: 0.0363 - val_acc: 0.9841\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0333 - acc: 0.9929 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0339 - acc: 0.9929 - val_loss: 0.0365 - val_acc: 0.9841\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0330 - acc: 0.9929 - val_loss: 0.0228 - val_acc: 0.9894\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0324 - acc: 0.9929 - val_loss: 0.0456 - val_acc: 0.9841\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0330 - acc: 0.9929 - val_loss: 0.0495 - val_acc: 0.9841\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0336 - acc: 0.9929 - val_loss: 0.0386 - val_acc: 0.9841\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0323 - acc: 0.9935 - val_loss: 0.0314 - val_acc: 0.9841\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0325 - acc: 0.9935 - val_loss: 0.0397 - val_acc: 0.9841\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0327 - acc: 0.9923 - val_loss: 0.0340 - val_acc: 0.9841\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0318 - acc: 0.9935 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0314 - acc: 0.9929 - val_loss: 0.0275 - val_acc: 1.0000\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0326 - acc: 0.9917 - val_loss: 0.0250 - val_acc: 0.9841\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0320 - acc: 0.9929 - val_loss: 0.0289 - val_acc: 0.9841\n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.0318 - acc: 0.9929 - val_loss: 0.0251 - val_acc: 0.9841\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0318 - acc: 0.9929 - val_loss: 0.0381 - val_acc: 0.9841\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0312 - acc: 0.9923 - val_loss: 0.0234 - val_acc: 0.9841\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0305 - acc: 0.9929 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.0309 - acc: 0.9935 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.0314 - acc: 0.9929 - val_loss: 0.0354 - val_acc: 0.9841\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0317 - acc: 0.9929 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0303 - acc: 0.9923 - val_loss: 0.0381 - val_acc: 0.9841\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0300 - acc: 0.9929 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0311 - acc: 0.9929 - val_loss: 0.0319 - val_acc: 0.9894\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0301 - acc: 0.9929 - val_loss: 0.0284 - val_acc: 0.9841\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0309 - acc: 0.9929 - val_loss: 0.0346 - val_acc: 0.9841\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0307 - acc: 0.9923 - val_loss: 0.0314 - val_acc: 0.9841\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0297 - acc: 0.9923 - val_loss: 0.0281 - val_acc: 0.9841\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0315 - acc: 0.9929 - val_loss: 0.0280 - val_acc: 0.9841\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0300 - acc: 0.9929 - val_loss: 0.0233 - val_acc: 0.9894\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0287 - acc: 0.9935 - val_loss: 0.0232 - val_acc: 0.9894\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0274 - val_acc: 0.9841\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0317 - val_acc: 0.9841\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0362 - val_acc: 0.9841\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0299 - acc: 0.9929 - val_loss: 0.0219 - val_acc: 0.9894\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.0291 - val_acc: 0.9841\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0276 - val_acc: 0.9894\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0294 - acc: 0.9935 - val_loss: 0.0190 - val_acc: 0.9894\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0290 - acc: 0.9929 - val_loss: 0.0300 - val_acc: 0.9841\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0230 - val_acc: 0.9841\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0286 - acc: 0.9935 - val_loss: 0.0328 - val_acc: 0.9841\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0287 - acc: 0.9935 - val_loss: 0.0217 - val_acc: 0.9841\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0285 - acc: 0.9935 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0287 - acc: 0.9935 - val_loss: 0.0214 - val_acc: 0.9894\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0278 - acc: 0.9941 - val_loss: 0.0287 - val_acc: 0.9841\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0228 - val_acc: 0.9894\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0289 - acc: 0.9917 - val_loss: 0.0228 - val_acc: 0.9894\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0282 - acc: 0.9935 - val_loss: 0.0233 - val_acc: 0.9894\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0285 - acc: 0.9935 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0283 - acc: 0.9929 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9894\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 0.9894\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0270 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9841\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0281 - acc: 0.9923 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.0124 - val_acc: 0.9947\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0279 - acc: 0.9935 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0268 - acc: 0.9947 - val_loss: 0.0221 - val_acc: 0.9894\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9894\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0270 - acc: 0.9947 - val_loss: 0.0190 - val_acc: 0.9894\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0268 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9894\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0266 - acc: 0.9929 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0269 - acc: 0.9929 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0269 - acc: 0.9941 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0256 - acc: 0.9947 - val_loss: 0.0229 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0263 - acc: 0.9935 - val_loss: 0.0205 - val_acc: 0.9894\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.0277 - val_acc: 0.9841\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0270 - acc: 0.9929 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9947 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0257 - acc: 0.9947 - val_loss: 0.0142 - val_acc: 0.9947\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0265 - acc: 0.9935 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0177 - val_acc: 0.9894\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0269 - acc: 0.9935 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9894\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9935 - val_loss: 0.0144 - val_acc: 0.9947\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0169 - val_acc: 0.9894\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0157 - val_acc: 0.9947\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0247 - acc: 0.9941 - val_loss: 0.0161 - val_acc: 0.9894\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0258 - acc: 0.9935 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0162 - val_acc: 0.9894\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0255 - acc: 0.9941 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0255 - acc: 0.9935 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0250 - acc: 0.9935 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0162 - val_acc: 0.9894\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0251 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9894\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0239 - acc: 0.9941 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0139 - val_acc: 0.9947\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0201 - val_acc: 0.9894\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0249 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9894\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0221 - val_acc: 0.9894\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0250 - acc: 0.9941 - val_loss: 0.0161 - val_acc: 0.9894\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0248 - acc: 0.9935 - val_loss: 0.0163 - val_acc: 0.9894\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0229 - val_acc: 0.9894\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0250 - acc: 0.9935 - val_loss: 0.0190 - val_acc: 0.9894\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0246 - acc: 0.9941 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.0156 - val_acc: 0.9894\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0248 - acc: 0.9941 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0194 - val_acc: 0.9894\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0150 - val_acc: 0.9894\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0237 - acc: 0.9935 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0244 - acc: 0.9941 - val_loss: 0.0125 - val_acc: 1.0000\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0238 - acc: 0.9935 - val_loss: 0.0167 - val_acc: 0.9894\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0242 - acc: 0.9941 - val_loss: 0.0090 - val_acc: 1.0000\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0144 - val_acc: 0.9894\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0174 - val_acc: 0.9894\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0232 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0231 - acc: 0.9941 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0232 - acc: 0.9941 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0222 - acc: 0.9941 - val_loss: 0.0136 - val_acc: 0.9894\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.0174 - val_acc: 0.9894\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 0.0149 - val_acc: 0.9947\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0118 - val_acc: 0.9947\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0202 - val_acc: 0.9894\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0108 - val_acc: 0.9947\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0231 - acc: 0.9941 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 299/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0231 - acc: 0.9941 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0234 - acc: 0.9941 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0221 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 0.9894\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0174 - val_acc: 0.9894\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0146 - val_acc: 0.9947\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9941 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0222 - acc: 0.9941 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0217 - acc: 0.9935 - val_loss: 0.0168 - val_acc: 0.9947\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0126 - val_acc: 0.9947\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0205 - acc: 0.9941 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0204 - acc: 0.9947 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0199 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0181 - acc: 0.9947 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0253 - val_acc: 0.9894\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0176 - acc: 0.9965 - val_loss: 0.0234 - val_acc: 0.9894\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0124 - val_acc: 0.9947\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9965 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0245 - val_acc: 0.9894\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.0109 - val_acc: 0.9947\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0143 - val_acc: 0.9947\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0180 - acc: 0.9965 - val_loss: 0.0166 - val_acc: 0.9947\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0210 - val_acc: 0.9894\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0210 - val_acc: 0.9894\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0172 - val_acc: 0.9947\n",
      "Epoch 358/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9965 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0170 - acc: 0.9976 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0126 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0180 - acc: 0.9965 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0145 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0227 - val_acc: 0.9894\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9965 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0106 - val_acc: 0.9947\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0102 - val_acc: 0.9947\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0172 - acc: 0.9965 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0167 - acc: 0.9965 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0149 - val_acc: 0.9947\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0230 - val_acc: 0.9894\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0074 - val_acc: 0.9947\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0166 - acc: 0.9970 - val_loss: 0.0127 - val_acc: 0.9947\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0155 - acc: 0.9970 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0155 - acc: 0.9970 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0162 - acc: 0.9965 - val_loss: 0.0086 - val_acc: 0.9947\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0089 - val_acc: 0.9947\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0077 - val_acc: 0.9947\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0150 - acc: 0.9970 - val_loss: 0.0158 - val_acc: 0.9947\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0127 - val_acc: 0.9947\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0154 - acc: 0.9970 - val_loss: 0.0082 - val_acc: 1.0000\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0141 - val_acc: 0.9947\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9976 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0152 - acc: 0.9970 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0147 - acc: 0.9976 - val_loss: 0.0118 - val_acc: 0.9947\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0185 - val_acc: 0.9894\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0147 - acc: 0.9976 - val_loss: 0.0142 - val_acc: 0.9947\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0154 - acc: 0.9976 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9976 - val_loss: 0.0106 - val_acc: 0.9947\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0120 - val_acc: 0.9947\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0149 - acc: 0.9976 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0149 - acc: 0.9976 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0145 - acc: 0.9976 - val_loss: 0.0171 - val_acc: 0.9947\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.0118 - val_acc: 0.9947\n",
      "Epoch 417/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0148 - acc: 0.9970 - val_loss: 0.0127 - val_acc: 0.9947\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0150 - acc: 0.9976 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0140 - acc: 0.9976 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9976 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0144 - acc: 0.9976 - val_loss: 0.0105 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0142 - acc: 0.9965 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0068 - val_acc: 0.9947\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0132 - acc: 0.9982 - val_loss: 0.0130 - val_acc: 0.9894\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0140 - val_acc: 0.9894\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0068 - val_acc: 0.9947\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0130 - acc: 0.9976 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.0128 - acc: 0.9976 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0073 - val_acc: 0.9947\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0073 - val_acc: 0.9947\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0102 - val_acc: 0.9947\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0147 - acc: 0.9965 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0142 - acc: 0.9976 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0136 - acc: 0.9965 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.0113 - val_acc: 0.9947\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0140 - acc: 0.9970 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0085 - val_acc: 0.9947\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0068 - val_acc: 0.9947\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0060 - val_acc: 0.9947\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.0142 - val_acc: 0.9894\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0066 - val_acc: 0.9947\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0134 - acc: 0.9953 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0064 - val_acc: 0.9947\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.0134 - acc: 0.9976 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0065 - val_acc: 0.9947\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 102us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0063 - val_acc: 0.9947\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 0.0060 - val_acc: 0.9947\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0132 - acc: 0.9976 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0067 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0062 - val_acc: 0.9947\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0078 - val_acc: 0.9947\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 149us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0080 - val_acc: 0.9947\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 145us/step - loss: 0.0127 - acc: 0.9976 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 128us/step - loss: 0.0132 - acc: 0.9965 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.0130 - acc: 0.9976 - val_loss: 0.0063 - val_acc: 0.9947\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 160us/step - loss: 0.0129 - acc: 0.9976 - val_loss: 0.0067 - val_acc: 0.9947\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0077 - val_acc: 1.0000\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0129 - acc: 0.9970 - val_loss: 0.0073 - val_acc: 0.9947\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0090 - val_acc: 0.9947\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0061 - val_acc: 1.0000\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0126 - acc: 0.9976 - val_loss: 0.0082 - val_acc: 0.9947\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0104 - acc: 0.9982 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 0.0087 - val_acc: 0.9947\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0089 - val_acc: 0.9947\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0125 - acc: 0.9976 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0055 - val_acc: 0.9947\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0140 - acc: 0.9970 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0056 - val_acc: 0.9947\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0055 - val_acc: 0.9947\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0115 - acc: 0.9982 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0046 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0100 - acc: 0.9982 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 0.9947\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 136us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0116 - acc: 0.9982 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0112 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0106 - acc: 0.9982 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0099 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0065 - val_acc: 0.9947\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0104 - acc: 0.9982 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0119 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0103 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 228us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0099 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0049 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0072 - acc: 0.9988 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9982 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0163 - val_acc: 0.9894\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0092 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0108 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0080 - acc: 0.9988 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 205us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0084 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0058 - val_acc: 0.9947\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0076 - acc: 0.9988 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0081 - val_acc: 0.9947\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 131us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.0053 - val_acc: 0.9947\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0061 - val_acc: 0.9947\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 7.9998e-04 - val_acc: 1.0000\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0049 - val_acc: 0.9947\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0064 - val_acc: 0.9947\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0090 - acc: 0.9965 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 9.8539e-04 - val_acc: 1.0000\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0012 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0064 - acc: 0.9976 - val_loss: 8.1637e-04 - val_acc: 1.0000\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0072 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0063 - acc: 0.9988 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0072 - acc: 0.9982 - val_loss: 8.2214e-04 - val_acc: 1.0000\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 9.0237e-04 - val_acc: 1.0000\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 9.9476e-04 - val_acc: 1.0000\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0079 - acc: 0.9970 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 8.6917e-04 - val_acc: 1.0000\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 8.4968e-04 - val_acc: 1.0000\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0017 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 4.4925e-04 - val_acc: 1.0000\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 7.3585e-04 - val_acc: 1.0000\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0068 - acc: 0.9988 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0066 - acc: 0.9988 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 7.7214e-04 - val_acc: 1.0000\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0075 - acc: 0.9965 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 8.9705e-04 - val_acc: 1.0000\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 8.9050e-04 - val_acc: 1.0000\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 8.8214e-04 - val_acc: 1.0000\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 7.3251e-04 - val_acc: 1.0000\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 9.0089e-04 - val_acc: 1.0000\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0100 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 363us/step - loss: 0.6441 - acc: 0.6383 - val_loss: 0.5786 - val_acc: 0.7302\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 153us/step - loss: 0.5368 - acc: 0.7589 - val_loss: 0.5168 - val_acc: 0.7725\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.4882 - acc: 0.7908 - val_loss: 0.4580 - val_acc: 0.8148\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.4491 - acc: 0.8079 - val_loss: 0.4080 - val_acc: 0.8677\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4155 - acc: 0.8327 - val_loss: 0.3646 - val_acc: 0.8677\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3838 - acc: 0.8505 - val_loss: 0.3290 - val_acc: 0.8624\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.3579 - acc: 0.8629 - val_loss: 0.3021 - val_acc: 0.8783\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3376 - acc: 0.8735 - val_loss: 0.2774 - val_acc: 0.8995\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3206 - acc: 0.8800 - val_loss: 0.2642 - val_acc: 0.9101\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.3049 - acc: 0.8889 - val_loss: 0.2385 - val_acc: 0.9259\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2911 - acc: 0.8936 - val_loss: 0.2221 - val_acc: 0.9312\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2771 - acc: 0.9025 - val_loss: 0.2084 - val_acc: 0.9365\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2669 - acc: 0.9048 - val_loss: 0.1973 - val_acc: 0.9471\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2558 - acc: 0.9090 - val_loss: 0.1817 - val_acc: 0.9471\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2428 - acc: 0.9184 - val_loss: 0.1706 - val_acc: 0.9471\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2352 - acc: 0.9202 - val_loss: 0.1645 - val_acc: 0.9735\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2238 - acc: 0.9291 - val_loss: 0.1578 - val_acc: 0.9735\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2151 - acc: 0.9303 - val_loss: 0.1490 - val_acc: 0.9735\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2055 - acc: 0.9344 - val_loss: 0.1342 - val_acc: 0.9735\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1955 - acc: 0.9350 - val_loss: 0.1263 - val_acc: 0.9735\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1901 - acc: 0.9379 - val_loss: 0.1280 - val_acc: 0.9788\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.1802 - acc: 0.9427 - val_loss: 0.1147 - val_acc: 0.9735\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1735 - acc: 0.9444 - val_loss: 0.1205 - val_acc: 0.9841\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1659 - acc: 0.9480 - val_loss: 0.1052 - val_acc: 0.9788\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1590 - acc: 0.9486 - val_loss: 0.1164 - val_acc: 0.9841\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1520 - acc: 0.9557 - val_loss: 0.0964 - val_acc: 0.9788\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1454 - acc: 0.9557 - val_loss: 0.0935 - val_acc: 0.9788\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1380 - acc: 0.9569 - val_loss: 0.0904 - val_acc: 0.9841\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.1350 - acc: 0.9610 - val_loss: 0.0949 - val_acc: 0.9841\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1265 - acc: 0.9622 - val_loss: 0.0977 - val_acc: 0.9841\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1214 - acc: 0.9663 - val_loss: 0.0983 - val_acc: 0.9841\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1154 - acc: 0.9669 - val_loss: 0.0820 - val_acc: 0.9841\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1137 - acc: 0.9663 - val_loss: 0.1047 - val_acc: 0.9841\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1084 - acc: 0.9687 - val_loss: 0.0976 - val_acc: 0.9841\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1018 - acc: 0.9687 - val_loss: 0.0767 - val_acc: 0.9841\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0991 - acc: 0.9716 - val_loss: 0.0764 - val_acc: 0.9841\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0944 - acc: 0.9728 - val_loss: 0.0796 - val_acc: 0.9841\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0909 - acc: 0.9740 - val_loss: 0.0800 - val_acc: 0.9841\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0868 - acc: 0.9758 - val_loss: 0.0708 - val_acc: 0.9841\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0855 - acc: 0.9764 - val_loss: 0.0725 - val_acc: 0.9841\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0810 - acc: 0.9781 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0797 - acc: 0.9775 - val_loss: 0.0686 - val_acc: 0.9841\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0754 - acc: 0.9787 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0787 - acc: 0.980 - 0s 58us/step - loss: 0.0749 - acc: 0.9811 - val_loss: 0.0761 - val_acc: 0.9841\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0715 - acc: 0.9817 - val_loss: 0.0745 - val_acc: 0.9841\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0679 - acc: 0.9823 - val_loss: 0.0643 - val_acc: 0.9841\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0669 - acc: 0.9840 - val_loss: 0.0664 - val_acc: 0.9841\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0663 - acc: 0.9846 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0621 - acc: 0.9823 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0597 - acc: 0.9840 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0586 - acc: 0.9846 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0584 - acc: 0.9858 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0575 - acc: 0.9852 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0540 - acc: 0.9864 - val_loss: 0.0614 - val_acc: 0.9841\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0527 - acc: 0.9852 - val_loss: 0.0614 - val_acc: 0.9894\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0532 - acc: 0.9852 - val_loss: 0.0569 - val_acc: 0.9894\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0507 - acc: 0.9858 - val_loss: 0.0659 - val_acc: 0.9894\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0489 - acc: 0.9864 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0494 - acc: 0.9882 - val_loss: 0.0587 - val_acc: 0.9894\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0476 - acc: 0.9882 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0464 - acc: 0.9882 - val_loss: 0.0603 - val_acc: 0.9894\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0452 - acc: 0.9870 - val_loss: 0.0550 - val_acc: 0.9841\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0451 - acc: 0.9888 - val_loss: 0.0558 - val_acc: 0.9841\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 0.0547 - val_acc: 0.9894\n",
      "Epoch 65/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0429 - acc: 0.9894 - val_loss: 0.0541 - val_acc: 0.9841\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0429 - acc: 0.9894 - val_loss: 0.0590 - val_acc: 0.9841\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0409 - acc: 0.9905 - val_loss: 0.0588 - val_acc: 0.9841\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0401 - acc: 0.9905 - val_loss: 0.0505 - val_acc: 0.9894\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0417 - acc: 0.9888 - val_loss: 0.0530 - val_acc: 0.9841\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0416 - acc: 0.9900 - val_loss: 0.0552 - val_acc: 0.9894\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0391 - acc: 0.9911 - val_loss: 0.0548 - val_acc: 0.9841\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0589 - val_acc: 0.9894\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0392 - acc: 0.9929 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0390 - acc: 0.9905 - val_loss: 0.0519 - val_acc: 0.9841\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0377 - acc: 0.9917 - val_loss: 0.0553 - val_acc: 0.9841\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0520 - val_acc: 0.9841\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0497 - val_acc: 0.9894\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.0509 - val_acc: 0.9894\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0354 - acc: 0.9929 - val_loss: 0.0568 - val_acc: 0.9841\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0372 - acc: 0.9917 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0351 - acc: 0.9911 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0378 - acc: 0.9917 - val_loss: 0.0429 - val_acc: 0.9894\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0514 - val_acc: 0.9894\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0355 - acc: 0.9911 - val_loss: 0.0468 - val_acc: 0.9894\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0362 - acc: 0.9929 - val_loss: 0.0425 - val_acc: 0.9894\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0340 - acc: 0.9941 - val_loss: 0.0482 - val_acc: 0.9841\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0356 - acc: 0.9935 - val_loss: 0.0500 - val_acc: 0.9841\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0336 - acc: 0.9929 - val_loss: 0.0518 - val_acc: 0.9841\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0336 - acc: 0.9923 - val_loss: 0.0427 - val_acc: 0.9894\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0322 - acc: 0.9929 - val_loss: 0.0482 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0489 - val_acc: 0.9841\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0347 - acc: 0.9923 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0347 - acc: 0.9941 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0336 - acc: 0.9929 - val_loss: 0.0358 - val_acc: 0.9894\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0327 - acc: 0.9929 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0327 - acc: 0.9947 - val_loss: 0.0475 - val_acc: 0.9841\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0334 - acc: 0.9923 - val_loss: 0.0419 - val_acc: 0.9894\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0323 - acc: 0.9941 - val_loss: 0.0492 - val_acc: 0.9841\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0327 - acc: 0.9929 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0429 - val_acc: 0.9841\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0318 - acc: 0.9947 - val_loss: 0.0375 - val_acc: 0.9894\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0327 - val_acc: 0.9947\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0293 - acc: 0.9947 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0319 - acc: 0.9935 - val_loss: 0.0360 - val_acc: 0.9947\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0295 - acc: 0.9947 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0305 - acc: 0.9947 - val_loss: 0.0317 - val_acc: 0.9947\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0321 - val_acc: 0.9947\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0288 - acc: 0.9953 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0306 - acc: 0.9935 - val_loss: 0.0373 - val_acc: 0.9841\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0292 - val_acc: 0.9947\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0302 - acc: 0.9935 - val_loss: 0.0334 - val_acc: 0.9841\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0275 - acc: 0.9941 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0312 - val_acc: 0.9841\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0285 - acc: 0.9953 - val_loss: 0.0344 - val_acc: 0.9894\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0278 - acc: 0.9941 - val_loss: 0.0311 - val_acc: 0.9894\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0293 - val_acc: 0.9947\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0320 - val_acc: 0.9947\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0290 - val_acc: 0.9947\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 0.0265 - val_acc: 0.9947\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0288 - acc: 0.9947 - val_loss: 0.0257 - val_acc: 0.9947\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0268 - acc: 0.9941 - val_loss: 0.0338 - val_acc: 0.9841\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0307 - val_acc: 0.9947\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0273 - val_acc: 0.9947\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0283 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9947\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0272 - acc: 0.9941 - val_loss: 0.0289 - val_acc: 0.9947\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0266 - val_acc: 0.9947\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0259 - acc: 0.9941 - val_loss: 0.0364 - val_acc: 0.9841\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0277 - acc: 0.9947 - val_loss: 0.0278 - val_acc: 0.9947\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0266 - val_acc: 0.9947\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0259 - acc: 0.9941 - val_loss: 0.0297 - val_acc: 0.9947\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0280 - acc: 0.9941 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0275 - acc: 0.9935 - val_loss: 0.0269 - val_acc: 0.9947\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0271 - acc: 0.9935 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0255 - acc: 0.9941 - val_loss: 0.0288 - val_acc: 0.9947\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0277 - acc: 0.9947 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0280 - acc: 0.9929 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0372 - val_acc: 0.9947\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0262 - acc: 0.9953 - val_loss: 0.0231 - val_acc: 0.9947\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0259 - acc: 0.9935 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0258 - val_acc: 0.9947\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0255 - acc: 0.9935 - val_loss: 0.0362 - val_acc: 0.9841\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0312 - val_acc: 0.9841\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9941 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0267 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0288 - val_acc: 0.9894\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0231 - val_acc: 0.9894\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9894\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0250 - acc: 0.9941 - val_loss: 0.0249 - val_acc: 0.9947\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0259 - acc: 0.9941 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0256 - acc: 0.9947 - val_loss: 0.0375 - val_acc: 0.9841\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0257 - acc: 0.9947 - val_loss: 0.0290 - val_acc: 0.9841\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0303 - val_acc: 0.9841\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0239 - val_acc: 0.9841\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0272 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0229 - val_acc: 0.9947\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0318 - val_acc: 0.9841\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0259 - acc: 0.9947 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0255 - val_acc: 0.9894\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0261 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 0.9894\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0242 - acc: 0.9941 - val_loss: 0.0141 - val_acc: 0.9947\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0220 - val_acc: 0.9894\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0257 - acc: 0.9947 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0282 - acc: 0.9935 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0236 - val_acc: 0.9894\n",
      "Epoch 183/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0242 - acc: 0.9941 - val_loss: 0.0164 - val_acc: 0.9947\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0253 - val_acc: 0.9841\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0231 - val_acc: 0.9894\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0255 - acc: 0.9953 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0250 - acc: 0.9941 - val_loss: 0.0171 - val_acc: 0.9894\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0237 - acc: 0.9941 - val_loss: 0.0248 - val_acc: 0.9894\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0139 - val_acc: 0.9947\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0237 - val_acc: 0.9894\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9959 - val_loss: 0.0251 - val_acc: 0.9841\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0142 - val_acc: 0.9947\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0243 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9894\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0171 - val_acc: 0.9894\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9894\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0134 - val_acc: 1.0000\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0252 - acc: 0.9947 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0141 - val_acc: 0.9894\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0113 - val_acc: 0.9947\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0141 - val_acc: 0.9894\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0196 - val_acc: 0.9894\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0162 - val_acc: 0.9894\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0127 - val_acc: 1.0000\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0285 - val_acc: 0.9841\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0232 - acc: 0.9959 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0219 - val_acc: 0.9841\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.0143 - val_acc: 0.9894\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0239 - acc: 0.9959 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0115 - val_acc: 1.0000\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0282 - val_acc: 0.9841\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0163 - val_acc: 0.9894\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0181 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0161 - val_acc: 0.9894\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0270 - val_acc: 0.9841\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0128 - val_acc: 0.9894\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0272 - val_acc: 0.9841\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0235 - acc: 0.9935 - val_loss: 0.0264 - val_acc: 0.9841\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0247 - val_acc: 0.9841\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0278 - acc: 0.9953 - val_loss: 0.0272 - val_acc: 0.9841\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0250 - val_acc: 0.9841\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0223 - acc: 0.9941 - val_loss: 0.0247 - val_acc: 0.9841\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0126 - val_acc: 0.9947\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.0187 - val_acc: 0.9841\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0232 - val_acc: 0.9841\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0224 - val_acc: 0.9841\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.0256 - val_acc: 0.9841\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0133 - val_acc: 0.9841\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0224 - acc: 0.9959 - val_loss: 0.0182 - val_acc: 0.9894\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0212 - val_acc: 0.9841\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0105 - val_acc: 1.0000\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0228 - acc: 0.9935 - val_loss: 0.0195 - val_acc: 0.9841\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0162 - val_acc: 0.9894\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0225 - acc: 0.9941 - val_loss: 0.0187 - val_acc: 0.9841\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0129 - val_acc: 0.9894\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0203 - val_acc: 0.9841\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0160 - val_acc: 0.9894\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0309 - val_acc: 0.9841\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0125 - val_acc: 0.9894\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0234 - val_acc: 0.9841\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0183 - val_acc: 0.9894\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0210 - acc: 0.9959 - val_loss: 0.0298 - val_acc: 1.0000\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.0088 - val_acc: 1.0000\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0252 - val_acc: 0.9841\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0242 - val_acc: 0.9841\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0118 - val_acc: 1.0000\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0122 - val_acc: 0.9894\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0141 - val_acc: 0.9894\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0156 - val_acc: 0.9894\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0160 - val_acc: 0.9894\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0197 - val_acc: 0.9894\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0207 - val_acc: 0.9894\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0105 - val_acc: 0.9947\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.0119 - val_acc: 0.9894\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0252 - acc: 0.9941 - val_loss: 0.0126 - val_acc: 0.9894\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0158 - val_acc: 0.9894\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0107 - val_acc: 1.0000\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0214 - acc: 0.9959 - val_loss: 0.0168 - val_acc: 0.9894\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0219 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 0.9894\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0220 - acc: 0.9959 - val_loss: 0.0176 - val_acc: 0.9894\n",
      "Epoch 301/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9894\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9894\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0225 - acc: 0.9959 - val_loss: 0.0151 - val_acc: 0.9894\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 0.9894\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0182 - val_acc: 0.9894\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 1.0000\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0301 - val_acc: 0.9894\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0168 - val_acc: 0.9894\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0137 - val_acc: 0.9894\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0195 - val_acc: 0.9894\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0110 - val_acc: 0.9947\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0232 - acc: 0.9959 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0165 - val_acc: 0.9894\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0248 - val_acc: 0.9894\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0235 - acc: 0.9959 - val_loss: 0.0113 - val_acc: 0.9947\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0143 - val_acc: 0.9894\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 0.9894\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0238 - val_acc: 0.9894\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0236 - val_acc: 0.9894\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0197 - acc: 0.9965 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0229 - acc: 0.9959 - val_loss: 0.0183 - val_acc: 0.9894\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0227 - acc: 0.9959 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0243 - val_acc: 0.9894\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0214 - val_acc: 0.9894\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9894\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 0.9894\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0225 - val_acc: 0.9894\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0186 - val_acc: 0.9894\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0173 - val_acc: 0.9894\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0187 - val_acc: 0.9894\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0182 - val_acc: 0.9894\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0229 - val_acc: 0.9894\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0244 - acc: 0.9959 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0171 - val_acc: 0.9894\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 0.9894\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0230 - acc: 0.9959 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0204 - acc: 0.9947 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0225 - acc: 0.9941 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0149 - val_acc: 0.9894\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 360/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0197 - val_acc: 0.9894\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0156 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0209 - acc: 0.9959 - val_loss: 0.0211 - val_acc: 0.9894\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0192 - val_acc: 0.9894\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0247 - acc: 0.9941 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0193 - val_acc: 0.9894\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0191 - val_acc: 0.9894\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0126 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0205 - val_acc: 0.9894\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0379 - val_acc: 0.9894\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0242 - val_acc: 0.9894\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0185 - val_acc: 0.9894\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0188 - val_acc: 0.9894\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0233 - val_acc: 0.9894\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0385 - val_acc: 0.9894\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0215 - acc: 0.9959 - val_loss: 0.0409 - val_acc: 0.9894\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0265 - acc: 0.9941 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0280 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0194 - acc: 0.9965 - val_loss: 0.0405 - val_acc: 0.9894\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0207 - val_acc: 0.9894\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9941 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0260 - val_acc: 0.9947\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0204 - acc: 0.9947 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0278 - val_acc: 0.9894\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0243 - acc: 0.9935 - val_loss: 0.0217 - val_acc: 0.9894\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0320 - val_acc: 0.9894\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0216 - acc: 0.9959 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0213 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 0.0210 - val_acc: 0.9894\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9965 - val_loss: 0.0445 - val_acc: 0.9894\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9894\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9894\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0269 - acc: 0.9917 - val_loss: 0.0427 - val_acc: 0.9894\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0393 - val_acc: 0.9894\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9959 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 419/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0419 - val_acc: 0.9894\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0397 - val_acc: 0.9894\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0467 - val_acc: 0.9894\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0270 - acc: 0.9894 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0526 - val_acc: 0.9894\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0229 - val_acc: 0.9894\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0354 - val_acc: 0.9894\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0272 - acc: 0.9929 - val_loss: 0.0334 - val_acc: 0.9894\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0328 - val_acc: 0.9894\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9894\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0361 - val_acc: 0.9894\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0403 - val_acc: 0.9894\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0539 - val_acc: 0.9894\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0282 - val_acc: 0.9947\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0417 - val_acc: 0.9894\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0220 - acc: 0.9941 - val_loss: 0.0500 - val_acc: 0.9894\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0421 - val_acc: 0.9894\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0203 - acc: 0.9965 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0290 - val_acc: 0.9947\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0480 - val_acc: 0.9894\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0213 - acc: 0.9959 - val_loss: 0.0517 - val_acc: 0.9894\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0509 - val_acc: 0.9894\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0195 - acc: 0.9965 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0443 - val_acc: 0.9894\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0345 - val_acc: 0.9894\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0181 - acc: 0.9965 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0441 - val_acc: 0.9894\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0204 - acc: 0.9941 - val_loss: 0.0523 - val_acc: 0.9894\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0547 - val_acc: 0.9894\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9894\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0451 - val_acc: 0.9894\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0442 - val_acc: 0.9894\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0388 - val_acc: 0.9894\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0209 - acc: 0.9941 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0538 - val_acc: 0.9894\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0212 - acc: 0.9959 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 0.0532 - val_acc: 0.9894\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0486 - val_acc: 0.9894\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0538 - val_acc: 0.9894\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0501 - val_acc: 0.9894\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9894\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0184 - acc: 0.9965 - val_loss: 0.0399 - val_acc: 0.9894\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0585 - val_acc: 0.9894\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0541 - val_acc: 0.9894\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0483 - val_acc: 0.9894\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0395 - val_acc: 0.9894\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0457 - val_acc: 0.9894\n",
      "Epoch 478/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0412 - val_acc: 0.9894\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0434 - val_acc: 0.9894\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0557 - val_acc: 0.9894\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0195 - acc: 0.9965 - val_loss: 0.0408 - val_acc: 0.9894\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0205 - acc: 0.9959 - val_loss: 0.0532 - val_acc: 0.9894\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0412 - val_acc: 0.9894\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0542 - val_acc: 0.9894\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9941 - val_loss: 0.0440 - val_acc: 0.9894\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0590 - val_acc: 0.9894\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 0.0585 - val_acc: 0.9894\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0189 - acc: 0.9965 - val_loss: 0.0541 - val_acc: 0.9894\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0499 - val_acc: 0.9894\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0473 - val_acc: 0.9894\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9894\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0202 - acc: 0.9965 - val_loss: 0.0525 - val_acc: 0.9894\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0483 - val_acc: 0.9894\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0440 - val_acc: 0.9894\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9965 - val_loss: 0.0529 - val_acc: 0.9894\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0179 - acc: 0.9965 - val_loss: 0.0498 - val_acc: 0.9894\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0450 - val_acc: 0.9894\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0455 - val_acc: 0.9894\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0493 - val_acc: 0.9894\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0433 - val_acc: 0.9894\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0497 - val_acc: 0.9894\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0518 - val_acc: 0.9894\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0562 - val_acc: 0.9894\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.0486 - val_acc: 0.9894\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0580 - val_acc: 0.9894\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0435 - val_acc: 0.9894\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0547 - val_acc: 0.9894\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0185 - acc: 0.9970 - val_loss: 0.0545 - val_acc: 0.9894\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0591 - val_acc: 0.9894\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0271 - acc: 0.9905 - val_loss: 0.0549 - val_acc: 0.9894\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0548 - val_acc: 0.9894\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0549 - val_acc: 0.9894\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.0551 - val_acc: 0.9894\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0476 - val_acc: 0.9894\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0564 - val_acc: 0.9894\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0519 - val_acc: 0.9894\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0559 - val_acc: 0.9894\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0155 - acc: 0.9947 - val_loss: 0.0586 - val_acc: 0.9894\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0575 - val_acc: 0.9894\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0542 - val_acc: 0.9894\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0524 - val_acc: 0.9894\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.0180 - acc: 0.9965 - val_loss: 0.0412 - val_acc: 0.9894\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0633 - val_acc: 0.9894\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0181 - acc: 0.9965 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0581 - val_acc: 0.9894\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0489 - val_acc: 0.9894\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0560 - val_acc: 0.9894\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0571 - val_acc: 0.9894\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0498 - val_acc: 0.9894\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0511 - val_acc: 0.9894\n",
      "Epoch 537/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0179 - acc: 0.9970 - val_loss: 0.0617 - val_acc: 0.9894\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0562 - val_acc: 0.9894\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0171 - acc: 0.9947 - val_loss: 0.0506 - val_acc: 0.9894\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0591 - val_acc: 0.9894\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0527 - val_acc: 0.9894\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 0.0522 - val_acc: 0.9894\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0511 - val_acc: 0.9894\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0587 - val_acc: 0.9894\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0449 - val_acc: 0.9894\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0516 - val_acc: 0.9894\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0227 - acc: 0.9923 - val_loss: 0.0529 - val_acc: 0.9894\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0569 - val_acc: 0.9894\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0468 - val_acc: 0.9894\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 128us/step - loss: 0.0178 - acc: 0.9965 - val_loss: 0.0473 - val_acc: 0.9894\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.0456 - val_acc: 0.9894\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 129us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0554 - val_acc: 0.9894\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0536 - val_acc: 0.9894\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0471 - val_acc: 0.9894\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0490 - val_acc: 0.9894\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0490 - val_acc: 0.9894\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0174 - acc: 0.9953 - val_loss: 0.0544 - val_acc: 0.9894\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0682 - val_acc: 0.9894\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0641 - val_acc: 0.9894\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0596 - val_acc: 0.9894\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0151 - acc: 0.9953 - val_loss: 0.0503 - val_acc: 0.9894\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0502 - val_acc: 0.9894\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0576 - val_acc: 0.9894\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0603 - val_acc: 0.9894\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0538 - val_acc: 0.9894\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0737 - val_acc: 0.9894\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0564 - val_acc: 0.9894\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0512 - val_acc: 0.9894\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0630 - val_acc: 0.9894\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.0524 - val_acc: 0.9894\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0533 - val_acc: 0.9894\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0662 - val_acc: 0.9894\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0506 - val_acc: 0.9894\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0643 - val_acc: 0.9894\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0582 - val_acc: 0.9894\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0492 - val_acc: 0.9894\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0645 - val_acc: 0.9894\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0689 - val_acc: 0.9894\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0645 - val_acc: 0.9894\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0184 - acc: 0.9941 - val_loss: 0.0650 - val_acc: 0.9894\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0646 - val_acc: 0.9894\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0620 - val_acc: 0.9894\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 0.0674 - val_acc: 0.9894\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0663 - val_acc: 0.9894\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0576 - val_acc: 0.9894\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0630 - val_acc: 0.9894\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0161 - acc: 0.9965 - val_loss: 0.0632 - val_acc: 0.9894\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0526 - val_acc: 0.9894\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0126 - acc: 0.9965 - val_loss: 0.0661 - val_acc: 0.9894\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0463 - val_acc: 0.9894\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0558 - val_acc: 0.9894\n",
      "Epoch 596/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0137 - acc: 0.9959 - val_loss: 0.0565 - val_acc: 0.9894\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0677 - val_acc: 0.9894\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0184 - acc: 0.9941 - val_loss: 0.0535 - val_acc: 0.9894\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0154 - acc: 0.9970 - val_loss: 0.0639 - val_acc: 0.9894\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0471 - val_acc: 0.9894\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0532 - val_acc: 0.9894\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0536 - val_acc: 0.9894\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0680 - val_acc: 0.9894\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0465 - val_acc: 0.9894\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0140 - acc: 0.9959 - val_loss: 0.0517 - val_acc: 0.9894\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0133 - acc: 0.9965 - val_loss: 0.0639 - val_acc: 0.9894\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0563 - val_acc: 0.9894\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0144 - acc: 0.9965 - val_loss: 0.0526 - val_acc: 0.9894\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.0534 - val_acc: 0.9894\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0123 - acc: 0.9976 - val_loss: 0.0695 - val_acc: 0.9894\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0549 - val_acc: 0.9894\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 0.0693 - val_acc: 0.9894\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0657 - val_acc: 0.9894\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0132 - acc: 0.9953 - val_loss: 0.0523 - val_acc: 0.9894\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0682 - val_acc: 0.9894\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0519 - val_acc: 0.9894\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0677 - val_acc: 0.9894\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0143 - acc: 0.9959 - val_loss: 0.0627 - val_acc: 0.9894\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9894\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0484 - val_acc: 0.9894\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0150 - acc: 0.9941 - val_loss: 0.0479 - val_acc: 0.9894\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 0.0505 - val_acc: 0.9894\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0549 - val_acc: 0.9894\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0583 - val_acc: 0.9894\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0727 - val_acc: 0.9894\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.0530 - val_acc: 0.9894\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0725 - val_acc: 0.9894\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0163 - acc: 0.9953 - val_loss: 0.0569 - val_acc: 0.9894\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0146 - acc: 0.9959 - val_loss: 0.0542 - val_acc: 0.9894\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0643 - val_acc: 0.9894\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0573 - val_acc: 0.9894\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0137 - acc: 0.9965 - val_loss: 0.0570 - val_acc: 0.9894\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0643 - val_acc: 0.9894\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0661 - val_acc: 0.9894\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0554 - val_acc: 0.9894\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 0.0587 - val_acc: 0.9894\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0641 - val_acc: 0.9894\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0596 - val_acc: 0.9894\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0152 - acc: 0.9947 - val_loss: 0.0610 - val_acc: 0.9894\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0545 - val_acc: 0.9894\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0600 - val_acc: 0.9894\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0651 - val_acc: 0.9894\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0593 - val_acc: 0.9894\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0583 - val_acc: 0.9894\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0733 - val_acc: 0.9894\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0609 - val_acc: 0.9894\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0648 - val_acc: 0.9894\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0559 - val_acc: 0.9894\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0666 - val_acc: 0.9894\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0609 - val_acc: 0.9894\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0566 - val_acc: 0.9894\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0757 - val_acc: 0.9894\n",
      "Epoch 655/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0631 - val_acc: 0.9894\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0685 - val_acc: 0.9894\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0654 - val_acc: 0.9894\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0609 - val_acc: 0.9894\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0712 - val_acc: 0.9894\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0601 - val_acc: 0.9894\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0658 - val_acc: 0.9894\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0688 - val_acc: 0.9894\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0134 - acc: 0.9947 - val_loss: 0.0559 - val_acc: 0.9894\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0761 - val_acc: 0.9894\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0717 - val_acc: 0.9894\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0605 - val_acc: 0.9894\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0720 - val_acc: 0.9894\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0766 - val_acc: 0.9894\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0125 - acc: 0.9970 - val_loss: 0.0529 - val_acc: 0.9894\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0652 - val_acc: 0.9894\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0154 - acc: 0.995 - 0s 47us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0703 - val_acc: 0.9894\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0116 - acc: 0.9965 - val_loss: 0.0768 - val_acc: 0.9894\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0573 - val_acc: 0.9894\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0600 - val_acc: 0.9894\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0674 - val_acc: 0.9894\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0623 - val_acc: 0.9894\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0662 - val_acc: 0.9894\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0739 - val_acc: 0.9894\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0705 - val_acc: 0.9894\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.0651 - val_acc: 0.9894\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0756 - val_acc: 0.9894\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 0.0804 - val_acc: 0.9894\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0570 - val_acc: 0.9894\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0708 - val_acc: 0.9894\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0662 - val_acc: 0.9894\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0184 - acc: 0.9935 - val_loss: 0.0790 - val_acc: 0.9894\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0663 - val_acc: 0.9894\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0773 - val_acc: 0.9894\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.996 - 0s 49us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0785 - val_acc: 0.9894\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0159 - acc: 0.9941 - val_loss: 0.0726 - val_acc: 0.9894\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0708 - val_acc: 0.9894\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0670 - val_acc: 0.9894\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0654 - val_acc: 0.9894\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0577 - val_acc: 0.9894\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0660 - val_acc: 0.9894\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0518 - val_acc: 0.9894\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0794 - val_acc: 0.9894\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0129 - acc: 0.9965 - val_loss: 0.0790 - val_acc: 0.9894\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0724 - val_acc: 0.9894\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0133 - acc: 0.9947 - val_loss: 0.0594 - val_acc: 0.9894\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0131 - acc: 0.9959 - val_loss: 0.0679 - val_acc: 0.9894\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0616 - val_acc: 0.9894\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0650 - val_acc: 0.9894\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0148 - acc: 0.9947 - val_loss: 0.0798 - val_acc: 0.9894\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0149 - acc: 0.9947 - val_loss: 0.0713 - val_acc: 0.9894\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0907 - val_acc: 0.9894\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0711 - val_acc: 0.9894\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0775 - val_acc: 0.9894\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0824 - val_acc: 0.9894\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0762 - val_acc: 0.9894\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0771 - val_acc: 0.9894\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0926 - val_acc: 0.9894\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.0728 - val_acc: 0.9894\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0691 - val_acc: 0.9894\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0802 - val_acc: 0.9894\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0854 - val_acc: 0.9894\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0109 - acc: 0.9959 - val_loss: 0.0686 - val_acc: 0.9894\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0693 - val_acc: 0.9894\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0120 - acc: 0.9982 - val_loss: 0.0750 - val_acc: 0.9894\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0781 - val_acc: 0.9894\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0856 - val_acc: 0.9894\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0095 - acc: 0.9959 - val_loss: 0.0857 - val_acc: 0.9894\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0770 - val_acc: 0.9894\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9953 - val_loss: 0.0771 - val_acc: 0.9894\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0814 - val_acc: 0.9894\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0172 - acc: 0.9929 - val_loss: 0.0786 - val_acc: 0.9894\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0810 - val_acc: 0.9894\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0110 - acc: 0.9959 - val_loss: 0.0782 - val_acc: 0.9894\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0978 - val_acc: 0.9630\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0131 - acc: 0.9965 - val_loss: 0.0722 - val_acc: 0.9894\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0848 - val_acc: 0.9894\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0090 - acc: 0.9988 - val_loss: 0.0921 - val_acc: 0.9894\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0756 - val_acc: 0.9894\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0751 - val_acc: 0.9894\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0724 - val_acc: 0.9894\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0670 - val_acc: 0.9894\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0850 - val_acc: 0.9894\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0903 - val_acc: 0.9894\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0915 - val_acc: 0.9894\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0903 - val_acc: 0.9894\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0681 - val_acc: 0.9894\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0827 - val_acc: 0.9894\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0941 - val_acc: 0.9894\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0901 - val_acc: 0.9894\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0841 - val_acc: 0.9894\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.1001 - val_acc: 0.9894\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0958 - val_acc: 0.9894\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0757 - val_acc: 0.9894\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.1101 - val_acc: 0.9841\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0093 - acc: 0.9965 - val_loss: 0.0891 - val_acc: 0.9894\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 129us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0968 - val_acc: 0.9894\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0993 - val_acc: 0.9894\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0803 - val_acc: 0.9841\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0886 - val_acc: 0.9894\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0910 - val_acc: 0.9894\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0117 - acc: 0.9959 - val_loss: 0.1227 - val_acc: 0.9894\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0122 - acc: 0.9976 - val_loss: 0.0939 - val_acc: 0.9894\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0924 - val_acc: 0.9894\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0137 - acc: 0.9959 - val_loss: 0.0880 - val_acc: 0.9894\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0805 - val_acc: 0.9894\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0125 - acc: 0.9953 - val_loss: 0.0824 - val_acc: 0.9894\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.0113 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9841\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0918 - val_acc: 0.9894\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0100 - acc: 0.9988 - val_loss: 0.0870 - val_acc: 0.9894\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0802 - val_acc: 0.9894\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0080 - acc: 0.9965 - val_loss: 0.0900 - val_acc: 0.9894\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.1051 - val_acc: 0.9894\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.1051 - val_acc: 0.9894\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0915 - val_acc: 0.9894\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.1135 - val_acc: 0.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0113 - acc: 0.9953 - val_loss: 0.1046 - val_acc: 0.9788\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0093 - acc: 0.9953 - val_loss: 0.1112 - val_acc: 0.9894\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0105 - acc: 0.9965 - val_loss: 0.1103 - val_acc: 0.9894\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0949 - val_acc: 0.9894\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0926 - val_acc: 0.9788\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0090 - acc: 0.9965 - val_loss: 0.0923 - val_acc: 0.9894\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0193 - acc: 0.9941 - val_loss: 0.1225 - val_acc: 0.9841\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0154 - acc: 0.9941 - val_loss: 0.0865 - val_acc: 0.9894\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1033 - val_acc: 0.9894\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0094 - acc: 0.9953 - val_loss: 0.0876 - val_acc: 0.9894\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0196 - acc: 0.9911 - val_loss: 0.0923 - val_acc: 0.9894\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0914 - val_acc: 0.9894\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0103 - acc: 0.9953 - val_loss: 0.0951 - val_acc: 0.9894\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0934 - val_acc: 0.9894\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.0955 - val_acc: 0.9894\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.0089 - acc: 0.9959 - val_loss: 0.0991 - val_acc: 0.9894\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.1003 - val_acc: 0.9894\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.1169 - val_acc: 0.9894\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.1062 - val_acc: 0.9841\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.1050 - val_acc: 0.9788\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.1059 - val_acc: 0.9894\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.1109 - val_acc: 0.9894\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0131 - acc: 0.9947 - val_loss: 0.1159 - val_acc: 0.9894\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.1143 - val_acc: 0.9894\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0931 - val_acc: 0.9894\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.1115 - val_acc: 0.9894\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0104 - acc: 0.9959 - val_loss: 0.1008 - val_acc: 0.9894\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0935 - val_acc: 0.9894\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.1093 - val_acc: 0.9894\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.1031 - val_acc: 0.9894\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.1110 - val_acc: 0.9894\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.1124 - val_acc: 0.9894\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0926 - val_acc: 0.9788\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0134 - acc: 0.9947 - val_loss: 0.1102 - val_acc: 0.9894\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0133 - acc: 0.9953 - val_loss: 0.0994 - val_acc: 0.9894\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.1050 - val_acc: 0.9894\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1116 - val_acc: 0.9788\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0095 - acc: 0.9959 - val_loss: 0.1146 - val_acc: 0.9841\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1080 - val_acc: 0.9894\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0989 - val_acc: 0.9894\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.1085 - val_acc: 0.9894\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.1057 - val_acc: 0.9841\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1188 - val_acc: 0.9894\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0995 - val_acc: 0.9788\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1124 - val_acc: 0.9788\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 0.0947 - val_acc: 0.9894\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0149 - acc: 0.9941 - val_loss: 0.1066 - val_acc: 0.9894\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.1038 - val_acc: 0.9894\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.1122 - val_acc: 0.9894\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.1213 - val_acc: 0.9894\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.1102 - val_acc: 0.9894\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.1115 - val_acc: 0.9841\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0965 - val_acc: 0.9841\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.1121 - val_acc: 0.9894\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0956 - val_acc: 0.9894\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.1010 - val_acc: 0.9841\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1080 - val_acc: 0.9841\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.1172 - val_acc: 0.9788\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0096 - acc: 0.9953 - val_loss: 0.0983 - val_acc: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0069 - acc: 0.9976 - val_loss: 0.0945 - val_acc: 0.9894\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.1050 - val_acc: 0.9894\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.1151 - val_acc: 0.9894\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.1123 - val_acc: 0.9894\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1023 - val_acc: 0.9841\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.1126 - val_acc: 0.9841\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.1200 - val_acc: 0.9788\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.1072 - val_acc: 0.9894\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0077 - acc: 0.9965 - val_loss: 0.1058 - val_acc: 0.9841\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0992 - val_acc: 0.9841\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 0.1112 - val_acc: 0.9841\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.1059 - val_acc: 0.9841\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 0.1120 - val_acc: 0.9894\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0950 - val_acc: 0.9894\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0084 - acc: 0.9988 - val_loss: 0.1178 - val_acc: 0.9894\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.1037 - val_acc: 0.9841\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.1082 - val_acc: 0.9841\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.1108 - val_acc: 0.9841\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0070 - acc: 0.9965 - val_loss: 0.1150 - val_acc: 0.9894\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0082 - acc: 0.9970 - val_loss: 0.1208 - val_acc: 0.9841\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.1103 - val_acc: 0.9894\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.1083 - val_acc: 0.9894\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.1139 - val_acc: 0.9894\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0977 - val_acc: 0.9841\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.1165 - val_acc: 0.9894\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0073 - acc: 0.9988 - val_loss: 0.1099 - val_acc: 0.9841\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0985 - val_acc: 0.9894\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0083 - acc: 0.9965 - val_loss: 0.1143 - val_acc: 0.9894\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0991 - val_acc: 0.9841\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0964 - val_acc: 0.9894\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0978 - val_acc: 0.9894\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0085 - acc: 0.9965 - val_loss: 0.0968 - val_acc: 0.9894\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0085 - acc: 0.9965 - val_loss: 0.0963 - val_acc: 0.9894\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0074 - acc: 0.9970 - val_loss: 0.0918 - val_acc: 0.9894\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0931 - val_acc: 0.9894\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0998 - val_acc: 0.9894\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0987 - val_acc: 0.9841\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0077 - acc: 0.9976 - val_loss: 0.0971 - val_acc: 0.9894\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0959 - val_acc: 0.9894\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0781 - val_acc: 0.9894\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0966 - val_acc: 0.9894\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0063 - acc: 0.9976 - val_loss: 0.0987 - val_acc: 0.9841\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0978 - val_acc: 0.9841\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0075 - acc: 0.9970 - val_loss: 0.0937 - val_acc: 0.9894\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0090 - acc: 0.9959 - val_loss: 0.0914 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.1001 - val_acc: 0.9894\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0074 - acc: 0.9982 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0071 - acc: 0.9965 - val_loss: 0.0870 - val_acc: 0.9947\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0954 - val_acc: 0.9894\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0929 - val_acc: 0.9894\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0894 - val_acc: 0.9947\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0103 - acc: 0.9953 - val_loss: 0.0929 - val_acc: 0.9894\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0060 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0927 - val_acc: 0.9894\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0979 - val_acc: 0.9894\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0882 - val_acc: 0.9947\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0997 - val_acc: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.1136 - val_acc: 0.9841\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.1057 - val_acc: 0.9894\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.1046 - val_acc: 0.9894\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0927 - val_acc: 0.9894\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1010 - val_acc: 0.9841\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0080 - acc: 0.9982 - val_loss: 0.0942 - val_acc: 0.9894\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0156 - acc: 0.9947 - val_loss: 0.0966 - val_acc: 0.9841\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0943 - val_acc: 0.9894\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0074 - acc: 0.9970 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0066 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0050 - acc: 0.9970 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.0897 - val_acc: 0.9947\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0094 - acc: 0.9965 - val_loss: 0.0836 - val_acc: 0.9894\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0897 - val_acc: 0.9894\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0855 - val_acc: 0.9947\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0896 - val_acc: 0.9894\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0908 - val_acc: 0.9894\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0914 - val_acc: 0.9894\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0067 - acc: 0.9988 - val_loss: 0.0889 - val_acc: 0.9947\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 0.9965 - val_loss: 0.0894 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0067 - acc: 0.9970 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0087 - acc: 0.9965 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0903 - val_acc: 0.9947\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0087 - acc: 0.9988 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0091 - acc: 0.9965 - val_loss: 0.0933 - val_acc: 0.9894\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0942 - val_acc: 0.9894\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0094 - acc: 0.9953 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0883 - val_acc: 0.9947\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0093 - acc: 0.9965 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0909 - val_acc: 0.9894\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0063 - acc: 0.9976 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0869 - val_acc: 0.9947\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.1075 - val_acc: 0.9894\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0901 - val_acc: 0.9947\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.0856 - val_acc: 0.9947\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0080 - acc: 0.9965 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0091 - acc: 0.9947 - val_loss: 0.0855 - val_acc: 0.9947\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0880 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0102 - acc: 0.9982 - val_loss: 0.0923 - val_acc: 0.9894\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0892 - val_acc: 0.9894\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0076 - acc: 0.9965 - val_loss: 0.0873 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0831 - val_acc: 0.9947\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9894\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0865 - val_acc: 0.9947\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0093 - acc: 0.9953 - val_loss: 0.0855 - val_acc: 0.9947\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0891 - val_acc: 0.9947\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0897 - val_acc: 0.9947\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0090 - acc: 0.9982 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0070 - acc: 0.9976 - val_loss: 0.0988 - val_acc: 0.9894\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0090 - acc: 0.9965 - val_loss: 0.0878 - val_acc: 0.9947\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0906 - val_acc: 0.9894\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0939 - val_acc: 0.9894\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0111 - acc: 0.9947 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.0875 - val_acc: 0.9947\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0069 - acc: 0.9965 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0879 - val_acc: 0.9947\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0068 - acc: 0.9970 - val_loss: 0.0866 - val_acc: 0.9947\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0864 - val_acc: 0.9947\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0124 - acc: 0.9976 - val_loss: 0.0881 - val_acc: 0.9947\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0072 - acc: 0.9970 - val_loss: 0.1074 - val_acc: 0.9894\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.0861 - val_acc: 0.9947\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1113 - val_acc: 0.9894\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.1193 - val_acc: 0.9841\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.0868 - val_acc: 0.9947\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0860 - val_acc: 0.9947\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0887 - val_acc: 0.9947\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0862 - val_acc: 0.9947\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 0.0991 - val_acc: 0.9894\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0888 - val_acc: 0.9947\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0971 - val_acc: 0.9894\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0872 - val_acc: 0.9947\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0857 - val_acc: 0.9947\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0874 - val_acc: 0.9947\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0049 - acc: 0.9976 - val_loss: 0.0856 - val_acc: 0.9947\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0963 - val_acc: 0.9894\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0873 - val_acc: 0.9947\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0908 - val_acc: 0.9894\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 0.0859 - val_acc: 0.9947\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9982 - val_loss: 0.0867 - val_acc: 0.9947\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0863 - val_acc: 0.9947\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0859 - val_acc: 0.9947\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "activation_function = ['relu', 'sigmoid', 'tanh']\n",
    "hum_con_act_accuracy = []\n",
    "for i in range(len(activation_function)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=18, activation='relu'))\n",
    "    model.add(Dense(8, activation= activation_function[i]))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model, iterating on the data in batches of 32 samples\n",
    "    summary = model.fit(x_train_human_con, y_train_human_con, validation_split = 0.1, nb_epoch=1000, batch_size=32)\n",
    "    scores = model.evaluate(x=x_test_human_con, y=y_test_human_con, verbose=0)\n",
    "    scores_acc = scores[1]\n",
    "    hum_con_act_accuracy.append(scores_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Activation Function Vs Accuracy')\n",
    "plt.plot(activation_function,hum_con_act_accuracy,'gs-', label='Accuracy')\n",
    "plt.axis([min(activation_function), max(activation_function), min(hum_con_act_accuracy)-0.1, max(hum_con_act_accuracy)+0.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('activation_function')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1b38967ef0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b389be240>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b389e8400>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b38a145c0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating plots for accuracy and loss using matplotlib package\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(summary.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model Accuracy\n",
    "scores = model.evaluate(x=x_test_human_con, y=y_test_human_con, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for Human Observed Dataset with Feature Concatentation \n",
      "\n",
      "Test Accuracy =  100%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for Human Observed Dataset with Feature Concatentation \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 407us/step - loss: 0.6950 - acc: 0.6643 - val_loss: 0.6670 - val_acc: 0.6455\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.6255 - acc: 0.6874 - val_loss: 0.6174 - val_acc: 0.6825\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.5926 - acc: 0.7163 - val_loss: 0.5830 - val_acc: 0.7037\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.5605 - acc: 0.7506 - val_loss: 0.5495 - val_acc: 0.7249\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5254 - acc: 0.7778 - val_loss: 0.5028 - val_acc: 0.7989\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4839 - acc: 0.7985 - val_loss: 0.4604 - val_acc: 0.8254\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.4416 - acc: 0.8203 - val_loss: 0.4102 - val_acc: 0.8571\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.4013 - acc: 0.8499 - val_loss: 0.3701 - val_acc: 0.8730\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3701 - acc: 0.8676 - val_loss: 0.3403 - val_acc: 0.9153\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3439 - acc: 0.8783 - val_loss: 0.3082 - val_acc: 0.9153\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3214 - acc: 0.8889 - val_loss: 0.2817 - val_acc: 0.9206\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2992 - acc: 0.8989 - val_loss: 0.2587 - val_acc: 0.9524\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2801 - acc: 0.9060 - val_loss: 0.2360 - val_acc: 0.9524\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2611 - acc: 0.9143 - val_loss: 0.2237 - val_acc: 0.9577\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2456 - acc: 0.9190 - val_loss: 0.2018 - val_acc: 0.9577\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2313 - acc: 0.9232 - val_loss: 0.1911 - val_acc: 0.9524\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2214 - acc: 0.9261 - val_loss: 0.1843 - val_acc: 0.9630\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2116 - acc: 0.9309 - val_loss: 0.1674 - val_acc: 0.9524\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2004 - acc: 0.9320 - val_loss: 0.1709 - val_acc: 0.9630\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1937 - acc: 0.9374 - val_loss: 0.1528 - val_acc: 0.9577\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1857 - acc: 0.9374 - val_loss: 0.1462 - val_acc: 0.9577\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1797 - acc: 0.9391 - val_loss: 0.1448 - val_acc: 0.9630\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1731 - acc: 0.9433 - val_loss: 0.1383 - val_acc: 0.9577\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1667 - acc: 0.9444 - val_loss: 0.1374 - val_acc: 0.9630\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1606 - acc: 0.9492 - val_loss: 0.1291 - val_acc: 0.9577\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1556 - acc: 0.9486 - val_loss: 0.1267 - val_acc: 0.9630\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1498 - acc: 0.9509 - val_loss: 0.1238 - val_acc: 0.9630\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1476 - acc: 0.9521 - val_loss: 0.1203 - val_acc: 0.9577\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1422 - acc: 0.9533 - val_loss: 0.1180 - val_acc: 0.9683\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1378 - acc: 0.9563 - val_loss: 0.1282 - val_acc: 0.9735\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1327 - acc: 0.9580 - val_loss: 0.1412 - val_acc: 0.9735\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1303 - acc: 0.9604 - val_loss: 0.1184 - val_acc: 0.9735\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1241 - acc: 0.9663 - val_loss: 0.1091 - val_acc: 0.9683\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1224 - acc: 0.9616 - val_loss: 0.1051 - val_acc: 0.9735\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1185 - acc: 0.9669 - val_loss: 0.1039 - val_acc: 0.9735\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1167 - acc: 0.9639 - val_loss: 0.1014 - val_acc: 0.9735\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1115 - acc: 0.9681 - val_loss: 0.1090 - val_acc: 0.9788\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1115 - acc: 0.9669 - val_loss: 0.1148 - val_acc: 0.9841\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1091 - acc: 0.9669 - val_loss: 0.0961 - val_acc: 0.9788\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1047 - acc: 0.9740 - val_loss: 0.0928 - val_acc: 0.9788\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1020 - acc: 0.9710 - val_loss: 0.1122 - val_acc: 0.9841\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0989 - acc: 0.9716 - val_loss: 0.0899 - val_acc: 0.9841\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0974 - acc: 0.9722 - val_loss: 0.0970 - val_acc: 0.9841\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0962 - acc: 0.9746 - val_loss: 0.0887 - val_acc: 0.9841\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0938 - acc: 0.9746 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0916 - acc: 0.9758 - val_loss: 0.0845 - val_acc: 0.9841\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0878 - acc: 0.9764 - val_loss: 0.0878 - val_acc: 0.9841\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0889 - acc: 0.9752 - val_loss: 0.0822 - val_acc: 0.9841\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0863 - acc: 0.9770 - val_loss: 0.0807 - val_acc: 0.9841\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0854 - acc: 0.9793 - val_loss: 0.0793 - val_acc: 0.9841\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0809 - acc: 0.9770 - val_loss: 0.0921 - val_acc: 0.9841\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0815 - acc: 0.9811 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0767 - acc: 0.9811 - val_loss: 0.0755 - val_acc: 0.9841\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0771 - acc: 0.9799 - val_loss: 0.0771 - val_acc: 0.9841\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0748 - acc: 0.9811 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0757 - acc: 0.9805 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0715 - acc: 0.9823 - val_loss: 0.0750 - val_acc: 0.9841\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0726 - acc: 0.9811 - val_loss: 0.0718 - val_acc: 0.9841\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0706 - acc: 0.9817 - val_loss: 0.0701 - val_acc: 0.9841\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0686 - acc: 0.9835 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0677 - acc: 0.9823 - val_loss: 0.0903 - val_acc: 0.9841\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0663 - acc: 0.9829 - val_loss: 0.0666 - val_acc: 0.9841\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0663 - acc: 0.9846 - val_loss: 0.0658 - val_acc: 0.9841\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0651 - acc: 0.9852 - val_loss: 0.0645 - val_acc: 0.9841\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0622 - acc: 0.9840 - val_loss: 0.0995 - val_acc: 0.9841\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0642 - acc: 0.9840 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0622 - acc: 0.9840 - val_loss: 0.0692 - val_acc: 0.9841\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0608 - acc: 0.9858 - val_loss: 0.0664 - val_acc: 0.9841\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0589 - acc: 0.9852 - val_loss: 0.0639 - val_acc: 0.9841\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0576 - acc: 0.9858 - val_loss: 0.0635 - val_acc: 0.9841\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0567 - acc: 0.9864 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0577 - acc: 0.9876 - val_loss: 0.0663 - val_acc: 0.9841\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0570 - acc: 0.9864 - val_loss: 0.0625 - val_acc: 0.9841\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0566 - acc: 0.9852 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0549 - acc: 0.9864 - val_loss: 0.0632 - val_acc: 0.9841\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0546 - acc: 0.9882 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0544 - acc: 0.9882 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0500 - acc: 0.9882 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0519 - acc: 0.9864 - val_loss: 0.0597 - val_acc: 0.9841\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0517 - acc: 0.9888 - val_loss: 0.0619 - val_acc: 0.9841\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0521 - acc: 0.9882 - val_loss: 0.0586 - val_acc: 0.9841\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0518 - acc: 0.9882 - val_loss: 0.0590 - val_acc: 0.9841\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0492 - acc: 0.9894 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0506 - acc: 0.9882 - val_loss: 0.0615 - val_acc: 0.9841\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0487 - acc: 0.9882 - val_loss: 0.0602 - val_acc: 0.9841\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0484 - acc: 0.9882 - val_loss: 0.0562 - val_acc: 0.9841\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0492 - acc: 0.9894 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0478 - acc: 0.9894 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0488 - acc: 0.9888 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0453 - acc: 0.9894 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0450 - acc: 0.9905 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0474 - acc: 0.9888 - val_loss: 0.0590 - val_acc: 0.9841\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0464 - acc: 0.9882 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0460 - acc: 0.9905 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0434 - acc: 0.9900 - val_loss: 0.0643 - val_acc: 0.9841\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0456 - acc: 0.9905 - val_loss: 0.0567 - val_acc: 0.9841\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0467 - acc: 0.9900 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0440 - acc: 0.9905 - val_loss: 0.0571 - val_acc: 0.9841\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0458 - acc: 0.9900 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0445 - acc: 0.9900 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0441 - acc: 0.9900 - val_loss: 0.0550 - val_acc: 0.9841\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0535 - val_acc: 0.9841\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0429 - acc: 0.9917 - val_loss: 0.0603 - val_acc: 0.9841\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0424 - acc: 0.9905 - val_loss: 0.0539 - val_acc: 0.9841\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0414 - acc: 0.9905 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0427 - acc: 0.9905 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0398 - acc: 0.9911 - val_loss: 0.0551 - val_acc: 0.9841\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0403 - acc: 0.9905 - val_loss: 0.0554 - val_acc: 0.9841\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0412 - acc: 0.9911 - val_loss: 0.0544 - val_acc: 0.9841\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0549 - val_acc: 0.9841\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0402 - acc: 0.9917 - val_loss: 0.0647 - val_acc: 0.9841\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0412 - acc: 0.9911 - val_loss: 0.0597 - val_acc: 0.9841\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0393 - acc: 0.9911 - val_loss: 0.0600 - val_acc: 0.9841\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0401 - acc: 0.9917 - val_loss: 0.0612 - val_acc: 0.9841\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0415 - acc: 0.9923 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0408 - acc: 0.9917 - val_loss: 0.0554 - val_acc: 0.9841\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0403 - acc: 0.9923 - val_loss: 0.0530 - val_acc: 0.9841\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0404 - acc: 0.9917 - val_loss: 0.0588 - val_acc: 0.9841\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0382 - acc: 0.9923 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0393 - acc: 0.9923 - val_loss: 0.0533 - val_acc: 0.9841\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0385 - acc: 0.9911 - val_loss: 0.0626 - val_acc: 0.9841\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0394 - acc: 0.9923 - val_loss: 0.0501 - val_acc: 0.9841\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0525 - val_acc: 0.9841\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0382 - acc: 0.9917 - val_loss: 0.0524 - val_acc: 0.9841\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0341 - acc: 0.9929 - val_loss: 0.0619 - val_acc: 0.9841\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0368 - acc: 0.9917 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0380 - acc: 0.9929 - val_loss: 0.0544 - val_acc: 0.9841\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0361 - acc: 0.9929 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0538 - val_acc: 0.9841\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0379 - acc: 0.9917 - val_loss: 0.0487 - val_acc: 0.9841\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0489 - val_acc: 0.9841\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0373 - acc: 0.9929 - val_loss: 0.0512 - val_acc: 0.9841\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0357 - acc: 0.9917 - val_loss: 0.0566 - val_acc: 0.9841\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0475 - val_acc: 0.9841\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0513 - val_acc: 0.9841\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0365 - acc: 0.9929 - val_loss: 0.0530 - val_acc: 0.9841\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0375 - acc: 0.9917 - val_loss: 0.0479 - val_acc: 0.9841\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0353 - acc: 0.9929 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0370 - acc: 0.9929 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0339 - acc: 0.9923 - val_loss: 0.0529 - val_acc: 0.9841\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0352 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0355 - acc: 0.9923 - val_loss: 0.0463 - val_acc: 0.9841\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0355 - acc: 0.9923 - val_loss: 0.0467 - val_acc: 0.9841\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0326 - acc: 0.9923 - val_loss: 0.0524 - val_acc: 0.9841\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0358 - acc: 0.9935 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0317 - acc: 0.9923 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0509 - val_acc: 0.9841\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0342 - acc: 0.9929 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0341 - acc: 0.9929 - val_loss: 0.0543 - val_acc: 0.9841\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0345 - acc: 0.9929 - val_loss: 0.0560 - val_acc: 0.9841\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0346 - acc: 0.9923 - val_loss: 0.0547 - val_acc: 0.9841\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0353 - acc: 0.9929 - val_loss: 0.0533 - val_acc: 0.9841\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0338 - acc: 0.9929 - val_loss: 0.0620 - val_acc: 0.9841\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0342 - acc: 0.9923 - val_loss: 0.0442 - val_acc: 0.9841\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0325 - acc: 0.9929 - val_loss: 0.0843 - val_acc: 0.9947\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0339 - acc: 0.9923 - val_loss: 0.0482 - val_acc: 0.9841\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0348 - acc: 0.9917 - val_loss: 0.0429 - val_acc: 0.9841\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0329 - acc: 0.9923 - val_loss: 0.0545 - val_acc: 0.9894\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0328 - acc: 0.9935 - val_loss: 0.0525 - val_acc: 0.9841\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0329 - acc: 0.9923 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0344 - acc: 0.9929 - val_loss: 0.0519 - val_acc: 0.9841\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0330 - acc: 0.9923 - val_loss: 0.0436 - val_acc: 0.9841\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0324 - acc: 0.9929 - val_loss: 0.0496 - val_acc: 0.9841\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0340 - acc: 0.9929 - val_loss: 0.0469 - val_acc: 0.9841\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0490 - val_acc: 0.9841\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0337 - acc: 0.9929 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0323 - acc: 0.9923 - val_loss: 0.0483 - val_acc: 0.9841\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0528 - val_acc: 0.9841\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0326 - acc: 0.9929 - val_loss: 0.0471 - val_acc: 0.9841\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0338 - acc: 0.9923 - val_loss: 0.0480 - val_acc: 0.9841\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0333 - acc: 0.9929 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0322 - acc: 0.9929 - val_loss: 0.0588 - val_acc: 0.9841\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0329 - acc: 0.9929 - val_loss: 0.0451 - val_acc: 0.9894\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0322 - acc: 0.9923 - val_loss: 0.0534 - val_acc: 0.9841\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0310 - acc: 0.9935 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0333 - acc: 0.9929 - val_loss: 0.0434 - val_acc: 0.9841\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0330 - acc: 0.9935 - val_loss: 0.0452 - val_acc: 0.9841\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0312 - acc: 0.9935 - val_loss: 0.0507 - val_acc: 0.9841\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0314 - acc: 0.9935 - val_loss: 0.0395 - val_acc: 0.9841\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0323 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9841\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0310 - acc: 0.9935 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0317 - acc: 0.9929 - val_loss: 0.0425 - val_acc: 0.9841\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0321 - acc: 0.9929 - val_loss: 0.0389 - val_acc: 0.9841\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0305 - acc: 0.9935 - val_loss: 0.0354 - val_acc: 0.9841\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0433 - val_acc: 0.9841\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.0447 - val_acc: 0.9841\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0293 - acc: 0.9935 - val_loss: 0.0431 - val_acc: 0.9841\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0307 - acc: 0.9929 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0371 - val_acc: 0.9841\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0313 - acc: 0.9935 - val_loss: 0.0508 - val_acc: 0.9841\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0317 - acc: 0.9929 - val_loss: 0.0365 - val_acc: 0.9894\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0381 - val_acc: 0.9841\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0322 - acc: 0.9935 - val_loss: 0.0335 - val_acc: 0.9841\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0298 - acc: 0.9935 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0301 - acc: 0.9935 - val_loss: 0.0565 - val_acc: 0.9841\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0303 - acc: 0.9935 - val_loss: 0.0373 - val_acc: 0.9841\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0449 - val_acc: 0.9841\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0306 - acc: 0.9923 - val_loss: 0.0361 - val_acc: 0.9841\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0375 - val_acc: 0.9841\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0297 - acc: 0.9935 - val_loss: 0.0413 - val_acc: 0.9841\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0292 - acc: 0.9935 - val_loss: 0.0510 - val_acc: 0.9841\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0287 - acc: 0.9929 - val_loss: 0.0446 - val_acc: 0.9841\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0308 - acc: 0.9935 - val_loss: 0.0396 - val_acc: 0.9841\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0358 - val_acc: 0.9841\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0397 - val_acc: 0.9841\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0390 - val_acc: 0.9841\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0437 - val_acc: 0.9841\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0443 - val_acc: 0.9841\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0303 - val_acc: 0.9894\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0295 - acc: 0.9947 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0449 - val_acc: 0.9841\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0287 - val_acc: 0.9894\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0278 - acc: 0.9941 - val_loss: 0.0368 - val_acc: 0.9841\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0299 - acc: 0.9947 - val_loss: 0.0312 - val_acc: 0.9841\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0352 - val_acc: 0.9841\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0347 - val_acc: 0.9841\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0278 - acc: 0.9941 - val_loss: 0.0422 - val_acc: 0.9841\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0293 - acc: 0.9935 - val_loss: 0.0327 - val_acc: 0.9841\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0282 - acc: 0.9947 - val_loss: 0.0276 - val_acc: 0.9894\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0287 - val_acc: 0.9894\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0297 - acc: 0.9935 - val_loss: 0.0321 - val_acc: 0.9841\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0362 - val_acc: 0.9841\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0270 - acc: 0.9947 - val_loss: 0.0405 - val_acc: 0.9841\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0290 - acc: 0.9947 - val_loss: 0.0361 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0278 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9841\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0267 - acc: 0.9953 - val_loss: 0.0370 - val_acc: 0.9841\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0269 - acc: 0.9941 - val_loss: 0.0393 - val_acc: 0.9841\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9947 - val_loss: 0.0316 - val_acc: 0.9841\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0268 - acc: 0.9941 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0281 - acc: 0.9941 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0288 - acc: 0.9947 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0282 - acc: 0.9941 - val_loss: 0.0294 - val_acc: 0.9841\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0285 - val_acc: 0.9947\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0273 - acc: 0.9947 - val_loss: 0.0242 - val_acc: 0.9894\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0283 - acc: 0.9947 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0281 - acc: 0.9947 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0285 - acc: 0.9941 - val_loss: 0.0279 - val_acc: 0.9841\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0288 - acc: 0.9947 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0258 - acc: 0.9959 - val_loss: 0.0234 - val_acc: 0.9894\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0273 - acc: 0.9947 - val_loss: 0.0272 - val_acc: 0.9894\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0280 - acc: 0.9947 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0248 - acc: 0.9941 - val_loss: 0.0369 - val_acc: 0.9841\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0295 - acc: 0.9947 - val_loss: 0.0239 - val_acc: 0.9894\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0285 - acc: 0.9947 - val_loss: 0.0298 - val_acc: 0.9841\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0281 - acc: 0.9947 - val_loss: 0.0235 - val_acc: 0.9894\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0277 - acc: 0.9935 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0277 - acc: 0.9947 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0275 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0289 - val_acc: 0.9841\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0276 - acc: 0.9947 - val_loss: 0.0360 - val_acc: 0.9841\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0273 - acc: 0.9947 - val_loss: 0.0192 - val_acc: 0.9894\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0267 - acc: 0.9941 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0313 - val_acc: 0.9841\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0310 - val_acc: 0.9841\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0282 - acc: 0.9947 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0269 - acc: 0.9947 - val_loss: 0.0190 - val_acc: 0.9894\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0298 - val_acc: 0.9841\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0267 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9894\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0268 - acc: 0.9953 - val_loss: 0.0197 - val_acc: 0.9894\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0267 - acc: 0.9953 - val_loss: 0.0198 - val_acc: 0.9894\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0271 - acc: 0.9947 - val_loss: 0.0186 - val_acc: 0.9894\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0275 - acc: 0.9953 - val_loss: 0.0206 - val_acc: 0.9894\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0252 - acc: 0.9947 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0265 - acc: 0.9947 - val_loss: 0.0195 - val_acc: 0.9894\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0268 - acc: 0.9947 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0208 - val_acc: 0.9894\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0286 - val_acc: 0.9841\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9947 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0238 - val_acc: 0.9894\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0266 - acc: 0.9953 - val_loss: 0.0283 - val_acc: 0.9841\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0283 - acc: 0.9953 - val_loss: 0.0192 - val_acc: 0.9894\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0277 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 0.9894\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0259 - acc: 0.9947 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0256 - acc: 0.9953 - val_loss: 0.0176 - val_acc: 0.9894\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0211 - val_acc: 0.9894\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0169 - val_acc: 0.9894\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0262 - acc: 0.9947 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0273 - acc: 0.9947 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9953 - val_loss: 0.0171 - val_acc: 0.9947\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.0186 - val_acc: 0.9894\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0168 - val_acc: 0.9947\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0261 - acc: 0.9953 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0264 - acc: 0.9935 - val_loss: 0.0204 - val_acc: 0.9894\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0263 - acc: 0.9953 - val_loss: 0.0238 - val_acc: 0.9894\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0270 - acc: 0.9947 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9894\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0256 - acc: 0.9947 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0167 - val_acc: 0.9894\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0135 - val_acc: 0.9894\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0218 - val_acc: 0.9894\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0317 - acc: 0.9923 - val_loss: 0.0205 - val_acc: 0.9894\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.0141 - val_acc: 0.9894\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0262 - acc: 0.9947 - val_loss: 0.0138 - val_acc: 0.9894\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0289 - val_acc: 0.9841\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0180 - val_acc: 0.9894\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0242 - val_acc: 0.9894\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0263 - acc: 0.9947 - val_loss: 0.0230 - val_acc: 0.9894\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9894\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0259 - acc: 0.9947 - val_loss: 0.0147 - val_acc: 0.9894\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0263 - acc: 0.9947 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0268 - acc: 0.9947 - val_loss: 0.0185 - val_acc: 0.9894\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0249 - acc: 0.9941 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0230 - val_acc: 0.9894\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0277 - acc: 0.9953 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0264 - acc: 0.9947 - val_loss: 0.0135 - val_acc: 0.9894\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0242 - acc: 0.9959 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9894\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0252 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 1.0000\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0242 - acc: 0.9947 - val_loss: 0.0181 - val_acc: 0.9894\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0187 - val_acc: 0.9894\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0197 - val_acc: 0.9894\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0178 - val_acc: 0.9894\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0202 - val_acc: 0.9894\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0265 - acc: 0.9947 - val_loss: 0.0234 - val_acc: 0.9894\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9947 - val_loss: 0.0125 - val_acc: 0.9894\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0164 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0277 - acc: 0.9947 - val_loss: 0.0164 - val_acc: 0.9894\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0268 - acc: 0.9947 - val_loss: 0.0151 - val_acc: 0.9894\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0261 - acc: 0.9953 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0171 - val_acc: 0.9894\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9941 - val_loss: 0.0141 - val_acc: 0.9894\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0194 - val_acc: 0.9894\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0127 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0262 - acc: 0.9947 - val_loss: 0.0168 - val_acc: 0.9894\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0151 - val_acc: 0.9894\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9959 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9959 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0228 - acc: 0.9959 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0140 - val_acc: 0.9894\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0262 - acc: 0.9953 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0131 - val_acc: 0.9894\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0127 - val_acc: 0.9894\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0256 - acc: 0.9947 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0193 - val_acc: 0.9894\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0225 - val_acc: 0.9894\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0155 - val_acc: 0.9894\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0238 - acc: 0.9935 - val_loss: 0.0189 - val_acc: 0.9894\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0162 - val_acc: 0.9894\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0143 - val_acc: 0.9894\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0220 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9894\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0267 - acc: 0.9947 - val_loss: 0.0168 - val_acc: 0.9894\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0148 - val_acc: 0.9894\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0163 - val_acc: 0.9894\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9947 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0256 - acc: 0.9953 - val_loss: 0.0190 - val_acc: 0.9894\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0271 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9894\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0237 - acc: 0.9959 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 0.9894\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0199 - val_acc: 0.9894\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0276 - acc: 0.9941 - val_loss: 0.0212 - val_acc: 0.9894\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9959 - val_loss: 0.0192 - val_acc: 0.9894\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0167 - val_acc: 0.9894\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0167 - val_acc: 0.9894\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0237 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9894\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0181 - val_acc: 0.9894\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0266 - acc: 0.9947 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0176 - val_acc: 0.9894\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0214 - val_acc: 0.9894\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0230 - val_acc: 0.9894\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0263 - acc: 0.9953 - val_loss: 0.0153 - val_acc: 0.9894\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0252 - acc: 0.9959 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0232 - acc: 0.9959 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0197 - val_acc: 0.9894\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0272 - acc: 0.9947 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0105 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0145 - val_acc: 0.9947\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0257 - acc: 0.9947 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0205 - val_acc: 0.9894\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0261 - acc: 0.9953 - val_loss: 0.0141 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0096 - val_acc: 0.9947\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0172 - val_acc: 0.9894\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0169 - val_acc: 0.9894\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0234 - val_acc: 0.9894\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0090 - val_acc: 0.9947\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9959 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0269 - acc: 0.9935 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0219 - acc: 0.9959 - val_loss: 0.0211 - val_acc: 0.9894\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0094 - val_acc: 0.9947\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0108 - val_acc: 0.9947\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0255 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0149 - val_acc: 0.9947\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9965 - val_loss: 0.0102 - val_acc: 0.9947\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0241 - acc: 0.9959 - val_loss: 0.0212 - val_acc: 0.9894\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0186 - val_acc: 0.9894\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0094 - val_acc: 0.9947\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0099 - val_acc: 0.9947\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0139 - val_acc: 0.9947\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0139 - val_acc: 0.9947\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0096 - val_acc: 1.0000\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0158 - val_acc: 0.9947\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0089 - val_acc: 0.9947\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0260 - acc: 0.9953 - val_loss: 0.0109 - val_acc: 0.9947\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0148 - val_acc: 0.9947\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0262 - acc: 0.9947 - val_loss: 0.0092 - val_acc: 0.9947\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0238 - acc: 0.9959 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0113 - val_acc: 1.0000\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0232 - acc: 0.9959 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0242 - acc: 0.9947 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0108 - val_acc: 0.9947\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0110 - val_acc: 0.9947\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0294 - acc: 0.9935 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0137 - val_acc: 0.9947\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0264 - acc: 0.9953 - val_loss: 0.0118 - val_acc: 0.9947\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0112 - val_acc: 1.0000\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0108 - val_acc: 0.9947\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0231 - acc: 0.9959 - val_loss: 0.0153 - val_acc: 0.9947\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0120 - val_acc: 0.9947\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0146 - val_acc: 0.9947\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0101 - val_acc: 0.9947\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0149 - val_acc: 0.9947\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0146 - val_acc: 0.9947\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0146 - val_acc: 0.9947\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0113 - val_acc: 0.9947\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0092 - val_acc: 0.9947\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0108 - val_acc: 0.9947\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0109 - val_acc: 0.9947\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0147 - val_acc: 1.0000\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0131 - val_acc: 0.9947\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0124 - val_acc: 0.9947\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0105 - val_acc: 0.9947\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 1.0000\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0102 - val_acc: 0.9947\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.0159 - val_acc: 0.9947\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9959 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0232 - val_acc: 1.0000\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0092 - val_acc: 0.9947\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0243 - acc: 0.9959 - val_loss: 0.0166 - val_acc: 0.9947\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0208 - acc: 0.9959 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0220 - acc: 0.9959 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0227 - acc: 0.9959 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0110 - val_acc: 0.9947\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0237 - acc: 0.9959 - val_loss: 0.0120 - val_acc: 0.9947\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0242 - acc: 0.9959 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0235 - acc: 0.9941 - val_loss: 0.0145 - val_acc: 0.9947\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0158 - val_acc: 0.9947\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0143 - val_acc: 0.9947\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0141 - val_acc: 0.9947\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0259 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9941 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0141 - val_acc: 0.9947\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0244 - acc: 0.9959 - val_loss: 0.0123 - val_acc: 0.9947\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0219 - acc: 0.9959 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9941 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0082 - val_acc: 0.9947   \n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0236 - acc: 0.9941 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0135 - val_acc: 0.9947\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0172 - val_acc: 0.9947\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0081 - val_acc: 0.9947\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0101 - val_acc: 1.0000\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0131 - val_acc: 1.0000\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0087 - val_acc: 0.9947\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9959 - val_loss: 0.0099 - val_acc: 0.9947\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0077 - val_acc: 0.9947\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0125 - val_acc: 0.9947\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0109 - val_acc: 0.9947\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0086 - val_acc: 0.9947\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0164 - val_acc: 0.9947\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0083 - val_acc: 0.9947\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0140 - val_acc: 1.0000\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0227 - acc: 0.9959 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0109 - val_acc: 1.0000\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0085 - val_acc: 0.9947\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0095 - val_acc: 0.9947\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0089 - val_acc: 0.9947\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0081 - val_acc: 0.9947\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0076 - val_acc: 1.0000\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0072 - val_acc: 0.9947\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0080 - val_acc: 0.9947\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0072 - val_acc: 0.9947\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0246 - acc: 0.9947 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.0236 - acc: 0.9941 - val_loss: 0.0115 - val_acc: 1.0000\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 155us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0099 - val_acc: 0.9947\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0139 - val_acc: 0.9947\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0225 - acc: 0.9959 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 139us/step - loss: 0.0241 - acc: 0.9941 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0084 - val_acc: 0.9947\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0106 - val_acc: 1.0000\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0126 - val_acc: 0.9947\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0097 - val_acc: 1.0000\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0241 - acc: 0.9935 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0111 - val_acc: 1.0000\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0234 - acc: 0.9953 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0084 - val_acc: 0.9947\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0229 - acc: 0.9941 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9959 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0230 - acc: 0.9941 - val_loss: 0.0067 - val_acc: 0.9947\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0096 - val_acc: 0.9947\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0235 - acc: 0.9947 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0074 - val_acc: 0.9947\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0099 - val_acc: 1.0000\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0098 - val_acc: 0.9947\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0066 - val_acc: 1.0000\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0197 - acc: 0.9941 - val_loss: 0.0077 - val_acc: 0.9947\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0087 - val_acc: 0.9947\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0247 - val_acc: 0.9947\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0247 - acc: 0.9941 - val_loss: 0.0057 - val_acc: 1.0000\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0094 - val_acc: 1.0000\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0058 - val_acc: 1.0000\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0121 - val_acc: 0.9947\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0177 - val_acc: 0.9947\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0241 - acc: 0.9941 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0080 - val_acc: 1.0000\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0124 - val_acc: 1.0000\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0107 - val_acc: 0.9947\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0074 - val_acc: 0.9947\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.0082 - val_acc: 0.9947\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.0078 - val_acc: 0.9947\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0078 - val_acc: 0.9947\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0115 - val_acc: 0.9947\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0220 - acc: 0.9941 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0080 - val_acc: 0.9947\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0117 - val_acc: 0.9947\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.0059 - val_acc: 1.0000\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0223 - acc: 0.9941 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0077 - val_acc: 0.9947\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0095 - val_acc: 1.0000\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0160 - val_acc: 0.9947\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0205 - acc: 0.9947 - val_loss: 0.0197 - val_acc: 1.0000\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0072 - val_acc: 0.9947\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0053 - val_acc: 1.0000\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0114 - val_acc: 1.0000\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0200 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9941 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0096 - val_acc: 0.9947\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0112 - val_acc: 0.9947\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0075 - val_acc: 0.9947\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.0074 - val_acc: 0.9947\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9935 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0143 - val_acc: 0.9947\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0194 - acc: 0.9941 - val_loss: 0.0081 - val_acc: 1.0000\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0181 - acc: 0.9941 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0114 - val_acc: 0.9947\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0072 - val_acc: 0.9947\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0194 - acc: 0.9935 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0161 - acc: 0.9953 - val_loss: 0.0090 - val_acc: 0.9947\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0074 - val_acc: 0.9947\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0093 - val_acc: 0.9947\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0158 - acc: 0.9941 - val_loss: 0.0055 - val_acc: 0.9947\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0147 - acc: 0.9965 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0137 - acc: 0.9959 - val_loss: 0.0089 - val_acc: 0.9947\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0100 - val_acc: 0.9947\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0058 - val_acc: 0.9947\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0153 - acc: 0.9947 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0165 - acc: 0.9947 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0164 - acc: 0.9935 - val_loss: 0.0142 - val_acc: 0.9947\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0159 - acc: 0.9953 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0161 - acc: 0.9941 - val_loss: 0.0055 - val_acc: 0.9947\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0143 - acc: 0.9959 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0143 - acc: 0.9965 - val_loss: 0.0058 - val_acc: 0.9947\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 0.0058 - val_acc: 0.9947\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0148 - acc: 0.9947 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0151 - acc: 0.9947 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0092 - val_acc: 0.9947\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0137 - acc: 0.9947 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0078 - val_acc: 0.9947\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0141 - acc: 0.9965 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0077 - val_acc: 0.9947\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0131 - acc: 0.9953 - val_loss: 0.0115 - val_acc: 1.0000\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0139 - acc: 0.9947 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0104 - val_acc: 0.9947\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0143 - acc: 0.9947 - val_loss: 0.0057 - val_acc: 0.9947\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0060 - val_acc: 1.0000\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0139 - val_acc: 0.9894\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.0062 - val_acc: 0.9947\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0129 - val_acc: 0.9947\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0138 - acc: 0.9959 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0132 - acc: 0.9965 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0123 - acc: 0.9959 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0116 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0082 - val_acc: 0.9947\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0067 - val_acc: 0.9947\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0120 - acc: 0.9965 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0057 - val_acc: 0.9947\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0086 - val_acc: 0.9947\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 0.0087 - val_acc: 0.9947\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.0057 - val_acc: 0.9947\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0097 - val_acc: 0.9947\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0068 - val_acc: 0.9947\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0121 - acc: 0.9953 - val_loss: 0.0068 - val_acc: 0.9947\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0194 - val_acc: 0.9894\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0108 - acc: 0.9982 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0115 - acc: 0.9976 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0124 - acc: 0.9953 - val_loss: 0.0071 - val_acc: 0.9947\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 0.0060 - val_acc: 0.9947\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0110 - acc: 0.9976 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0112 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0110 - acc: 0.9982 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0100 - acc: 0.9976 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0125 - acc: 0.9959 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0099 - acc: 0.9982 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0117 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0112 - acc: 0.9959 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0122 - val_acc: 0.9947\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0066 - val_acc: 0.9947\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0076 - val_acc: 0.9947\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0079 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0107 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0102 - acc: 0.9982 - val_loss: 0.0051 - val_acc: 1.0000\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0096 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0105 - acc: 0.9976 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0104 - acc: 0.9982 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9947\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0111 - val_acc: 0.9947\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0124 - val_acc: 0.9947\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0069 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0091 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0092 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0111 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0101 - acc: 0.9976 - val_loss: 0.0088 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0099 - val_acc: 0.9947\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0097 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0106 - acc: 0.9982 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0097 - acc: 0.9982 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0102 - acc: 0.9982 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0063 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0065 - val_acc: 0.9947\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9947\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0027 - val_acc: 1.0000\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0147 - val_acc: 0.9947\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0067 - val_acc: 1.0000\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0095 - acc: 0.9965 - val_loss: 0.0103 - val_acc: 0.9947\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0089 - acc: 0.9982 - val_loss: 0.0059 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0081 - val_acc: 0.9947\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0070 - val_acc: 0.9947\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0085 - acc: 0.9982 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0089 - acc: 0.9976 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0094 - acc: 0.9976 - val_loss: 0.0039 - val_acc: 1.0000\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0036 - val_acc: 1.0000\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0093 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0133 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0085 - val_acc: 0.9947\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0087 - acc: 0.9976 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0100 - acc: 0.9965 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0087 - acc: 0.9982 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.0024 - val_acc: 1.0000\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.0066 - val_acc: 0.9947\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0105 - val_acc: 0.9947\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.0046 - val_acc: 0.9947\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0090 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 1.0000\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0084 - acc: 0.9976 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0142 - val_acc: 0.9947\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 426us/step - loss: 0.6221 - acc: 0.6921 - val_loss: 0.5782 - val_acc: 0.7619\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.5403 - acc: 0.7660 - val_loss: 0.4974 - val_acc: 0.8307\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4731 - acc: 0.8162 - val_loss: 0.4339 - val_acc: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.4256 - acc: 0.839 - 0s 59us/step - loss: 0.4255 - acc: 0.8416 - val_loss: 0.3884 - val_acc: 0.9048\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3924 - acc: 0.8611 - val_loss: 0.3549 - val_acc: 0.9101\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3682 - acc: 0.8717 - val_loss: 0.3290 - val_acc: 0.9206\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3490 - acc: 0.8818 - val_loss: 0.3097 - val_acc: 0.9153\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3329 - acc: 0.8889 - val_loss: 0.2957 - val_acc: 0.9101\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3200 - acc: 0.8942 - val_loss: 0.2795 - val_acc: 0.9259\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3075 - acc: 0.8978 - val_loss: 0.2674 - val_acc: 0.9365\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2970 - acc: 0.9001 - val_loss: 0.2577 - val_acc: 0.9206\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2887 - acc: 0.9031 - val_loss: 0.2473 - val_acc: 0.9365\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2802 - acc: 0.9072 - val_loss: 0.2425 - val_acc: 0.9365\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2730 - acc: 0.9096 - val_loss: 0.2310 - val_acc: 0.9312\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2657 - acc: 0.9113 - val_loss: 0.2248 - val_acc: 0.9365\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2590 - acc: 0.9131 - val_loss: 0.2213 - val_acc: 0.9365\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2532 - acc: 0.9196 - val_loss: 0.2125 - val_acc: 0.9365\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2476 - acc: 0.9190 - val_loss: 0.2074 - val_acc: 0.9471\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2420 - acc: 0.9226 - val_loss: 0.2023 - val_acc: 0.9471\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2371 - acc: 0.9249 - val_loss: 0.1969 - val_acc: 0.9418\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2322 - acc: 0.9249 - val_loss: 0.1955 - val_acc: 0.9471\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2280 - acc: 0.9285 - val_loss: 0.1930 - val_acc: 0.9471\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2239 - acc: 0.9314 - val_loss: 0.1840 - val_acc: 0.9524\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2193 - acc: 0.9291 - val_loss: 0.1836 - val_acc: 0.9524\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2159 - acc: 0.9332 - val_loss: 0.1768 - val_acc: 0.9524\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2124 - acc: 0.9332 - val_loss: 0.1751 - val_acc: 0.9577\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2095 - acc: 0.9362 - val_loss: 0.1701 - val_acc: 0.9577\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2055 - acc: 0.9362 - val_loss: 0.1692 - val_acc: 0.9630\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2027 - acc: 0.9379 - val_loss: 0.1650 - val_acc: 0.9630\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1998 - acc: 0.9391 - val_loss: 0.1632 - val_acc: 0.9683\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1969 - acc: 0.9397 - val_loss: 0.1588 - val_acc: 0.9683\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1941 - acc: 0.9391 - val_loss: 0.1577 - val_acc: 0.9683\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1916 - acc: 0.9415 - val_loss: 0.1559 - val_acc: 0.9683\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1888 - acc: 0.9444 - val_loss: 0.1514 - val_acc: 0.9683\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1867 - acc: 0.9433 - val_loss: 0.1494 - val_acc: 0.9683\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1841 - acc: 0.9439 - val_loss: 0.1483 - val_acc: 0.9683\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1818 - acc: 0.9450 - val_loss: 0.1457 - val_acc: 0.9735\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1796 - acc: 0.9456 - val_loss: 0.1450 - val_acc: 0.9735\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1776 - acc: 0.9456 - val_loss: 0.1429 - val_acc: 0.9735\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1756 - acc: 0.9462 - val_loss: 0.1430 - val_acc: 0.9735\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1732 - acc: 0.9468 - val_loss: 0.1384 - val_acc: 0.9735\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1719 - acc: 0.9468 - val_loss: 0.1380 - val_acc: 0.9735\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1701 - acc: 0.9486 - val_loss: 0.1362 - val_acc: 0.9735\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1684 - acc: 0.9486 - val_loss: 0.1358 - val_acc: 0.9735\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1666 - acc: 0.9480 - val_loss: 0.1358 - val_acc: 0.9735\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1651 - acc: 0.9504 - val_loss: 0.1346 - val_acc: 0.9735\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1632 - acc: 0.9492 - val_loss: 0.1324 - val_acc: 0.9735\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1617 - acc: 0.9498 - val_loss: 0.1337 - val_acc: 0.9735\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1602 - acc: 0.9498 - val_loss: 0.1295 - val_acc: 0.9735\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1589 - acc: 0.9498 - val_loss: 0.1314 - val_acc: 0.9735\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1572 - acc: 0.9515 - val_loss: 0.1313 - val_acc: 0.9735\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1562 - acc: 0.9527 - val_loss: 0.1291 - val_acc: 0.9735\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1547 - acc: 0.9533 - val_loss: 0.1261 - val_acc: 0.9735\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1533 - acc: 0.9521 - val_loss: 0.1250 - val_acc: 0.9735\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1522 - acc: 0.9539 - val_loss: 0.1249 - val_acc: 0.9735\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1508 - acc: 0.9563 - val_loss: 0.1228 - val_acc: 0.9735\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1497 - acc: 0.9539 - val_loss: 0.1234 - val_acc: 0.9735\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1481 - acc: 0.9586 - val_loss: 0.1208 - val_acc: 0.9735\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1469 - acc: 0.9557 - val_loss: 0.1220 - val_acc: 0.9735\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1460 - acc: 0.9574 - val_loss: 0.1212 - val_acc: 0.9735\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1447 - acc: 0.9580 - val_loss: 0.1204 - val_acc: 0.9735\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1436 - acc: 0.9592 - val_loss: 0.1194 - val_acc: 0.9735\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1424 - acc: 0.9586 - val_loss: 0.1209 - val_acc: 0.9735\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1411 - acc: 0.9598 - val_loss: 0.1174 - val_acc: 0.9735\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1401 - acc: 0.9574 - val_loss: 0.1214 - val_acc: 0.9788\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1392 - acc: 0.9622 - val_loss: 0.1154 - val_acc: 0.9735\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1384 - acc: 0.9586 - val_loss: 0.1168 - val_acc: 0.9735\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1371 - acc: 0.9622 - val_loss: 0.1137 - val_acc: 0.9735\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1366 - acc: 0.9610 - val_loss: 0.1139 - val_acc: 0.9735\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1355 - acc: 0.9628 - val_loss: 0.1134 - val_acc: 0.9735\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1345 - acc: 0.9628 - val_loss: 0.1155 - val_acc: 0.9788\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1335 - acc: 0.9628 - val_loss: 0.1137 - val_acc: 0.9788\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1328 - acc: 0.9628 - val_loss: 0.1121 - val_acc: 0.9788\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1317 - acc: 0.9634 - val_loss: 0.1148 - val_acc: 0.9788\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1308 - acc: 0.9645 - val_loss: 0.1118 - val_acc: 0.9788\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1298 - acc: 0.9639 - val_loss: 0.1085 - val_acc: 0.9735\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1291 - acc: 0.9628 - val_loss: 0.1132 - val_acc: 0.9788\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.1285 - acc: 0.9639 - val_loss: 0.1107 - val_acc: 0.9788\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1275 - acc: 0.9639 - val_loss: 0.1109 - val_acc: 0.9788\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1268 - acc: 0.9651 - val_loss: 0.1080 - val_acc: 0.9788\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1258 - acc: 0.9645 - val_loss: 0.1099 - val_acc: 0.9788\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1250 - acc: 0.9657 - val_loss: 0.1116 - val_acc: 0.9841\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1245 - acc: 0.9663 - val_loss: 0.1094 - val_acc: 0.9788\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1235 - acc: 0.9657 - val_loss: 0.1054 - val_acc: 0.9788\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1231 - acc: 0.9663 - val_loss: 0.1066 - val_acc: 0.9788\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1223 - acc: 0.9657 - val_loss: 0.1077 - val_acc: 0.9788\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1219 - acc: 0.9645 - val_loss: 0.1079 - val_acc: 0.9841\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1209 - acc: 0.9663 - val_loss: 0.1051 - val_acc: 0.9788\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1203 - acc: 0.9663 - val_loss: 0.1062 - val_acc: 0.9841\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1196 - acc: 0.9657 - val_loss: 0.1054 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1187 - acc: 0.9663 - val_loss: 0.1080 - val_acc: 0.9841\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1183 - acc: 0.9657 - val_loss: 0.1052 - val_acc: 0.9841\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1177 - acc: 0.9669 - val_loss: 0.1043 - val_acc: 0.9841\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1171 - acc: 0.9669 - val_loss: 0.1027 - val_acc: 0.9841\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1163 - acc: 0.9669 - val_loss: 0.1055 - val_acc: 0.9841\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1156 - acc: 0.9669 - val_loss: 0.1021 - val_acc: 0.9841\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1155 - acc: 0.9669 - val_loss: 0.1031 - val_acc: 0.9841\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1146 - acc: 0.9669 - val_loss: 0.1024 - val_acc: 0.9841\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1141 - acc: 0.9669 - val_loss: 0.1044 - val_acc: 0.9841\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1135 - acc: 0.9669 - val_loss: 0.1051 - val_acc: 0.9841\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1134 - acc: 0.9669 - val_loss: 0.1039 - val_acc: 0.9841\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1124 - acc: 0.9669 - val_loss: 0.1029 - val_acc: 0.9841\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1117 - acc: 0.9669 - val_loss: 0.1006 - val_acc: 0.9841\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1115 - acc: 0.9669 - val_loss: 0.1008 - val_acc: 0.9841\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1105 - acc: 0.9675 - val_loss: 0.0985 - val_acc: 0.9841\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1103 - acc: 0.9669 - val_loss: 0.1028 - val_acc: 0.9841\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1098 - acc: 0.9669 - val_loss: 0.1004 - val_acc: 0.9841\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1093 - acc: 0.9669 - val_loss: 0.0988 - val_acc: 0.9841\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1085 - acc: 0.9681 - val_loss: 0.1017 - val_acc: 0.9841\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1082 - acc: 0.9675 - val_loss: 0.0998 - val_acc: 0.9841\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1078 - acc: 0.9669 - val_loss: 0.0995 - val_acc: 0.9841\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1071 - acc: 0.9681 - val_loss: 0.0977 - val_acc: 0.9841\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1068 - acc: 0.9687 - val_loss: 0.0986 - val_acc: 0.9841\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1063 - acc: 0.9681 - val_loss: 0.1008 - val_acc: 0.9841\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 155us/step - loss: 0.1058 - acc: 0.9681 - val_loss: 0.0983 - val_acc: 0.9841\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1053 - acc: 0.9693 - val_loss: 0.0975 - val_acc: 0.9841\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1049 - acc: 0.9681 - val_loss: 0.0963 - val_acc: 0.9841\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1047 - acc: 0.9693 - val_loss: 0.0979 - val_acc: 0.9841\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.1040 - acc: 0.9693 - val_loss: 0.0996 - val_acc: 0.9841\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1032 - acc: 0.9699 - val_loss: 0.0958 - val_acc: 0.9841\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1032 - acc: 0.9693 - val_loss: 0.0962 - val_acc: 0.9841\n",
      "Epoch 122/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1027 - acc: 0.9681 - val_loss: 0.0972 - val_acc: 0.9841\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1023 - acc: 0.9699 - val_loss: 0.0977 - val_acc: 0.9841\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1019 - acc: 0.9693 - val_loss: 0.0953 - val_acc: 0.9841\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1014 - acc: 0.9693 - val_loss: 0.0949 - val_acc: 0.9841\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1010 - acc: 0.9699 - val_loss: 0.0941 - val_acc: 0.9841\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1009 - acc: 0.9699 - val_loss: 0.0954 - val_acc: 0.9841\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1004 - acc: 0.9704 - val_loss: 0.0960 - val_acc: 0.9841\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0998 - acc: 0.9699 - val_loss: 0.0954 - val_acc: 0.9841\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0995 - acc: 0.9710 - val_loss: 0.0954 - val_acc: 0.9841\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0991 - acc: 0.9710 - val_loss: 0.0943 - val_acc: 0.9841\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0986 - acc: 0.9710 - val_loss: 0.0945 - val_acc: 0.9841\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0980 - acc: 0.9699 - val_loss: 0.0978 - val_acc: 0.9841\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0978 - acc: 0.9710 - val_loss: 0.0930 - val_acc: 0.9841\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0976 - acc: 0.9710 - val_loss: 0.0929 - val_acc: 0.9841\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0973 - acc: 0.9704 - val_loss: 0.0948 - val_acc: 0.9841\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0966 - acc: 0.9710 - val_loss: 0.0918 - val_acc: 0.9841\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0965 - acc: 0.9710 - val_loss: 0.0939 - val_acc: 0.9841\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0962 - acc: 0.9716 - val_loss: 0.0927 - val_acc: 0.9841\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0959 - acc: 0.9704 - val_loss: 0.0949 - val_acc: 0.9841\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0954 - acc: 0.9716 - val_loss: 0.0927 - val_acc: 0.9841\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0951 - acc: 0.9710 - val_loss: 0.0931 - val_acc: 0.9841\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0947 - acc: 0.9710 - val_loss: 0.0932 - val_acc: 0.9841\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0941 - acc: 0.9716 - val_loss: 0.0912 - val_acc: 0.9841\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0940 - acc: 0.9716 - val_loss: 0.0931 - val_acc: 0.9841\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0936 - acc: 0.9722 - val_loss: 0.0916 - val_acc: 0.9841\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0935 - acc: 0.9716 - val_loss: 0.0911 - val_acc: 0.9841\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0931 - acc: 0.9716 - val_loss: 0.0917 - val_acc: 0.9841\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0927 - acc: 0.9716 - val_loss: 0.0915 - val_acc: 0.9841\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0923 - acc: 0.9716 - val_loss: 0.0925 - val_acc: 0.9841\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0921 - acc: 0.9722 - val_loss: 0.0920 - val_acc: 0.9841\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0917 - acc: 0.9728 - val_loss: 0.0909 - val_acc: 0.9841\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0912 - acc: 0.9722 - val_loss: 0.0939 - val_acc: 0.9841\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0910 - acc: 0.9734 - val_loss: 0.0900 - val_acc: 0.9841\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0907 - acc: 0.9716 - val_loss: 0.0906 - val_acc: 0.9841\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0905 - acc: 0.9722 - val_loss: 0.0907 - val_acc: 0.9841\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0902 - acc: 0.9722 - val_loss: 0.0905 - val_acc: 0.9841\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0899 - acc: 0.9722 - val_loss: 0.0911 - val_acc: 0.9841\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0896 - acc: 0.9734 - val_loss: 0.0901 - val_acc: 0.9841\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0892 - acc: 0.9722 - val_loss: 0.0919 - val_acc: 0.9841\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0887 - acc: 0.9740 - val_loss: 0.0888 - val_acc: 0.9841\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0886 - acc: 0.9728 - val_loss: 0.0907 - val_acc: 0.9841\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0883 - acc: 0.9728 - val_loss: 0.0912 - val_acc: 0.9841\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0880 - acc: 0.9734 - val_loss: 0.0888 - val_acc: 0.9841\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0878 - acc: 0.9728 - val_loss: 0.0895 - val_acc: 0.9841\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0875 - acc: 0.9722 - val_loss: 0.0905 - val_acc: 0.9841\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0874 - acc: 0.9740 - val_loss: 0.0899 - val_acc: 0.9841\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0869 - acc: 0.9728 - val_loss: 0.0906 - val_acc: 0.9841\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0867 - acc: 0.9734 - val_loss: 0.0903 - val_acc: 0.9841\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0862 - acc: 0.9740 - val_loss: 0.0887 - val_acc: 0.9841\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0862 - acc: 0.9740 - val_loss: 0.0898 - val_acc: 0.9841\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0859 - acc: 0.9746 - val_loss: 0.0891 - val_acc: 0.9841\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0856 - acc: 0.9734 - val_loss: 0.0893 - val_acc: 0.9841\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.0897 - val_acc: 0.9841\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0851 - acc: 0.9752 - val_loss: 0.0877 - val_acc: 0.9841\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0848 - acc: 0.9740 - val_loss: 0.0885 - val_acc: 0.9841\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0845 - acc: 0.9746 - val_loss: 0.0880 - val_acc: 0.9841\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0842 - acc: 0.9758 - val_loss: 0.0878 - val_acc: 0.9841\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0842 - acc: 0.9740 - val_loss: 0.0890 - val_acc: 0.9841\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0836 - acc: 0.9752 - val_loss: 0.0882 - val_acc: 0.9841\n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0836 - acc: 0.9752 - val_loss: 0.0880 - val_acc: 0.9841\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0834 - acc: 0.9746 - val_loss: 0.0886 - val_acc: 0.9841\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0828 - acc: 0.9764 - val_loss: 0.0865 - val_acc: 0.9841\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0828 - acc: 0.9746 - val_loss: 0.0873 - val_acc: 0.9841\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0826 - acc: 0.9746 - val_loss: 0.0890 - val_acc: 0.9841\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0823 - acc: 0.9746 - val_loss: 0.0879 - val_acc: 0.9841\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0820 - acc: 0.9758 - val_loss: 0.0888 - val_acc: 0.9841\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0816 - acc: 0.9764 - val_loss: 0.0866 - val_acc: 0.9841\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0818 - acc: 0.9758 - val_loss: 0.0875 - val_acc: 0.9841\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0812 - acc: 0.9770 - val_loss: 0.0864 - val_acc: 0.9841\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0812 - acc: 0.9758 - val_loss: 0.0880 - val_acc: 0.9841\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0809 - acc: 0.9764 - val_loss: 0.0870 - val_acc: 0.9841\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0806 - acc: 0.9752 - val_loss: 0.0892 - val_acc: 0.9841\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0806 - acc: 0.9764 - val_loss: 0.0887 - val_acc: 0.9841\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0802 - acc: 0.9770 - val_loss: 0.0876 - val_acc: 0.9841\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0799 - acc: 0.9775 - val_loss: 0.0868 - val_acc: 0.9841\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0798 - acc: 0.9764 - val_loss: 0.0871 - val_acc: 0.9841\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0794 - acc: 0.9775 - val_loss: 0.0864 - val_acc: 0.9841\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0793 - acc: 0.9764 - val_loss: 0.0867 - val_acc: 0.9841\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0791 - acc: 0.9764 - val_loss: 0.0873 - val_acc: 0.9841\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0789 - acc: 0.9770 - val_loss: 0.0861 - val_acc: 0.9841\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0788 - acc: 0.9758 - val_loss: 0.0871 - val_acc: 0.9841\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0785 - acc: 0.9764 - val_loss: 0.0868 - val_acc: 0.9841\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0781 - acc: 0.9770 - val_loss: 0.0879 - val_acc: 0.9841\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0780 - acc: 0.9775 - val_loss: 0.0873 - val_acc: 0.9841\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0778 - acc: 0.9775 - val_loss: 0.0865 - val_acc: 0.9841\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0776 - acc: 0.9764 - val_loss: 0.0872 - val_acc: 0.9841\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0773 - acc: 0.9770 - val_loss: 0.0878 - val_acc: 0.9841\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0772 - acc: 0.9775 - val_loss: 0.0865 - val_acc: 0.9841\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0768 - acc: 0.9775 - val_loss: 0.0858 - val_acc: 0.9841\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0767 - acc: 0.9775 - val_loss: 0.0861 - val_acc: 0.9841\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0765 - acc: 0.9787 - val_loss: 0.0855 - val_acc: 0.9841\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0765 - acc: 0.9775 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0761 - acc: 0.9775 - val_loss: 0.0860 - val_acc: 0.9841\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0759 - acc: 0.9781 - val_loss: 0.0868 - val_acc: 0.9841\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0758 - acc: 0.9781 - val_loss: 0.0859 - val_acc: 0.9841\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0756 - acc: 0.9793 - val_loss: 0.0861 - val_acc: 0.9841\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0753 - acc: 0.9781 - val_loss: 0.0867 - val_acc: 0.9841\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0751 - acc: 0.9787 - val_loss: 0.0869 - val_acc: 0.9841\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0750 - acc: 0.9787 - val_loss: 0.0854 - val_acc: 0.9841\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0748 - acc: 0.9787 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0744 - acc: 0.9787 - val_loss: 0.0849 - val_acc: 0.9841\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0744 - acc: 0.9787 - val_loss: 0.0847 - val_acc: 0.9841\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0741 - acc: 0.9787 - val_loss: 0.0854 - val_acc: 0.9841\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0741 - acc: 0.9787 - val_loss: 0.0850 - val_acc: 0.9841\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0737 - acc: 0.9787 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0736 - acc: 0.9793 - val_loss: 0.0851 - val_acc: 0.9841\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0734 - acc: 0.9793 - val_loss: 0.0847 - val_acc: 0.9841\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0733 - acc: 0.9793 - val_loss: 0.0849 - val_acc: 0.9841\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0729 - acc: 0.9787 - val_loss: 0.0858 - val_acc: 0.9841\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0730 - acc: 0.9793 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0728 - acc: 0.9793 - val_loss: 0.0847 - val_acc: 0.9841\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0725 - acc: 0.9793 - val_loss: 0.0841 - val_acc: 0.9841\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0723 - acc: 0.9793 - val_loss: 0.0837 - val_acc: 0.9841\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0723 - acc: 0.9793 - val_loss: 0.0846 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0718 - acc: 0.9805 - val_loss: 0.0834 - val_acc: 0.9841\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0719 - acc: 0.9787 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0718 - acc: 0.9793 - val_loss: 0.0843 - val_acc: 0.9841\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0714 - acc: 0.9793 - val_loss: 0.0841 - val_acc: 0.9841\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0714 - acc: 0.9793 - val_loss: 0.0845 - val_acc: 0.9841\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0712 - acc: 0.9799 - val_loss: 0.0839 - val_acc: 0.9841\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0710 - acc: 0.9793 - val_loss: 0.0831 - val_acc: 0.9841\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0708 - acc: 0.9793 - val_loss: 0.0836 - val_acc: 0.9841\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0706 - acc: 0.9793 - val_loss: 0.0834 - val_acc: 0.9841\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0703 - acc: 0.9793 - val_loss: 0.0828 - val_acc: 0.9841\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0703 - acc: 0.9793 - val_loss: 0.0844 - val_acc: 0.9841\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0702 - acc: 0.9793 - val_loss: 0.0837 - val_acc: 0.9841\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0700 - acc: 0.9793 - val_loss: 0.0837 - val_acc: 0.9841\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0698 - acc: 0.9799 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0697 - acc: 0.9793 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0694 - acc: 0.9799 - val_loss: 0.0830 - val_acc: 0.9841\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0693 - acc: 0.9799 - val_loss: 0.0838 - val_acc: 0.9841\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0691 - acc: 0.9805 - val_loss: 0.0830 - val_acc: 0.9841\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0692 - acc: 0.9799 - val_loss: 0.0828 - val_acc: 0.9841\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0689 - acc: 0.9799 - val_loss: 0.0841 - val_acc: 0.9841\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0687 - acc: 0.9805 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0687 - acc: 0.9805 - val_loss: 0.0838 - val_acc: 0.9841\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0685 - acc: 0.9805 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0682 - acc: 0.9793 - val_loss: 0.0842 - val_acc: 0.9841\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0682 - acc: 0.9805 - val_loss: 0.0831 - val_acc: 0.9841\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0680 - acc: 0.9811 - val_loss: 0.0825 - val_acc: 0.9841\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0680 - acc: 0.9805 - val_loss: 0.0830 - val_acc: 0.9841\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0678 - acc: 0.9805 - val_loss: 0.0839 - val_acc: 0.9841\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0675 - acc: 0.9817 - val_loss: 0.0828 - val_acc: 0.9841\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0674 - acc: 0.9805 - val_loss: 0.0834 - val_acc: 0.9841\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0672 - acc: 0.9823 - val_loss: 0.0825 - val_acc: 0.9841\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0672 - acc: 0.9811 - val_loss: 0.0826 - val_acc: 0.9841\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0670 - acc: 0.9817 - val_loss: 0.0829 - val_acc: 0.9841\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0667 - acc: 0.9823 - val_loss: 0.0820 - val_acc: 0.9841\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0667 - acc: 0.9811 - val_loss: 0.0824 - val_acc: 0.9841\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0664 - acc: 0.9811 - val_loss: 0.0824 - val_acc: 0.9841\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0664 - acc: 0.9811 - val_loss: 0.0827 - val_acc: 0.9841\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0662 - acc: 0.9811 - val_loss: 0.0831 - val_acc: 0.9841\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0662 - acc: 0.9817 - val_loss: 0.0835 - val_acc: 0.9841\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0659 - acc: 0.9823 - val_loss: 0.0829 - val_acc: 0.9841\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0657 - acc: 0.9823 - val_loss: 0.0820 - val_acc: 0.9841\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0657 - acc: 0.9817 - val_loss: 0.0824 - val_acc: 0.9841\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0655 - acc: 0.9823 - val_loss: 0.0822 - val_acc: 0.9841\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0654 - acc: 0.9823 - val_loss: 0.0818 - val_acc: 0.9841\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0652 - acc: 0.9823 - val_loss: 0.0821 - val_acc: 0.9841\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0651 - acc: 0.9823 - val_loss: 0.0823 - val_acc: 0.9841\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0649 - acc: 0.9835 - val_loss: 0.0817 - val_acc: 0.9841\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0647 - acc: 0.9823 - val_loss: 0.0825 - val_acc: 0.9841\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0647 - acc: 0.9840 - val_loss: 0.0822 - val_acc: 0.9841\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0645 - acc: 0.9835 - val_loss: 0.0816 - val_acc: 0.9841\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0644 - acc: 0.9829 - val_loss: 0.0820 - val_acc: 0.9841\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0643 - acc: 0.9840 - val_loss: 0.0816 - val_acc: 0.9841\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0642 - acc: 0.9840 - val_loss: 0.0822 - val_acc: 0.9841\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0640 - acc: 0.9835 - val_loss: 0.0826 - val_acc: 0.9841\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0638 - acc: 0.9835 - val_loss: 0.0823 - val_acc: 0.9841\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0637 - acc: 0.9840 - val_loss: 0.0818 - val_acc: 0.9841\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0636 - acc: 0.9835 - val_loss: 0.0819 - val_acc: 0.9841\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0635 - acc: 0.9840 - val_loss: 0.0823 - val_acc: 0.9841\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0633 - acc: 0.9852 - val_loss: 0.0812 - val_acc: 0.9841\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0633 - acc: 0.9840 - val_loss: 0.0816 - val_acc: 0.9841\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0631 - acc: 0.9846 - val_loss: 0.0814 - val_acc: 0.9841\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0630 - acc: 0.9840 - val_loss: 0.0810 - val_acc: 0.9841\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0629 - acc: 0.9840 - val_loss: 0.0819 - val_acc: 0.9841\n",
      "Epoch 299/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0627 - acc: 0.9835 - val_loss: 0.0818 - val_acc: 0.9841\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0626 - acc: 0.9852 - val_loss: 0.0809 - val_acc: 0.9841\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0624 - acc: 0.9846 - val_loss: 0.0806 - val_acc: 0.9841\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0623 - acc: 0.9840 - val_loss: 0.0812 - val_acc: 0.9841\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0622 - acc: 0.9846 - val_loss: 0.0806 - val_acc: 0.9841\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0621 - acc: 0.9846 - val_loss: 0.0806 - val_acc: 0.9841\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0620 - acc: 0.9846 - val_loss: 0.0806 - val_acc: 0.9841\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0619 - acc: 0.9840 - val_loss: 0.0812 - val_acc: 0.9841\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0617 - acc: 0.9852 - val_loss: 0.0810 - val_acc: 0.9841\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0616 - acc: 0.9852 - val_loss: 0.0804 - val_acc: 0.9841\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0615 - acc: 0.9846 - val_loss: 0.0814 - val_acc: 0.9841\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0615 - acc: 0.9840 - val_loss: 0.0805 - val_acc: 0.9841\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0612 - acc: 0.9852 - val_loss: 0.0803 - val_acc: 0.9841\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0612 - acc: 0.9840 - val_loss: 0.0811 - val_acc: 0.9841\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0610 - acc: 0.9852 - val_loss: 0.0803 - val_acc: 0.9841\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0609 - acc: 0.9846 - val_loss: 0.0798 - val_acc: 0.9841\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0608 - acc: 0.9846 - val_loss: 0.0796 - val_acc: 0.9841\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0607 - acc: 0.9846 - val_loss: 0.0808 - val_acc: 0.9841\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0607 - acc: 0.9852 - val_loss: 0.0808 - val_acc: 0.9841\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0605 - acc: 0.9852 - val_loss: 0.0800 - val_acc: 0.9841\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0603 - acc: 0.9852 - val_loss: 0.0800 - val_acc: 0.9841\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0603 - acc: 0.9852 - val_loss: 0.0803 - val_acc: 0.9841\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0600 - acc: 0.9846 - val_loss: 0.0796 - val_acc: 0.9841\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0601 - acc: 0.9852 - val_loss: 0.0799 - val_acc: 0.9841\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0599 - acc: 0.9846 - val_loss: 0.0801 - val_acc: 0.9841\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0600 - acc: 0.9852 - val_loss: 0.0797 - val_acc: 0.9841\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0597 - acc: 0.9840 - val_loss: 0.0802 - val_acc: 0.9841\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0596 - acc: 0.9846 - val_loss: 0.0795 - val_acc: 0.9841\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0596 - acc: 0.9852 - val_loss: 0.0795 - val_acc: 0.9841\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0595 - acc: 0.9846 - val_loss: 0.0797 - val_acc: 0.9841\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0593 - acc: 0.9852 - val_loss: 0.0802 - val_acc: 0.9841\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0592 - acc: 0.9852 - val_loss: 0.0792 - val_acc: 0.9841\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0591 - acc: 0.9852 - val_loss: 0.0789 - val_acc: 0.9841\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.0798 - val_acc: 0.9841\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0589 - acc: 0.9852 - val_loss: 0.0794 - val_acc: 0.9841\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0589 - acc: 0.9852 - val_loss: 0.0799 - val_acc: 0.9841\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0587 - acc: 0.9846 - val_loss: 0.0803 - val_acc: 0.9841\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0587 - acc: 0.9852 - val_loss: 0.0797 - val_acc: 0.9841\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0584 - acc: 0.9852 - val_loss: 0.0804 - val_acc: 0.9841\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0586 - acc: 0.9852 - val_loss: 0.0801 - val_acc: 0.9841\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0584 - acc: 0.9858 - val_loss: 0.0792 - val_acc: 0.9841\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0583 - acc: 0.9852 - val_loss: 0.0793 - val_acc: 0.9841\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0581 - acc: 0.9852 - val_loss: 0.0795 - val_acc: 0.9841\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0580 - acc: 0.9858 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0580 - acc: 0.9852 - val_loss: 0.0790 - val_acc: 0.9841\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0578 - acc: 0.9846 - val_loss: 0.0795 - val_acc: 0.9841\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0578 - acc: 0.9852 - val_loss: 0.0789 - val_acc: 0.9841\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0577 - acc: 0.9858 - val_loss: 0.0789 - val_acc: 0.9841\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.0577 - acc: 0.9858 - val_loss: 0.0790 - val_acc: 0.9841\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.0574 - acc: 0.9858 - val_loss: 0.0786 - val_acc: 0.9841\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0574 - acc: 0.9852 - val_loss: 0.0792 - val_acc: 0.9841\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.0573 - acc: 0.9852 - val_loss: 0.0791 - val_acc: 0.9841\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0573 - acc: 0.9852 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.0572 - acc: 0.9858 - val_loss: 0.0791 - val_acc: 0.9841\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0570 - acc: 0.9864 - val_loss: 0.0785 - val_acc: 0.9841\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0569 - acc: 0.9852 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0568 - acc: 0.9852 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0568 - acc: 0.9858 - val_loss: 0.0785 - val_acc: 0.9841\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0567 - acc: 0.9852 - val_loss: 0.0786 - val_acc: 0.9841\n",
      "Epoch 358/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0566 - acc: 0.9858 - val_loss: 0.0783 - val_acc: 0.9841\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0565 - acc: 0.9858 - val_loss: 0.0783 - val_acc: 0.9841\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0564 - acc: 0.9858 - val_loss: 0.0780 - val_acc: 0.9841\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0563 - acc: 0.9864 - val_loss: 0.0782 - val_acc: 0.9841\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0563 - acc: 0.9852 - val_loss: 0.0787 - val_acc: 0.9841\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0562 - acc: 0.9864 - val_loss: 0.0785 - val_acc: 0.9841\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0561 - acc: 0.9858 - val_loss: 0.0788 - val_acc: 0.9841\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0560 - acc: 0.9864 - val_loss: 0.0779 - val_acc: 0.9841\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0559 - acc: 0.9858 - val_loss: 0.0781 - val_acc: 0.9841\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0558 - acc: 0.9858 - val_loss: 0.0782 - val_acc: 0.9841\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0558 - acc: 0.9864 - val_loss: 0.0784 - val_acc: 0.9841\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0557 - acc: 0.9858 - val_loss: 0.0786 - val_acc: 0.9841\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0556 - acc: 0.9864 - val_loss: 0.0786 - val_acc: 0.9841\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0556 - acc: 0.9864 - val_loss: 0.0780 - val_acc: 0.9841\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0554 - acc: 0.9858 - val_loss: 0.0787 - val_acc: 0.9841\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0554 - acc: 0.9864 - val_loss: 0.0785 - val_acc: 0.9841\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0553 - acc: 0.9864 - val_loss: 0.0780 - val_acc: 0.9841\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0552 - acc: 0.9858 - val_loss: 0.0782 - val_acc: 0.9841\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0551 - acc: 0.9864 - val_loss: 0.0781 - val_acc: 0.9841\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0549 - acc: 0.9864 - val_loss: 0.0776 - val_acc: 0.9841\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0551 - acc: 0.9858 - val_loss: 0.0779 - val_acc: 0.9841\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0548 - acc: 0.9864 - val_loss: 0.0783 - val_acc: 0.9841\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0549 - acc: 0.9864 - val_loss: 0.0779 - val_acc: 0.9841\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0546 - acc: 0.9864 - val_loss: 0.0773 - val_acc: 0.9841\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0548 - acc: 0.9864 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0546 - acc: 0.9858 - val_loss: 0.0779 - val_acc: 0.9841\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0546 - acc: 0.9864 - val_loss: 0.0776 - val_acc: 0.9841\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0544 - acc: 0.9864 - val_loss: 0.0778 - val_acc: 0.9841\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0544 - acc: 0.9864 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0544 - acc: 0.9870 - val_loss: 0.0774 - val_acc: 0.9841\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0541 - acc: 0.9864 - val_loss: 0.0776 - val_acc: 0.9841\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0541 - acc: 0.9864 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0540 - acc: 0.9864 - val_loss: 0.0775 - val_acc: 0.9841\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0541 - acc: 0.9864 - val_loss: 0.0773 - val_acc: 0.9841\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0539 - acc: 0.9864 - val_loss: 0.0770 - val_acc: 0.9841\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0537 - acc: 0.9864 - val_loss: 0.0779 - val_acc: 0.9841\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0538 - acc: 0.9870 - val_loss: 0.0771 - val_acc: 0.9841\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0537 - acc: 0.9864 - val_loss: 0.0774 - val_acc: 0.9841\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0536 - acc: 0.9870 - val_loss: 0.0770 - val_acc: 0.9841\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0534 - acc: 0.9870 - val_loss: 0.0773 - val_acc: 0.9841\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0535 - acc: 0.9870 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0535 - acc: 0.9864 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0534 - acc: 0.9870 - val_loss: 0.0769 - val_acc: 0.9841\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0533 - acc: 0.9870 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0532 - acc: 0.9870 - val_loss: 0.0770 - val_acc: 0.9841\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0531 - acc: 0.9876 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0531 - acc: 0.9870 - val_loss: 0.0766 - val_acc: 0.9841\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0530 - acc: 0.9870 - val_loss: 0.0764 - val_acc: 0.9841\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0529 - acc: 0.9882 - val_loss: 0.0763 - val_acc: 0.9841\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0529 - acc: 0.9876 - val_loss: 0.0762 - val_acc: 0.9841\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0528 - acc: 0.9870 - val_loss: 0.0766 - val_acc: 0.9841\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0527 - acc: 0.9876 - val_loss: 0.0762 - val_acc: 0.9841\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0527 - acc: 0.9870 - val_loss: 0.0764 - val_acc: 0.9841\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0526 - acc: 0.9876 - val_loss: 0.0761 - val_acc: 0.9841\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0525 - acc: 0.9870 - val_loss: 0.0758 - val_acc: 0.9841\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0525 - acc: 0.9876 - val_loss: 0.0757 - val_acc: 0.9841\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0524 - acc: 0.9876 - val_loss: 0.0760 - val_acc: 0.9841\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0523 - acc: 0.9876 - val_loss: 0.0757 - val_acc: 0.9841\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0522 - acc: 0.9882 - val_loss: 0.0754 - val_acc: 0.9841\n",
      "Epoch 417/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0522 - acc: 0.9876 - val_loss: 0.0754 - val_acc: 0.9841\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0521 - acc: 0.9876 - val_loss: 0.0754 - val_acc: 0.9841\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0520 - acc: 0.9876 - val_loss: 0.0754 - val_acc: 0.9841\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0520 - acc: 0.9888 - val_loss: 0.0752 - val_acc: 0.9841\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0520 - acc: 0.9876 - val_loss: 0.0752 - val_acc: 0.9841\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0519 - acc: 0.9876 - val_loss: 0.0752 - val_acc: 0.9841\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0518 - acc: 0.9882 - val_loss: 0.0752 - val_acc: 0.9841\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0517 - acc: 0.9882 - val_loss: 0.0754 - val_acc: 0.9841\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0517 - acc: 0.9876 - val_loss: 0.0755 - val_acc: 0.9841\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0515 - acc: 0.9882 - val_loss: 0.0750 - val_acc: 0.9841\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0515 - acc: 0.9882 - val_loss: 0.0750 - val_acc: 0.9841\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0515 - acc: 0.9876 - val_loss: 0.0750 - val_acc: 0.9841\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0515 - acc: 0.9876 - val_loss: 0.0751 - val_acc: 0.9841\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0514 - acc: 0.9882 - val_loss: 0.0749 - val_acc: 0.9841\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 155us/step - loss: 0.0513 - acc: 0.9876 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0512 - acc: 0.9882 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0512 - acc: 0.9882 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0511 - acc: 0.9882 - val_loss: 0.0746 - val_acc: 0.9841\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0511 - acc: 0.9876 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0510 - acc: 0.9876 - val_loss: 0.0749 - val_acc: 0.9841\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0509 - acc: 0.9888 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0509 - acc: 0.9876 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0508 - acc: 0.9882 - val_loss: 0.0745 - val_acc: 0.9841\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0508 - acc: 0.9876 - val_loss: 0.0744 - val_acc: 0.9841\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0507 - acc: 0.9882 - val_loss: 0.0746 - val_acc: 0.9841\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0506 - acc: 0.9882 - val_loss: 0.0743 - val_acc: 0.9841\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0506 - acc: 0.9882 - val_loss: 0.0743 - val_acc: 0.9841\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0506 - acc: 0.9882 - val_loss: 0.0742 - val_acc: 0.9841\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0505 - acc: 0.9888 - val_loss: 0.0741 - val_acc: 0.9841\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0504 - acc: 0.9888 - val_loss: 0.0740 - val_acc: 0.9841\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0504 - acc: 0.9876 - val_loss: 0.0741 - val_acc: 0.9841\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0503 - acc: 0.9882 - val_loss: 0.0741 - val_acc: 0.9841\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0502 - acc: 0.9882 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0502 - acc: 0.9882 - val_loss: 0.0740 - val_acc: 0.9841\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0502 - acc: 0.9876 - val_loss: 0.0741 - val_acc: 0.9841\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0500 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0500 - acc: 0.9876 - val_loss: 0.0742 - val_acc: 0.9841\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0500 - acc: 0.9882 - val_loss: 0.0740 - val_acc: 0.9841\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0499 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0498 - acc: 0.9888 - val_loss: 0.0738 - val_acc: 0.9841\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.0499 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0497 - acc: 0.9882 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0497 - acc: 0.9882 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 137us/step - loss: 0.0496 - acc: 0.9888 - val_loss: 0.0736 - val_acc: 0.9841\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0496 - acc: 0.9888 - val_loss: 0.0736 - val_acc: 0.9841\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0496 - acc: 0.9888 - val_loss: 0.0734 - val_acc: 0.9841\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0495 - acc: 0.9888 - val_loss: 0.0734 - val_acc: 0.9841\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0494 - acc: 0.9882 - val_loss: 0.0737 - val_acc: 0.9841\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0494 - acc: 0.9888 - val_loss: 0.0733 - val_acc: 0.9841\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0493 - acc: 0.9888 - val_loss: 0.0734 - val_acc: 0.9841\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0494 - acc: 0.9888 - val_loss: 0.0731 - val_acc: 0.9841\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0493 - acc: 0.9888 - val_loss: 0.0732 - val_acc: 0.9841\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0492 - acc: 0.9888 - val_loss: 0.0731 - val_acc: 0.9841\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0491 - acc: 0.9888 - val_loss: 0.0733 - val_acc: 0.9841\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0491 - acc: 0.9888 - val_loss: 0.0733 - val_acc: 0.9841\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0491 - acc: 0.9888 - val_loss: 0.0734 - val_acc: 0.9841\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0490 - acc: 0.9888 - val_loss: 0.0730 - val_acc: 0.9841\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0489 - acc: 0.9888 - val_loss: 0.0729 - val_acc: 0.9841\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0489 - acc: 0.9888 - val_loss: 0.0729 - val_acc: 0.9841\n",
      "Epoch 476/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0489 - acc: 0.9888 - val_loss: 0.0729 - val_acc: 0.9841\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0488 - acc: 0.9888 - val_loss: 0.0728 - val_acc: 0.9841\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0488 - acc: 0.9888 - val_loss: 0.0728 - val_acc: 0.9841\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0487 - acc: 0.9888 - val_loss: 0.0730 - val_acc: 0.9841\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0487 - acc: 0.9888 - val_loss: 0.0726 - val_acc: 0.9841\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0486 - acc: 0.9888 - val_loss: 0.0727 - val_acc: 0.9841\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0485 - acc: 0.9888 - val_loss: 0.0726 - val_acc: 0.9841\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0485 - acc: 0.9888 - val_loss: 0.0725 - val_acc: 0.9841\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0486 - acc: 0.9888 - val_loss: 0.0725 - val_acc: 0.9841\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0484 - acc: 0.9888 - val_loss: 0.0725 - val_acc: 0.9841\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0484 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9841\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0483 - acc: 0.9888 - val_loss: 0.0723 - val_acc: 0.9841\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0483 - acc: 0.9888 - val_loss: 0.0723 - val_acc: 0.9841\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0482 - acc: 0.9882 - val_loss: 0.0723 - val_acc: 0.9841\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0482 - acc: 0.9888 - val_loss: 0.0723 - val_acc: 0.9841\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0481 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9841\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0482 - acc: 0.9888 - val_loss: 0.0723 - val_acc: 0.9841\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0481 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9841\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0479 - acc: 0.9888 - val_loss: 0.0722 - val_acc: 0.9841\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0479 - acc: 0.9888 - val_loss: 0.0721 - val_acc: 0.9841\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0479 - acc: 0.9888 - val_loss: 0.0721 - val_acc: 0.9841\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0478 - acc: 0.9888 - val_loss: 0.0722 - val_acc: 0.9841\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0478 - acc: 0.9888 - val_loss: 0.0721 - val_acc: 0.9841\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0478 - acc: 0.9888 - val_loss: 0.0720 - val_acc: 0.9841\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0477 - acc: 0.9888 - val_loss: 0.0720 - val_acc: 0.9841\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0476 - acc: 0.9888 - val_loss: 0.0719 - val_acc: 0.9841\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0477 - acc: 0.9882 - val_loss: 0.0718 - val_acc: 0.9841\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0475 - acc: 0.9888 - val_loss: 0.0717 - val_acc: 0.9841\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0475 - acc: 0.9888 - val_loss: 0.0718 - val_acc: 0.9841\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0475 - acc: 0.9888 - val_loss: 0.0718 - val_acc: 0.9841\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0474 - acc: 0.9888 - val_loss: 0.0717 - val_acc: 0.9841\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0474 - acc: 0.9888 - val_loss: 0.0717 - val_acc: 0.9841\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0474 - acc: 0.9888 - val_loss: 0.0717 - val_acc: 0.9841\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0474 - acc: 0.9888 - val_loss: 0.0716 - val_acc: 0.9841\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0472 - acc: 0.9888 - val_loss: 0.0717 - val_acc: 0.9841\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0472 - acc: 0.9888 - val_loss: 0.0714 - val_acc: 0.9841\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0470 - acc: 0.9888 - val_loss: 0.0719 - val_acc: 0.9841\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.0472 - acc: 0.9888 - val_loss: 0.0716 - val_acc: 0.9841\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0471 - acc: 0.9888 - val_loss: 0.0714 - val_acc: 0.9841\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0471 - acc: 0.9888 - val_loss: 0.0713 - val_acc: 0.9841\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0470 - acc: 0.9882 - val_loss: 0.0713 - val_acc: 0.9841\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0470 - acc: 0.9888 - val_loss: 0.0712 - val_acc: 0.9841\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0470 - acc: 0.9888 - val_loss: 0.0713 - val_acc: 0.9841\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0470 - acc: 0.9888 - val_loss: 0.0714 - val_acc: 0.9841\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0468 - acc: 0.9888 - val_loss: 0.0711 - val_acc: 0.9841\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0468 - acc: 0.9888 - val_loss: 0.0710 - val_acc: 0.9841\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0468 - acc: 0.9888 - val_loss: 0.0710 - val_acc: 0.9841\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0467 - acc: 0.9888 - val_loss: 0.0711 - val_acc: 0.9841\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0467 - acc: 0.9888 - val_loss: 0.0711 - val_acc: 0.9841\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0466 - acc: 0.9888 - val_loss: 0.0711 - val_acc: 0.9841\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0466 - acc: 0.9888 - val_loss: 0.0710 - val_acc: 0.9841\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0465 - acc: 0.9888 - val_loss: 0.0709 - val_acc: 0.9841\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0465 - acc: 0.9888 - val_loss: 0.0709 - val_acc: 0.9841\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0465 - acc: 0.9888 - val_loss: 0.0709 - val_acc: 0.9841\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0465 - acc: 0.9888 - val_loss: 0.0708 - val_acc: 0.9841\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9841\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0464 - acc: 0.9888 - val_loss: 0.0708 - val_acc: 0.9841\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9841\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0706 - val_acc: 0.9841\n",
      "Epoch 535/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0463 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9841\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0462 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9841\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0462 - acc: 0.9888 - val_loss: 0.0706 - val_acc: 0.9841\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0461 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0461 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0703 - val_acc: 0.9841\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0459 - acc: 0.9888 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0459 - acc: 0.9888 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0460 - acc: 0.9888 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0458 - acc: 0.9888 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0458 - acc: 0.9888 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0701 - val_acc: 0.9841\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0458 - acc: 0.9888 - val_loss: 0.0701 - val_acc: 0.9841\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0703 - val_acc: 0.9841\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0701 - val_acc: 0.9841\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0699 - val_acc: 0.9841\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0456 - acc: 0.9888 - val_loss: 0.0700 - val_acc: 0.9841\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0455 - acc: 0.9888 - val_loss: 0.0699 - val_acc: 0.9841\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0455 - acc: 0.9888 - val_loss: 0.0698 - val_acc: 0.9841\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0698 - val_acc: 0.9841\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0697 - val_acc: 0.9841\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0453 - acc: 0.9888 - val_loss: 0.0698 - val_acc: 0.9841\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0698 - val_acc: 0.9841\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0697 - val_acc: 0.9841\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0451 - acc: 0.9888 - val_loss: 0.0700 - val_acc: 0.9841\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0452 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0451 - acc: 0.9888 - val_loss: 0.0695 - val_acc: 0.9841\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0452 - acc: 0.9888 - val_loss: 0.0695 - val_acc: 0.9841\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0451 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0694 - val_acc: 0.9841\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0694 - val_acc: 0.9841\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0695 - val_acc: 0.9841\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0450 - acc: 0.9888 - val_loss: 0.0694 - val_acc: 0.9841\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0693 - val_acc: 0.9841\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0448 - acc: 0.9888 - val_loss: 0.0693 - val_acc: 0.9841\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0448 - acc: 0.9888 - val_loss: 0.0692 - val_acc: 0.9841\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0448 - acc: 0.9888 - val_loss: 0.0692 - val_acc: 0.9841\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0447 - acc: 0.9888 - val_loss: 0.0691 - val_acc: 0.9841\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0447 - acc: 0.9888 - val_loss: 0.0690 - val_acc: 0.9841\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0447 - acc: 0.9888 - val_loss: 0.0691 - val_acc: 0.9841\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0445 - acc: 0.9894 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0446 - acc: 0.9888 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0445 - acc: 0.9888 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0445 - acc: 0.9888 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0445 - acc: 0.9888 - val_loss: 0.0688 - val_acc: 0.9841\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0444 - acc: 0.9888 - val_loss: 0.0688 - val_acc: 0.9841\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0687 - val_acc: 0.9841\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0445 - acc: 0.9888 - val_loss: 0.0687 - val_acc: 0.9841\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0444 - acc: 0.9888 - val_loss: 0.0687 - val_acc: 0.9841\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0686 - val_acc: 0.9841\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0443 - acc: 0.9894 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 594/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0442 - acc: 0.9888 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0442 - acc: 0.9894 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0442 - acc: 0.9888 - val_loss: 0.0686 - val_acc: 0.9841\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0441 - acc: 0.9888 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0442 - acc: 0.9888 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0441 - acc: 0.9888 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0439 - acc: 0.9900 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0439 - acc: 0.9894 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0439 - acc: 0.9900 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0439 - acc: 0.9894 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0439 - acc: 0.9894 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0438 - acc: 0.9888 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0438 - acc: 0.9900 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0438 - acc: 0.9900 - val_loss: 0.0681 - val_acc: 0.9841\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0680 - val_acc: 0.9841\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0680 - val_acc: 0.9841\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0680 - val_acc: 0.9841\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0436 - acc: 0.9900 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0436 - acc: 0.9888 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0678 - val_acc: 0.9841\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0436 - acc: 0.9900 - val_loss: 0.0678 - val_acc: 0.9841\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 0.0679 - val_acc: 0.9841\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0435 - acc: 0.9900 - val_loss: 0.0678 - val_acc: 0.9841\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0433 - acc: 0.9900 - val_loss: 0.0677 - val_acc: 0.9841\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0433 - acc: 0.9888 - val_loss: 0.0677 - val_acc: 0.9841\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0433 - acc: 0.9900 - val_loss: 0.0676 - val_acc: 0.9841\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0433 - acc: 0.9900 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0434 - acc: 0.9894 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0433 - acc: 0.9894 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0432 - acc: 0.9894 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0432 - acc: 0.9900 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0432 - acc: 0.9900 - val_loss: 0.0674 - val_acc: 0.9841\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 0.0674 - val_acc: 0.9841\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 0.0675 - val_acc: 0.9841\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0431 - acc: 0.9900 - val_loss: 0.0673 - val_acc: 0.9841\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0430 - acc: 0.9894 - val_loss: 0.0673 - val_acc: 0.9841\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0430 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0429 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0429 - acc: 0.9900 - val_loss: 0.0671 - val_acc: 0.9841\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0428 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0429 - acc: 0.9900 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0428 - acc: 0.9900 - val_loss: 0.0672 - val_acc: 0.9841\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0428 - acc: 0.9900 - val_loss: 0.0671 - val_acc: 0.9841\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0427 - acc: 0.9900 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0427 - acc: 0.9900 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0428 - acc: 0.9900 - val_loss: 0.0669 - val_acc: 0.9841\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0426 - acc: 0.9894 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0427 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0426 - acc: 0.9894 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0426 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0426 - acc: 0.9900 - val_loss: 0.0667 - val_acc: 0.9841\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0426 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0425 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0425 - acc: 0.9900 - val_loss: 0.0668 - val_acc: 0.9841\n",
      "Epoch 653/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0425 - acc: 0.9900 - val_loss: 0.0667 - val_acc: 0.9841\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0667 - val_acc: 0.9841\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0424 - acc: 0.9900 - val_loss: 0.0666 - val_acc: 0.9841\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0425 - acc: 0.9900 - val_loss: 0.0666 - val_acc: 0.9841\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0666 - val_acc: 0.9841\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0665 - val_acc: 0.9841\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0665 - val_acc: 0.9841\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0424 - acc: 0.9900 - val_loss: 0.0665 - val_acc: 0.9841\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0663 - val_acc: 0.9841\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0422 - acc: 0.9900 - val_loss: 0.0663 - val_acc: 0.9841\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0421 - acc: 0.9900 - val_loss: 0.0663 - val_acc: 0.9841\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0421 - acc: 0.9900 - val_loss: 0.0662 - val_acc: 0.9841\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0422 - acc: 0.9900 - val_loss: 0.0662 - val_acc: 0.9841\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0421 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9841\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0663 - val_acc: 0.9841\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0421 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9841\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0421 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9841\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9841\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.0661 - val_acc: 0.9841\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 0.0660 - val_acc: 0.9841\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 0.0659 - val_acc: 0.9841\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.0659 - val_acc: 0.9841\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0417 - acc: 0.9900 - val_loss: 0.0659 - val_acc: 0.9841\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0418 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9841\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0417 - acc: 0.9900 - val_loss: 0.0659 - val_acc: 0.9841\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0417 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9841\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0416 - acc: 0.9900 - val_loss: 0.0658 - val_acc: 0.9841\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0416 - acc: 0.9900 - val_loss: 0.0657 - val_acc: 0.9841\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0417 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0415 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0415 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0416 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0416 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0415 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0415 - acc: 0.9900 - val_loss: 0.0655 - val_acc: 0.9841\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0414 - acc: 0.9900 - val_loss: 0.0656 - val_acc: 0.9841\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0414 - acc: 0.9900 - val_loss: 0.0655 - val_acc: 0.9841\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0414 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0414 - acc: 0.9905 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0412 - acc: 0.9900 - val_loss: 0.0652 - val_acc: 0.9841\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0412 - acc: 0.9905 - val_loss: 0.0652 - val_acc: 0.9841\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0413 - acc: 0.9900 - val_loss: 0.0652 - val_acc: 0.9841\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0412 - acc: 0.9900 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 0.9900 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0412 - acc: 0.9900 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0411 - acc: 0.9905 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0411 - acc: 0.9900 - val_loss: 0.0650 - val_acc: 0.9841\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0411 - acc: 0.9900 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 0.9900 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0411 - acc: 0.9905 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 712/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.0650 - val_acc: 0.9841\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0409 - acc: 0.9900 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0409 - acc: 0.9900 - val_loss: 0.0647 - val_acc: 0.9841\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0408 - acc: 0.9905 - val_loss: 0.0647 - val_acc: 0.9841\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0646 - val_acc: 0.9841\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0408 - acc: 0.9905 - val_loss: 0.0647 - val_acc: 0.9841\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0646 - val_acc: 0.9841\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0646 - val_acc: 0.9841\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0407 - acc: 0.9905 - val_loss: 0.0646 - val_acc: 0.9841\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 0.0645 - val_acc: 0.9841\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 0.0645 - val_acc: 0.9841\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0408 - acc: 0.9905 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0407 - acc: 0.9905 - val_loss: 0.0645 - val_acc: 0.9841\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0406 - acc: 0.9900 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0406 - acc: 0.9900 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 0.0643 - val_acc: 0.9841\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0405 - acc: 0.9905 - val_loss: 0.0643 - val_acc: 0.9841\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0406 - acc: 0.9905 - val_loss: 0.0642 - val_acc: 0.9841\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0642 - val_acc: 0.9841\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0405 - acc: 0.9905 - val_loss: 0.0642 - val_acc: 0.9841\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 0.0642 - val_acc: 0.9841\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 0.0641 - val_acc: 0.9841\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0641 - val_acc: 0.9841\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0641 - val_acc: 0.9841\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0404 - acc: 0.9905 - val_loss: 0.0640 - val_acc: 0.9841\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0404 - acc: 0.9905 - val_loss: 0.0640 - val_acc: 0.9841\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0403 - acc: 0.9911 - val_loss: 0.0640 - val_acc: 0.9841\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0404 - acc: 0.9900 - val_loss: 0.0639 - val_acc: 0.9841\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0403 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0403 - acc: 0.9905 - val_loss: 0.0639 - val_acc: 0.9841\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0402 - acc: 0.9900 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0403 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0402 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0402 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0402 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9841\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0402 - acc: 0.9900 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0401 - acc: 0.9905 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0401 - acc: 0.9900 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0401 - acc: 0.9917 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0401 - acc: 0.9905 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0400 - acc: 0.9911 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0400 - acc: 0.9917 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0399 - acc: 0.9911 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0400 - acc: 0.9911 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0398 - acc: 0.9900 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0400 - acc: 0.9917 - val_loss: 0.0635 - val_acc: 0.9841\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0399 - acc: 0.9917 - val_loss: 0.0635 - val_acc: 0.9841\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0399 - acc: 0.9911 - val_loss: 0.0636 - val_acc: 0.9841\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0399 - acc: 0.9905 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0398 - acc: 0.9911 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0398 - acc: 0.9911 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0398 - acc: 0.9911 - val_loss: 0.0634 - val_acc: 0.9841\n",
      "Epoch 771/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.0633 - val_acc: 0.9841\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0399 - acc: 0.9911 - val_loss: 0.0633 - val_acc: 0.9841\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0398 - acc: 0.9917 - val_loss: 0.0633 - val_acc: 0.9841\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0398 - acc: 0.9905 - val_loss: 0.0632 - val_acc: 0.9841\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0633 - val_acc: 0.9841\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0633 - val_acc: 0.9841\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0632 - val_acc: 0.9841\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0632 - val_acc: 0.9841\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0631 - val_acc: 0.9841\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0631 - val_acc: 0.9841\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0631 - val_acc: 0.9841\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0395 - acc: 0.9917 - val_loss: 0.0631 - val_acc: 0.9841\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0396 - acc: 0.9917 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0395 - acc: 0.9911 - val_loss: 0.0632 - val_acc: 0.9841\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0395 - acc: 0.9917 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0394 - acc: 0.9923 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0395 - acc: 0.9911 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.9911 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0393 - acc: 0.9917 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0394 - acc: 0.9917 - val_loss: 0.0630 - val_acc: 0.9841\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0395 - acc: 0.9917 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0393 - acc: 0.9917 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0393 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.9917 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0393 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0392 - acc: 0.9923 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0392 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0392 - acc: 0.9917 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0392 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0391 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0391 - acc: 0.9923 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.9917 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0391 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0390 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0391 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0390 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0390 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0625 - val_acc: 0.9841\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0626 - val_acc: 0.9841\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0625 - val_acc: 0.9841\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0625 - val_acc: 0.9841\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0626 - val_acc: 0.9841\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0627 - val_acc: 0.9841\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0389 - acc: 0.9923 - val_loss: 0.0624 - val_acc: 0.9841\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0625 - val_acc: 0.9841\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0624 - val_acc: 0.9841\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0624 - val_acc: 0.9841\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0388 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0621 - val_acc: 0.9841\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0386 - acc: 0.9923 - val_loss: 0.0621 - val_acc: 0.9841\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0387 - acc: 0.9923 - val_loss: 0.0621 - val_acc: 0.9841\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0385 - acc: 0.9923 - val_loss: 0.0620 - val_acc: 0.9841\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0386 - acc: 0.9923 - val_loss: 0.0621 - val_acc: 0.9841\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0385 - acc: 0.9923 - val_loss: 0.0620 - val_acc: 0.9841\n",
      "Epoch 830/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0385 - acc: 0.9923 - val_loss: 0.0622 - val_acc: 0.9841\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0386 - acc: 0.9917 - val_loss: 0.0621 - val_acc: 0.9841\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0385 - acc: 0.9923 - val_loss: 0.0620 - val_acc: 0.9841\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0385 - acc: 0.9923 - val_loss: 0.0619 - val_acc: 0.9841\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0618 - val_acc: 0.9841\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0384 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0617 - val_acc: 0.9841\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0616 - val_acc: 0.9841\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0616 - val_acc: 0.9841\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0615 - val_acc: 0.9841\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0615 - val_acc: 0.9841\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0382 - acc: 0.9923 - val_loss: 0.0616 - val_acc: 0.9841\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0383 - acc: 0.9923 - val_loss: 0.0614 - val_acc: 0.9841\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0382 - acc: 0.9923 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0382 - acc: 0.9923 - val_loss: 0.0612 - val_acc: 0.9841\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0382 - acc: 0.9923 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0612 - val_acc: 0.9841\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0612 - val_acc: 0.9841\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0612 - val_acc: 0.9841\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0611 - val_acc: 0.9841\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0610 - val_acc: 0.9841\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0381 - acc: 0.9923 - val_loss: 0.0610 - val_acc: 0.9841\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0609 - val_acc: 0.9841\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0608 - val_acc: 0.9841\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0610 - val_acc: 0.9841\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0378 - acc: 0.9917 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0379 - acc: 0.9923 - val_loss: 0.0609 - val_acc: 0.9841\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0379 - acc: 0.9923 - val_loss: 0.0608 - val_acc: 0.9841\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0380 - acc: 0.9923 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0379 - acc: 0.9923 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0607 - val_acc: 0.9841\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0603 - val_acc: 0.9841\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0603 - val_acc: 0.9841\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0378 - acc: 0.9923 - val_loss: 0.0603 - val_acc: 0.9841\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0602 - val_acc: 0.9841\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0377 - acc: 0.9923 - val_loss: 0.0602 - val_acc: 0.9841\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0376 - acc: 0.9923 - val_loss: 0.0599 - val_acc: 0.9841\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0376 - acc: 0.9923 - val_loss: 0.0599 - val_acc: 0.9841\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0376 - acc: 0.9923 - val_loss: 0.0599 - val_acc: 0.9841\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0601 - val_acc: 0.9841\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0602 - val_acc: 0.9841\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0376 - acc: 0.9923 - val_loss: 0.0600 - val_acc: 0.9841\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0598 - val_acc: 0.9841\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0598 - val_acc: 0.9841\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0597 - val_acc: 0.9841\n",
      "Epoch 889/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0303 - acc: 0.994 - 0s 51us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0597 - val_acc: 0.9841\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0374 - acc: 0.9923 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0595 - val_acc: 0.9841\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0374 - acc: 0.9923 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0375 - acc: 0.9923 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0374 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.0374 - acc: 0.9923 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 132us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0596 - val_acc: 0.9841\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0592 - val_acc: 0.9841\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0372 - acc: 0.9923 - val_loss: 0.0591 - val_acc: 0.9841\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0372 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0373 - acc: 0.9923 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0372 - acc: 0.9923 - val_loss: 0.0592 - val_acc: 0.9841\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0371 - acc: 0.9923 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0371 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0371 - acc: 0.9923 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0372 - acc: 0.9923 - val_loss: 0.0592 - val_acc: 0.9841\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0371 - acc: 0.9923 - val_loss: 0.0590 - val_acc: 0.9841\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0500 - acc: 0.987 - 0s 55us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0371 - acc: 0.9923 - val_loss: 0.0592 - val_acc: 0.9841\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0588 - val_acc: 0.9841\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0369 - acc: 0.9923 - val_loss: 0.0592 - val_acc: 0.9841\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0369 - acc: 0.9923 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0588 - val_acc: 0.9841\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0369 - acc: 0.9923 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0369 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0370 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0590 - val_acc: 0.9841\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0369 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0586 - val_acc: 0.9841\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0586 - val_acc: 0.9841\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0586 - val_acc: 0.9841\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0368 - acc: 0.9923 - val_loss: 0.0586 - val_acc: 0.9841\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0585 - val_acc: 0.9841\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0366 - acc: 0.9923 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0367 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0585 - val_acc: 0.9841\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0364 - acc: 0.9923 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0576 - val_acc: 0.9841\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0578 - val_acc: 0.9841\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0575 - val_acc: 0.9841\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0579 - val_acc: 0.9841\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 0.0578 - val_acc: 0.9841\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.0574 - val_acc: 0.9841\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 0.0576 - val_acc: 0.9841\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0362 - acc: 0.9923 - val_loss: 0.0575 - val_acc: 0.9841\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0576 - val_acc: 0.9841\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0573 - val_acc: 0.9841\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0571 - val_acc: 0.9841\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0573 - val_acc: 0.9841\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0575 - val_acc: 0.9841\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0361 - acc: 0.9923 - val_loss: 0.0573 - val_acc: 0.9841\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0575 - val_acc: 0.9841\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0574 - val_acc: 0.9841\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0571 - val_acc: 0.9841\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0360 - acc: 0.9923 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9841\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0571 - val_acc: 0.9841\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0567 - val_acc: 0.9841\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0359 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9841\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9841\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0357 - acc: 0.9923 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0357 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0357 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0358 - acc: 0.9923 - val_loss: 0.0568 - val_acc: 0.9841\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0357 - acc: 0.9923 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 378us/step - loss: 0.7830 - acc: 0.3824 - val_loss: 0.6818 - val_acc: 0.4709\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.6731 - acc: 0.6017 - val_loss: 0.6566 - val_acc: 0.6772\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.6542 - acc: 0.6820 - val_loss: 0.6448 - val_acc: 0.7249\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.6435 - acc: 0.7098 - val_loss: 0.6362 - val_acc: 0.7143\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.6355 - acc: 0.7092 - val_loss: 0.6292 - val_acc: 0.7090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.6288 - acc: 0.7128 - val_loss: 0.6234 - val_acc: 0.7196\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.6227 - acc: 0.7122 - val_loss: 0.6171 - val_acc: 0.7196\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.6169 - acc: 0.7151 - val_loss: 0.6115 - val_acc: 0.7249\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.6111 - acc: 0.7216 - val_loss: 0.6053 - val_acc: 0.7302\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.6052 - acc: 0.7240 - val_loss: 0.5990 - val_acc: 0.7249\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5994 - acc: 0.7293 - val_loss: 0.5927 - val_acc: 0.7354\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.5930 - acc: 0.7340 - val_loss: 0.5861 - val_acc: 0.7460\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.5866 - acc: 0.7382 - val_loss: 0.5788 - val_acc: 0.7619\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5796 - acc: 0.7476 - val_loss: 0.5716 - val_acc: 0.7619\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5721 - acc: 0.7547 - val_loss: 0.5636 - val_acc: 0.7672\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.5642 - acc: 0.7595 - val_loss: 0.5550 - val_acc: 0.7672\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.5562 - acc: 0.7671 - val_loss: 0.5460 - val_acc: 0.7672\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.5474 - acc: 0.7689 - val_loss: 0.5361 - val_acc: 0.7831\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.5382 - acc: 0.7801 - val_loss: 0.5264 - val_acc: 0.7884\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.5283 - acc: 0.7896 - val_loss: 0.5154 - val_acc: 0.8042\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5183 - acc: 0.7961 - val_loss: 0.5050 - val_acc: 0.8095\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.5083 - acc: 0.8020 - val_loss: 0.4935 - val_acc: 0.8254\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4975 - acc: 0.8091 - val_loss: 0.4814 - val_acc: 0.8360\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4869 - acc: 0.8144 - val_loss: 0.4694 - val_acc: 0.8413\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.4761 - acc: 0.8180 - val_loss: 0.4560 - val_acc: 0.8571\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.4645 - acc: 0.8304 - val_loss: 0.4432 - val_acc: 0.8571\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4538 - acc: 0.8351 - val_loss: 0.4306 - val_acc: 0.8677\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4425 - acc: 0.8404 - val_loss: 0.4177 - val_acc: 0.8783\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4302 - acc: 0.8528 - val_loss: 0.4045 - val_acc: 0.8783\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4181 - acc: 0.8617 - val_loss: 0.3902 - val_acc: 0.8783\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4066 - acc: 0.8676 - val_loss: 0.3767 - val_acc: 0.8836\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3945 - acc: 0.8741 - val_loss: 0.3644 - val_acc: 0.8889\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3836 - acc: 0.8800 - val_loss: 0.3516 - val_acc: 0.8995\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3725 - acc: 0.8877 - val_loss: 0.3404 - val_acc: 0.9101\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3618 - acc: 0.8930 - val_loss: 0.3312 - val_acc: 0.8995\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3527 - acc: 0.8983 - val_loss: 0.3200 - val_acc: 0.9259\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3426 - acc: 0.9037 - val_loss: 0.3096 - val_acc: 0.9312\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3342 - acc: 0.9102 - val_loss: 0.3013 - val_acc: 0.9312\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3251 - acc: 0.9143 - val_loss: 0.2929 - val_acc: 0.9312\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.3166 - acc: 0.9184 - val_loss: 0.2860 - val_acc: 0.9312\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3093 - acc: 0.9178 - val_loss: 0.2787 - val_acc: 0.9312\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3013 - acc: 0.9232 - val_loss: 0.2711 - val_acc: 0.9312\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2940 - acc: 0.9249 - val_loss: 0.2641 - val_acc: 0.9312\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2863 - acc: 0.9273 - val_loss: 0.2617 - val_acc: 0.9259\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2800 - acc: 0.9261 - val_loss: 0.2517 - val_acc: 0.9365\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2738 - acc: 0.9285 - val_loss: 0.2479 - val_acc: 0.9365\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2676 - acc: 0.9291 - val_loss: 0.2416 - val_acc: 0.9418\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2619 - acc: 0.9303 - val_loss: 0.2367 - val_acc: 0.9418\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2563 - acc: 0.9314 - val_loss: 0.2326 - val_acc: 0.9418\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2510 - acc: 0.9314 - val_loss: 0.2299 - val_acc: 0.9471\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2463 - acc: 0.9332 - val_loss: 0.2251 - val_acc: 0.9418\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2419 - acc: 0.9374 - val_loss: 0.2211 - val_acc: 0.9418\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2377 - acc: 0.9391 - val_loss: 0.2175 - val_acc: 0.9418\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2333 - acc: 0.9415 - val_loss: 0.2144 - val_acc: 0.9365\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2295 - acc: 0.9427 - val_loss: 0.2111 - val_acc: 0.9365\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2258 - acc: 0.9433 - val_loss: 0.2082 - val_acc: 0.9471\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2226 - acc: 0.9433 - val_loss: 0.2062 - val_acc: 0.9365\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2186 - acc: 0.9439 - val_loss: 0.2036 - val_acc: 0.9365\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2155 - acc: 0.9439 - val_loss: 0.1987 - val_acc: 0.9418\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2123 - acc: 0.9456 - val_loss: 0.1968 - val_acc: 0.9365\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2095 - acc: 0.9450 - val_loss: 0.1935 - val_acc: 0.9577\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2069 - acc: 0.9450 - val_loss: 0.1924 - val_acc: 0.9471\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2042 - acc: 0.9474 - val_loss: 0.1890 - val_acc: 0.9524\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2011 - acc: 0.9474 - val_loss: 0.1867 - val_acc: 0.9471\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1987 - acc: 0.9504 - val_loss: 0.1847 - val_acc: 0.9577\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1962 - acc: 0.9486 - val_loss: 0.1841 - val_acc: 0.9418\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1930 - acc: 0.9521 - val_loss: 0.1796 - val_acc: 0.9577\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1907 - acc: 0.9515 - val_loss: 0.1814 - val_acc: 0.9418\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1893 - acc: 0.9509 - val_loss: 0.1753 - val_acc: 0.9524\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1869 - acc: 0.9515 - val_loss: 0.1729 - val_acc: 0.9577\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1850 - acc: 0.9527 - val_loss: 0.1714 - val_acc: 0.9524\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1830 - acc: 0.9521 - val_loss: 0.1690 - val_acc: 0.9577\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1815 - acc: 0.9527 - val_loss: 0.1680 - val_acc: 0.9524\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1792 - acc: 0.9521 - val_loss: 0.1679 - val_acc: 0.9577\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1777 - acc: 0.9533 - val_loss: 0.1642 - val_acc: 0.9577\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1757 - acc: 0.9533 - val_loss: 0.1626 - val_acc: 0.9524\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1740 - acc: 0.9551 - val_loss: 0.1624 - val_acc: 0.9524\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1725 - acc: 0.9545 - val_loss: 0.1588 - val_acc: 0.9524\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1706 - acc: 0.9551 - val_loss: 0.1574 - val_acc: 0.9471\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1691 - acc: 0.9563 - val_loss: 0.1571 - val_acc: 0.9471\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1678 - acc: 0.9557 - val_loss: 0.1554 - val_acc: 0.9577\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1664 - acc: 0.9551 - val_loss: 0.1535 - val_acc: 0.9524\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1647 - acc: 0.9563 - val_loss: 0.1512 - val_acc: 0.9577\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1632 - acc: 0.9563 - val_loss: 0.1484 - val_acc: 0.9577\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1621 - acc: 0.9569 - val_loss: 0.1477 - val_acc: 0.9524\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1610 - acc: 0.9574 - val_loss: 0.1453 - val_acc: 0.9577\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1598 - acc: 0.9563 - val_loss: 0.1438 - val_acc: 0.9577\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1581 - acc: 0.9586 - val_loss: 0.1424 - val_acc: 0.9577\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1565 - acc: 0.9604 - val_loss: 0.1419 - val_acc: 0.9577\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1554 - acc: 0.9592 - val_loss: 0.1393 - val_acc: 0.9577\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1543 - acc: 0.9580 - val_loss: 0.1437 - val_acc: 0.9630\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1531 - acc: 0.9598 - val_loss: 0.1393 - val_acc: 0.9471\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1520 - acc: 0.9592 - val_loss: 0.1365 - val_acc: 0.9577\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1510 - acc: 0.9598 - val_loss: 0.1330 - val_acc: 0.9577\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1499 - acc: 0.9580 - val_loss: 0.1327 - val_acc: 0.9577\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1488 - acc: 0.9610 - val_loss: 0.1308 - val_acc: 0.9630\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.1477 - acc: 0.9592 - val_loss: 0.1291 - val_acc: 0.9630\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1464 - acc: 0.9598 - val_loss: 0.1294 - val_acc: 0.9630\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1453 - acc: 0.9604 - val_loss: 0.1267 - val_acc: 0.9524\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1442 - acc: 0.9616 - val_loss: 0.1248 - val_acc: 0.9630\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1425 - acc: 0.9622 - val_loss: 0.1266 - val_acc: 0.9577\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1420 - acc: 0.9616 - val_loss: 0.1223 - val_acc: 0.9524\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1409 - acc: 0.9622 - val_loss: 0.1233 - val_acc: 0.9577\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1399 - acc: 0.9634 - val_loss: 0.1179 - val_acc: 0.9630\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1384 - acc: 0.9622 - val_loss: 0.1197 - val_acc: 0.9683\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1374 - acc: 0.9639 - val_loss: 0.1170 - val_acc: 0.9630\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1363 - acc: 0.9645 - val_loss: 0.1158 - val_acc: 0.9630\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1356 - acc: 0.9645 - val_loss: 0.1160 - val_acc: 0.9735\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1348 - acc: 0.9657 - val_loss: 0.1110 - val_acc: 0.9735\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1336 - acc: 0.9663 - val_loss: 0.1114 - val_acc: 0.9735\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1329 - acc: 0.9657 - val_loss: 0.1112 - val_acc: 0.9735\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1311 - acc: 0.9675 - val_loss: 0.1091 - val_acc: 0.9735\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1305 - acc: 0.9675 - val_loss: 0.1065 - val_acc: 0.9735\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1295 - acc: 0.9675 - val_loss: 0.1048 - val_acc: 0.9735\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1288 - acc: 0.9675 - val_loss: 0.1043 - val_acc: 0.9735\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1275 - acc: 0.9675 - val_loss: 0.1017 - val_acc: 0.9788\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1267 - acc: 0.9687 - val_loss: 0.1021 - val_acc: 0.9735\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1256 - acc: 0.9693 - val_loss: 0.1039 - val_acc: 0.9735\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1247 - acc: 0.9693 - val_loss: 0.0984 - val_acc: 0.9788\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1235 - acc: 0.9693 - val_loss: 0.0999 - val_acc: 0.9788\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1228 - acc: 0.9699 - val_loss: 0.0985 - val_acc: 0.9735\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1219 - acc: 0.9699 - val_loss: 0.0982 - val_acc: 0.9735\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1208 - acc: 0.9704 - val_loss: 0.0950 - val_acc: 0.9788\n",
      "Epoch 124/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1201 - acc: 0.9704 - val_loss: 0.0954 - val_acc: 0.9788\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1189 - acc: 0.9699 - val_loss: 0.0935 - val_acc: 0.9788\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1180 - acc: 0.9693 - val_loss: 0.0921 - val_acc: 0.9788\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1172 - acc: 0.9699 - val_loss: 0.0939 - val_acc: 0.9788\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1165 - acc: 0.9710 - val_loss: 0.0923 - val_acc: 0.9788\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1157 - acc: 0.9699 - val_loss: 0.0928 - val_acc: 0.9788\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 151us/step - loss: 0.1146 - acc: 0.9699 - val_loss: 0.0893 - val_acc: 0.9788\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 150us/step - loss: 0.1136 - acc: 0.9704 - val_loss: 0.0886 - val_acc: 0.9841\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.1131 - acc: 0.9699 - val_loss: 0.0901 - val_acc: 0.9735\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1122 - acc: 0.9704 - val_loss: 0.0891 - val_acc: 0.9735\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.1107 - acc: 0.9710 - val_loss: 0.0888 - val_acc: 0.9735\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 151us/step - loss: 0.1107 - acc: 0.9716 - val_loss: 0.0869 - val_acc: 0.9788\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.1100 - acc: 0.9722 - val_loss: 0.0857 - val_acc: 0.9788\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.1095 - acc: 0.9722 - val_loss: 0.0870 - val_acc: 0.9735\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1080 - acc: 0.9722 - val_loss: 0.0847 - val_acc: 0.9788\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1072 - acc: 0.9728 - val_loss: 0.0840 - val_acc: 0.9788\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1059 - acc: 0.9722 - val_loss: 0.0855 - val_acc: 0.9788\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1059 - acc: 0.9722 - val_loss: 0.0822 - val_acc: 0.9788\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1046 - acc: 0.9728 - val_loss: 0.0834 - val_acc: 0.9735\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1041 - acc: 0.9728 - val_loss: 0.0807 - val_acc: 0.9788\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1037 - acc: 0.9728 - val_loss: 0.0801 - val_acc: 0.9788\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1025 - acc: 0.9734 - val_loss: 0.0788 - val_acc: 0.9788\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 116us/step - loss: 0.1016 - acc: 0.9722 - val_loss: 0.0788 - val_acc: 0.9788\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.1017 - acc: 0.9734 - val_loss: 0.0788 - val_acc: 0.9735\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1004 - acc: 0.9740 - val_loss: 0.0779 - val_acc: 0.9788\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0998 - acc: 0.9740 - val_loss: 0.0787 - val_acc: 0.9735\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 147us/step - loss: 0.0988 - acc: 0.9752 - val_loss: 0.0759 - val_acc: 0.9788\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 127us/step - loss: 0.0983 - acc: 0.9746 - val_loss: 0.0758 - val_acc: 0.9788\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0979 - acc: 0.9764 - val_loss: 0.0763 - val_acc: 0.9735\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.0967 - acc: 0.9752 - val_loss: 0.0754 - val_acc: 0.9735\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0965 - acc: 0.9746 - val_loss: 0.0763 - val_acc: 0.9788\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 129us/step - loss: 0.0965 - acc: 0.9758 - val_loss: 0.0732 - val_acc: 0.9735\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0951 - acc: 0.9758 - val_loss: 0.0744 - val_acc: 0.9788\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0951 - acc: 0.9764 - val_loss: 0.0719 - val_acc: 0.9788\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0942 - acc: 0.9770 - val_loss: 0.0724 - val_acc: 0.9841\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 140us/step - loss: 0.0933 - acc: 0.9770 - val_loss: 0.0723 - val_acc: 0.9788\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0929 - acc: 0.9781 - val_loss: 0.0741 - val_acc: 0.9788\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 156us/step - loss: 0.0928 - acc: 0.9781 - val_loss: 0.0735 - val_acc: 0.9788\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 152us/step - loss: 0.0919 - acc: 0.9775 - val_loss: 0.0729 - val_acc: 0.9735\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.0914 - acc: 0.9781 - val_loss: 0.0696 - val_acc: 0.9735\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.0912 - acc: 0.9781 - val_loss: 0.0714 - val_acc: 0.9735\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.0906 - acc: 0.9787 - val_loss: 0.0691 - val_acc: 0.9735\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.0899 - acc: 0.9781 - val_loss: 0.0698 - val_acc: 0.9788\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0896 - acc: 0.9770 - val_loss: 0.0679 - val_acc: 0.9894\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0887 - acc: 0.9793 - val_loss: 0.0671 - val_acc: 0.9735\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0885 - acc: 0.9793 - val_loss: 0.0664 - val_acc: 0.9735\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0878 - acc: 0.9781 - val_loss: 0.0670 - val_acc: 0.9841\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0877 - acc: 0.9781 - val_loss: 0.0654 - val_acc: 0.9841\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0870 - acc: 0.9781 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0867 - acc: 0.9793 - val_loss: 0.0664 - val_acc: 0.9735\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0861 - acc: 0.9805 - val_loss: 0.0690 - val_acc: 0.9788\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0861 - acc: 0.9799 - val_loss: 0.0670 - val_acc: 0.9788\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0854 - acc: 0.9799 - val_loss: 0.0641 - val_acc: 0.9788\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0851 - acc: 0.9793 - val_loss: 0.0643 - val_acc: 0.9788\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0842 - acc: 0.9811 - val_loss: 0.0644 - val_acc: 0.9788\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0843 - acc: 0.9799 - val_loss: 0.0636 - val_acc: 0.9788\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0840 - acc: 0.9811 - val_loss: 0.0623 - val_acc: 0.9841\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.0835 - acc: 0.9811 - val_loss: 0.0633 - val_acc: 0.9788\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0832 - acc: 0.9811 - val_loss: 0.0614 - val_acc: 0.9788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0828 - acc: 0.9811 - val_loss: 0.0611 - val_acc: 0.9841\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0831 - acc: 0.9805 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0822 - acc: 0.9811 - val_loss: 0.0599 - val_acc: 0.9788\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0820 - acc: 0.9799 - val_loss: 0.0597 - val_acc: 0.9841\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0818 - acc: 0.9817 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0810 - acc: 0.9811 - val_loss: 0.0601 - val_acc: 0.9788\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0808 - acc: 0.9817 - val_loss: 0.0626 - val_acc: 0.9841\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.0804 - acc: 0.9811 - val_loss: 0.0593 - val_acc: 0.9841\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0804 - acc: 0.9799 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0798 - acc: 0.9811 - val_loss: 0.0567 - val_acc: 0.9894\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0789 - acc: 0.9811 - val_loss: 0.0652 - val_acc: 0.9841\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0791 - acc: 0.9823 - val_loss: 0.0598 - val_acc: 0.9841\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0790 - acc: 0.9823 - val_loss: 0.0576 - val_acc: 0.9788\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0784 - acc: 0.9823 - val_loss: 0.0609 - val_acc: 0.9894\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0782 - acc: 0.9823 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0779 - acc: 0.9829 - val_loss: 0.0562 - val_acc: 0.9841\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0779 - acc: 0.9829 - val_loss: 0.0564 - val_acc: 0.9894\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0770 - acc: 0.9829 - val_loss: 0.0555 - val_acc: 0.9894\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0770 - acc: 0.9829 - val_loss: 0.0560 - val_acc: 0.9894\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0766 - acc: 0.9835 - val_loss: 0.0563 - val_acc: 0.9894\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0764 - acc: 0.9829 - val_loss: 0.0534 - val_acc: 0.9894\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0762 - acc: 0.9829 - val_loss: 0.0538 - val_acc: 0.9894\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0759 - acc: 0.9829 - val_loss: 0.0551 - val_acc: 0.9894\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0757 - acc: 0.9840 - val_loss: 0.0550 - val_acc: 0.9947\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0755 - acc: 0.9835 - val_loss: 0.0541 - val_acc: 0.9894\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0749 - acc: 0.9835 - val_loss: 0.0518 - val_acc: 0.9894\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0749 - acc: 0.9846 - val_loss: 0.0521 - val_acc: 0.9947\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0746 - acc: 0.9829 - val_loss: 0.0520 - val_acc: 0.9947\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0745 - acc: 0.9840 - val_loss: 0.0527 - val_acc: 0.9894\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0745 - acc: 0.9835 - val_loss: 0.0511 - val_acc: 0.9947\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0736 - acc: 0.9835 - val_loss: 0.0523 - val_acc: 0.9894\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0736 - acc: 0.9846 - val_loss: 0.0501 - val_acc: 0.9894\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0734 - acc: 0.9840 - val_loss: 0.0511 - val_acc: 0.9894\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0732 - acc: 0.9829 - val_loss: 0.0526 - val_acc: 0.9894\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0730 - acc: 0.9840 - val_loss: 0.0496 - val_acc: 0.9947\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0729 - acc: 0.9829 - val_loss: 0.0497 - val_acc: 0.9894\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0718 - acc: 0.9846 - val_loss: 0.0500 - val_acc: 0.9894\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0717 - acc: 0.9846 - val_loss: 0.0523 - val_acc: 0.9894\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0719 - acc: 0.9835 - val_loss: 0.0502 - val_acc: 0.9894\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0718 - acc: 0.9846 - val_loss: 0.0524 - val_acc: 0.9894\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0715 - acc: 0.9846 - val_loss: 0.0492 - val_acc: 0.9894\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0711 - acc: 0.9840 - val_loss: 0.0480 - val_acc: 0.9947\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0715 - acc: 0.9840 - val_loss: 0.0494 - val_acc: 0.9894\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0705 - acc: 0.9840 - val_loss: 0.0502 - val_acc: 0.9894\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0702 - acc: 0.9840 - val_loss: 0.0491 - val_acc: 0.9894\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0701 - acc: 0.9846 - val_loss: 0.0487 - val_acc: 0.9894\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0698 - acc: 0.9846 - val_loss: 0.0489 - val_acc: 0.9894\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0699 - acc: 0.9846 - val_loss: 0.0477 - val_acc: 0.9894\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0696 - acc: 0.9846 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0692 - acc: 0.9852 - val_loss: 0.0490 - val_acc: 0.9894\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0693 - acc: 0.9852 - val_loss: 0.0458 - val_acc: 0.9947\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0688 - acc: 0.9846 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0690 - acc: 0.9846 - val_loss: 0.0453 - val_acc: 0.9947\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0683 - acc: 0.9858 - val_loss: 0.0465 - val_acc: 0.9894\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0684 - acc: 0.9858 - val_loss: 0.0496 - val_acc: 0.9894\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0680 - acc: 0.9858 - val_loss: 0.0464 - val_acc: 0.9894\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0679 - acc: 0.9852 - val_loss: 0.0469 - val_acc: 0.9894\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0675 - acc: 0.9858 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0675 - acc: 0.9858 - val_loss: 0.0464 - val_acc: 0.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0675 - acc: 0.9858 - val_loss: 0.0442 - val_acc: 1.0000\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0669 - acc: 0.9858 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0668 - acc: 0.9864 - val_loss: 0.0447 - val_acc: 0.9894\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0667 - acc: 0.9858 - val_loss: 0.0458 - val_acc: 0.9947\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0663 - acc: 0.9852 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0663 - acc: 0.9858 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0659 - acc: 0.9858 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0660 - acc: 0.9858 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0657 - acc: 0.9864 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0650 - acc: 0.9864 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0654 - acc: 0.9870 - val_loss: 0.0444 - val_acc: 0.9894\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0649 - acc: 0.9858 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0649 - acc: 0.9864 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0647 - acc: 0.9864 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0650 - acc: 0.9858 - val_loss: 0.0421 - val_acc: 0.9894\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0644 - acc: 0.9858 - val_loss: 0.0438 - val_acc: 0.9894\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0640 - acc: 0.9870 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0642 - acc: 0.9864 - val_loss: 0.0414 - val_acc: 0.9947\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0636 - acc: 0.9870 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0637 - acc: 0.9858 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0632 - acc: 0.9858 - val_loss: 0.0411 - val_acc: 0.9894\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0633 - acc: 0.9858 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0633 - acc: 0.9858 - val_loss: 0.0402 - val_acc: 0.9947\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0626 - acc: 0.9858 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0625 - acc: 0.9858 - val_loss: 0.0398 - val_acc: 0.9947\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0622 - acc: 0.9858 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0623 - acc: 0.9864 - val_loss: 0.0413 - val_acc: 0.9894\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0624 - acc: 0.9870 - val_loss: 0.0402 - val_acc: 0.9894\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0622 - acc: 0.9864 - val_loss: 0.0411 - val_acc: 0.9894\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0620 - acc: 0.9858 - val_loss: 0.0398 - val_acc: 0.9894\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0613 - acc: 0.9870 - val_loss: 0.0391 - val_acc: 0.9947\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0619 - acc: 0.9870 - val_loss: 0.0401 - val_acc: 0.9947\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0611 - acc: 0.9870 - val_loss: 0.0391 - val_acc: 0.9947\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0610 - acc: 0.9858 - val_loss: 0.0402 - val_acc: 0.9894\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0611 - acc: 0.9864 - val_loss: 0.0410 - val_acc: 0.9894\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0604 - acc: 0.9870 - val_loss: 0.0396 - val_acc: 0.9894\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0604 - acc: 0.9870 - val_loss: 0.0396 - val_acc: 0.9894\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0603 - acc: 0.9870 - val_loss: 0.0390 - val_acc: 0.9894\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0598 - acc: 0.9876 - val_loss: 0.0603 - val_acc: 0.9947\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0605 - acc: 0.9864 - val_loss: 0.0392 - val_acc: 0.9947\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0597 - acc: 0.9876 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0594 - acc: 0.9864 - val_loss: 0.0387 - val_acc: 0.9894\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0593 - acc: 0.9876 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0591 - acc: 0.9876 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0588 - acc: 0.9882 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0594 - acc: 0.9864 - val_loss: 0.0384 - val_acc: 0.9894\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0586 - acc: 0.9882 - val_loss: 0.0372 - val_acc: 0.9947\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0587 - acc: 0.9882 - val_loss: 0.0375 - val_acc: 0.9894\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0581 - acc: 0.9870 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0581 - acc: 0.9882 - val_loss: 0.0384 - val_acc: 0.9894\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0580 - acc: 0.9876 - val_loss: 0.0367 - val_acc: 0.9947\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0579 - acc: 0.9870 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0576 - acc: 0.9882 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0576 - acc: 0.9870 - val_loss: 0.0361 - val_acc: 0.9947\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0573 - acc: 0.9888 - val_loss: 0.0365 - val_acc: 0.9947\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0571 - acc: 0.9882 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0568 - acc: 0.9882 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0568 - acc: 0.9882 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0563 - acc: 0.9888 - val_loss: 0.0418 - val_acc: 0.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0565 - acc: 0.9888 - val_loss: 0.0365 - val_acc: 0.9894\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0564 - acc: 0.9882 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0562 - acc: 0.9876 - val_loss: 0.0353 - val_acc: 0.9894\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0560 - acc: 0.9882 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0560 - acc: 0.9888 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0557 - acc: 0.9882 - val_loss: 0.0379 - val_acc: 0.9894\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0556 - acc: 0.9882 - val_loss: 0.0349 - val_acc: 0.9894\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0553 - acc: 0.9882 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0552 - acc: 0.9888 - val_loss: 0.0334 - val_acc: 1.0000\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0550 - acc: 0.9888 - val_loss: 0.0345 - val_acc: 0.9894\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0544 - acc: 0.9888 - val_loss: 0.0342 - val_acc: 0.9947\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0545 - acc: 0.9888 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0543 - acc: 0.9882 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0541 - acc: 0.9882 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0542 - acc: 0.9888 - val_loss: 0.0334 - val_acc: 0.9947\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0539 - acc: 0.9882 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0537 - acc: 0.9882 - val_loss: 0.0329 - val_acc: 1.0000\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0536 - acc: 0.9888 - val_loss: 0.0375 - val_acc: 0.9894\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0537 - acc: 0.9894 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0534 - acc: 0.9888 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0526 - acc: 0.9888 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0534 - acc: 0.9894 - val_loss: 0.0315 - val_acc: 0.9947\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0527 - acc: 0.9894 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0526 - acc: 0.9888 - val_loss: 0.0338 - val_acc: 0.9947\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0526 - acc: 0.9894 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0522 - acc: 0.9888 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0525 - acc: 0.9894 - val_loss: 0.0349 - val_acc: 0.9894\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0521 - acc: 0.9894 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0519 - acc: 0.9894 - val_loss: 0.0334 - val_acc: 0.9894\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0519 - acc: 0.9888 - val_loss: 0.0327 - val_acc: 0.9947\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0514 - acc: 0.9894 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0517 - acc: 0.9894 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0512 - acc: 0.9894 - val_loss: 0.0310 - val_acc: 0.9947\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0510 - acc: 0.9894 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0511 - acc: 0.9894 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0509 - acc: 0.9894 - val_loss: 0.0326 - val_acc: 0.9894\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0510 - acc: 0.9888 - val_loss: 0.0313 - val_acc: 0.9947\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0501 - acc: 0.9894 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0504 - acc: 0.9894 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0499 - acc: 0.9894 - val_loss: 0.0317 - val_acc: 1.0000\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0505 - acc: 0.9888 - val_loss: 0.0326 - val_acc: 0.9894\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0500 - acc: 0.9894 - val_loss: 0.0314 - val_acc: 0.9947\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0498 - acc: 0.9894 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0500 - acc: 0.9888 - val_loss: 0.0318 - val_acc: 0.9894\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0493 - acc: 0.9888 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0492 - acc: 0.9894 - val_loss: 0.0330 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0495 - acc: 0.9894 - val_loss: 0.0309 - val_acc: 0.9947\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0494 - acc: 0.9888 - val_loss: 0.0320 - val_acc: 0.9894\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0486 - acc: 0.9888 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0489 - acc: 0.9894 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0487 - acc: 0.9888 - val_loss: 0.0307 - val_acc: 0.9947\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0488 - acc: 0.9900 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0487 - acc: 0.9888 - val_loss: 0.0293 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0483 - acc: 0.9894 - val_loss: 0.0296 - val_acc: 0.9947\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0481 - acc: 0.9894 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0482 - acc: 0.9905 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0480 - acc: 0.9900 - val_loss: 0.0310 - val_acc: 0.9894\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0480 - acc: 0.9900 - val_loss: 0.0286 - val_acc: 0.9947\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0478 - acc: 0.9900 - val_loss: 0.0314 - val_acc: 0.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0476 - acc: 0.9900 - val_loss: 0.0295 - val_acc: 0.9947\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0477 - acc: 0.9900 - val_loss: 0.0294 - val_acc: 0.9947\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0475 - acc: 0.9905 - val_loss: 0.0288 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0471 - acc: 0.9900 - val_loss: 0.0309 - val_acc: 0.9947\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0476 - acc: 0.9900 - val_loss: 0.0290 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0475 - acc: 0.9900 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0469 - acc: 0.9905 - val_loss: 0.0309 - val_acc: 0.9947\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0469 - acc: 0.9894 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0471 - acc: 0.9894 - val_loss: 0.0295 - val_acc: 0.9947\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0467 - acc: 0.9905 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0468 - acc: 0.9905 - val_loss: 0.0292 - val_acc: 0.9947\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0467 - acc: 0.9905 - val_loss: 0.0296 - val_acc: 0.9947\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0464 - acc: 0.9905 - val_loss: 0.0287 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0463 - acc: 0.9905 - val_loss: 0.0286 - val_acc: 0.9947\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0465 - acc: 0.9905 - val_loss: 0.0303 - val_acc: 0.9894\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0463 - acc: 0.9905 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0509 - acc: 0.988 - 0s 51us/step - loss: 0.0460 - acc: 0.9900 - val_loss: 0.0297 - val_acc: 0.9947\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0459 - acc: 0.9905 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0459 - acc: 0.9905 - val_loss: 0.0278 - val_acc: 0.9947\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0455 - acc: 0.9900 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0454 - acc: 0.9905 - val_loss: 0.0290 - val_acc: 1.0000\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0454 - acc: 0.9911 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0456 - acc: 0.9905 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0452 - acc: 0.9905 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0452 - acc: 0.9905 - val_loss: 0.0275 - val_acc: 0.9947\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0452 - acc: 0.9905 - val_loss: 0.0281 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0452 - acc: 0.9911 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0452 - acc: 0.9905 - val_loss: 0.0265 - val_acc: 0.9947\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0446 - acc: 0.9911 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0446 - acc: 0.9905 - val_loss: 0.0278 - val_acc: 0.9947\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0448 - acc: 0.9911 - val_loss: 0.0279 - val_acc: 0.9947\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0447 - acc: 0.9905 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0446 - acc: 0.9905 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0446 - acc: 0.9905 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0445 - acc: 0.9905 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0443 - acc: 0.9905 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0439 - acc: 0.9905 - val_loss: 0.0266 - val_acc: 0.9947\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0439 - acc: 0.9911 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0441 - acc: 0.9905 - val_loss: 0.0269 - val_acc: 0.9947\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0438 - acc: 0.9911 - val_loss: 0.0271 - val_acc: 0.9947\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0434 - acc: 0.9917 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0440 - acc: 0.9905 - val_loss: 0.0269 - val_acc: 0.9947\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0438 - acc: 0.9905 - val_loss: 0.0294 - val_acc: 0.9947\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0437 - acc: 0.9917 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0435 - acc: 0.9911 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0431 - acc: 0.9911 - val_loss: 0.0267 - val_acc: 1.0000\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0431 - acc: 0.9917 - val_loss: 0.0252 - val_acc: 1.0000\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0432 - acc: 0.9917 - val_loss: 0.0264 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0432 - acc: 0.9905 - val_loss: 0.0262 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0431 - acc: 0.9917 - val_loss: 0.0260 - val_acc: 0.9947\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0427 - acc: 0.9917 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0430 - acc: 0.9917 - val_loss: 0.0285 - val_acc: 0.9894\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0429 - acc: 0.9911 - val_loss: 0.0266 - val_acc: 0.9947\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0425 - acc: 0.9911 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0425 - acc: 0.9917 - val_loss: 0.0278 - val_acc: 0.9894\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0423 - acc: 0.9923 - val_loss: 0.0267 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0422 - acc: 0.9917 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0424 - acc: 0.9917 - val_loss: 0.0261 - val_acc: 0.9894\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0424 - acc: 0.9923 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0422 - acc: 0.9917 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0417 - acc: 0.9911 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0421 - acc: 0.9923 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0419 - acc: 0.9923 - val_loss: 0.0256 - val_acc: 0.9947\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0418 - acc: 0.9923 - val_loss: 0.0260 - val_acc: 0.9947\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0421 - acc: 0.9917 - val_loss: 0.0280 - val_acc: 0.9894\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0417 - acc: 0.9917 - val_loss: 0.0285 - val_acc: 0.9894\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0412 - acc: 0.9923 - val_loss: 0.0264 - val_acc: 0.9947\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0417 - acc: 0.9923 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0417 - acc: 0.9923 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0414 - acc: 0.9923 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0416 - acc: 0.9923 - val_loss: 0.0272 - val_acc: 0.9894\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0414 - acc: 0.9923 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0416 - acc: 0.9923 - val_loss: 0.0303 - val_acc: 0.9894\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0413 - acc: 0.9923 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0412 - acc: 0.9923 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0407 - acc: 0.9923 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0407 - acc: 0.9923 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0408 - acc: 0.9923 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0408 - acc: 0.9923 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0416 - acc: 0.9929 - val_loss: 0.0252 - val_acc: 0.9947\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0402 - acc: 0.9929 - val_loss: 0.0261 - val_acc: 1.0000\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0409 - acc: 0.9923 - val_loss: 0.0253 - val_acc: 1.0000\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0409 - acc: 0.9923 - val_loss: 0.0278 - val_acc: 0.9894\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0401 - acc: 0.9923 - val_loss: 0.0373 - val_acc: 0.9894\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0406 - acc: 0.9929 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0405 - acc: 0.9929 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0401 - acc: 0.9929 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0404 - acc: 0.9929 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0402 - acc: 0.9923 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0403 - acc: 0.9929 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0404 - acc: 0.9917 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0402 - acc: 0.9929 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0399 - acc: 0.9929 - val_loss: 0.0252 - val_acc: 0.9894\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0397 - acc: 0.9929 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0397 - acc: 0.9929 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0398 - acc: 0.9935 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0394 - acc: 0.9929 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0395 - acc: 0.9923 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0397 - acc: 0.9929 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0395 - acc: 0.9935 - val_loss: 0.0253 - val_acc: 0.9894\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0393 - acc: 0.9929 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0394 - acc: 0.9929 - val_loss: 0.0288 - val_acc: 0.9894\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0394 - acc: 0.9929 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0394 - acc: 0.9929 - val_loss: 0.0263 - val_acc: 0.9947\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0391 - acc: 0.9929 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0393 - acc: 0.9929 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.9929 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0393 - acc: 0.9929 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0392 - acc: 0.9935 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0388 - acc: 0.9929 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0391 - acc: 0.9929 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0390 - acc: 0.9929 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0395 - acc: 0.9929 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 0.9929 - val_loss: 0.0250 - val_acc: 0.9947\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0385 - acc: 0.9929 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0388 - acc: 0.9929 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0391 - acc: 0.9929 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 477/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0385 - acc: 0.9929 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0385 - acc: 0.9929 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0387 - acc: 0.9929 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0387 - acc: 0.9929 - val_loss: 0.0255 - val_acc: 0.9894\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0384 - acc: 0.9929 - val_loss: 0.0245 - val_acc: 0.9894\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0384 - acc: 0.9929 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0381 - acc: 0.9935 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0383 - acc: 0.9929 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0380 - acc: 0.9929 - val_loss: 0.0253 - val_acc: 0.9894\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0385 - acc: 0.9935 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0385 - acc: 0.9929 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0382 - acc: 0.9935 - val_loss: 0.0235 - val_acc: 0.9947\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0383 - acc: 0.9929 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0379 - acc: 0.9929 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0378 - acc: 0.9929 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0378 - acc: 0.9929 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0379 - acc: 0.9929 - val_loss: 0.0233 - val_acc: 1.0000\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0374 - acc: 0.9929 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0380 - acc: 0.9929 - val_loss: 0.0292 - val_acc: 1.0000\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0385 - acc: 0.9929 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0378 - acc: 0.9929 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0375 - acc: 0.9935 - val_loss: 0.0240 - val_acc: 0.9947\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0376 - acc: 0.9935 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0375 - acc: 0.9935 - val_loss: 0.0231 - val_acc: 1.0000\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0374 - acc: 0.9935 - val_loss: 0.0255 - val_acc: 0.9894\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0376 - acc: 0.9929 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0377 - acc: 0.9929 - val_loss: 0.0229 - val_acc: 0.9947\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0375 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0374 - acc: 0.9929 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0372 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0372 - acc: 0.9929 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0368 - acc: 0.9935 - val_loss: 0.0272 - val_acc: 0.9894\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0374 - acc: 0.9935 - val_loss: 0.0249 - val_acc: 0.9894\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0371 - acc: 0.9935 - val_loss: 0.0275 - val_acc: 0.9947\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0371 - acc: 0.9935 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0369 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0371 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0373 - acc: 0.9935 - val_loss: 0.0245 - val_acc: 0.9947\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0367 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0369 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9935 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0370 - acc: 0.9941 - val_loss: 0.0228 - val_acc: 0.9947\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0369 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0370 - acc: 0.9935 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0371 - acc: 0.9935 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0367 - acc: 0.9935 - val_loss: 0.0235 - val_acc: 0.9947\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0366 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0370 - acc: 0.9935 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0367 - acc: 0.9935 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0368 - acc: 0.9935 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0362 - acc: 0.9935 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0361 - acc: 0.9935 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0365 - acc: 0.9935 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0361 - acc: 0.9941 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0365 - acc: 0.9935 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9935 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9935 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0364 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0362 - acc: 0.9935 - val_loss: 0.0247 - val_acc: 0.9947\n",
      "Epoch 536/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0359 - acc: 0.9935 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0373 - acc: 0.9935 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0363 - acc: 0.9935 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0363 - acc: 0.9935 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0361 - acc: 0.9935 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0362 - acc: 0.9935 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0359 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0360 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0362 - acc: 0.9935 - val_loss: 0.0262 - val_acc: 0.9947\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0358 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0357 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0362 - acc: 0.9935 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0355 - acc: 0.9941 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0360 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0357 - acc: 0.9935 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0357 - acc: 0.9935 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0357 - acc: 0.9941 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0357 - acc: 0.9935 - val_loss: 0.0210 - val_acc: 0.9947\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0359 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0359 - acc: 0.9935 - val_loss: 0.0252 - val_acc: 0.9894\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0357 - acc: 0.9935 - val_loss: 0.0235 - val_acc: 0.9947\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0356 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0357 - acc: 0.9935 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0353 - acc: 0.9935 - val_loss: 0.0231 - val_acc: 0.9947\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0353 - acc: 0.9935 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0354 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0358 - acc: 0.9935 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0354 - acc: 0.9935 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0356 - acc: 0.9935 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0354 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0349 - acc: 0.9935 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0349 - acc: 0.9935 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0354 - acc: 0.9935 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0353 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0351 - acc: 0.9935 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0350 - acc: 0.9935 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0351 - acc: 0.9935 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0350 - acc: 0.9935 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0350 - acc: 0.9935 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0348 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0348 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 1.0000\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0348 - acc: 0.9941 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0349 - acc: 0.9935 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0346 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0354 - acc: 0.9935 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0349 - acc: 0.9941 - val_loss: 0.0235 - val_acc: 0.9894\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0348 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0346 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0345 - acc: 0.9935 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0352 - acc: 0.9935 - val_loss: 0.0229 - val_acc: 0.9947\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0343 - acc: 0.9935 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0349 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0346 - acc: 0.9929 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0349 - acc: 0.9935 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0342 - acc: 0.9935 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0348 - acc: 0.9935 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0342 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0346 - acc: 0.9935 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0346 - acc: 0.9935 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.0342 - acc: 0.9941 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0346 - acc: 0.9935 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0342 - acc: 0.9935 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0342 - acc: 0.9941 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0340 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0345 - acc: 0.9935 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0340 - acc: 0.9935 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0342 - acc: 0.9935 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0339 - acc: 0.9935 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0342 - acc: 0.9935 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0338 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0335 - acc: 0.9935 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0337 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0336 - acc: 0.9941 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0341 - acc: 0.9935 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0341 - acc: 0.9935 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0336 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0336 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0338 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.0336 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 102us/step - loss: 0.0336 - acc: 0.9941 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0338 - acc: 0.9935 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0337 - acc: 0.9935 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0340 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0337 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0338 - acc: 0.9941 - val_loss: 0.0219 - val_acc: 0.9947\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0334 - acc: 0.9941 - val_loss: 0.0185 - val_acc: 1.0000\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0337 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0339 - acc: 0.9941 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0335 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0336 - acc: 0.9935 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0334 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0335 - acc: 0.9935 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0327 - acc: 0.9941 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0335 - acc: 0.9935 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0332 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0333 - acc: 0.9935 - val_loss: 0.0228 - val_acc: 0.9947\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0330 - acc: 0.9935 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0331 - acc: 0.9941 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0334 - acc: 0.9941 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0330 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0330 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0332 - acc: 0.9941 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 654/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0330 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0330 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0332 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0333 - acc: 0.9935 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0331 - acc: 0.9935 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0327 - acc: 0.9935 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0331 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0333 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0328 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0328 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0328 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0330 - acc: 0.9941 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0328 - acc: 0.9935 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0331 - acc: 0.9941 - val_loss: 0.0210 - val_acc: 0.9947\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0327 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0328 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 1.0000\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0327 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 1.0000\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0323 - acc: 0.9941 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0229 - val_acc: 0.9947\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0324 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0256 - val_acc: 0.9841\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0323 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0323 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0328 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 713/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0322 - acc: 0.9941 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0171 - val_acc: 1.0000\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0321 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 1.0000\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0308 - val_acc: 0.9788\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0318 - acc: 0.9935 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0187 - val_acc: 1.0000\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0270 - val_acc: 0.9841\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0211 - val_acc: 0.9947\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 1.0000\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0320 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0319 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0249 - val_acc: 0.9947\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0234 - val_acc: 0.9947\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0273 - val_acc: 0.9841\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0317 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0224 - val_acc: 0.9894\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0230 - val_acc: 0.9894\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0316 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0228 - val_acc: 0.9894\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0177 - val_acc: 0.9947\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0318 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0314 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0154 - val_acc: 1.0000\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0315 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 1.0000\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0242 - val_acc: 0.9894\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 116us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0312 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 1.0000\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0166 - val_acc: 0.9947\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0233 - val_acc: 0.9894\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0159 - val_acc: 1.0000\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0161 - val_acc: 1.0000\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0311 - acc: 0.9941 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0225 - val_acc: 0.9894\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0309 - acc: 0.9941 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0310 - acc: 0.9941 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0172 - val_acc: 0.9947\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0169 - val_acc: 0.9947\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0313 - acc: 0.9941 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 831/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9894\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0169 - val_acc: 1.0000\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0305 - acc: 0.9941 - val_loss: 0.0154 - val_acc: 1.0000\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0171 - val_acc: 0.9947\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 40us/step - loss: 0.0308 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0159 - val_acc: 0.9947\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0309 - acc: 0.9935 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0307 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0159 - val_acc: 0.9947\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0164 - val_acc: 0.9947\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9947\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0300 - acc: 0.9947 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.0302 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0157 - val_acc: 0.9947\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0301 - acc: 0.9941 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0303 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0306 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0171 - val_acc: 0.9947\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 890/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0143 - val_acc: 1.0000\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0167 - val_acc: 0.9947\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0210 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9947 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0300 - acc: 0.9947 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0156 - val_acc: 0.9947\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0151 - val_acc: 1.0000\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0243 - val_acc: 0.9894\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0300 - acc: 0.9941 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 1.0000\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0298 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0239 - val_acc: 0.9947\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0291 - acc: 0.9947 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0160 - val_acc: 1.0000\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0299 - acc: 0.9941 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0148 - val_acc: 1.0000\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0176 - val_acc: 0.9947\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0165 - val_acc: 0.9947\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0168 - val_acc: 1.0000\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0296 - acc: 0.9941 - val_loss: 0.0187 - val_acc: 0.9947\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 1.0000\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0166 - val_acc: 0.9947\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0295 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 949/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0294 - acc: 0.9947 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0228 - val_acc: 0.9894\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0293 - acc: 0.9941 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 190us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0240 - val_acc: 0.9894\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0193 - val_acc: 0.9947\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0287 - acc: 0.9947 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0167 - val_acc: 0.9947\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0181 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0288 - acc: 0.9947 - val_loss: 0.0187 - val_acc: 0.9947\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0294 - acc: 0.9941 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0222 - val_acc: 0.9894\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0291 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9947\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0285 - acc: 0.9941 - val_loss: 0.0227 - val_acc: 0.9894\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0290 - acc: 0.9947 - val_loss: 0.0164 - val_acc: 0.9947\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0215 - val_acc: 0.9894\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9947\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0226 - val_acc: 0.9894\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0229 - val_acc: 0.9894\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0197 - val_acc: 0.9947\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0231 - val_acc: 0.9894\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0289 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.0290 - acc: 0.9941 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0286 - acc: 0.9941 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0232 - val_acc: 0.9894\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0285 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.0284 - acc: 0.9941 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0283 - acc: 0.9947 - val_loss: 0.0187 - val_acc: 0.9947\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 477us/step - loss: 0.6532 - acc: 0.6436 - val_loss: 0.6172 - val_acc: 0.6614\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.6071 - acc: 0.6939 - val_loss: 0.5758 - val_acc: 0.7143\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.5710 - acc: 0.7524 - val_loss: 0.5209 - val_acc: 0.7989\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.5332 - acc: 0.7730 - val_loss: 0.4837 - val_acc: 0.8519\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.5042 - acc: 0.7896 - val_loss: 0.4520 - val_acc: 0.8466\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4748 - acc: 0.8109 - val_loss: 0.4211 - val_acc: 0.8466\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4474 - acc: 0.8239 - val_loss: 0.3936 - val_acc: 0.8413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4241 - acc: 0.8345 - val_loss: 0.3697 - val_acc: 0.8466\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4028 - acc: 0.8475 - val_loss: 0.3531 - val_acc: 0.8624\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3856 - acc: 0.8511 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3705 - acc: 0.8641 - val_loss: 0.3209 - val_acc: 0.8836\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3579 - acc: 0.8664 - val_loss: 0.3089 - val_acc: 0.8889\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.3450 - acc: 0.8771 - val_loss: 0.2973 - val_acc: 0.8942\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.3338 - acc: 0.8788 - val_loss: 0.2868 - val_acc: 0.8889\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.3229 - acc: 0.8871 - val_loss: 0.2781 - val_acc: 0.8942\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3135 - acc: 0.8859 - val_loss: 0.2672 - val_acc: 0.8995\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3052 - acc: 0.8913 - val_loss: 0.2601 - val_acc: 0.9101\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.2957 - acc: 0.8942 - val_loss: 0.2532 - val_acc: 0.9101\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2879 - acc: 0.8960 - val_loss: 0.2444 - val_acc: 0.9153\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2794 - acc: 0.9043 - val_loss: 0.2366 - val_acc: 0.9153\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2708 - acc: 0.9060 - val_loss: 0.2287 - val_acc: 0.9206\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2651 - acc: 0.9043 - val_loss: 0.2230 - val_acc: 0.9312\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2563 - acc: 0.9119 - val_loss: 0.2141 - val_acc: 0.9206\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2488 - acc: 0.9149 - val_loss: 0.2042 - val_acc: 0.9312\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2417 - acc: 0.9167 - val_loss: 0.1968 - val_acc: 0.9312\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2356 - acc: 0.9161 - val_loss: 0.1912 - val_acc: 0.9418\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2282 - acc: 0.9202 - val_loss: 0.1815 - val_acc: 0.9312\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.2232 - acc: 0.9202 - val_loss: 0.1753 - val_acc: 0.9577\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 138us/step - loss: 0.2160 - acc: 0.9273 - val_loss: 0.1696 - val_acc: 0.9577\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.2101 - acc: 0.9291 - val_loss: 0.1637 - val_acc: 0.9630\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.2060 - acc: 0.9285 - val_loss: 0.1621 - val_acc: 0.9418\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1991 - acc: 0.9314 - val_loss: 0.1536 - val_acc: 0.9735\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1940 - acc: 0.9356 - val_loss: 0.1498 - val_acc: 0.9788\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1910 - acc: 0.9374 - val_loss: 0.1453 - val_acc: 0.9788\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1849 - acc: 0.9368 - val_loss: 0.1488 - val_acc: 0.9735\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1820 - acc: 0.9409 - val_loss: 0.1396 - val_acc: 0.9788\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1768 - acc: 0.9427 - val_loss: 0.1331 - val_acc: 0.9788\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1738 - acc: 0.9415 - val_loss: 0.1322 - val_acc: 0.9788\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1695 - acc: 0.9456 - val_loss: 0.1297 - val_acc: 0.9788\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1654 - acc: 0.9480 - val_loss: 0.1282 - val_acc: 0.9788\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9439 - val_loss: 0.1239 - val_acc: 0.9841\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1605 - acc: 0.9456 - val_loss: 0.1305 - val_acc: 0.9788\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1565 - acc: 0.9492 - val_loss: 0.1177 - val_acc: 0.9788\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1531 - acc: 0.9492 - val_loss: 0.1181 - val_acc: 0.9841\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1504 - acc: 0.9486 - val_loss: 0.1241 - val_acc: 0.9788\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1469 - acc: 0.9515 - val_loss: 0.1172 - val_acc: 0.9841\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1436 - acc: 0.9557 - val_loss: 0.1185 - val_acc: 0.9788\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1426 - acc: 0.9557 - val_loss: 0.1083 - val_acc: 0.9841\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1384 - acc: 0.9569 - val_loss: 0.1126 - val_acc: 0.9841\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1344 - acc: 0.9574 - val_loss: 0.1083 - val_acc: 0.9841\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1314 - acc: 0.9586 - val_loss: 0.1093 - val_acc: 0.9788\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1309 - acc: 0.9598 - val_loss: 0.1045 - val_acc: 0.9841\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1271 - acc: 0.9610 - val_loss: 0.0996 - val_acc: 0.9841\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1246 - acc: 0.9610 - val_loss: 0.1031 - val_acc: 0.9841\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1248 - acc: 0.9616 - val_loss: 0.1057 - val_acc: 0.9788\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1216 - acc: 0.9616 - val_loss: 0.0986 - val_acc: 0.9841\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1180 - acc: 0.9628 - val_loss: 0.0952 - val_acc: 0.9841\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1152 - acc: 0.9628 - val_loss: 0.0955 - val_acc: 0.9841\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1135 - acc: 0.9628 - val_loss: 0.0937 - val_acc: 0.9841\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1115 - acc: 0.9645 - val_loss: 0.0930 - val_acc: 0.9841\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1090 - acc: 0.9634 - val_loss: 0.0943 - val_acc: 0.9841\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1083 - acc: 0.9651 - val_loss: 0.0897 - val_acc: 0.9841\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1055 - acc: 0.9687 - val_loss: 0.0879 - val_acc: 0.9841\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1048 - acc: 0.9681 - val_loss: 0.0897 - val_acc: 0.9841\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1035 - acc: 0.9699 - val_loss: 0.0915 - val_acc: 0.9841\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 138us/step - loss: 0.1013 - acc: 0.9710 - val_loss: 0.0926 - val_acc: 0.9841\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1008 - acc: 0.9722 - val_loss: 0.0896 - val_acc: 0.9841\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.0973 - acc: 0.9740 - val_loss: 0.0847 - val_acc: 0.9841\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0960 - acc: 0.9740 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0941 - acc: 0.9710 - val_loss: 0.0918 - val_acc: 0.9841\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0932 - acc: 0.9758 - val_loss: 0.0861 - val_acc: 0.9841\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0921 - acc: 0.9758 - val_loss: 0.0829 - val_acc: 0.9841\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0901 - acc: 0.9758 - val_loss: 0.0824 - val_acc: 0.9841\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0895 - acc: 0.9752 - val_loss: 0.0856 - val_acc: 0.9841\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0881 - acc: 0.9758 - val_loss: 0.0813 - val_acc: 0.9841\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0867 - acc: 0.9752 - val_loss: 0.0825 - val_acc: 0.9841\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0852 - acc: 0.9781 - val_loss: 0.0797 - val_acc: 0.9841\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0843 - acc: 0.9770 - val_loss: 0.0848 - val_acc: 0.9841\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0832 - acc: 0.9787 - val_loss: 0.0793 - val_acc: 0.9841\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0815 - acc: 0.9770 - val_loss: 0.0791 - val_acc: 0.9841\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0807 - acc: 0.9787 - val_loss: 0.0791 - val_acc: 0.9841\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0795 - acc: 0.9799 - val_loss: 0.0801 - val_acc: 0.9841\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0786 - acc: 0.9799 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0772 - acc: 0.9805 - val_loss: 0.0764 - val_acc: 0.9841\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0765 - acc: 0.9787 - val_loss: 0.0776 - val_acc: 0.9841\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0773 - acc: 0.9781 - val_loss: 0.0776 - val_acc: 0.9841\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0747 - acc: 0.9811 - val_loss: 0.0757 - val_acc: 0.9841\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0735 - acc: 0.9799 - val_loss: 0.0753 - val_acc: 0.9841\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0727 - acc: 0.9805 - val_loss: 0.0767 - val_acc: 0.9841\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0732 - acc: 0.9805 - val_loss: 0.0768 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0730 - acc: 0.9793 - val_loss: 0.0756 - val_acc: 0.9841\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0710 - acc: 0.9805 - val_loss: 0.0747 - val_acc: 0.9841\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0698 - acc: 0.9811 - val_loss: 0.0733 - val_acc: 0.9841\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0720 - acc: 0.9793 - val_loss: 0.0756 - val_acc: 0.9841\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0683 - acc: 0.9817 - val_loss: 0.0738 - val_acc: 0.9841\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0672 - acc: 0.9817 - val_loss: 0.0732 - val_acc: 0.9841\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0670 - acc: 0.9817 - val_loss: 0.0728 - val_acc: 0.9841\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0663 - acc: 0.9811 - val_loss: 0.0722 - val_acc: 0.9841\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0657 - acc: 0.9817 - val_loss: 0.0721 - val_acc: 0.9841\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0653 - acc: 0.9829 - val_loss: 0.0728 - val_acc: 0.9841\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0648 - acc: 0.9811 - val_loss: 0.0706 - val_acc: 0.9841\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0631 - acc: 0.9823 - val_loss: 0.0710 - val_acc: 0.9841\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0622 - acc: 0.9835 - val_loss: 0.0710 - val_acc: 0.9841\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0624 - acc: 0.9823 - val_loss: 0.0704 - val_acc: 0.9841\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0613 - acc: 0.9823 - val_loss: 0.0696 - val_acc: 0.9841\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0616 - acc: 0.9823 - val_loss: 0.0702 - val_acc: 0.9841\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0599 - acc: 0.9823 - val_loss: 0.0685 - val_acc: 0.9841\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0590 - acc: 0.9823 - val_loss: 0.0698 - val_acc: 0.9841\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0596 - acc: 0.9823 - val_loss: 0.0687 - val_acc: 0.9841\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0584 - acc: 0.9829 - val_loss: 0.0689 - val_acc: 0.9841\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0583 - acc: 0.9823 - val_loss: 0.0681 - val_acc: 0.9841\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0572 - acc: 0.9835 - val_loss: 0.0697 - val_acc: 0.9841\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0558 - acc: 0.9840 - val_loss: 0.0684 - val_acc: 0.9841\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0558 - acc: 0.9829 - val_loss: 0.0678 - val_acc: 0.9841\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0558 - acc: 0.9840 - val_loss: 0.0684 - val_acc: 0.9841\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0546 - acc: 0.9840 - val_loss: 0.0682 - val_acc: 0.9841\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0547 - acc: 0.9823 - val_loss: 0.0691 - val_acc: 0.9841\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0543 - acc: 0.9840 - val_loss: 0.0695 - val_acc: 0.9841\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0530 - acc: 0.9846 - val_loss: 0.0659 - val_acc: 0.9841\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0523 - acc: 0.9864 - val_loss: 0.0664 - val_acc: 0.9841\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0524 - acc: 0.9864 - val_loss: 0.0664 - val_acc: 0.9841\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0515 - acc: 0.9852 - val_loss: 0.0664 - val_acc: 0.9841\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0511 - acc: 0.9888 - val_loss: 0.0676 - val_acc: 0.9841\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0513 - acc: 0.9846 - val_loss: 0.0667 - val_acc: 0.9841\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0502 - acc: 0.9876 - val_loss: 0.0644 - val_acc: 0.9841\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0494 - acc: 0.9858 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0491 - acc: 0.9864 - val_loss: 0.0651 - val_acc: 0.9841\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0490 - acc: 0.9870 - val_loss: 0.0649 - val_acc: 0.9841\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0486 - acc: 0.9876 - val_loss: 0.0641 - val_acc: 0.9841\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0491 - acc: 0.9864 - val_loss: 0.0648 - val_acc: 0.9841\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0483 - acc: 0.9882 - val_loss: 0.0637 - val_acc: 0.9841\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0473 - acc: 0.9876 - val_loss: 0.0624 - val_acc: 0.9841\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0471 - acc: 0.9864 - val_loss: 0.0628 - val_acc: 0.9841\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0470 - acc: 0.9888 - val_loss: 0.0618 - val_acc: 0.9841\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0460 - acc: 0.9882 - val_loss: 0.0618 - val_acc: 0.9841\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0455 - acc: 0.9882 - val_loss: 0.0631 - val_acc: 0.9841\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0451 - acc: 0.9894 - val_loss: 0.0613 - val_acc: 0.9841\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0452 - acc: 0.9894 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0447 - acc: 0.9900 - val_loss: 0.0618 - val_acc: 0.9841\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0452 - acc: 0.9876 - val_loss: 0.0577 - val_acc: 0.9841\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0440 - acc: 0.9894 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0436 - acc: 0.9894 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0426 - acc: 0.9900 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0428 - acc: 0.9900 - val_loss: 0.0581 - val_acc: 0.9841\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0425 - acc: 0.9888 - val_loss: 0.0567 - val_acc: 0.9841\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0425 - acc: 0.9894 - val_loss: 0.0573 - val_acc: 0.9841\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0422 - acc: 0.9894 - val_loss: 0.0565 - val_acc: 0.9841\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0418 - acc: 0.9894 - val_loss: 0.0589 - val_acc: 0.9841\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0423 - acc: 0.9900 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0413 - acc: 0.9905 - val_loss: 0.0578 - val_acc: 0.9841\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0412 - acc: 0.9888 - val_loss: 0.0574 - val_acc: 0.9841\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0408 - acc: 0.9905 - val_loss: 0.0548 - val_acc: 0.9841\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0401 - acc: 0.9911 - val_loss: 0.0562 - val_acc: 0.9841\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0395 - acc: 0.9905 - val_loss: 0.0608 - val_acc: 0.9841\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0408 - acc: 0.9888 - val_loss: 0.0552 - val_acc: 0.9841\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0394 - acc: 0.9905 - val_loss: 0.0555 - val_acc: 0.9841\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0400 - acc: 0.9900 - val_loss: 0.0539 - val_acc: 0.9841\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0400 - acc: 0.9894 - val_loss: 0.0526 - val_acc: 0.9841\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0388 - acc: 0.9905 - val_loss: 0.0529 - val_acc: 0.9841\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0390 - acc: 0.9905 - val_loss: 0.0551 - val_acc: 0.9841\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.9905 - val_loss: 0.0538 - val_acc: 0.9841\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0388 - acc: 0.9900 - val_loss: 0.0541 - val_acc: 0.9841\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0381 - acc: 0.9911 - val_loss: 0.0515 - val_acc: 0.9841\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0377 - acc: 0.9900 - val_loss: 0.0574 - val_acc: 0.9841\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0380 - acc: 0.9905 - val_loss: 0.0518 - val_acc: 0.9841\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0375 - acc: 0.9905 - val_loss: 0.0539 - val_acc: 0.9841\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0370 - acc: 0.9911 - val_loss: 0.0570 - val_acc: 0.9841\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0379 - acc: 0.9905 - val_loss: 0.0513 - val_acc: 0.9841\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0365 - acc: 0.9911 - val_loss: 0.0525 - val_acc: 0.9841\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0366 - acc: 0.9911 - val_loss: 0.0508 - val_acc: 0.9841\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0367 - acc: 0.9911 - val_loss: 0.0580 - val_acc: 0.9841\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0363 - acc: 0.9905 - val_loss: 0.0502 - val_acc: 0.9841\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0366 - acc: 0.9905 - val_loss: 0.0510 - val_acc: 0.9841\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0357 - acc: 0.9911 - val_loss: 0.0520 - val_acc: 0.9841\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0356 - acc: 0.9911 - val_loss: 0.0499 - val_acc: 0.9841\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0356 - acc: 0.9911 - val_loss: 0.0489 - val_acc: 0.9841\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0351 - acc: 0.9911 - val_loss: 0.0512 - val_acc: 0.9841\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0349 - acc: 0.9905 - val_loss: 0.0480 - val_acc: 0.9894\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0351 - acc: 0.9917 - val_loss: 0.0511 - val_acc: 0.9841\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0349 - acc: 0.9917 - val_loss: 0.0481 - val_acc: 0.9841\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0350 - acc: 0.9911 - val_loss: 0.0505 - val_acc: 0.9841\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0348 - acc: 0.9911 - val_loss: 0.0522 - val_acc: 0.9841\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0340 - acc: 0.9917 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0333 - acc: 0.9917 - val_loss: 0.0561 - val_acc: 0.9841\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0351 - acc: 0.9905 - val_loss: 0.0504 - val_acc: 0.9841\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0336 - acc: 0.9917 - val_loss: 0.0539 - val_acc: 0.9841\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0340 - acc: 0.9911 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0331 - acc: 0.9917 - val_loss: 0.0522 - val_acc: 0.9841\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0340 - acc: 0.9905 - val_loss: 0.0489 - val_acc: 0.9841\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0336 - acc: 0.9917 - val_loss: 0.0503 - val_acc: 0.9841\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0332 - acc: 0.9911 - val_loss: 0.0523 - val_acc: 0.9841\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0330 - acc: 0.9923 - val_loss: 0.0478 - val_acc: 0.9841\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0328 - acc: 0.9923 - val_loss: 0.0486 - val_acc: 0.9841\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0330 - acc: 0.9929 - val_loss: 0.0512 - val_acc: 0.9841\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0331 - acc: 0.9929 - val_loss: 0.0488 - val_acc: 0.9841\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0339 - acc: 0.9929 - val_loss: 0.0467 - val_acc: 0.9841\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0322 - acc: 0.9929 - val_loss: 0.0507 - val_acc: 0.9841\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0325 - acc: 0.9929 - val_loss: 0.0466 - val_acc: 0.9894\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0318 - acc: 0.9929 - val_loss: 0.0525 - val_acc: 0.9841\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0320 - acc: 0.9929 - val_loss: 0.0445 - val_acc: 0.9894\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0315 - acc: 0.9935 - val_loss: 0.0497 - val_acc: 0.9841\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0315 - acc: 0.9935 - val_loss: 0.0493 - val_acc: 0.9841\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.0463 - val_acc: 0.9841\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.0457 - val_acc: 0.9841\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0313 - acc: 0.9929 - val_loss: 0.0554 - val_acc: 0.9841\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0325 - acc: 0.9929 - val_loss: 0.0533 - val_acc: 0.9841\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0329 - acc: 0.9923 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.0449 - val_acc: 0.9894\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0309 - acc: 0.9935 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0320 - acc: 0.9929 - val_loss: 0.0434 - val_acc: 0.9841\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0316 - acc: 0.9935 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.0460 - val_acc: 0.9841\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0315 - acc: 0.9935 - val_loss: 0.0434 - val_acc: 0.9894\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0307 - acc: 0.9935 - val_loss: 0.0428 - val_acc: 0.9894\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0302 - acc: 0.9935 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0306 - acc: 0.9935 - val_loss: 0.0443 - val_acc: 0.9894\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0302 - acc: 0.9935 - val_loss: 0.0495 - val_acc: 0.9841\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0306 - acc: 0.9935 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0302 - acc: 0.9935 - val_loss: 0.0439 - val_acc: 0.9841\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0299 - acc: 0.9935 - val_loss: 0.0453 - val_acc: 0.9894\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0296 - acc: 0.9935 - val_loss: 0.0435 - val_acc: 0.9894\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0294 - acc: 0.9935 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0300 - acc: 0.9935 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0297 - acc: 0.9935 - val_loss: 0.0449 - val_acc: 0.9894\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0291 - acc: 0.9935 - val_loss: 0.0454 - val_acc: 0.9841\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0298 - acc: 0.9935 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0291 - acc: 0.9935 - val_loss: 0.0437 - val_acc: 0.9894\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0297 - acc: 0.9935 - val_loss: 0.0449 - val_acc: 0.9841\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0291 - acc: 0.9935 - val_loss: 0.0411 - val_acc: 0.9947\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0449 - val_acc: 0.9841\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0467 - val_acc: 0.9841\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0292 - acc: 0.9935 - val_loss: 0.0491 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0286 - acc: 0.9935 - val_loss: 0.0396 - val_acc: 0.9947\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0292 - acc: 0.9935 - val_loss: 0.0436 - val_acc: 0.9894\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0283 - acc: 0.9935 - val_loss: 0.0447 - val_acc: 0.9841\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0285 - acc: 0.9935 - val_loss: 0.0440 - val_acc: 0.9894\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0281 - acc: 0.9935 - val_loss: 0.0410 - val_acc: 0.9947\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0281 - acc: 0.9935 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0285 - acc: 0.9935 - val_loss: 0.0407 - val_acc: 0.9947\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0290 - acc: 0.9935 - val_loss: 0.0387 - val_acc: 0.9947\n",
      "Epoch 244/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0462 - val_acc: 0.9841\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0403 - val_acc: 0.9947\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0407 - val_acc: 0.9947\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0275 - acc: 0.9935 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0467 - val_acc: 0.9841\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0283 - acc: 0.9935 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0272 - acc: 0.9935 - val_loss: 0.0433 - val_acc: 0.9894\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0276 - acc: 0.9935 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0269 - acc: 0.9935 - val_loss: 0.0391 - val_acc: 0.9947\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0259 - acc: 0.9935 - val_loss: 0.0480 - val_acc: 0.9841\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0279 - acc: 0.9935 - val_loss: 0.0396 - val_acc: 0.9947\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0274 - acc: 0.9935 - val_loss: 0.0451 - val_acc: 0.9894\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0268 - acc: 0.9935 - val_loss: 0.0404 - val_acc: 0.9947\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0269 - acc: 0.9941 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0271 - acc: 0.9935 - val_loss: 0.0431 - val_acc: 0.9894\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.0400 - val_acc: 0.9894\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0271 - acc: 0.9935 - val_loss: 0.0458 - val_acc: 0.9841\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.0437 - val_acc: 0.9894\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0274 - acc: 0.9935 - val_loss: 0.0396 - val_acc: 0.9947\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.0400 - val_acc: 0.9947\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9894\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0263 - acc: 0.9935 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0268 - acc: 0.9935 - val_loss: 0.0438 - val_acc: 0.9894\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0265 - acc: 0.9935 - val_loss: 0.0409 - val_acc: 0.9947\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0262 - acc: 0.9935 - val_loss: 0.0395 - val_acc: 0.9947\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0366 - val_acc: 0.9947\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 0.0369 - val_acc: 0.9947\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0261 - acc: 0.9935 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0257 - acc: 0.9941 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0267 - acc: 0.9935 - val_loss: 0.0371 - val_acc: 0.9947\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0260 - acc: 0.9941 - val_loss: 0.0392 - val_acc: 0.9947\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0257 - acc: 0.9941 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0268 - acc: 0.9935 - val_loss: 0.0358 - val_acc: 0.9947\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0253 - acc: 0.9935 - val_loss: 0.0368 - val_acc: 0.9947\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0259 - acc: 0.9935 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0378 - val_acc: 0.9947\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0253 - acc: 0.9935 - val_loss: 0.0400 - val_acc: 0.9947\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0258 - acc: 0.9935 - val_loss: 0.0380 - val_acc: 0.9947\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0447 - val_acc: 0.9894\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0266 - acc: 0.9935 - val_loss: 0.0432 - val_acc: 0.9894\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0257 - acc: 0.9929 - val_loss: 0.0393 - val_acc: 0.9947\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0373 - val_acc: 0.9947\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9894\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0252 - acc: 0.9935 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0456 - val_acc: 0.9894\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0259 - acc: 0.9947 - val_loss: 0.0433 - val_acc: 0.9894\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0252 - acc: 0.9935 - val_loss: 0.0358 - val_acc: 0.9947\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0437 - val_acc: 0.9894\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0252 - acc: 0.9941 - val_loss: 0.0422 - val_acc: 0.9894\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0405 - val_acc: 0.9947\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0249 - acc: 0.9941 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0244 - acc: 0.9935 - val_loss: 0.0366 - val_acc: 0.9947\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0247 - acc: 0.9941 - val_loss: 0.0373 - val_acc: 0.9947\n",
      "Epoch 303/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0255 - acc: 0.9941 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0246 - acc: 0.9941 - val_loss: 0.0385 - val_acc: 0.9947\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 0.9935 - val_loss: 0.0391 - val_acc: 0.9947\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0246 - acc: 0.9941 - val_loss: 0.0358 - val_acc: 0.9947\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0445 - val_acc: 0.9894\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.0399 - val_acc: 0.9894\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0444 - val_acc: 0.9894\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0241 - acc: 0.9941 - val_loss: 0.0356 - val_acc: 0.9947\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0374 - val_acc: 0.9947\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0242 - acc: 0.9947 - val_loss: 0.0382 - val_acc: 0.9947\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9894\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0243 - acc: 0.9941 - val_loss: 0.0363 - val_acc: 0.9947\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9941 - val_loss: 0.0363 - val_acc: 0.9947\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0408 - val_acc: 0.9894\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9941 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0374 - val_acc: 0.9947\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0345 - val_acc: 0.9947\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0239 - acc: 0.9935 - val_loss: 0.0401 - val_acc: 0.9894\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0249 - acc: 0.9941 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0369 - val_acc: 0.9947\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0243 - acc: 0.9941 - val_loss: 0.0357 - val_acc: 0.9947\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0237 - acc: 0.9941 - val_loss: 0.0352 - val_acc: 0.9947\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0234 - acc: 0.9941 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0240 - acc: 0.9941 - val_loss: 0.0365 - val_acc: 0.9947\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0363 - val_acc: 0.9947\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0386 - val_acc: 0.9947\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0431 - val_acc: 0.9894\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0348 - val_acc: 0.9947\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0371 - val_acc: 0.9947\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0242 - acc: 0.9941 - val_loss: 0.0416 - val_acc: 0.9894\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0340 - val_acc: 0.9947\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0248 - acc: 0.9941 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0340 - val_acc: 0.9947\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0355 - val_acc: 0.9947\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0389 - val_acc: 0.9947\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0419 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9894\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0385 - val_acc: 0.9894\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0384 - val_acc: 0.9894\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0413 - val_acc: 0.9894\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9947\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0322 - val_acc: 0.9947\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0384 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0327 - val_acc: 0.9947\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0365 - val_acc: 0.9947\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0332 - val_acc: 0.9947\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0355 - val_acc: 0.9947\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0347 - val_acc: 0.9947\n",
      "Epoch 362/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0346 - val_acc: 0.9947\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9947\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0343 - val_acc: 0.9947\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0400 - val_acc: 0.9894\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0334 - val_acc: 0.9947\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0343 - val_acc: 0.9947\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0330 - val_acc: 0.9947\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0323 - val_acc: 0.9947\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9894\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0398 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9947\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0345 - val_acc: 0.9947\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0213 - acc: 0.9965 - val_loss: 0.0442 - val_acc: 0.9894\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0429 - val_acc: 0.9894\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0319 - val_acc: 0.9947\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0298 - val_acc: 0.9947\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0335 - val_acc: 0.9947\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0323 - val_acc: 0.9947\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0402 - val_acc: 0.9894\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0335 - val_acc: 0.9947\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0393 - val_acc: 0.9894\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0326 - val_acc: 0.9947\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0309 - val_acc: 0.9947\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0334 - val_acc: 0.9947\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0388 - val_acc: 0.9894\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0399 - val_acc: 0.9894\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0391 - val_acc: 0.9894\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0314 - val_acc: 0.9947\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0342 - val_acc: 0.9947\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0352 - val_acc: 0.9947\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0406 - val_acc: 0.9894\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0302 - val_acc: 0.9947\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0340 - val_acc: 0.9894\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0340 - val_acc: 0.9947\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0319 - val_acc: 0.9947\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9894\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0319 - val_acc: 0.9947\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0386 - val_acc: 0.9894\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0335 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0331 - val_acc: 0.9947\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0398 - val_acc: 0.9894\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0305 - val_acc: 0.9947\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0301 - val_acc: 0.9947\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0318 - val_acc: 0.9947\n",
      "Epoch 421/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0215 - acc: 0.9959 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0339 - val_acc: 0.9947\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0301 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0339 - val_acc: 0.9894\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0326 - val_acc: 0.9947\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0321 - val_acc: 0.9947\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0306 - val_acc: 0.9947\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0324 - val_acc: 0.9947\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0212 - acc: 0.9947 - val_loss: 0.0291 - val_acc: 0.9947\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0304 - val_acc: 0.9947\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0315 - val_acc: 0.9947\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0381 - val_acc: 0.9894\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0306 - val_acc: 0.9947\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0311 - val_acc: 0.9947\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0320 - val_acc: 0.9947\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0210 - acc: 0.9959 - val_loss: 0.0346 - val_acc: 0.9894\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0297 - val_acc: 0.9947\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0212 - acc: 0.9947 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0310 - val_acc: 0.9947\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0205 - acc: 0.9947 - val_loss: 0.0326 - val_acc: 0.9894\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9947\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0301 - val_acc: 0.9947\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9947\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0344 - val_acc: 0.9947\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0299 - val_acc: 0.9947\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0203 - acc: 0.9947 - val_loss: 0.0278 - val_acc: 0.9947\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0202 - acc: 0.9965 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0346 - val_acc: 0.9894\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0312 - val_acc: 0.9947\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0322 - val_acc: 0.9947\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0304 - val_acc: 0.9947\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0386 - val_acc: 0.9894\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0303 - val_acc: 0.9947\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0328 - val_acc: 0.9894\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0282 - val_acc: 0.9947\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0310 - val_acc: 0.9947\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0355 - val_acc: 0.9894\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0308 - val_acc: 0.9947\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0330 - val_acc: 0.9894\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0276 - val_acc: 0.9947\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 480/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0355 - val_acc: 0.9894\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0339 - val_acc: 0.9894\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0327 - val_acc: 0.9894\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0330 - val_acc: 0.9894\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0342 - val_acc: 0.9894\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0393 - val_acc: 0.9894\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0285 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0196 - acc: 0.9959 - val_loss: 0.0302 - val_acc: 0.9947\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0361 - val_acc: 0.9894\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0315 - val_acc: 0.9894\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0190 - acc: 0.9953 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0326 - val_acc: 0.9894\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0200 - acc: 0.9965 - val_loss: 0.0353 - val_acc: 0.9894\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0188 - acc: 0.9965 - val_loss: 0.0349 - val_acc: 0.9894\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0388 - val_acc: 0.9894\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0406 - val_acc: 0.9894\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9947\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9965 - val_loss: 0.0315 - val_acc: 0.9894\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0240 - val_acc: 0.9947\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9894\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.0268 - val_acc: 0.9947\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0190 - acc: 0.9953 - val_loss: 0.0279 - val_acc: 0.9947\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0205 - acc: 0.9947 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0282 - val_acc: 0.9947\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0196 - acc: 0.9959 - val_loss: 0.0352 - val_acc: 0.9894\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0372 - val_acc: 0.9894\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0390 - val_acc: 0.9894\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0310 - val_acc: 0.9894\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0421 - val_acc: 0.9894\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0267 - val_acc: 0.9947\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0245 - val_acc: 0.9947\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9894\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0372 - val_acc: 0.9894\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0319 - val_acc: 0.9894\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0398 - val_acc: 0.9894\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0270 - val_acc: 0.9947\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0395 - val_acc: 0.9894\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0406 - val_acc: 0.9894\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0361 - val_acc: 0.9894\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0416 - val_acc: 0.9894\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9894\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0189 - acc: 0.9953 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.0384 - val_acc: 0.9894\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0257 - val_acc: 0.9947\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0179 - acc: 0.9965 - val_loss: 0.0397 - val_acc: 0.9894\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0307 - val_acc: 0.9894\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0278 - val_acc: 0.9947\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9894\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0314 - val_acc: 0.9894\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0397 - val_acc: 0.9894\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0189 - acc: 0.9953 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0372 - val_acc: 0.9894\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0318 - val_acc: 0.9894\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0270 - val_acc: 0.9947\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9965 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 598/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0260 - val_acc: 0.9947\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0330 - val_acc: 0.9894\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0255 - val_acc: 0.9947\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.0287 - val_acc: 0.9894\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0296 - val_acc: 0.9894\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9894\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0264 - val_acc: 0.9947\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0504 - val_acc: 0.9894\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0353 - val_acc: 0.9894\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0187 - acc: 0.9965 - val_loss: 0.0291 - val_acc: 0.9894\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0325 - val_acc: 0.9894\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0307 - val_acc: 0.9894\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0260 - val_acc: 0.9947\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0319 - val_acc: 0.9894\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9894\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0343 - val_acc: 0.9894\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9953 - val_loss: 0.0290 - val_acc: 0.9894\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0263 - val_acc: 0.9947\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0346 - val_acc: 0.9894\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0288 - val_acc: 0.9894\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0307 - val_acc: 0.9894\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0255 - val_acc: 0.9947\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0275 - val_acc: 0.9947\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0186 - acc: 0.9953 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0388 - val_acc: 0.9894\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.0399 - val_acc: 0.9894\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0355 - val_acc: 0.9894\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 657/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0167 - acc: 0.9970 - val_loss: 0.0369 - val_acc: 0.9894\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0363 - val_acc: 0.9894\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0181 - acc: 0.9947 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0332 - val_acc: 0.9894\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9965 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0352 - val_acc: 0.9894\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0419 - val_acc: 0.9894\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0406 - val_acc: 0.9894\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0408 - val_acc: 0.9894\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0177 - acc: 0.9953 - val_loss: 0.0235 - val_acc: 0.9947\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0349 - val_acc: 0.9894\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0276 - val_acc: 0.9894\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0241 - val_acc: 0.9947\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0376 - val_acc: 0.9894\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0311 - val_acc: 0.9894\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0320 - val_acc: 0.9894\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0436 - val_acc: 0.9894\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0257 - val_acc: 0.9947\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0334 - val_acc: 0.9894\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0178 - acc: 0.9965 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0303 - val_acc: 0.9894\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0353 - val_acc: 0.9894\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0183 - acc: 0.9953 - val_loss: 0.0249 - val_acc: 0.9947\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0327 - val_acc: 0.9894\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0288 - val_acc: 0.9894\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0179 - acc: 0.9953 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0287 - val_acc: 0.9894\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0318 - val_acc: 0.9894\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0174 - acc: 0.9953 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0172 - acc: 0.9947 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 716/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0362 - val_acc: 0.9894\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.0336 - val_acc: 0.9894\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0167 - acc: 0.9965 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0405 - val_acc: 0.9894\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0172 - acc: 0.9947 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0174 - acc: 0.9953 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0286 - val_acc: 0.9894\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0235 - val_acc: 0.9947\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0161 - acc: 0.9965 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0348 - val_acc: 0.9894\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0368 - val_acc: 0.9894\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0167 - acc: 0.9965 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0255 - val_acc: 0.9947\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0168 - acc: 0.9970 - val_loss: 0.0276 - val_acc: 0.9894\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0170 - acc: 0.9970 - val_loss: 0.0360 - val_acc: 0.9894\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0277 - val_acc: 0.9894\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0242 - val_acc: 0.9894\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0359 - val_acc: 0.9894\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0373 - val_acc: 0.9894\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 0.0328 - val_acc: 0.9894\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0182 - acc: 0.9941 - val_loss: 0.0428 - val_acc: 0.9894\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0289 - val_acc: 0.9894\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0303 - val_acc: 0.9894\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0244 - val_acc: 0.9894\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0227 - val_acc: 0.9894\n",
      "Epoch 775/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0159 - acc: 0.9970 - val_loss: 0.0379 - val_acc: 0.9894\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0219 - val_acc: 0.9947\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0261 - val_acc: 0.9947\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0220 - val_acc: 0.9947\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0285 - val_acc: 0.9894\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0258 - val_acc: 0.9894\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0349 - val_acc: 0.9894\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0167 - acc: 0.9965 - val_loss: 0.0357 - val_acc: 0.9894\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.0388 - val_acc: 0.9894\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0243 - val_acc: 0.9894\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0168 - acc: 0.9965 - val_loss: 0.0252 - val_acc: 0.9947\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0170 - acc: 0.9953 - val_loss: 0.0236 - val_acc: 0.9947\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0167 - acc: 0.9965 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0308 - val_acc: 0.9894\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0398 - val_acc: 0.9894\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0417 - val_acc: 0.9894\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0280 - val_acc: 0.9894\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0161 - acc: 0.9970 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0315 - val_acc: 0.9894\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0328 - val_acc: 0.9894\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0165 - acc: 0.9970 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0163 - acc: 0.9953 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0170 - acc: 0.9965 - val_loss: 0.0186 - val_acc: 0.9947\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0272 - val_acc: 0.9894\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0171 - acc: 0.9965 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0231 - val_acc: 0.9894\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0161 - acc: 0.9970 - val_loss: 0.0334 - val_acc: 0.9894\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0162 - acc: 0.9970 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 0.0234 - val_acc: 0.9894\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 0.0248 - val_acc: 0.9894\n",
      "Epoch 834/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0234 - val_acc: 0.9947\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0231 - val_acc: 0.9947\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0261 - val_acc: 0.9894\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0171 - acc: 0.9953 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0239 - val_acc: 0.9947\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0162 - acc: 0.9965 - val_loss: 0.0313 - val_acc: 0.9894\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0247 - val_acc: 0.9947\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0241 - val_acc: 0.9894\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0162 - acc: 0.9965 - val_loss: 0.0315 - val_acc: 0.9894\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0251 - val_acc: 0.9894\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0314 - val_acc: 0.9894\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0165 - acc: 0.9965 - val_loss: 0.0261 - val_acc: 0.9894\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0268 - val_acc: 0.9894\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0329 - val_acc: 0.9894\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0170 - acc: 0.9970 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0161 - acc: 0.9965 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0377 - val_acc: 0.9894\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0293 - val_acc: 0.9894\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0305 - val_acc: 0.9894\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0319 - val_acc: 0.9894\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0192 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0269 - val_acc: 0.9894\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0256 - val_acc: 0.9894\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0155 - acc: 0.9970 - val_loss: 0.0323 - val_acc: 0.9894\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0318 - val_acc: 0.9894\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0310 - val_acc: 0.9894\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0164 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9947\n",
      "Epoch 893/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0262 - val_acc: 0.9894\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0270 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0245 - val_acc: 0.9947\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0260 - val_acc: 0.9894\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0156 - acc: 0.9970 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0169 - val_acc: 0.9947\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0309 - val_acc: 0.9894\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9894\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0237 - val_acc: 0.9894\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.0265 - val_acc: 0.9894\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0160 - acc: 0.9953 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0160 - acc: 0.9953 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9970 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0161 - acc: 0.9965 - val_loss: 0.0278 - val_acc: 0.9894\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0149 - acc: 0.9970 - val_loss: 0.0333 - val_acc: 0.9894\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0249 - val_acc: 0.9894\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9970 - val_loss: 0.0247 - val_acc: 0.9947\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0311 - val_acc: 0.9894\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0154 - acc: 0.9970 - val_loss: 0.0326 - val_acc: 0.9894\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0397 - val_acc: 0.9894\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0166 - acc: 0.9965 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0252 - val_acc: 0.9894\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0250 - val_acc: 0.9947\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0296 - val_acc: 0.9894\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0294 - val_acc: 0.9894\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0298 - val_acc: 0.9894\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0285 - val_acc: 0.9894\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0273 - val_acc: 0.9894\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0212 - val_acc: 0.9947\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0225 - val_acc: 0.9947\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0228 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0354 - val_acc: 0.9894\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0160 - acc: 0.9965 - val_loss: 0.0347 - val_acc: 0.9894\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0248 - val_acc: 0.9947\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0266 - val_acc: 0.9894\n",
      "Epoch 952/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.0274 - val_acc: 0.9894\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0154 - acc: 0.9965 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.0292 - val_acc: 0.9894\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0280 - val_acc: 0.9894\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0162 - acc: 0.9965 - val_loss: 0.0300 - val_acc: 0.9894\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0157 - acc: 0.9965 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0371 - val_acc: 0.9894\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0357 - val_acc: 0.9894\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0285 - val_acc: 0.9894\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0208 - val_acc: 0.9947\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0163 - acc: 0.9965 - val_loss: 0.0213 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0272 - val_acc: 0.9894\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0259 - val_acc: 0.9947\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0251 - val_acc: 0.9947\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 0.9965 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0214 - val_acc: 0.9947\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0162 - acc: 0.9970 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0150 - acc: 0.9970 - val_loss: 0.0382 - val_acc: 0.9894\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0246 - val_acc: 0.9894\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0330 - val_acc: 0.9894\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0299 - val_acc: 0.9894\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0263 - val_acc: 0.9894\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0410 - val_acc: 0.9894\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0215 - val_acc: 0.9947\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0148 - acc: 0.9965 - val_loss: 0.0271 - val_acc: 0.9894\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0153 - acc: 0.9970 - val_loss: 0.0279 - val_acc: 0.9894\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0250 - val_acc: 0.9894\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0156 - acc: 0.9965 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0201 - val_acc: 0.9947\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0275 - val_acc: 0.9894\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0232 - val_acc: 0.9894\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0210 - val_acc: 0.9947\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.0264 - val_acc: 0.9894\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0158 - acc: 0.9965 - val_loss: 0.0270 - val_acc: 0.9894\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0284 - val_acc: 0.9894\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0144 - val_acc: 0.9947\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0210 - val_acc: 0.9947\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 495us/step - loss: 0.6564 - acc: 0.6389 - val_loss: 0.6543 - val_acc: 0.5979\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.6190 - acc: 0.6554 - val_loss: 0.6156 - val_acc: 0.6296\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5832 - acc: 0.6968 - val_loss: 0.5683 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5388 - acc: 0.7382 - val_loss: 0.5069 - val_acc: 0.7513\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4867 - acc: 0.7878 - val_loss: 0.4422 - val_acc: 0.7937\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4384 - acc: 0.8268 - val_loss: 0.3866 - val_acc: 0.8571\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3982 - acc: 0.8440 - val_loss: 0.3459 - val_acc: 0.8836\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3666 - acc: 0.8617 - val_loss: 0.3208 - val_acc: 0.8942\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3448 - acc: 0.8812 - val_loss: 0.2941 - val_acc: 0.9153\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3278 - acc: 0.8830 - val_loss: 0.2703 - val_acc: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3109 - acc: 0.8924 - val_loss: 0.2582 - val_acc: 0.9312\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2984 - acc: 0.8960 - val_loss: 0.2446 - val_acc: 0.9259\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2830 - acc: 0.9001 - val_loss: 0.2263 - val_acc: 0.9312\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2697 - acc: 0.9072 - val_loss: 0.2133 - val_acc: 0.9312\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2574 - acc: 0.9113 - val_loss: 0.2079 - val_acc: 0.9365\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2448 - acc: 0.9208 - val_loss: 0.1903 - val_acc: 0.9365\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2318 - acc: 0.9243 - val_loss: 0.1784 - val_acc: 0.9471\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2215 - acc: 0.9261 - val_loss: 0.1847 - val_acc: 0.9788\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2099 - acc: 0.9285 - val_loss: 0.1608 - val_acc: 0.9735\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2011 - acc: 0.9385 - val_loss: 0.1469 - val_acc: 0.9683\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1923 - acc: 0.9409 - val_loss: 0.1406 - val_acc: 0.9788\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1840 - acc: 0.9409 - val_loss: 0.1348 - val_acc: 0.9788\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1768 - acc: 0.9450 - val_loss: 0.1304 - val_acc: 0.9788\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1694 - acc: 0.9504 - val_loss: 0.1342 - val_acc: 0.9788\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1640 - acc: 0.9515 - val_loss: 0.1292 - val_acc: 0.9788\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1581 - acc: 0.9545 - val_loss: 0.1225 - val_acc: 0.9788\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1533 - acc: 0.9551 - val_loss: 0.1163 - val_acc: 0.9788\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1481 - acc: 0.9580 - val_loss: 0.1127 - val_acc: 0.9788\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1445 - acc: 0.9586 - val_loss: 0.1048 - val_acc: 0.9788\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1414 - acc: 0.9610 - val_loss: 0.1113 - val_acc: 0.9788\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1364 - acc: 0.9634 - val_loss: 0.1049 - val_acc: 0.9788\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1326 - acc: 0.9663 - val_loss: 0.1010 - val_acc: 0.9788\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1311 - acc: 0.9639 - val_loss: 0.0975 - val_acc: 0.9788\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1282 - acc: 0.9657 - val_loss: 0.1049 - val_acc: 0.9788\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1262 - acc: 0.9651 - val_loss: 0.1025 - val_acc: 0.9788\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1228 - acc: 0.9675 - val_loss: 0.0983 - val_acc: 0.9788\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1213 - acc: 0.9699 - val_loss: 0.0928 - val_acc: 0.9788\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1192 - acc: 0.9699 - val_loss: 0.0935 - val_acc: 0.9788\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1169 - acc: 0.9699 - val_loss: 0.0898 - val_acc: 0.9788\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1156 - acc: 0.9699 - val_loss: 0.0894 - val_acc: 0.9788\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1137 - acc: 0.9699 - val_loss: 0.0950 - val_acc: 0.9788\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1122 - acc: 0.9699 - val_loss: 0.0880 - val_acc: 0.9788\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1109 - acc: 0.9704 - val_loss: 0.0904 - val_acc: 0.9788\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1098 - acc: 0.9716 - val_loss: 0.0863 - val_acc: 0.9788\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1083 - acc: 0.9710 - val_loss: 0.0855 - val_acc: 0.9788\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1072 - acc: 0.9710 - val_loss: 0.0843 - val_acc: 0.9788\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1055 - acc: 0.9710 - val_loss: 0.0882 - val_acc: 0.9788\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1046 - acc: 0.9716 - val_loss: 0.0884 - val_acc: 0.9788\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1045 - acc: 0.9716 - val_loss: 0.0836 - val_acc: 0.9788\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1030 - acc: 0.9722 - val_loss: 0.0904 - val_acc: 0.9788\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1008 - acc: 0.9722 - val_loss: 0.0834 - val_acc: 0.9788\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1000 - acc: 0.9722 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0994 - acc: 0.9710 - val_loss: 0.0811 - val_acc: 0.9788\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0966 - acc: 0.9740 - val_loss: 0.0800 - val_acc: 0.9788\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0967 - acc: 0.9734 - val_loss: 0.0822 - val_acc: 0.9788\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0961 - acc: 0.9722 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0950 - acc: 0.9722 - val_loss: 0.0803 - val_acc: 0.9788\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0936 - acc: 0.9722 - val_loss: 0.0836 - val_acc: 0.9788\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0920 - acc: 0.9734 - val_loss: 0.0789 - val_acc: 0.9788\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0931 - acc: 0.9740 - val_loss: 0.0827 - val_acc: 0.9788\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0927 - acc: 0.9734 - val_loss: 0.0797 - val_acc: 0.9788\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0914 - acc: 0.9734 - val_loss: 0.0804 - val_acc: 0.9788\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0896 - acc: 0.9740 - val_loss: 0.0803 - val_acc: 0.9788\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0901 - acc: 0.9722 - val_loss: 0.0786 - val_acc: 0.9788\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0894 - acc: 0.9728 - val_loss: 0.0813 - val_acc: 0.9788\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0864 - acc: 0.9740 - val_loss: 0.0798 - val_acc: 0.9788\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0872 - acc: 0.9746 - val_loss: 0.0820 - val_acc: 0.9788\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0863 - acc: 0.9734 - val_loss: 0.0792 - val_acc: 0.9788\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0844 - acc: 0.9746 - val_loss: 0.0800 - val_acc: 0.9788\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0857 - acc: 0.9728 - val_loss: 0.0842 - val_acc: 0.9788\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0848 - acc: 0.9770 - val_loss: 0.0797 - val_acc: 0.9788\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0840 - acc: 0.9752 - val_loss: 0.0792 - val_acc: 0.9788\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0836 - acc: 0.9752 - val_loss: 0.0780 - val_acc: 0.9788\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0821 - acc: 0.9775 - val_loss: 0.0788 - val_acc: 0.9788\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0829 - acc: 0.9758 - val_loss: 0.0770 - val_acc: 0.9788\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0811 - acc: 0.9770 - val_loss: 0.0787 - val_acc: 0.9788\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0816 - acc: 0.9775 - val_loss: 0.0789 - val_acc: 0.9788\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0803 - acc: 0.9781 - val_loss: 0.0787 - val_acc: 0.9788\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0805 - acc: 0.9764 - val_loss: 0.0926 - val_acc: 0.9735\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0782 - acc: 0.9793 - val_loss: 0.0765 - val_acc: 0.9788\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0782 - acc: 0.9781 - val_loss: 0.0772 - val_acc: 0.9788\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0766 - acc: 0.9781 - val_loss: 0.0800 - val_acc: 0.9788\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0776 - acc: 0.9793 - val_loss: 0.0785 - val_acc: 0.9788\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0756 - acc: 0.9799 - val_loss: 0.0802 - val_acc: 0.9788\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0754 - acc: 0.9787 - val_loss: 0.0779 - val_acc: 0.9788\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0749 - acc: 0.9781 - val_loss: 0.1040 - val_acc: 0.9630\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0765 - acc: 0.9787 - val_loss: 0.0784 - val_acc: 0.9788\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0751 - acc: 0.9805 - val_loss: 0.0756 - val_acc: 0.9788\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0743 - acc: 0.9793 - val_loss: 0.0746 - val_acc: 0.9788\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0740 - acc: 0.9805 - val_loss: 0.0798 - val_acc: 0.9841\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0708 - acc: 0.9805 - val_loss: 0.0750 - val_acc: 0.9735\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0718 - acc: 0.9793 - val_loss: 0.0745 - val_acc: 0.9735\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0719 - acc: 0.9805 - val_loss: 0.0760 - val_acc: 0.9788\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0710 - acc: 0.9799 - val_loss: 0.0821 - val_acc: 0.9788\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0709 - acc: 0.9805 - val_loss: 0.0787 - val_acc: 0.9735\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0712 - acc: 0.9799 - val_loss: 0.0748 - val_acc: 0.9735\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0708 - acc: 0.9799 - val_loss: 0.0772 - val_acc: 0.9788\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0711 - acc: 0.9811 - val_loss: 0.0754 - val_acc: 0.9735\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0685 - acc: 0.9817 - val_loss: 0.0764 - val_acc: 0.9735\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0688 - acc: 0.9805 - val_loss: 0.0774 - val_acc: 0.9788\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0691 - acc: 0.9811 - val_loss: 0.0770 - val_acc: 0.9735\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0677 - acc: 0.9811 - val_loss: 0.0755 - val_acc: 0.9735\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0666 - acc: 0.9817 - val_loss: 0.0769 - val_acc: 0.9735\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0653 - acc: 0.9811 - val_loss: 0.0752 - val_acc: 0.9788\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0663 - acc: 0.9823 - val_loss: 0.0753 - val_acc: 0.9735\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0655 - acc: 0.9805 - val_loss: 0.0759 - val_acc: 0.9735\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0632 - acc: 0.9829 - val_loss: 0.0839 - val_acc: 0.9735\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0642 - acc: 0.9829 - val_loss: 0.0768 - val_acc: 0.9735\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0638 - acc: 0.9811 - val_loss: 0.0768 - val_acc: 0.9735\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0630 - acc: 0.9823 - val_loss: 0.0801 - val_acc: 0.9788\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0626 - acc: 0.9840 - val_loss: 0.0746 - val_acc: 0.9735\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0627 - acc: 0.9817 - val_loss: 0.0732 - val_acc: 0.9735\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0616 - acc: 0.9817 - val_loss: 0.0746 - val_acc: 0.9735\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0613 - acc: 0.9823 - val_loss: 0.0731 - val_acc: 0.9735\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0616 - acc: 0.9835 - val_loss: 0.0748 - val_acc: 0.9735\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0607 - acc: 0.9840 - val_loss: 0.0759 - val_acc: 0.9735\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0590 - acc: 0.9852 - val_loss: 0.0776 - val_acc: 0.9735\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0598 - acc: 0.9817 - val_loss: 0.0752 - val_acc: 0.9735\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0599 - acc: 0.9835 - val_loss: 0.0749 - val_acc: 0.9735\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0588 - acc: 0.9835 - val_loss: 0.0781 - val_acc: 0.9735\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0587 - acc: 0.9852 - val_loss: 0.0733 - val_acc: 0.9735\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0584 - acc: 0.9846 - val_loss: 0.0741 - val_acc: 0.9735\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0579 - acc: 0.9852 - val_loss: 0.0757 - val_acc: 0.9735\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0573 - acc: 0.9835 - val_loss: 0.0758 - val_acc: 0.9735\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0564 - acc: 0.9852 - val_loss: 0.0780 - val_acc: 0.9735\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0571 - acc: 0.9852 - val_loss: 0.0757 - val_acc: 0.9735\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0552 - acc: 0.9858 - val_loss: 0.0783 - val_acc: 0.9735\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0563 - acc: 0.9852 - val_loss: 0.0781 - val_acc: 0.9735\n",
      "Epoch 129/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0563 - acc: 0.9852 - val_loss: 0.0740 - val_acc: 0.9735\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0554 - acc: 0.9864 - val_loss: 0.0749 - val_acc: 0.9735\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0556 - acc: 0.9858 - val_loss: 0.0764 - val_acc: 0.9735\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0559 - acc: 0.9864 - val_loss: 0.0798 - val_acc: 0.9735\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0552 - acc: 0.9864 - val_loss: 0.0799 - val_acc: 0.9735\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0548 - acc: 0.9864 - val_loss: 0.0828 - val_acc: 0.9735\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0552 - acc: 0.9864 - val_loss: 0.0802 - val_acc: 0.9735\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0550 - acc: 0.9858 - val_loss: 0.0750 - val_acc: 0.9735\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0528 - acc: 0.9858 - val_loss: 0.0771 - val_acc: 0.9735\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0530 - acc: 0.9870 - val_loss: 0.0740 - val_acc: 0.9788\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0533 - acc: 0.9870 - val_loss: 0.0776 - val_acc: 0.9735\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0528 - acc: 0.9864 - val_loss: 0.0774 - val_acc: 0.9735\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0529 - acc: 0.9870 - val_loss: 0.0783 - val_acc: 0.9735\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0524 - acc: 0.9870 - val_loss: 0.0752 - val_acc: 0.9735\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0530 - acc: 0.9870 - val_loss: 0.0780 - val_acc: 0.9735\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0517 - acc: 0.9864 - val_loss: 0.0810 - val_acc: 0.9683\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0526 - acc: 0.9870 - val_loss: 0.0731 - val_acc: 0.9735\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0515 - acc: 0.9870 - val_loss: 0.0751 - val_acc: 0.9735\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0523 - acc: 0.9876 - val_loss: 0.0799 - val_acc: 0.9735\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0511 - acc: 0.9870 - val_loss: 0.0775 - val_acc: 0.9735\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0497 - acc: 0.9870 - val_loss: 0.0790 - val_acc: 0.9735\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0482 - acc: 0.9876 - val_loss: 0.0775 - val_acc: 0.9735\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0502 - acc: 0.9870 - val_loss: 0.0798 - val_acc: 0.9735\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0484 - acc: 0.9870 - val_loss: 0.0792 - val_acc: 0.9735\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0484 - acc: 0.9876 - val_loss: 0.0769 - val_acc: 0.9735\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0501 - acc: 0.9870 - val_loss: 0.0776 - val_acc: 0.9735\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0492 - acc: 0.9870 - val_loss: 0.0747 - val_acc: 0.9735\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0499 - acc: 0.9870 - val_loss: 0.0794 - val_acc: 0.9735\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0496 - acc: 0.9870 - val_loss: 0.0772 - val_acc: 0.9735\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0491 - acc: 0.9870 - val_loss: 0.0767 - val_acc: 0.9788\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0490 - acc: 0.9876 - val_loss: 0.0768 - val_acc: 0.9735\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0480 - acc: 0.9882 - val_loss: 0.0779 - val_acc: 0.9735\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0479 - acc: 0.9882 - val_loss: 0.0766 - val_acc: 0.9735\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0479 - acc: 0.9888 - val_loss: 0.0792 - val_acc: 0.9735\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0481 - acc: 0.9870 - val_loss: 0.0737 - val_acc: 0.9735\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0475 - acc: 0.9882 - val_loss: 0.0847 - val_acc: 0.9735\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0479 - acc: 0.9876 - val_loss: 0.0764 - val_acc: 0.9788\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0471 - acc: 0.9876 - val_loss: 0.0720 - val_acc: 0.9788\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0473 - acc: 0.9882 - val_loss: 0.0800 - val_acc: 0.9735\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0475 - acc: 0.9888 - val_loss: 0.0753 - val_acc: 0.9788\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0468 - acc: 0.9876 - val_loss: 0.0750 - val_acc: 0.9788\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0476 - acc: 0.9888 - val_loss: 0.0756 - val_acc: 0.9735\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0473 - acc: 0.9876 - val_loss: 0.0716 - val_acc: 0.9788\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0465 - acc: 0.9882 - val_loss: 0.0723 - val_acc: 0.9788\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0462 - acc: 0.9888 - val_loss: 0.0737 - val_acc: 0.9788\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0468 - acc: 0.9876 - val_loss: 0.0728 - val_acc: 0.9788\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0468 - acc: 0.9888 - val_loss: 0.0755 - val_acc: 0.9788\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0459 - acc: 0.9876 - val_loss: 0.0733 - val_acc: 0.9788\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0464 - acc: 0.9882 - val_loss: 0.0751 - val_acc: 0.9788\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0451 - acc: 0.9888 - val_loss: 0.0746 - val_acc: 0.9788\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0457 - acc: 0.9888 - val_loss: 0.0707 - val_acc: 0.9788\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0455 - acc: 0.9882 - val_loss: 0.0750 - val_acc: 0.9788\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0440 - acc: 0.9888 - val_loss: 0.0805 - val_acc: 0.9735\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0458 - acc: 0.9888 - val_loss: 0.0744 - val_acc: 0.9788\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0454 - acc: 0.9888 - val_loss: 0.0713 - val_acc: 0.9788\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0451 - acc: 0.9894 - val_loss: 0.0741 - val_acc: 0.9788\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0448 - acc: 0.9888 - val_loss: 0.0726 - val_acc: 0.9788\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.0751 - val_acc: 0.9788\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0443 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9788\n",
      "Epoch 188/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0445 - acc: 0.9882 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0433 - acc: 0.9894 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0441 - acc: 0.9882 - val_loss: 0.0776 - val_acc: 0.9788\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0441 - acc: 0.9894 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0434 - acc: 0.9888 - val_loss: 0.0725 - val_acc: 0.9788\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0431 - acc: 0.9894 - val_loss: 0.0721 - val_acc: 0.9788\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0435 - acc: 0.9888 - val_loss: 0.0812 - val_acc: 0.9788\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0435 - acc: 0.9894 - val_loss: 0.0728 - val_acc: 0.9788\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0434 - acc: 0.9894 - val_loss: 0.0823 - val_acc: 0.9788\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0434 - acc: 0.9894 - val_loss: 0.0766 - val_acc: 0.9788\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0437 - acc: 0.9894 - val_loss: 0.0741 - val_acc: 0.9788\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0424 - acc: 0.9894 - val_loss: 0.0792 - val_acc: 0.9788\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0436 - acc: 0.9882 - val_loss: 0.0760 - val_acc: 0.9788\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0420 - acc: 0.9888 - val_loss: 0.0798 - val_acc: 0.9788\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0421 - acc: 0.9888 - val_loss: 0.0746 - val_acc: 0.9788\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0425 - acc: 0.9888 - val_loss: 0.0755 - val_acc: 0.9788\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0429 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 0.9788\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0423 - acc: 0.9888 - val_loss: 0.0790 - val_acc: 0.9788\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0422 - acc: 0.9894 - val_loss: 0.0763 - val_acc: 0.9788\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0418 - acc: 0.9888 - val_loss: 0.0781 - val_acc: 0.9788\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0414 - acc: 0.9894 - val_loss: 0.0764 - val_acc: 0.9788\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0416 - acc: 0.9888 - val_loss: 0.0772 - val_acc: 0.9788\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0416 - acc: 0.9888 - val_loss: 0.0768 - val_acc: 0.9788\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0413 - acc: 0.9888 - val_loss: 0.0725 - val_acc: 0.9788\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0412 - acc: 0.9894 - val_loss: 0.0760 - val_acc: 0.9788\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0414 - acc: 0.9894 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0419 - acc: 0.9900 - val_loss: 0.0780 - val_acc: 0.9788\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0411 - acc: 0.9894 - val_loss: 0.0762 - val_acc: 0.9788\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 0.0856 - val_acc: 0.9788\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0403 - acc: 0.9888 - val_loss: 0.0792 - val_acc: 0.9788\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0413 - acc: 0.9894 - val_loss: 0.0727 - val_acc: 0.9788\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0411 - acc: 0.9888 - val_loss: 0.0758 - val_acc: 0.9788\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0408 - acc: 0.9900 - val_loss: 0.0799 - val_acc: 0.9788\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0405 - acc: 0.9894 - val_loss: 0.0762 - val_acc: 0.9788\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0397 - acc: 0.9888 - val_loss: 0.0806 - val_acc: 0.9788\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0398 - acc: 0.9900 - val_loss: 0.0828 - val_acc: 0.9788\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0401 - acc: 0.9894 - val_loss: 0.0771 - val_acc: 0.9788\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0392 - acc: 0.9900 - val_loss: 0.0687 - val_acc: 0.9788\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0402 - acc: 0.9894 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0398 - acc: 0.9905 - val_loss: 0.0805 - val_acc: 0.9788\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0397 - acc: 0.9905 - val_loss: 0.0777 - val_acc: 0.9788\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0388 - acc: 0.9900 - val_loss: 0.0755 - val_acc: 0.9788\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0397 - acc: 0.9894 - val_loss: 0.0725 - val_acc: 0.9788\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0399 - acc: 0.9905 - val_loss: 0.0764 - val_acc: 0.9788\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0390 - acc: 0.9894 - val_loss: 0.0743 - val_acc: 0.9788\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.9900 - val_loss: 0.0723 - val_acc: 0.9788\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0379 - acc: 0.9900 - val_loss: 0.0716 - val_acc: 0.9841\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0385 - acc: 0.9894 - val_loss: 0.0780 - val_acc: 0.9788\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0394 - acc: 0.9900 - val_loss: 0.0733 - val_acc: 0.9788\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0384 - acc: 0.9900 - val_loss: 0.0803 - val_acc: 0.9788\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0384 - acc: 0.9905 - val_loss: 0.0777 - val_acc: 0.9788\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0390 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 0.9788\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0382 - acc: 0.9900 - val_loss: 0.0733 - val_acc: 0.9788\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0387 - acc: 0.9894 - val_loss: 0.0747 - val_acc: 0.9788\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0388 - acc: 0.9900 - val_loss: 0.0788 - val_acc: 0.9788\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0382 - acc: 0.9900 - val_loss: 0.0674 - val_acc: 0.9841\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0379 - acc: 0.9900 - val_loss: 0.0667 - val_acc: 0.9841\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0372 - acc: 0.9900 - val_loss: 0.0705 - val_acc: 0.9788\n",
      "Epoch 247/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0376 - acc: 0.9905 - val_loss: 0.0653 - val_acc: 0.9841\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0383 - acc: 0.9900 - val_loss: 0.0740 - val_acc: 0.9788\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0368 - acc: 0.9911 - val_loss: 0.0754 - val_acc: 0.9788\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0368 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9788\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0371 - acc: 0.9900 - val_loss: 0.0701 - val_acc: 0.9788\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0374 - acc: 0.9900 - val_loss: 0.0655 - val_acc: 0.9841\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0373 - acc: 0.9900 - val_loss: 0.0718 - val_acc: 0.9788\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0374 - acc: 0.9905 - val_loss: 0.0662 - val_acc: 0.9788\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0369 - acc: 0.9911 - val_loss: 0.0641 - val_acc: 0.9788\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0371 - acc: 0.9911 - val_loss: 0.0624 - val_acc: 0.9841\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0367 - acc: 0.9905 - val_loss: 0.0688 - val_acc: 0.9788\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0359 - acc: 0.9905 - val_loss: 0.0606 - val_acc: 0.9841\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0355 - acc: 0.9905 - val_loss: 0.0674 - val_acc: 0.9788\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0359 - acc: 0.9905 - val_loss: 0.0602 - val_acc: 0.9841\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0366 - acc: 0.9900 - val_loss: 0.0662 - val_acc: 0.9788\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0360 - acc: 0.9905 - val_loss: 0.0638 - val_acc: 0.9788\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0355 - acc: 0.9905 - val_loss: 0.0571 - val_acc: 0.9841\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0354 - acc: 0.9905 - val_loss: 0.0668 - val_acc: 0.9788\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.0552 - val_acc: 0.9841\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0352 - acc: 0.9905 - val_loss: 0.0610 - val_acc: 0.9788\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0347 - acc: 0.9905 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0351 - acc: 0.9905 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0355 - acc: 0.9917 - val_loss: 0.0583 - val_acc: 0.9841\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0340 - acc: 0.9911 - val_loss: 0.0620 - val_acc: 0.9841\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0348 - acc: 0.9923 - val_loss: 0.0604 - val_acc: 0.9841\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0338 - acc: 0.9917 - val_loss: 0.0700 - val_acc: 0.9788\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0350 - acc: 0.9905 - val_loss: 0.0569 - val_acc: 0.9841\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0348 - acc: 0.9905 - val_loss: 0.0529 - val_acc: 0.9841\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0341 - acc: 0.9923 - val_loss: 0.0497 - val_acc: 0.9841\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0345 - acc: 0.9917 - val_loss: 0.0643 - val_acc: 0.9788\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0340 - acc: 0.9917 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0338 - acc: 0.9923 - val_loss: 0.0661 - val_acc: 0.9788\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0339 - acc: 0.9905 - val_loss: 0.0587 - val_acc: 0.9841\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0341 - acc: 0.9911 - val_loss: 0.0560 - val_acc: 0.9841\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0338 - acc: 0.9917 - val_loss: 0.0495 - val_acc: 0.9841\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0335 - acc: 0.9917 - val_loss: 0.0576 - val_acc: 0.9841\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0330 - acc: 0.9911 - val_loss: 0.0631 - val_acc: 0.9788\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0331 - acc: 0.9923 - val_loss: 0.0531 - val_acc: 0.9841\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0332 - acc: 0.9917 - val_loss: 0.0475 - val_acc: 0.9841\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0333 - acc: 0.9917 - val_loss: 0.0526 - val_acc: 0.9841\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0326 - acc: 0.9923 - val_loss: 0.0526 - val_acc: 0.9841\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0333 - acc: 0.9917 - val_loss: 0.0558 - val_acc: 0.9841\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0332 - acc: 0.9917 - val_loss: 0.0598 - val_acc: 0.9841\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0329 - acc: 0.9917 - val_loss: 0.0576 - val_acc: 0.9788\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0331 - acc: 0.9917 - val_loss: 0.0519 - val_acc: 0.9841\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0319 - acc: 0.9923 - val_loss: 0.0584 - val_acc: 0.9841\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0322 - acc: 0.9923 - val_loss: 0.0504 - val_acc: 0.9841\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0334 - acc: 0.9911 - val_loss: 0.0487 - val_acc: 0.9841\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0329 - acc: 0.9917 - val_loss: 0.0464 - val_acc: 0.9841\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0324 - acc: 0.9923 - val_loss: 0.0582 - val_acc: 0.9841\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0320 - acc: 0.9929 - val_loss: 0.0470 - val_acc: 0.9894\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0327 - acc: 0.9923 - val_loss: 0.0495 - val_acc: 0.9841\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0312 - acc: 0.9917 - val_loss: 0.0618 - val_acc: 0.9841\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0317 - acc: 0.9917 - val_loss: 0.0493 - val_acc: 0.9894\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0327 - acc: 0.9923 - val_loss: 0.0496 - val_acc: 0.9841\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0322 - acc: 0.9923 - val_loss: 0.0502 - val_acc: 0.9841\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0318 - acc: 0.9929 - val_loss: 0.0516 - val_acc: 0.9841\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0316 - acc: 0.9929 - val_loss: 0.0535 - val_acc: 0.9841\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0318 - acc: 0.9923 - val_loss: 0.0490 - val_acc: 0.9841\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0322 - acc: 0.9917 - val_loss: 0.0545 - val_acc: 0.9841\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0310 - acc: 0.9929 - val_loss: 0.0547 - val_acc: 0.9841\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0311 - acc: 0.9929 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0302 - acc: 0.9923 - val_loss: 0.0560 - val_acc: 0.9841\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0317 - acc: 0.9923 - val_loss: 0.0527 - val_acc: 0.9841\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0305 - acc: 0.9929 - val_loss: 0.0514 - val_acc: 0.9894\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0301 - acc: 0.9929 - val_loss: 0.0552 - val_acc: 0.9841\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0304 - acc: 0.9929 - val_loss: 0.0480 - val_acc: 0.9841\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0307 - acc: 0.9929 - val_loss: 0.0498 - val_acc: 0.9841\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0304 - acc: 0.9923 - val_loss: 0.0471 - val_acc: 0.9841\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0301 - acc: 0.9929 - val_loss: 0.0544 - val_acc: 0.9841\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0309 - acc: 0.9929 - val_loss: 0.0535 - val_acc: 0.9841\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.0610 - val_acc: 0.9841\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0286 - acc: 0.9929 - val_loss: 0.0585 - val_acc: 0.9841\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0310 - acc: 0.9935 - val_loss: 0.0542 - val_acc: 0.9841\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0294 - acc: 0.9935 - val_loss: 0.0555 - val_acc: 0.9841\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0299 - acc: 0.9935 - val_loss: 0.0530 - val_acc: 0.9841\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0285 - acc: 0.9935 - val_loss: 0.0507 - val_acc: 0.9841\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0297 - acc: 0.9941 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0293 - acc: 0.9935 - val_loss: 0.0486 - val_acc: 0.9894\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0296 - acc: 0.9929 - val_loss: 0.0485 - val_acc: 0.9841\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0288 - acc: 0.9941 - val_loss: 0.0492 - val_acc: 0.9947\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0287 - acc: 0.9935 - val_loss: 0.0594 - val_acc: 0.9841\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0508 - val_acc: 0.9841\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0537 - val_acc: 0.9841\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0286 - acc: 0.9935 - val_loss: 0.0509 - val_acc: 0.9841\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0292 - acc: 0.9941 - val_loss: 0.0473 - val_acc: 0.9841\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0284 - acc: 0.9935 - val_loss: 0.0503 - val_acc: 0.9841\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0293 - acc: 0.9935 - val_loss: 0.0509 - val_acc: 0.9841\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0267 - acc: 0.9941 - val_loss: 0.0554 - val_acc: 0.9841\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0280 - acc: 0.9935 - val_loss: 0.0514 - val_acc: 0.9841\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0285 - acc: 0.9941 - val_loss: 0.0489 - val_acc: 0.9841\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0276 - acc: 0.9941 - val_loss: 0.0501 - val_acc: 0.9841\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0282 - acc: 0.9935 - val_loss: 0.0509 - val_acc: 0.9841\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0281 - acc: 0.9941 - val_loss: 0.0450 - val_acc: 0.9894\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0275 - acc: 0.9935 - val_loss: 0.0445 - val_acc: 0.9894\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0282 - acc: 0.9941 - val_loss: 0.0523 - val_acc: 0.9841\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0283 - acc: 0.9941 - val_loss: 0.0506 - val_acc: 0.9841\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0281 - acc: 0.9935 - val_loss: 0.0490 - val_acc: 0.9841\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0524 - val_acc: 0.9947\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0287 - acc: 0.9941 - val_loss: 0.0451 - val_acc: 0.9894\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0467 - val_acc: 0.9841\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0274 - acc: 0.9941 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0461 - val_acc: 0.9947\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0276 - acc: 0.9935 - val_loss: 0.0446 - val_acc: 0.9894\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0279 - acc: 0.9941 - val_loss: 0.0443 - val_acc: 0.9894\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0503 - val_acc: 0.9841\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0275 - acc: 0.9935 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0271 - acc: 0.9941 - val_loss: 0.0480 - val_acc: 0.9894\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0274 - acc: 0.9935 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0274 - acc: 0.9947 - val_loss: 0.0473 - val_acc: 0.9894\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0275 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0428 - val_acc: 0.9894\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0269 - acc: 0.9941 - val_loss: 0.0444 - val_acc: 0.9947\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0271 - acc: 0.9947 - val_loss: 0.0464 - val_acc: 0.9894\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0274 - acc: 0.9941 - val_loss: 0.0426 - val_acc: 0.9894\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0270 - acc: 0.9941 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0275 - acc: 0.9941 - val_loss: 0.0451 - val_acc: 0.9894\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0270 - acc: 0.9935 - val_loss: 0.0455 - val_acc: 0.9894\n",
      "Epoch 365/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0272 - acc: 0.9947 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0448 - val_acc: 0.9947\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0259 - acc: 0.9941 - val_loss: 0.0490 - val_acc: 0.9894\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0268 - acc: 0.9941 - val_loss: 0.0484 - val_acc: 0.9841\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0273 - acc: 0.9935 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0411 - val_acc: 0.9947\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0262 - acc: 0.9935 - val_loss: 0.0414 - val_acc: 0.9947\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0265 - acc: 0.9941 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0431 - val_acc: 0.9947\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0261 - acc: 0.9941 - val_loss: 0.0466 - val_acc: 0.9947\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0259 - acc: 0.9941 - val_loss: 0.0452 - val_acc: 0.9894\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0265 - acc: 0.9941 - val_loss: 0.0468 - val_acc: 0.9947\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0272 - acc: 0.9941 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0262 - acc: 0.9941 - val_loss: 0.0481 - val_acc: 0.9894\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0493 - val_acc: 0.9894\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0260 - acc: 0.9941 - val_loss: 0.0450 - val_acc: 0.9947\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0267 - acc: 0.9941 - val_loss: 0.0450 - val_acc: 0.9947\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0262 - acc: 0.9941 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0260 - acc: 0.9935 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0257 - acc: 0.9941 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0269 - acc: 0.9941 - val_loss: 0.0443 - val_acc: 0.9947\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0262 - acc: 0.9941 - val_loss: 0.0435 - val_acc: 0.9947\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0257 - acc: 0.9941 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0266 - acc: 0.9941 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0268 - acc: 0.9941 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0273 - acc: 0.9941 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0439 - val_acc: 0.9947\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0454 - val_acc: 0.9947\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0270 - acc: 0.9941 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0252 - acc: 0.9941 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0263 - acc: 0.9941 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0256 - acc: 0.9947 - val_loss: 0.0456 - val_acc: 0.9947\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0454 - val_acc: 0.9947\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0449 - val_acc: 0.9947\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0264 - acc: 0.9941 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0251 - acc: 0.9935 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0260 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0262 - acc: 0.9947 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0252 - acc: 0.9941 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0264 - acc: 0.9947 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0259 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9947\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0258 - acc: 0.9947 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0258 - acc: 0.9941 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0253 - acc: 0.9941 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0485 - val_acc: 0.9947\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0404 - val_acc: 0.9947\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0262 - acc: 0.9953 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0261 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0240 - acc: 0.9947 - val_loss: 0.0549 - val_acc: 0.9947\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0417 - val_acc: 0.9947\n",
      "Epoch 424/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0258 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0410 - val_acc: 0.9947\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0255 - acc: 0.9953 - val_loss: 0.0441 - val_acc: 0.9947\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0255 - acc: 0.9953 - val_loss: 0.0447 - val_acc: 0.9947\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0452 - val_acc: 0.9947\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0257 - acc: 0.9947 - val_loss: 0.0417 - val_acc: 0.9947\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9947\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0205 - acc: 0.996 - 0s 50us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9947\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0254 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0450 - val_acc: 0.9894\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0249 - acc: 0.9947 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0255 - acc: 0.9947 - val_loss: 0.0448 - val_acc: 0.9947\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0501 - val_acc: 0.9894\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0457 - val_acc: 0.9894\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0464 - val_acc: 0.9894\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0417 - val_acc: 0.9947\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0250 - acc: 0.9947 - val_loss: 0.0451 - val_acc: 0.9947\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0257 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0253 - acc: 0.9953 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0243 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0255 - acc: 0.9953 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0256 - acc: 0.9953 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0419 - val_acc: 0.9947\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0254 - acc: 0.9947 - val_loss: 0.0448 - val_acc: 0.9894\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0443 - val_acc: 0.9947\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9947\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0248 - acc: 0.9947 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9947\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0414 - val_acc: 0.9947\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0468 - val_acc: 0.9894\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0245 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0472 - val_acc: 0.9894\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0435 - val_acc: 0.9947\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0435 - val_acc: 0.9947\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9894\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0462 - val_acc: 0.9894\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0441 - val_acc: 0.9947\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0251 - acc: 0.9947 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0242 - acc: 0.9947 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0252 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0251 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0237 - acc: 0.9947 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0470 - val_acc: 0.9894\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0249 - acc: 0.9953 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.994 - 0s 51us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9894\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0462 - val_acc: 0.9947\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0248 - acc: 0.9953 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0431 - val_acc: 0.9947\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0244 - acc: 0.9947 - val_loss: 0.0457 - val_acc: 0.9947\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0435 - val_acc: 0.9947\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0447 - val_acc: 0.9894\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0451 - val_acc: 0.9947\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0490 - val_acc: 0.9947\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0245 - acc: 0.9947 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0439 - val_acc: 0.9947\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9947\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0439 - val_acc: 0.9947\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0463 - val_acc: 0.9894\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0243 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9894\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9894\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9894\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0242 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0237 - acc: 0.9953 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0455 - val_acc: 0.9947\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0439 - val_acc: 0.9947\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0452 - val_acc: 0.9947\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0447 - val_acc: 0.9947\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0464 - val_acc: 0.9947\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0489 - val_acc: 0.9894\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0241 - acc: 0.9953 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0447 - val_acc: 0.9947\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0463 - val_acc: 0.9947\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0242 - acc: 0.9947 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0427 - val_acc: 0.9947\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0450 - val_acc: 0.9947\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0482 - val_acc: 0.9894\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0191 - acc: 0.995 - 0s 51us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0467 - val_acc: 0.9894\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0450 - val_acc: 0.9947\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0238 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0448 - val_acc: 0.9947\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0525 - val_acc: 0.9894\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0441 - val_acc: 0.9947\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9947\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0474 - val_acc: 0.9947\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0481 - val_acc: 0.9947\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0239 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9947\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0483 - val_acc: 0.9947\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0427 - val_acc: 0.9947\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0784 - val_acc: 0.9894\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0226 - acc: 0.9959 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0481 - val_acc: 0.9894\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0451 - val_acc: 0.9947\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0439 - val_acc: 0.9947\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9947\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0470 - val_acc: 0.9894\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0470 - val_acc: 0.9894\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0472 - val_acc: 0.9894\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0414 - val_acc: 0.9947\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0467 - val_acc: 0.9894\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9947\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0465 - val_acc: 0.9894\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0431 - val_acc: 0.9947\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0233 - acc: 0.9953 - val_loss: 0.0451 - val_acc: 0.9947\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0430 - val_acc: 0.9947\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0455 - val_acc: 0.9894\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0419 - val_acc: 0.9947\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0235 - acc: 0.9953 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0231 - acc: 0.9953 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0427 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0434 - val_acc: 0.9947\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0445 - val_acc: 0.9947\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9894\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0227 - acc: 0.9959 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0460 - val_acc: 0.9894\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0481 - val_acc: 0.9894\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0443 - val_acc: 0.9947\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0502 - val_acc: 0.9894\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.0487 - val_acc: 0.9947\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0236 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0466 - val_acc: 0.9894\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0428 - val_acc: 0.9947\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9947\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0485 - val_acc: 0.9894\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0441 - val_acc: 0.9947\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0471 - val_acc: 0.9894\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0497 - val_acc: 0.9894\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0448 - val_acc: 0.9947\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0444 - val_acc: 0.9947\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0456 - val_acc: 0.9894\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0214 - acc: 0.9959 - val_loss: 0.0472 - val_acc: 0.9894\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0420 - val_acc: 0.9947\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0447 - val_acc: 0.9894\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0216 - acc: 0.9959 - val_loss: 0.0457 - val_acc: 0.9894\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0449 - val_acc: 0.9947\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0211 - acc: 0.9959 - val_loss: 0.0452 - val_acc: 0.9947\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0468 - val_acc: 0.9894\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0495 - val_acc: 0.9894\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.0470 - val_acc: 0.9894\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0409 - val_acc: 0.9947\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0448 - val_acc: 0.9894\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0232 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0449 - val_acc: 0.9947\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0427 - val_acc: 0.9947\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0439 - val_acc: 0.9894\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9894\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0421 - val_acc: 0.9947\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0486 - val_acc: 0.9947\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0487 - val_acc: 0.9894\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0228 - acc: 0.9953 - val_loss: 0.0438 - val_acc: 0.9947\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9894\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0401 - val_acc: 0.9947\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0412 - val_acc: 0.9947\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0222 - acc: 0.9959 - val_loss: 0.0431 - val_acc: 0.9947\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0224 - acc: 0.9941 - val_loss: 0.0436 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0419 - val_acc: 0.9947\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0419 - val_acc: 0.9947\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0405 - val_acc: 0.9947\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0458 - val_acc: 0.9894\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0435 - val_acc: 0.9894\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0492 - val_acc: 0.9894\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.0412 - val_acc: 0.9947\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0445 - val_acc: 0.9947\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9947 - val_loss: 0.0397 - val_acc: 0.9947\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9894\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9947\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0212 - acc: 0.9959 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0426 - val_acc: 0.9894\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0404 - val_acc: 0.9947\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0434 - val_acc: 0.9894\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0411 - val_acc: 0.9947\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9947\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0567 - val_acc: 0.9894\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0228 - acc: 0.9941 - val_loss: 0.0426 - val_acc: 0.9947\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0401 - val_acc: 0.9947\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0432 - val_acc: 0.9947\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0222 - acc: 0.9959 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0429 - val_acc: 0.9947\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9894\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0440 - val_acc: 0.9894\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0224 - acc: 0.9953 - val_loss: 0.0418 - val_acc: 0.9947\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0474 - val_acc: 0.9894\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0405 - val_acc: 0.9947\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0391 - val_acc: 0.9947\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.0230 - acc: 0.9953 - val_loss: 0.0390 - val_acc: 0.9947\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0427 - val_acc: 0.9894\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0414 - val_acc: 0.9947\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0225 - acc: 0.9953 - val_loss: 0.0379 - val_acc: 0.9947\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0422 - val_acc: 0.9947\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0424 - val_acc: 0.9947\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0435 - val_acc: 0.9947\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0399 - val_acc: 0.9947\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.0219 - acc: 0.9941 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0400 - val_acc: 0.9947\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0394 - val_acc: 0.9947\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0400 - val_acc: 0.9947\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0392 - val_acc: 0.9947\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0222 - acc: 0.9953 - val_loss: 0.0502 - val_acc: 0.9894\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0406 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0450 - val_acc: 0.9894\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0454 - val_acc: 0.9894\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0463 - val_acc: 0.9894\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0211 - acc: 0.9959 - val_loss: 0.0592 - val_acc: 0.9894\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0219 - acc: 0.9959 - val_loss: 0.0390 - val_acc: 0.9947\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9947\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0465 - val_acc: 0.9894\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0215 - acc: 0.9959 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9947\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0226 - acc: 0.9953 - val_loss: 0.0442 - val_acc: 0.9894\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0395 - val_acc: 0.9947\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0416 - val_acc: 0.9947\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0216 - acc: 0.9959 - val_loss: 0.0400 - val_acc: 0.9947\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0390 - val_acc: 0.9947\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0567 - val_acc: 0.9894\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0434 - val_acc: 0.9894\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9894\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0440 - val_acc: 0.9894\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0405 - val_acc: 0.9947\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0388 - val_acc: 0.9947\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0220 - acc: 0.9941 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 0.0406 - val_acc: 0.9894\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0216 - acc: 0.9959 - val_loss: 0.0413 - val_acc: 0.9947\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0404 - val_acc: 0.9894\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0220 - acc: 0.9953 - val_loss: 0.0377 - val_acc: 0.9947\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9947\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0402 - val_acc: 0.9947\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0389 - val_acc: 0.9947\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0431 - val_acc: 0.9894\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0212 - acc: 0.9959 - val_loss: 0.0426 - val_acc: 0.9894\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0423 - val_acc: 0.9894\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9894\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0409 - val_acc: 0.9894\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0420 - val_acc: 0.9894\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0395 - val_acc: 0.9947\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0209 - acc: 0.9959 - val_loss: 0.0387 - val_acc: 0.9947\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0395 - val_acc: 0.9947\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0400 - val_acc: 0.9894\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0373 - val_acc: 0.9947\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0371 - val_acc: 0.9947\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0407 - val_acc: 0.9894\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0393 - val_acc: 0.9894\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0375 - val_acc: 0.9947\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0219 - acc: 0.9953 - val_loss: 0.0395 - val_acc: 0.9947\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0379 - val_acc: 0.9947\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.0365 - val_acc: 0.9947\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0401 - val_acc: 0.9894\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0405 - val_acc: 0.9894\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0386 - val_acc: 0.9947\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0379 - val_acc: 0.9947\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0408 - val_acc: 0.9947\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0374 - val_acc: 0.9947\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9894\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0398 - val_acc: 0.9947\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0377 - val_acc: 0.9947\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0394 - val_acc: 0.9894\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0372 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.1347 - val_acc: 0.9683\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0345 - val_acc: 0.9947\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0217 - acc: 0.9953 - val_loss: 0.0407 - val_acc: 0.9894\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0531 - val_acc: 0.9841\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0357 - val_acc: 0.9947\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0403 - val_acc: 0.9894\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.0373 - val_acc: 0.9947\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0556 - val_acc: 0.9841\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0372 - val_acc: 0.9947\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0370 - val_acc: 0.9947\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0380 - val_acc: 0.9894\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0354 - val_acc: 0.9947\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0415 - val_acc: 0.9894\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0402 - val_acc: 0.9894\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0362 - val_acc: 0.9947\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0398 - val_acc: 0.9947\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0347 - val_acc: 0.9947\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0892 - val_acc: 0.9788\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0213 - acc: 0.9959 - val_loss: 0.0440 - val_acc: 0.9947\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.0375 - val_acc: 0.9947\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0216 - acc: 0.9953 - val_loss: 0.0349 - val_acc: 0.9947\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0656 - val_acc: 0.9894\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0227 - acc: 0.9953 - val_loss: 0.0399 - val_acc: 0.9894\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0390 - val_acc: 0.9894\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0383 - val_acc: 0.9947\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0351 - val_acc: 0.9947\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0346 - val_acc: 0.9947\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0371 - val_acc: 0.9947\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0356 - val_acc: 0.9947\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0366 - val_acc: 0.9894\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0205 - acc: 0.9959 - val_loss: 0.0409 - val_acc: 0.9894\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0358 - val_acc: 0.9947\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0377 - val_acc: 0.9894\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0209 - acc: 0.9959 - val_loss: 0.0372 - val_acc: 0.9894\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0395 - val_acc: 0.9894\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0333 - val_acc: 0.9947\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0370 - val_acc: 0.9894\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0339 - val_acc: 0.9947\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0199 - acc: 0.9965 - val_loss: 0.0378 - val_acc: 0.9894\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0353 - val_acc: 0.9947\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0214 - acc: 0.9941 - val_loss: 0.0383 - val_acc: 0.9894\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0196 - acc: 0.9959 - val_loss: 0.0343 - val_acc: 0.9947\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0339 - val_acc: 0.9947\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0329 - val_acc: 0.9947\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0325 - val_acc: 0.9947\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0495 - val_acc: 0.9788\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0215 - acc: 0.9953 - val_loss: 0.0329 - val_acc: 0.9947\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0346 - val_acc: 0.9947\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0313 - val_acc: 0.9947\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0356 - val_acc: 0.9894\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0203 - acc: 0.9965 - val_loss: 0.0296 - val_acc: 0.9947\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0316 - val_acc: 0.9947\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0338 - val_acc: 0.9947\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0350 - val_acc: 0.9894\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0209 - acc: 0.9959 - val_loss: 0.0340 - val_acc: 0.9894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0335 - val_acc: 0.9894\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0335 - val_acc: 0.9947\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0208 - acc: 0.9959 - val_loss: 0.0304 - val_acc: 0.9947\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0297 - val_acc: 0.9947\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0308 - val_acc: 0.9947\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0300 - val_acc: 0.9947\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.0207 - acc: 0.9953 - val_loss: 0.0270 - val_acc: 0.9947\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0205 - acc: 0.9959 - val_loss: 0.0283 - val_acc: 0.9947\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0208 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9947\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0209 - acc: 0.9953 - val_loss: 0.0306 - val_acc: 0.9947\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0211 - acc: 0.9959 - val_loss: 0.0285 - val_acc: 0.9947\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.0312 - val_acc: 0.9947\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0291 - val_acc: 0.9947\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0211 - acc: 0.9959 - val_loss: 0.0291 - val_acc: 0.9947\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0285 - val_acc: 0.9947\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0297 - val_acc: 0.9894\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0284 - val_acc: 0.9947\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0324 - val_acc: 0.9894\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0274 - val_acc: 0.9947\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0305 - val_acc: 0.9947\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0256 - val_acc: 0.9947\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0274 - val_acc: 0.9947\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0298 - val_acc: 0.9947\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0296 - val_acc: 0.9894\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0289 - val_acc: 0.9947\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0364 - val_acc: 0.9947\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0207 - acc: 0.9959 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0267 - val_acc: 0.9894\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0283 - val_acc: 0.9894\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0413 - val_acc: 0.9894\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0211 - acc: 0.9953 - val_loss: 0.0277 - val_acc: 0.9947\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0284 - val_acc: 0.9947\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0306 - val_acc: 0.9947\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0205 - acc: 0.9959 - val_loss: 0.0341 - val_acc: 0.9894\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.0302 - val_acc: 0.9841\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0203 - acc: 0.9947 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0240 - val_acc: 0.9947\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0256 - val_acc: 0.9947\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0166 - acc: 0.9970 - val_loss: 0.0282 - val_acc: 0.9894\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0206 - acc: 0.9959 - val_loss: 0.0254 - val_acc: 0.9947\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0249 - val_acc: 0.9947\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.0218 - acc: 0.995 - 0s 59us/step - loss: 0.0195 - acc: 0.9965 - val_loss: 0.0291 - val_acc: 0.9947\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0299 - val_acc: 0.9947\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0257 - val_acc: 0.9894\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0232 - val_acc: 0.9947\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0230 - val_acc: 0.9947\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0341 - val_acc: 0.9841\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0190 - acc: 0.9965 - val_loss: 0.0234 - val_acc: 0.9947\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0239 - val_acc: 0.9894\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0281 - val_acc: 0.9894\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0253 - val_acc: 0.9947\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0234 - val_acc: 0.9947\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0240 - val_acc: 0.9894\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0246 - val_acc: 0.9947\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0196 - acc: 0.9953 - val_loss: 0.0216 - val_acc: 0.9947\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0204 - acc: 0.9959 - val_loss: 0.0204 - val_acc: 0.9947\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0208 - acc: 0.9959 - val_loss: 0.0233 - val_acc: 0.9947\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0224 - val_acc: 0.9947\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0239 - val_acc: 0.9894\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0182 - val_acc: 0.9947\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0217 - val_acc: 0.9947\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0222 - val_acc: 0.9947\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.0203 - acc: 0.9959 - val_loss: 0.0221 - val_acc: 0.9947\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0202 - acc: 0.9953 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0203 - val_acc: 0.9947\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0184 - acc: 0.9953 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0183 - val_acc: 0.9947\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0240 - val_acc: 0.9894\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0191 - val_acc: 0.9947\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0240 - val_acc: 0.9894\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0215 - val_acc: 0.9894\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0207 - val_acc: 0.9947\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0193 - val_acc: 0.9894\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0179 - val_acc: 0.9894\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0213 - val_acc: 0.9894\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0203 - acc: 0.9959 - val_loss: 0.0182 - val_acc: 0.9894\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0199 - val_acc: 0.9894\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0199 - val_acc: 0.9947\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0189 - acc: 0.9965 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0222 - val_acc: 0.9894\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0161 - val_acc: 0.9947\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0257 - val_acc: 0.9947\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0218 - val_acc: 0.9947\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0187 - acc: 0.9953 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0156 - val_acc: 0.9947\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0175 - val_acc: 0.9947\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0201 - val_acc: 0.9894\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0190 - acc: 0.9953 - val_loss: 0.0194 - val_acc: 0.9894\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0211 - val_acc: 0.9894\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0203 - val_acc: 0.9894\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0205 - acc: 0.9953 - val_loss: 0.0184 - val_acc: 0.9947\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0181 - acc: 0.9970 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0169 - val_acc: 0.9947\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0180 - val_acc: 0.9947\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0202 - val_acc: 0.9894\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.0189 - val_acc: 0.9947\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0166 - val_acc: 0.9947\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0163 - val_acc: 0.9947\n",
      "Epoch 953/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0188 - acc: 0.9965 - val_loss: 0.0173 - val_acc: 0.9947\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0170 - val_acc: 0.9894\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.0194 - acc: 0.9959 - val_loss: 0.0171 - val_acc: 0.9947\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0191 - acc: 0.9953 - val_loss: 0.0243 - val_acc: 0.9947\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0203 - acc: 0.9959 - val_loss: 0.0167 - val_acc: 0.9947\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0187 - val_acc: 0.9894\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0194 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9894\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0193 - val_acc: 0.9894\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0182 - val_acc: 0.9894\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0158 - val_acc: 0.9947\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.0200 - acc: 0.9959 - val_loss: 0.0178 - val_acc: 0.9894\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0192 - acc: 0.9953 - val_loss: 0.0136 - val_acc: 0.9947\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0157 - val_acc: 0.9947\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0134 - val_acc: 0.9947\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0161 - val_acc: 0.9894\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0188 - acc: 0.9959 - val_loss: 0.0169 - val_acc: 0.9894\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.0201 - acc: 0.9959 - val_loss: 0.0120 - val_acc: 0.9947\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0175 - val_acc: 0.9894\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0190 - acc: 0.9959 - val_loss: 0.0152 - val_acc: 0.9947\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0199 - val_acc: 0.9894\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0185 - val_acc: 0.9894\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0138 - val_acc: 0.9947\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0130 - val_acc: 0.9947\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0159 - val_acc: 0.9947\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0209 - val_acc: 0.9894\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0151 - val_acc: 0.9947\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0188 - acc: 0.9953 - val_loss: 0.0154 - val_acc: 0.9947\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0196 - acc: 0.9959 - val_loss: 0.0148 - val_acc: 0.9894\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0157 - val_acc: 0.9947\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0153 - val_acc: 0.9947\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0146 - val_acc: 0.9947\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0119 - val_acc: 0.9947\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0159 - val_acc: 0.9947\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0116 - val_acc: 0.9947\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0193 - acc: 0.9953 - val_loss: 0.0124 - val_acc: 0.9947\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.0181 - acc: 0.9965 - val_loss: 0.0149 - val_acc: 0.9947\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0231 - val_acc: 0.9947\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0132 - val_acc: 0.9947\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.0191 - acc: 0.9965 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.0128 - val_acc: 0.9947\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.0156 - val_acc: 0.9894\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.0198 - acc: 0.9959 - val_loss: 0.0163 - val_acc: 0.9894\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0173 - val_acc: 0.9947\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "optimizer_var = ['rmsprop', 'adagrad', 'SGD', 'adamax', 'adadelta']\n",
    "hum_con_opt_accuracy = []\n",
    "for i in range(len(optimizer_var)):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=18, activation='relu'))\n",
    "    model.add(Dense(8, activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer_var[i], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model, iterating on the data in batches of 32 samples\n",
    "    summary = model.fit(x_train_human_con, y_train_human_con, validation_split = 0.1, nb_epoch=1000, batch_size=32)\n",
    "    scores = model.evaluate(x=x_test_human_con, y=y_test_human_con, verbose=0)\n",
    "    scores_acc = scores[1]\n",
    "    hum_con_opt_accuracy.append(scores_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Optimizer Vs Accuracy')\n",
    "plt.plot(optimizer_var,hum_con_opt_accuracy,'mo-', label='Accuracy')\n",
    "#plt.axis([min(optimizer_var), max(optimizer_var), min(hum_con_opt)-0.01, max(hum_con_opt)])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Optimizer')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Observed Dataset with Feature Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 9)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing datasets for Human Features with Feature Subtraction\n",
    "x_train_human2 = train_human.iloc[:,6].values.tolist()\n",
    "x_train_human_sub = np.array(x_train_human2)\n",
    "y_train_human2 = train_human.iloc[:,2].values.tolist()\n",
    "y_train_human_sub = np.array(y_train_human2)\n",
    "x_test_human2 = test_human.iloc[:,6].values.tolist()\n",
    "x_test_human_sub = np.array(x_test_human2)\n",
    "y_test_human2 = test_human.iloc[:,2].values.tolist()\n",
    "y_test_human_sub = np.array(y_test_human2)\n",
    "x_test_human_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 442us/step - loss: 0.6844 - acc: 0.5981 - val_loss: 0.6881 - val_acc: 0.6085\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.6594 - acc: 0.6223 - val_loss: 0.6765 - val_acc: 0.6190\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.6426 - acc: 0.6395 - val_loss: 0.6669 - val_acc: 0.6138\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.6282 - acc: 0.6667 - val_loss: 0.6587 - val_acc: 0.6402\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.6146 - acc: 0.6915 - val_loss: 0.6516 - val_acc: 0.6508\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.6020 - acc: 0.6986 - val_loss: 0.6460 - val_acc: 0.6561\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.5905 - acc: 0.7134 - val_loss: 0.6400 - val_acc: 0.6614\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.5790 - acc: 0.7199 - val_loss: 0.6342 - val_acc: 0.6825\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.5675 - acc: 0.7252 - val_loss: 0.6274 - val_acc: 0.6878\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.5559 - acc: 0.7358 - val_loss: 0.6198 - val_acc: 0.6984\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.5436 - acc: 0.7453 - val_loss: 0.6113 - val_acc: 0.6931\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5315 - acc: 0.7624 - val_loss: 0.6046 - val_acc: 0.7037\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.5188 - acc: 0.7766 - val_loss: 0.5973 - val_acc: 0.7143\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.5061 - acc: 0.7813 - val_loss: 0.5890 - val_acc: 0.7090\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.4940 - acc: 0.7949 - val_loss: 0.5820 - val_acc: 0.7354\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.4821 - acc: 0.7949 - val_loss: 0.5752 - val_acc: 0.7143\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4714 - acc: 0.8061 - val_loss: 0.5683 - val_acc: 0.7302\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4612 - acc: 0.8109 - val_loss: 0.5602 - val_acc: 0.7354\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.4518 - acc: 0.8180 - val_loss: 0.5549 - val_acc: 0.7513\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4423 - acc: 0.8197 - val_loss: 0.5472 - val_acc: 0.7407\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4340 - acc: 0.8280 - val_loss: 0.5419 - val_acc: 0.7513\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.4253 - acc: 0.8304 - val_loss: 0.5356 - val_acc: 0.7566\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4173 - acc: 0.8363 - val_loss: 0.5303 - val_acc: 0.7672\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.4104 - acc: 0.8398 - val_loss: 0.5241 - val_acc: 0.7725\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.4030 - acc: 0.8434 - val_loss: 0.5172 - val_acc: 0.7725\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3964 - acc: 0.8469 - val_loss: 0.5109 - val_acc: 0.7778\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3901 - acc: 0.8469 - val_loss: 0.5058 - val_acc: 0.7778\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3837 - acc: 0.8452 - val_loss: 0.5028 - val_acc: 0.7778\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3779 - acc: 0.8511 - val_loss: 0.4949 - val_acc: 0.7778\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3727 - acc: 0.8540 - val_loss: 0.4933 - val_acc: 0.7884\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3680 - acc: 0.8564 - val_loss: 0.4887 - val_acc: 0.7937\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.3625 - acc: 0.8558 - val_loss: 0.4817 - val_acc: 0.7778\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.3578 - acc: 0.8587 - val_loss: 0.4773 - val_acc: 0.7884\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.3528 - acc: 0.8599 - val_loss: 0.4706 - val_acc: 0.7831\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3489 - acc: 0.8652 - val_loss: 0.4711 - val_acc: 0.7937\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3453 - acc: 0.8641 - val_loss: 0.4631 - val_acc: 0.7884\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3416 - acc: 0.8664 - val_loss: 0.4616 - val_acc: 0.8042\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3378 - acc: 0.8676 - val_loss: 0.4579 - val_acc: 0.7989\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3344 - acc: 0.8741 - val_loss: 0.4562 - val_acc: 0.8095\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3310 - acc: 0.8729 - val_loss: 0.4545 - val_acc: 0.8095\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3275 - acc: 0.8741 - val_loss: 0.4485 - val_acc: 0.8042\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3242 - acc: 0.8800 - val_loss: 0.4517 - val_acc: 0.8148\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3220 - acc: 0.8800 - val_loss: 0.4478 - val_acc: 0.8201\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3190 - acc: 0.8759 - val_loss: 0.4493 - val_acc: 0.8201\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3162 - acc: 0.8794 - val_loss: 0.4458 - val_acc: 0.8254\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3137 - acc: 0.8830 - val_loss: 0.4459 - val_acc: 0.8148\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3110 - acc: 0.8842 - val_loss: 0.4450 - val_acc: 0.8148\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3085 - acc: 0.8865 - val_loss: 0.4400 - val_acc: 0.8254\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3053 - acc: 0.8859 - val_loss: 0.4391 - val_acc: 0.8201\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3033 - acc: 0.8871 - val_loss: 0.4351 - val_acc: 0.8254\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3015 - acc: 0.8877 - val_loss: 0.4338 - val_acc: 0.8254\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2986 - acc: 0.8913 - val_loss: 0.4377 - val_acc: 0.8254\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2968 - acc: 0.8930 - val_loss: 0.4382 - val_acc: 0.8254\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2951 - acc: 0.8913 - val_loss: 0.4352 - val_acc: 0.8254\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2933 - acc: 0.8936 - val_loss: 0.4300 - val_acc: 0.8254\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2906 - acc: 0.8954 - val_loss: 0.4285 - val_acc: 0.8307\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2892 - acc: 0.8978 - val_loss: 0.4248 - val_acc: 0.8254\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2876 - acc: 0.8972 - val_loss: 0.4256 - val_acc: 0.8307\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2857 - acc: 0.8978 - val_loss: 0.4229 - val_acc: 0.8466\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2836 - acc: 0.9013 - val_loss: 0.4209 - val_acc: 0.8360\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2823 - acc: 0.9025 - val_loss: 0.4233 - val_acc: 0.8413\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2803 - acc: 0.9007 - val_loss: 0.4262 - val_acc: 0.8413\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2795 - acc: 0.9001 - val_loss: 0.4189 - val_acc: 0.8360\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2777 - acc: 0.9025 - val_loss: 0.4204 - val_acc: 0.8413\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2756 - acc: 0.9031 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2750 - acc: 0.9037 - val_loss: 0.4180 - val_acc: 0.8466\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2733 - acc: 0.9037 - val_loss: 0.4125 - val_acc: 0.8519\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2720 - acc: 0.9037 - val_loss: 0.4159 - val_acc: 0.8466\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2712 - acc: 0.9078 - val_loss: 0.4128 - val_acc: 0.8519\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2697 - acc: 0.9072 - val_loss: 0.4076 - val_acc: 0.8466\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2682 - acc: 0.9096 - val_loss: 0.4050 - val_acc: 0.8466\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2665 - acc: 0.9078 - val_loss: 0.4170 - val_acc: 0.8466\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2658 - acc: 0.9102 - val_loss: 0.4098 - val_acc: 0.8519\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2642 - acc: 0.9096 - val_loss: 0.4111 - val_acc: 0.8519\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2632 - acc: 0.9125 - val_loss: 0.4074 - val_acc: 0.8519\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2624 - acc: 0.9090 - val_loss: 0.4081 - val_acc: 0.8519\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2608 - acc: 0.9096 - val_loss: 0.4104 - val_acc: 0.8519\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2592 - acc: 0.9096 - val_loss: 0.4019 - val_acc: 0.8519\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2584 - acc: 0.9131 - val_loss: 0.4056 - val_acc: 0.8519\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2575 - acc: 0.9131 - val_loss: 0.4032 - val_acc: 0.8571\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2566 - acc: 0.9113 - val_loss: 0.4035 - val_acc: 0.8571\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2557 - acc: 0.9108 - val_loss: 0.4049 - val_acc: 0.8571\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2540 - acc: 0.9102 - val_loss: 0.4025 - val_acc: 0.8519\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2537 - acc: 0.9137 - val_loss: 0.4016 - val_acc: 0.8519\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2520 - acc: 0.9125 - val_loss: 0.4092 - val_acc: 0.8466\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2516 - acc: 0.9119 - val_loss: 0.3991 - val_acc: 0.8571\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2501 - acc: 0.9161 - val_loss: 0.4058 - val_acc: 0.8466\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2503 - acc: 0.9096 - val_loss: 0.3974 - val_acc: 0.8519\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2492 - acc: 0.9113 - val_loss: 0.3953 - val_acc: 0.8571\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2483 - acc: 0.9143 - val_loss: 0.3994 - val_acc: 0.8466\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2478 - acc: 0.9125 - val_loss: 0.3997 - val_acc: 0.8466\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2469 - acc: 0.9143 - val_loss: 0.3913 - val_acc: 0.8519\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2463 - acc: 0.9143 - val_loss: 0.3934 - val_acc: 0.8519\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2453 - acc: 0.9149 - val_loss: 0.3976 - val_acc: 0.8466\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2444 - acc: 0.9143 - val_loss: 0.3957 - val_acc: 0.8413\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2438 - acc: 0.9143 - val_loss: 0.3942 - val_acc: 0.8466\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2432 - acc: 0.9143 - val_loss: 0.3924 - val_acc: 0.8466\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2417 - acc: 0.9149 - val_loss: 0.3976 - val_acc: 0.8466\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2415 - acc: 0.9113 - val_loss: 0.3902 - val_acc: 0.8466\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2407 - acc: 0.9149 - val_loss: 0.3937 - val_acc: 0.8466\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2400 - acc: 0.9143 - val_loss: 0.3947 - val_acc: 0.8466\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2394 - acc: 0.9137 - val_loss: 0.3906 - val_acc: 0.8519\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2381 - acc: 0.9119 - val_loss: 0.3941 - val_acc: 0.8466\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2379 - acc: 0.9137 - val_loss: 0.3930 - val_acc: 0.8466\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2372 - acc: 0.9184 - val_loss: 0.3956 - val_acc: 0.8466\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2364 - acc: 0.9173 - val_loss: 0.3906 - val_acc: 0.8466\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2358 - acc: 0.9167 - val_loss: 0.3885 - val_acc: 0.8413\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2353 - acc: 0.9137 - val_loss: 0.3890 - val_acc: 0.8413\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2339 - acc: 0.9155 - val_loss: 0.3908 - val_acc: 0.8519\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2337 - acc: 0.9137 - val_loss: 0.3949 - val_acc: 0.8466\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2333 - acc: 0.9149 - val_loss: 0.3916 - val_acc: 0.8466\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2319 - acc: 0.9167 - val_loss: 0.3901 - val_acc: 0.8413\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2318 - acc: 0.9184 - val_loss: 0.3901 - val_acc: 0.8466\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2306 - acc: 0.9155 - val_loss: 0.3946 - val_acc: 0.8466\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2307 - acc: 0.9161 - val_loss: 0.3894 - val_acc: 0.8519\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2291 - acc: 0.9178 - val_loss: 0.3875 - val_acc: 0.8466\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2290 - acc: 0.9173 - val_loss: 0.3854 - val_acc: 0.8466\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2281 - acc: 0.9190 - val_loss: 0.3855 - val_acc: 0.8466\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2270 - acc: 0.9178 - val_loss: 0.3876 - val_acc: 0.8519\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2268 - acc: 0.9178 - val_loss: 0.3859 - val_acc: 0.8519\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2258 - acc: 0.9149 - val_loss: 0.3928 - val_acc: 0.8519\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2255 - acc: 0.9184 - val_loss: 0.3906 - val_acc: 0.8466\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2251 - acc: 0.9208 - val_loss: 0.3858 - val_acc: 0.8519\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2242 - acc: 0.9178 - val_loss: 0.3844 - val_acc: 0.8519\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2236 - acc: 0.9173 - val_loss: 0.3954 - val_acc: 0.8466\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2237 - acc: 0.9173 - val_loss: 0.3934 - val_acc: 0.8466\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2212 - acc: 0.9190 - val_loss: 0.3837 - val_acc: 0.8571\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2229 - acc: 0.9196 - val_loss: 0.3858 - val_acc: 0.8466\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2213 - acc: 0.9190 - val_loss: 0.3886 - val_acc: 0.8466\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2213 - acc: 0.9202 - val_loss: 0.3976 - val_acc: 0.8466\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2212 - acc: 0.9184 - val_loss: 0.3889 - val_acc: 0.8466\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2194 - acc: 0.9196 - val_loss: 0.4034 - val_acc: 0.8519\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2204 - acc: 0.9190 - val_loss: 0.3979 - val_acc: 0.8466\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2200 - acc: 0.9161 - val_loss: 0.3949 - val_acc: 0.8466\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2192 - acc: 0.9190 - val_loss: 0.3912 - val_acc: 0.8466\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2191 - acc: 0.9202 - val_loss: 0.3926 - val_acc: 0.8466\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2191 - acc: 0.9214 - val_loss: 0.3976 - val_acc: 0.8519\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2179 - acc: 0.9214 - val_loss: 0.3898 - val_acc: 0.8519\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2180 - acc: 0.9220 - val_loss: 0.3962 - val_acc: 0.8466\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2180 - acc: 0.9202 - val_loss: 0.3918 - val_acc: 0.8466\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2171 - acc: 0.9196 - val_loss: 0.3876 - val_acc: 0.8571\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2158 - acc: 0.9226 - val_loss: 0.4052 - val_acc: 0.8571\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2169 - acc: 0.9173 - val_loss: 0.3906 - val_acc: 0.8466\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2160 - acc: 0.9214 - val_loss: 0.3894 - val_acc: 0.8571\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2153 - acc: 0.9214 - val_loss: 0.3877 - val_acc: 0.8519\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2152 - acc: 0.9214 - val_loss: 0.3838 - val_acc: 0.8413\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2156 - acc: 0.9184 - val_loss: 0.3861 - val_acc: 0.8571\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2150 - acc: 0.9214 - val_loss: 0.3923 - val_acc: 0.8571\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2153 - acc: 0.9208 - val_loss: 0.3905 - val_acc: 0.8519\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2144 - acc: 0.9214 - val_loss: 0.3977 - val_acc: 0.8466\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2140 - acc: 0.9190 - val_loss: 0.3882 - val_acc: 0.8519\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2140 - acc: 0.9220 - val_loss: 0.3949 - val_acc: 0.8519\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2130 - acc: 0.9196 - val_loss: 0.3862 - val_acc: 0.8519\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2127 - acc: 0.9232 - val_loss: 0.3897 - val_acc: 0.8519\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2132 - acc: 0.9196 - val_loss: 0.3936 - val_acc: 0.8519\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2128 - acc: 0.9238 - val_loss: 0.3954 - val_acc: 0.8571\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2124 - acc: 0.9208 - val_loss: 0.3924 - val_acc: 0.8519\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2121 - acc: 0.9208 - val_loss: 0.3869 - val_acc: 0.8519\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2123 - acc: 0.9208 - val_loss: 0.3902 - val_acc: 0.8519\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2115 - acc: 0.9220 - val_loss: 0.3888 - val_acc: 0.8466\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2110 - acc: 0.9214 - val_loss: 0.4003 - val_acc: 0.8519\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2116 - acc: 0.9196 - val_loss: 0.3943 - val_acc: 0.8571\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2117 - acc: 0.9220 - val_loss: 0.3901 - val_acc: 0.8519\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2107 - acc: 0.9196 - val_loss: 0.3912 - val_acc: 0.8519\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2106 - acc: 0.9238 - val_loss: 0.3876 - val_acc: 0.8519\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2102 - acc: 0.9249 - val_loss: 0.3842 - val_acc: 0.8519\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2105 - acc: 0.9220 - val_loss: 0.3944 - val_acc: 0.8466\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2097 - acc: 0.9208 - val_loss: 0.3883 - val_acc: 0.8519\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2100 - acc: 0.9255 - val_loss: 0.3892 - val_acc: 0.8519\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2087 - acc: 0.9249 - val_loss: 0.3931 - val_acc: 0.8571\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2091 - acc: 0.9220 - val_loss: 0.3945 - val_acc: 0.8624\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2094 - acc: 0.9220 - val_loss: 0.3904 - val_acc: 0.8571\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2088 - acc: 0.9214 - val_loss: 0.3879 - val_acc: 0.8519\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2086 - acc: 0.9226 - val_loss: 0.3863 - val_acc: 0.8519\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2085 - acc: 0.9214 - val_loss: 0.3879 - val_acc: 0.8571\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2073 - acc: 0.9267 - val_loss: 0.3892 - val_acc: 0.8571\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2082 - acc: 0.9243 - val_loss: 0.3864 - val_acc: 0.8519\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2072 - acc: 0.9208 - val_loss: 0.3891 - val_acc: 0.8571\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2075 - acc: 0.9255 - val_loss: 0.3913 - val_acc: 0.8571\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2071 - acc: 0.9243 - val_loss: 0.3914 - val_acc: 0.8571\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2063 - acc: 0.9226 - val_loss: 0.3889 - val_acc: 0.8571\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2068 - acc: 0.9238 - val_loss: 0.3829 - val_acc: 0.8519\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2061 - acc: 0.9261 - val_loss: 0.3839 - val_acc: 0.8519\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2060 - acc: 0.9249 - val_loss: 0.3847 - val_acc: 0.8571\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2052 - acc: 0.9226 - val_loss: 0.3798 - val_acc: 0.8519\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2045 - acc: 0.9214 - val_loss: 0.3855 - val_acc: 0.8571\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2050 - acc: 0.9232 - val_loss: 0.3852 - val_acc: 0.8571\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2050 - acc: 0.9261 - val_loss: 0.3852 - val_acc: 0.8571\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2042 - acc: 0.9267 - val_loss: 0.3858 - val_acc: 0.8571\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2039 - acc: 0.9267 - val_loss: 0.3902 - val_acc: 0.8571\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2033 - acc: 0.9214 - val_loss: 0.3783 - val_acc: 0.8624\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2038 - acc: 0.9261 - val_loss: 0.3852 - val_acc: 0.8519\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2030 - acc: 0.9255 - val_loss: 0.3849 - val_acc: 0.8624\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2025 - acc: 0.9202 - val_loss: 0.3792 - val_acc: 0.8624\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2031 - acc: 0.9285 - val_loss: 0.3844 - val_acc: 0.8571\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2028 - acc: 0.9255 - val_loss: 0.3816 - val_acc: 0.8624\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2023 - acc: 0.9285 - val_loss: 0.3835 - val_acc: 0.8677\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2022 - acc: 0.9261 - val_loss: 0.3787 - val_acc: 0.8624\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2021 - acc: 0.9273 - val_loss: 0.3820 - val_acc: 0.8677\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2006 - acc: 0.9291 - val_loss: 0.3937 - val_acc: 0.8571\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2022 - acc: 0.9249 - val_loss: 0.3850 - val_acc: 0.8571\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2017 - acc: 0.9267 - val_loss: 0.3843 - val_acc: 0.8571\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2009 - acc: 0.9261 - val_loss: 0.3875 - val_acc: 0.8677\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2007 - acc: 0.9267 - val_loss: 0.3830 - val_acc: 0.8624\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2009 - acc: 0.9279 - val_loss: 0.3772 - val_acc: 0.8624\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2001 - acc: 0.9249 - val_loss: 0.3788 - val_acc: 0.8624\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1999 - acc: 0.9279 - val_loss: 0.3860 - val_acc: 0.8571\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2000 - acc: 0.9285 - val_loss: 0.3851 - val_acc: 0.8519\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1995 - acc: 0.9285 - val_loss: 0.3850 - val_acc: 0.8571\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1995 - acc: 0.9261 - val_loss: 0.3800 - val_acc: 0.8624\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1994 - acc: 0.9261 - val_loss: 0.3826 - val_acc: 0.8624\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1990 - acc: 0.9267 - val_loss: 0.3904 - val_acc: 0.8571\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1990 - acc: 0.9273 - val_loss: 0.3926 - val_acc: 0.8571\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1988 - acc: 0.9273 - val_loss: 0.3861 - val_acc: 0.8624\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1988 - acc: 0.9279 - val_loss: 0.3911 - val_acc: 0.8571\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1993 - acc: 0.9273 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1990 - acc: 0.9267 - val_loss: 0.3874 - val_acc: 0.8571\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1980 - acc: 0.9261 - val_loss: 0.3902 - val_acc: 0.8571\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1979 - acc: 0.9267 - val_loss: 0.3920 - val_acc: 0.8571\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1976 - acc: 0.9249 - val_loss: 0.3850 - val_acc: 0.8624\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1978 - acc: 0.9279 - val_loss: 0.3868 - val_acc: 0.8624\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1974 - acc: 0.9267 - val_loss: 0.3919 - val_acc: 0.8677\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1977 - acc: 0.9267 - val_loss: 0.3887 - val_acc: 0.8571\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1971 - acc: 0.9255 - val_loss: 0.3942 - val_acc: 0.8624\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1972 - acc: 0.9285 - val_loss: 0.3933 - val_acc: 0.8571\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1961 - acc: 0.9285 - val_loss: 0.3885 - val_acc: 0.8519\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1964 - acc: 0.9261 - val_loss: 0.3885 - val_acc: 0.8624\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1968 - acc: 0.9255 - val_loss: 0.3971 - val_acc: 0.8571\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1966 - acc: 0.9273 - val_loss: 0.3891 - val_acc: 0.8624\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1948 - acc: 0.9291 - val_loss: 0.3928 - val_acc: 0.8624\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1957 - acc: 0.9279 - val_loss: 0.3941 - val_acc: 0.8571\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1954 - acc: 0.9285 - val_loss: 0.3875 - val_acc: 0.8624\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1949 - acc: 0.9273 - val_loss: 0.3855 - val_acc: 0.8624\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1955 - acc: 0.9279 - val_loss: 0.3868 - val_acc: 0.8624\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1950 - acc: 0.9267 - val_loss: 0.3938 - val_acc: 0.8624\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1950 - acc: 0.9279 - val_loss: 0.3878 - val_acc: 0.8624\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1949 - acc: 0.9285 - val_loss: 0.3869 - val_acc: 0.8624\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 102us/step - loss: 0.1947 - acc: 0.9291 - val_loss: 0.3878 - val_acc: 0.8677\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1943 - acc: 0.9279 - val_loss: 0.3895 - val_acc: 0.8624\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1935 - acc: 0.9291 - val_loss: 0.3935 - val_acc: 0.8571\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1931 - acc: 0.9332 - val_loss: 0.3987 - val_acc: 0.8624\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1940 - acc: 0.9267 - val_loss: 0.3874 - val_acc: 0.8677\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1938 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8571\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1936 - acc: 0.9279 - val_loss: 0.3897 - val_acc: 0.8571\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1928 - acc: 0.9320 - val_loss: 0.3870 - val_acc: 0.8624\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1929 - acc: 0.9320 - val_loss: 0.3890 - val_acc: 0.8571\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1927 - acc: 0.9291 - val_loss: 0.3870 - val_acc: 0.8571\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1921 - acc: 0.9309 - val_loss: 0.3965 - val_acc: 0.8571\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1925 - acc: 0.9303 - val_loss: 0.3914 - val_acc: 0.8571\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1924 - acc: 0.9338 - val_loss: 0.3863 - val_acc: 0.8677\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1916 - acc: 0.9314 - val_loss: 0.3937 - val_acc: 0.8624\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1917 - acc: 0.9332 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1917 - acc: 0.9326 - val_loss: 0.3952 - val_acc: 0.8571\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1919 - acc: 0.9309 - val_loss: 0.3860 - val_acc: 0.8624\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1915 - acc: 0.9297 - val_loss: 0.3870 - val_acc: 0.8624\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1910 - acc: 0.9350 - val_loss: 0.3943 - val_acc: 0.8571\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1915 - acc: 0.9338 - val_loss: 0.3858 - val_acc: 0.8624\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1916 - acc: 0.9297 - val_loss: 0.3903 - val_acc: 0.8571\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1905 - acc: 0.9338 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1908 - acc: 0.9320 - val_loss: 0.3957 - val_acc: 0.8624\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1913 - acc: 0.9291 - val_loss: 0.3856 - val_acc: 0.8571\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1901 - acc: 0.9338 - val_loss: 0.3854 - val_acc: 0.8677\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1906 - acc: 0.9338 - val_loss: 0.3841 - val_acc: 0.8571\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1903 - acc: 0.9332 - val_loss: 0.3899 - val_acc: 0.8571\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1904 - acc: 0.9332 - val_loss: 0.3827 - val_acc: 0.8571\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1891 - acc: 0.9344 - val_loss: 0.3921 - val_acc: 0.8571\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1897 - acc: 0.9356 - val_loss: 0.3851 - val_acc: 0.8624\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1895 - acc: 0.9350 - val_loss: 0.3809 - val_acc: 0.8571\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1892 - acc: 0.9362 - val_loss: 0.3803 - val_acc: 0.8571\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1899 - acc: 0.9344 - val_loss: 0.3791 - val_acc: 0.8571\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1887 - acc: 0.9332 - val_loss: 0.3753 - val_acc: 0.8624\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1900 - acc: 0.9338 - val_loss: 0.3784 - val_acc: 0.8571\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1893 - acc: 0.9350 - val_loss: 0.3812 - val_acc: 0.8624\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1896 - acc: 0.9338 - val_loss: 0.3813 - val_acc: 0.8571\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1873 - acc: 0.9362 - val_loss: 0.3902 - val_acc: 0.8571\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1893 - acc: 0.9350 - val_loss: 0.3785 - val_acc: 0.8624\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1889 - acc: 0.9362 - val_loss: 0.3797 - val_acc: 0.8571\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1875 - acc: 0.9356 - val_loss: 0.3740 - val_acc: 0.8624\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1886 - acc: 0.9326 - val_loss: 0.3765 - val_acc: 0.8624\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1879 - acc: 0.9350 - val_loss: 0.3783 - val_acc: 0.8571\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1883 - acc: 0.9362 - val_loss: 0.3720 - val_acc: 0.8571\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1883 - acc: 0.9344 - val_loss: 0.3761 - val_acc: 0.8571\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1882 - acc: 0.9344 - val_loss: 0.3744 - val_acc: 0.8571\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1872 - acc: 0.9320 - val_loss: 0.3717 - val_acc: 0.8624\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1874 - acc: 0.9374 - val_loss: 0.3757 - val_acc: 0.8571\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1876 - acc: 0.9374 - val_loss: 0.3793 - val_acc: 0.8571\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1876 - acc: 0.9332 - val_loss: 0.3835 - val_acc: 0.8624\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1877 - acc: 0.9338 - val_loss: 0.3760 - val_acc: 0.8571\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1872 - acc: 0.9320 - val_loss: 0.3807 - val_acc: 0.8624\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1877 - acc: 0.9350 - val_loss: 0.3788 - val_acc: 0.8571\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1868 - acc: 0.9344 - val_loss: 0.3831 - val_acc: 0.8571\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1869 - acc: 0.9362 - val_loss: 0.3751 - val_acc: 0.8571\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1866 - acc: 0.9362 - val_loss: 0.3763 - val_acc: 0.8624\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1862 - acc: 0.9362 - val_loss: 0.3861 - val_acc: 0.8571\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1865 - acc: 0.9332 - val_loss: 0.3712 - val_acc: 0.8624\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1858 - acc: 0.9356 - val_loss: 0.3746 - val_acc: 0.8624\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1860 - acc: 0.9356 - val_loss: 0.3823 - val_acc: 0.8571\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1860 - acc: 0.9350 - val_loss: 0.3728 - val_acc: 0.8571\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1866 - acc: 0.9350 - val_loss: 0.3730 - val_acc: 0.8624\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1861 - acc: 0.9374 - val_loss: 0.3718 - val_acc: 0.8677\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1867 - acc: 0.9338 - val_loss: 0.3802 - val_acc: 0.8624\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1850 - acc: 0.9326 - val_loss: 0.3718 - val_acc: 0.8571\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1860 - acc: 0.9374 - val_loss: 0.3777 - val_acc: 0.8571\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1866 - acc: 0.9344 - val_loss: 0.3744 - val_acc: 0.8624\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1857 - acc: 0.9344 - val_loss: 0.3768 - val_acc: 0.8624\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1860 - acc: 0.9368 - val_loss: 0.3819 - val_acc: 0.8571\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1862 - acc: 0.9362 - val_loss: 0.3817 - val_acc: 0.8571\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1856 - acc: 0.9356 - val_loss: 0.3772 - val_acc: 0.8571\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1854 - acc: 0.9356 - val_loss: 0.3753 - val_acc: 0.8624\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1858 - acc: 0.9350 - val_loss: 0.3702 - val_acc: 0.8624\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1853 - acc: 0.9356 - val_loss: 0.3788 - val_acc: 0.8624\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1857 - acc: 0.9344 - val_loss: 0.3729 - val_acc: 0.8677\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1846 - acc: 0.9332 - val_loss: 0.3692 - val_acc: 0.8677\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1855 - acc: 0.9374 - val_loss: 0.3773 - val_acc: 0.8677\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1854 - acc: 0.9350 - val_loss: 0.3738 - val_acc: 0.8571\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1851 - acc: 0.9362 - val_loss: 0.3744 - val_acc: 0.8571\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1850 - acc: 0.9379 - val_loss: 0.3696 - val_acc: 0.8677\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1846 - acc: 0.9350 - val_loss: 0.3778 - val_acc: 0.8571\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1847 - acc: 0.9385 - val_loss: 0.3727 - val_acc: 0.8571\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1841 - acc: 0.9356 - val_loss: 0.3724 - val_acc: 0.8571\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1839 - acc: 0.9368 - val_loss: 0.3720 - val_acc: 0.8624\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1844 - acc: 0.9356 - val_loss: 0.3778 - val_acc: 0.8624\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1841 - acc: 0.9379 - val_loss: 0.3711 - val_acc: 0.8677\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1839 - acc: 0.9374 - val_loss: 0.3783 - val_acc: 0.8519\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1850 - acc: 0.9368 - val_loss: 0.3713 - val_acc: 0.8624\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1846 - acc: 0.9368 - val_loss: 0.3726 - val_acc: 0.8624\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1842 - acc: 0.9374 - val_loss: 0.3678 - val_acc: 0.8677\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1848 - acc: 0.9374 - val_loss: 0.3756 - val_acc: 0.8624\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1847 - acc: 0.9368 - val_loss: 0.3742 - val_acc: 0.8624\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1841 - acc: 0.9344 - val_loss: 0.3726 - val_acc: 0.8624\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1842 - acc: 0.9344 - val_loss: 0.3724 - val_acc: 0.8571\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1837 - acc: 0.9344 - val_loss: 0.3785 - val_acc: 0.8571\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1848 - acc: 0.9368 - val_loss: 0.3716 - val_acc: 0.8677\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1842 - acc: 0.9379 - val_loss: 0.3693 - val_acc: 0.8677\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1836 - acc: 0.9368 - val_loss: 0.3774 - val_acc: 0.8571\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1844 - acc: 0.9368 - val_loss: 0.3691 - val_acc: 0.8571\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1831 - acc: 0.9350 - val_loss: 0.3711 - val_acc: 0.8730\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1840 - acc: 0.9374 - val_loss: 0.3734 - val_acc: 0.8677\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1832 - acc: 0.9362 - val_loss: 0.3620 - val_acc: 0.8677\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1846 - acc: 0.9374 - val_loss: 0.3643 - val_acc: 0.8730\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1836 - acc: 0.9368 - val_loss: 0.3726 - val_acc: 0.8624\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1833 - acc: 0.9374 - val_loss: 0.3706 - val_acc: 0.8677\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1830 - acc: 0.9356 - val_loss: 0.3702 - val_acc: 0.8677\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1836 - acc: 0.9356 - val_loss: 0.3695 - val_acc: 0.8624\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1837 - acc: 0.9338 - val_loss: 0.3740 - val_acc: 0.8677\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1828 - acc: 0.9368 - val_loss: 0.3759 - val_acc: 0.8730\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1838 - acc: 0.9356 - val_loss: 0.3709 - val_acc: 0.8624\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1828 - acc: 0.9374 - val_loss: 0.3714 - val_acc: 0.8571\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1835 - acc: 0.9350 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1830 - acc: 0.9320 - val_loss: 0.3713 - val_acc: 0.8677\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1828 - acc: 0.9368 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1829 - acc: 0.9350 - val_loss: 0.3749 - val_acc: 0.8624\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1778 - acc: 0.934 - 0s 50us/step - loss: 0.1830 - acc: 0.9332 - val_loss: 0.3791 - val_acc: 0.8624\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1830 - acc: 0.9356 - val_loss: 0.3766 - val_acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1830 - acc: 0.9368 - val_loss: 0.3680 - val_acc: 0.8571\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1827 - acc: 0.9338 - val_loss: 0.3726 - val_acc: 0.8624\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1830 - acc: 0.9374 - val_loss: 0.3707 - val_acc: 0.8624\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1817 - acc: 0.9350 - val_loss: 0.3629 - val_acc: 0.8730\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1826 - acc: 0.9362 - val_loss: 0.3766 - val_acc: 0.8624\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1823 - acc: 0.9350 - val_loss: 0.3678 - val_acc: 0.8677\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1820 - acc: 0.9362 - val_loss: 0.3798 - val_acc: 0.8677\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1826 - acc: 0.9356 - val_loss: 0.3712 - val_acc: 0.8624\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1819 - acc: 0.9385 - val_loss: 0.3703 - val_acc: 0.8624\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1827 - acc: 0.9362 - val_loss: 0.3708 - val_acc: 0.8624\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1823 - acc: 0.9374 - val_loss: 0.3675 - val_acc: 0.8571\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1820 - acc: 0.9338 - val_loss: 0.3697 - val_acc: 0.8677\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1824 - acc: 0.9379 - val_loss: 0.3718 - val_acc: 0.8677\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1820 - acc: 0.9374 - val_loss: 0.3669 - val_acc: 0.8624\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1823 - acc: 0.9350 - val_loss: 0.3747 - val_acc: 0.8571\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1816 - acc: 0.9391 - val_loss: 0.3762 - val_acc: 0.8624\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1822 - acc: 0.9350 - val_loss: 0.3678 - val_acc: 0.8571\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1829 - acc: 0.936 - 0s 49us/step - loss: 0.1818 - acc: 0.9385 - val_loss: 0.3737 - val_acc: 0.8571\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1838 - acc: 0.939 - 0s 50us/step - loss: 0.1817 - acc: 0.9362 - val_loss: 0.3712 - val_acc: 0.8624\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1817 - acc: 0.9379 - val_loss: 0.3690 - val_acc: 0.8677\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1821 - acc: 0.9356 - val_loss: 0.3674 - val_acc: 0.8571\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1817 - acc: 0.9374 - val_loss: 0.3675 - val_acc: 0.8677\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1813 - acc: 0.9356 - val_loss: 0.3768 - val_acc: 0.8677\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1819 - acc: 0.9356 - val_loss: 0.3763 - val_acc: 0.8571\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1804 - acc: 0.9338 - val_loss: 0.3683 - val_acc: 0.8519\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1825 - acc: 0.9374 - val_loss: 0.3745 - val_acc: 0.8624\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1814 - acc: 0.9368 - val_loss: 0.3698 - val_acc: 0.8677\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1819 - acc: 0.9368 - val_loss: 0.3690 - val_acc: 0.8677\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1814 - acc: 0.9374 - val_loss: 0.3691 - val_acc: 0.8624\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1810 - acc: 0.9379 - val_loss: 0.3739 - val_acc: 0.8571\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1817 - acc: 0.9368 - val_loss: 0.3783 - val_acc: 0.8624\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1810 - acc: 0.9368 - val_loss: 0.3840 - val_acc: 0.8677\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1817 - acc: 0.9356 - val_loss: 0.3782 - val_acc: 0.8677\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1810 - acc: 0.9326 - val_loss: 0.3696 - val_acc: 0.8677\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1812 - acc: 0.9368 - val_loss: 0.3717 - val_acc: 0.8571\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1803 - acc: 0.9368 - val_loss: 0.3739 - val_acc: 0.8519\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1819 - acc: 0.9350 - val_loss: 0.3766 - val_acc: 0.8571\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1805 - acc: 0.9368 - val_loss: 0.3750 - val_acc: 0.8624\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1812 - acc: 0.9374 - val_loss: 0.3738 - val_acc: 0.8677\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1806 - acc: 0.9368 - val_loss: 0.3767 - val_acc: 0.8730\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1811 - acc: 0.9344 - val_loss: 0.3693 - val_acc: 0.8624\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1806 - acc: 0.9368 - val_loss: 0.3723 - val_acc: 0.8730\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1813 - acc: 0.9368 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1805 - acc: 0.9379 - val_loss: 0.3628 - val_acc: 0.8677\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1800 - acc: 0.9368 - val_loss: 0.3793 - val_acc: 0.8677\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1811 - acc: 0.9362 - val_loss: 0.3717 - val_acc: 0.8730\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1807 - acc: 0.9356 - val_loss: 0.3667 - val_acc: 0.8730\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1798 - acc: 0.9374 - val_loss: 0.3643 - val_acc: 0.8571\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1805 - acc: 0.9368 - val_loss: 0.3678 - val_acc: 0.8624\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1808 - acc: 0.9374 - val_loss: 0.3634 - val_acc: 0.8624\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1807 - acc: 0.9362 - val_loss: 0.3716 - val_acc: 0.8677\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1803 - acc: 0.9362 - val_loss: 0.3657 - val_acc: 0.8624\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1806 - acc: 0.9403 - val_loss: 0.3723 - val_acc: 0.8730\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1799 - acc: 0.9350 - val_loss: 0.3682 - val_acc: 0.8730\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1809 - acc: 0.9368 - val_loss: 0.3639 - val_acc: 0.8624\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1804 - acc: 0.9362 - val_loss: 0.3671 - val_acc: 0.8677\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1795 - acc: 0.9385 - val_loss: 0.3717 - val_acc: 0.8624\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1803 - acc: 0.9356 - val_loss: 0.3719 - val_acc: 0.8624\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1797 - acc: 0.9374 - val_loss: 0.3771 - val_acc: 0.8677\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1800 - acc: 0.9374 - val_loss: 0.3761 - val_acc: 0.8730\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1801 - acc: 0.9374 - val_loss: 0.3649 - val_acc: 0.8677\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1802 - acc: 0.9344 - val_loss: 0.3746 - val_acc: 0.8677\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1789 - acc: 0.9403 - val_loss: 0.3644 - val_acc: 0.8677\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1805 - acc: 0.9356 - val_loss: 0.3677 - val_acc: 0.8677\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1799 - acc: 0.9379 - val_loss: 0.3652 - val_acc: 0.8730\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1801 - acc: 0.9379 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1796 - acc: 0.9344 - val_loss: 0.3616 - val_acc: 0.8730\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1801 - acc: 0.9356 - val_loss: 0.3717 - val_acc: 0.8677\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1797 - acc: 0.9362 - val_loss: 0.3637 - val_acc: 0.8677\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1789 - acc: 0.9362 - val_loss: 0.3705 - val_acc: 0.8730\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1796 - acc: 0.9314 - val_loss: 0.3696 - val_acc: 0.8730\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1795 - acc: 0.9356 - val_loss: 0.3657 - val_acc: 0.8677\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1796 - acc: 0.9344 - val_loss: 0.3729 - val_acc: 0.8677\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1791 - acc: 0.9356 - val_loss: 0.3620 - val_acc: 0.8730\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1784 - acc: 0.9320 - val_loss: 0.3595 - val_acc: 0.8730\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1791 - acc: 0.9374 - val_loss: 0.3631 - val_acc: 0.8730\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1787 - acc: 0.9350 - val_loss: 0.3729 - val_acc: 0.8730\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1784 - acc: 0.9385 - val_loss: 0.3674 - val_acc: 0.8783\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1791 - acc: 0.9385 - val_loss: 0.3688 - val_acc: 0.8730\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1784 - acc: 0.9391 - val_loss: 0.3773 - val_acc: 0.8624\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1790 - acc: 0.9362 - val_loss: 0.3759 - val_acc: 0.8730\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1789 - acc: 0.9362 - val_loss: 0.3638 - val_acc: 0.8677\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1794 - acc: 0.9356 - val_loss: 0.3713 - val_acc: 0.8730\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1789 - acc: 0.9385 - val_loss: 0.3615 - val_acc: 0.8624\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1782 - acc: 0.9374 - val_loss: 0.3638 - val_acc: 0.8730\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1784 - acc: 0.9350 - val_loss: 0.3625 - val_acc: 0.8730\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1776 - acc: 0.9356 - val_loss: 0.3654 - val_acc: 0.8730\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1785 - acc: 0.9368 - val_loss: 0.3721 - val_acc: 0.8730\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1785 - acc: 0.9356 - val_loss: 0.3610 - val_acc: 0.8677\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1788 - acc: 0.9368 - val_loss: 0.3691 - val_acc: 0.8677\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1786 - acc: 0.9362 - val_loss: 0.3740 - val_acc: 0.8677\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1786 - acc: 0.9362 - val_loss: 0.3606 - val_acc: 0.8677\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1785 - acc: 0.9350 - val_loss: 0.3683 - val_acc: 0.8730\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1782 - acc: 0.9356 - val_loss: 0.3591 - val_acc: 0.8730\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1784 - acc: 0.9391 - val_loss: 0.3646 - val_acc: 0.8730\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1788 - acc: 0.9350 - val_loss: 0.3645 - val_acc: 0.8730\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1782 - acc: 0.9385 - val_loss: 0.3637 - val_acc: 0.8730\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1779 - acc: 0.9368 - val_loss: 0.3599 - val_acc: 0.8677\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1780 - acc: 0.9379 - val_loss: 0.3657 - val_acc: 0.8730\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1778 - acc: 0.9362 - val_loss: 0.3547 - val_acc: 0.8677\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1782 - acc: 0.9356 - val_loss: 0.3616 - val_acc: 0.8730\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1780 - acc: 0.9385 - val_loss: 0.3600 - val_acc: 0.8730\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1773 - acc: 0.9374 - val_loss: 0.3729 - val_acc: 0.8730\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1780 - acc: 0.9356 - val_loss: 0.3605 - val_acc: 0.8624\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1777 - acc: 0.9338 - val_loss: 0.3610 - val_acc: 0.8730\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1777 - acc: 0.9344 - val_loss: 0.3634 - val_acc: 0.8677\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1770 - acc: 0.9350 - val_loss: 0.3752 - val_acc: 0.8730\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1761 - acc: 0.9350 - val_loss: 0.3562 - val_acc: 0.8730\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1773 - acc: 0.9362 - val_loss: 0.3605 - val_acc: 0.8677\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1772 - acc: 0.9344 - val_loss: 0.3553 - val_acc: 0.8783\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1777 - acc: 0.9344 - val_loss: 0.3608 - val_acc: 0.8624\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1776 - acc: 0.9374 - val_loss: 0.3623 - val_acc: 0.8730\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1772 - acc: 0.9368 - val_loss: 0.3581 - val_acc: 0.8677\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1772 - acc: 0.9368 - val_loss: 0.3668 - val_acc: 0.8730\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1771 - acc: 0.9391 - val_loss: 0.3643 - val_acc: 0.8730\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1770 - acc: 0.9368 - val_loss: 0.3691 - val_acc: 0.8730\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1780 - acc: 0.9356 - val_loss: 0.3625 - val_acc: 0.8730\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1772 - acc: 0.9397 - val_loss: 0.3597 - val_acc: 0.8677\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1770 - acc: 0.9385 - val_loss: 0.3598 - val_acc: 0.8624\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1773 - acc: 0.9385 - val_loss: 0.3562 - val_acc: 0.8624\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1771 - acc: 0.9350 - val_loss: 0.3537 - val_acc: 0.8730\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1773 - acc: 0.9379 - val_loss: 0.3588 - val_acc: 0.8730\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1771 - acc: 0.9379 - val_loss: 0.3623 - val_acc: 0.8677\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1768 - acc: 0.9379 - val_loss: 0.3630 - val_acc: 0.8677\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1776 - acc: 0.9391 - val_loss: 0.3658 - val_acc: 0.8677\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1760 - acc: 0.9391 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1765 - acc: 0.9391 - val_loss: 0.3650 - val_acc: 0.8677\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1765 - acc: 0.9368 - val_loss: 0.3646 - val_acc: 0.8730\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1763 - acc: 0.9356 - val_loss: 0.3559 - val_acc: 0.8783\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1772 - acc: 0.9379 - val_loss: 0.3581 - val_acc: 0.8624\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1759 - acc: 0.9403 - val_loss: 0.3536 - val_acc: 0.8624\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1764 - acc: 0.9391 - val_loss: 0.3585 - val_acc: 0.8677\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1764 - acc: 0.9374 - val_loss: 0.3524 - val_acc: 0.8677\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1764 - acc: 0.9368 - val_loss: 0.3654 - val_acc: 0.8624\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1767 - acc: 0.9379 - val_loss: 0.3611 - val_acc: 0.8624\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1765 - acc: 0.9362 - val_loss: 0.3616 - val_acc: 0.8730\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1761 - acc: 0.9344 - val_loss: 0.3610 - val_acc: 0.8677\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1762 - acc: 0.9374 - val_loss: 0.3551 - val_acc: 0.8783\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1758 - acc: 0.9362 - val_loss: 0.3602 - val_acc: 0.8677\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1760 - acc: 0.9385 - val_loss: 0.3613 - val_acc: 0.8730\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1758 - acc: 0.9385 - val_loss: 0.3657 - val_acc: 0.8677\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1762 - acc: 0.9374 - val_loss: 0.3600 - val_acc: 0.8677\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1761 - acc: 0.9379 - val_loss: 0.3625 - val_acc: 0.8677\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1756 - acc: 0.9385 - val_loss: 0.3651 - val_acc: 0.8730\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1761 - acc: 0.9368 - val_loss: 0.3646 - val_acc: 0.8677\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1758 - acc: 0.9403 - val_loss: 0.3569 - val_acc: 0.8730\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1758 - acc: 0.9374 - val_loss: 0.3589 - val_acc: 0.8730\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1754 - acc: 0.9379 - val_loss: 0.3641 - val_acc: 0.8730\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1749 - acc: 0.9344 - val_loss: 0.3741 - val_acc: 0.8730\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1759 - acc: 0.9368 - val_loss: 0.3651 - val_acc: 0.8730\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1752 - acc: 0.9409 - val_loss: 0.3744 - val_acc: 0.8730\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1754 - acc: 0.9385 - val_loss: 0.3664 - val_acc: 0.8730\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1756 - acc: 0.9374 - val_loss: 0.3594 - val_acc: 0.8730\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1746 - acc: 0.9379 - val_loss: 0.3657 - val_acc: 0.8624\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1745 - acc: 0.9379 - val_loss: 0.3726 - val_acc: 0.8677\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1757 - acc: 0.9379 - val_loss: 0.3599 - val_acc: 0.8624\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1750 - acc: 0.9427 - val_loss: 0.3633 - val_acc: 0.8730\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1741 - acc: 0.9385 - val_loss: 0.3699 - val_acc: 0.8677\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1741 - acc: 0.9415 - val_loss: 0.3592 - val_acc: 0.8677\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1753 - acc: 0.9385 - val_loss: 0.3550 - val_acc: 0.8783\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1750 - acc: 0.9403 - val_loss: 0.3616 - val_acc: 0.8677\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1750 - acc: 0.9379 - val_loss: 0.3655 - val_acc: 0.8624\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1757 - acc: 0.9409 - val_loss: 0.3611 - val_acc: 0.8730\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1749 - acc: 0.9391 - val_loss: 0.3646 - val_acc: 0.8677\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1754 - acc: 0.9391 - val_loss: 0.3652 - val_acc: 0.8730\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1745 - acc: 0.9391 - val_loss: 0.3648 - val_acc: 0.8730\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1745 - acc: 0.9379 - val_loss: 0.3643 - val_acc: 0.8677\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1751 - acc: 0.9374 - val_loss: 0.3653 - val_acc: 0.8730\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1750 - acc: 0.9391 - val_loss: 0.3613 - val_acc: 0.8677\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1741 - acc: 0.9403 - val_loss: 0.3592 - val_acc: 0.8624\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1743 - acc: 0.9385 - val_loss: 0.3713 - val_acc: 0.8730\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1730 - acc: 0.9391 - val_loss: 0.3706 - val_acc: 0.8677\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1750 - acc: 0.9403 - val_loss: 0.3648 - val_acc: 0.8677\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1746 - acc: 0.9397 - val_loss: 0.3578 - val_acc: 0.8677\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1725 - acc: 0.9403 - val_loss: 0.3562 - val_acc: 0.8677\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1740 - acc: 0.9368 - val_loss: 0.3601 - val_acc: 0.8677\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1742 - acc: 0.9391 - val_loss: 0.3634 - val_acc: 0.8624\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1745 - acc: 0.9385 - val_loss: 0.3592 - val_acc: 0.8624\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1742 - acc: 0.9397 - val_loss: 0.3669 - val_acc: 0.8677\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1739 - acc: 0.9379 - val_loss: 0.3640 - val_acc: 0.8677\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1740 - acc: 0.9403 - val_loss: 0.3564 - val_acc: 0.8624\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1730 - acc: 0.9379 - val_loss: 0.3614 - val_acc: 0.8624\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1735 - acc: 0.9379 - val_loss: 0.3598 - val_acc: 0.8783\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1726 - acc: 0.9421 - val_loss: 0.3664 - val_acc: 0.8677\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1721 - acc: 0.9397 - val_loss: 0.3736 - val_acc: 0.8677\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1743 - acc: 0.9379 - val_loss: 0.3588 - val_acc: 0.8677\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1733 - acc: 0.9415 - val_loss: 0.3742 - val_acc: 0.8730\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1737 - acc: 0.9403 - val_loss: 0.3646 - val_acc: 0.8624\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1731 - acc: 0.9385 - val_loss: 0.3623 - val_acc: 0.8624\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1739 - acc: 0.9391 - val_loss: 0.3598 - val_acc: 0.8624\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1736 - acc: 0.9385 - val_loss: 0.3592 - val_acc: 0.8624\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1732 - acc: 0.9385 - val_loss: 0.3684 - val_acc: 0.8730\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1729 - acc: 0.9391 - val_loss: 0.3625 - val_acc: 0.8677\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1735 - acc: 0.9391 - val_loss: 0.3616 - val_acc: 0.8730\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1730 - acc: 0.9397 - val_loss: 0.3575 - val_acc: 0.8677\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1732 - acc: 0.9403 - val_loss: 0.3561 - val_acc: 0.8624\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1737 - acc: 0.9391 - val_loss: 0.3637 - val_acc: 0.8677\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1720 - acc: 0.9409 - val_loss: 0.3784 - val_acc: 0.8730\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1738 - acc: 0.9368 - val_loss: 0.3604 - val_acc: 0.8677\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1722 - acc: 0.9403 - val_loss: 0.3671 - val_acc: 0.8677\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1726 - acc: 0.9421 - val_loss: 0.3599 - val_acc: 0.8571\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1718 - acc: 0.9421 - val_loss: 0.3600 - val_acc: 0.8677\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1718 - acc: 0.9374 - val_loss: 0.3733 - val_acc: 0.8730\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1732 - acc: 0.9362 - val_loss: 0.3583 - val_acc: 0.8677\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1712 - acc: 0.9379 - val_loss: 0.3591 - val_acc: 0.8624\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1715 - acc: 0.9385 - val_loss: 0.3684 - val_acc: 0.8571\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1733 - acc: 0.9403 - val_loss: 0.3625 - val_acc: 0.8677\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1732 - acc: 0.9409 - val_loss: 0.3617 - val_acc: 0.8730\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1726 - acc: 0.9385 - val_loss: 0.3631 - val_acc: 0.8730\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1726 - acc: 0.9403 - val_loss: 0.3567 - val_acc: 0.8624\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1724 - acc: 0.9385 - val_loss: 0.3563 - val_acc: 0.8624\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1728 - acc: 0.9385 - val_loss: 0.3570 - val_acc: 0.8624\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1722 - acc: 0.9385 - val_loss: 0.3568 - val_acc: 0.8730\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1727 - acc: 0.9391 - val_loss: 0.3557 - val_acc: 0.8677\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1725 - acc: 0.9444 - val_loss: 0.3581 - val_acc: 0.8677\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1726 - acc: 0.9397 - val_loss: 0.3601 - val_acc: 0.8677\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1720 - acc: 0.9385 - val_loss: 0.3559 - val_acc: 0.8624\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1723 - acc: 0.9379 - val_loss: 0.3593 - val_acc: 0.8677\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1717 - acc: 0.9391 - val_loss: 0.3688 - val_acc: 0.8730\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1723 - acc: 0.9409 - val_loss: 0.3583 - val_acc: 0.8677\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1717 - acc: 0.9403 - val_loss: 0.3569 - val_acc: 0.8730\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1721 - acc: 0.9403 - val_loss: 0.3616 - val_acc: 0.8571\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1715 - acc: 0.9415 - val_loss: 0.3667 - val_acc: 0.8677\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1720 - acc: 0.9409 - val_loss: 0.3591 - val_acc: 0.8677\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1717 - acc: 0.9391 - val_loss: 0.3564 - val_acc: 0.8624\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1715 - acc: 0.9391 - val_loss: 0.3637 - val_acc: 0.8677\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1717 - acc: 0.9409 - val_loss: 0.3609 - val_acc: 0.8624\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1723 - acc: 0.9391 - val_loss: 0.3577 - val_acc: 0.8624\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1721 - acc: 0.9379 - val_loss: 0.3560 - val_acc: 0.8624\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1715 - acc: 0.9421 - val_loss: 0.3586 - val_acc: 0.8730\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1718 - acc: 0.9415 - val_loss: 0.3557 - val_acc: 0.8624\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1710 - acc: 0.9391 - val_loss: 0.3575 - val_acc: 0.8624\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1713 - acc: 0.9409 - val_loss: 0.3573 - val_acc: 0.8624\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1715 - acc: 0.9409 - val_loss: 0.3642 - val_acc: 0.8677\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1712 - acc: 0.9403 - val_loss: 0.3606 - val_acc: 0.8624\n",
      "Epoch 590/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1706 - acc: 0.9415 - val_loss: 0.3689 - val_acc: 0.8677\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1708 - acc: 0.9368 - val_loss: 0.3600 - val_acc: 0.8624\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1705 - acc: 0.9397 - val_loss: 0.3560 - val_acc: 0.8677\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1704 - acc: 0.9421 - val_loss: 0.3645 - val_acc: 0.8624\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1708 - acc: 0.9374 - val_loss: 0.3636 - val_acc: 0.8624\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1712 - acc: 0.9403 - val_loss: 0.3604 - val_acc: 0.8677\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1696 - acc: 0.9415 - val_loss: 0.3715 - val_acc: 0.8730\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1710 - acc: 0.9433 - val_loss: 0.3568 - val_acc: 0.8624\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1710 - acc: 0.9385 - val_loss: 0.3588 - val_acc: 0.8624\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1710 - acc: 0.9415 - val_loss: 0.3609 - val_acc: 0.8677\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1694 - acc: 0.9415 - val_loss: 0.3686 - val_acc: 0.8571\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1706 - acc: 0.9397 - val_loss: 0.3689 - val_acc: 0.8624\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1706 - acc: 0.9427 - val_loss: 0.3690 - val_acc: 0.8624\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1709 - acc: 0.9403 - val_loss: 0.3624 - val_acc: 0.8624\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1704 - acc: 0.9397 - val_loss: 0.3591 - val_acc: 0.8571\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1705 - acc: 0.9391 - val_loss: 0.3559 - val_acc: 0.8624\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1695 - acc: 0.9415 - val_loss: 0.3648 - val_acc: 0.8571\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1701 - acc: 0.9403 - val_loss: 0.3660 - val_acc: 0.8624\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1705 - acc: 0.9421 - val_loss: 0.3568 - val_acc: 0.8624\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1701 - acc: 0.9403 - val_loss: 0.3614 - val_acc: 0.8624\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1703 - acc: 0.9397 - val_loss: 0.3628 - val_acc: 0.8624\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1699 - acc: 0.9397 - val_loss: 0.3668 - val_acc: 0.8624\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1693 - acc: 0.9409 - val_loss: 0.3615 - val_acc: 0.8571\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1697 - acc: 0.9391 - val_loss: 0.3610 - val_acc: 0.8624\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1687 - acc: 0.9415 - val_loss: 0.3710 - val_acc: 0.8624\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1696 - acc: 0.9397 - val_loss: 0.3719 - val_acc: 0.8730\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1696 - acc: 0.9385 - val_loss: 0.3558 - val_acc: 0.8624\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1692 - acc: 0.9421 - val_loss: 0.3645 - val_acc: 0.8624\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1694 - acc: 0.9403 - val_loss: 0.3665 - val_acc: 0.8624\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1700 - acc: 0.9385 - val_loss: 0.3687 - val_acc: 0.8624\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1699 - acc: 0.9415 - val_loss: 0.3688 - val_acc: 0.8677\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.3705 - val_acc: 0.8624\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1680 - acc: 0.9403 - val_loss: 0.3589 - val_acc: 0.8624\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1699 - acc: 0.9421 - val_loss: 0.3603 - val_acc: 0.8624\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.3589 - val_acc: 0.8677\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1695 - acc: 0.9379 - val_loss: 0.3630 - val_acc: 0.8624\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1692 - acc: 0.9409 - val_loss: 0.3616 - val_acc: 0.8624\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1697 - acc: 0.9439 - val_loss: 0.3598 - val_acc: 0.8677\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1690 - acc: 0.9391 - val_loss: 0.3584 - val_acc: 0.8677\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1690 - acc: 0.9421 - val_loss: 0.3541 - val_acc: 0.8677\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1689 - acc: 0.9427 - val_loss: 0.3624 - val_acc: 0.8624\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1688 - acc: 0.9433 - val_loss: 0.3615 - val_acc: 0.8624\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1686 - acc: 0.9409 - val_loss: 0.3664 - val_acc: 0.8624\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1692 - acc: 0.9397 - val_loss: 0.3652 - val_acc: 0.8677\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1680 - acc: 0.9421 - val_loss: 0.3771 - val_acc: 0.8571\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1686 - acc: 0.9403 - val_loss: 0.3709 - val_acc: 0.8677\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1689 - acc: 0.9409 - val_loss: 0.3619 - val_acc: 0.8624\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1679 - acc: 0.9421 - val_loss: 0.3669 - val_acc: 0.8571\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1679 - acc: 0.9385 - val_loss: 0.3613 - val_acc: 0.8677\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1687 - acc: 0.9409 - val_loss: 0.3621 - val_acc: 0.8624\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1683 - acc: 0.9427 - val_loss: 0.3649 - val_acc: 0.8624\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1686 - acc: 0.9391 - val_loss: 0.3639 - val_acc: 0.8624\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1683 - acc: 0.9421 - val_loss: 0.3609 - val_acc: 0.8677\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1684 - acc: 0.9415 - val_loss: 0.3663 - val_acc: 0.8624\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1679 - acc: 0.9409 - val_loss: 0.3697 - val_acc: 0.8624\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1692 - acc: 0.9409 - val_loss: 0.3620 - val_acc: 0.8571\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1677 - acc: 0.9409 - val_loss: 0.3678 - val_acc: 0.8624\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1686 - acc: 0.9409 - val_loss: 0.3661 - val_acc: 0.8624\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1681 - acc: 0.9409 - val_loss: 0.3630 - val_acc: 0.8677\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1678 - acc: 0.9409 - val_loss: 0.3667 - val_acc: 0.8730\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1683 - acc: 0.9385 - val_loss: 0.3593 - val_acc: 0.8677\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1671 - acc: 0.9415 - val_loss: 0.3653 - val_acc: 0.8677\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1680 - acc: 0.9415 - val_loss: 0.3574 - val_acc: 0.8624\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1678 - acc: 0.9374 - val_loss: 0.3615 - val_acc: 0.8677\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1674 - acc: 0.9421 - val_loss: 0.3575 - val_acc: 0.8677\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1686 - acc: 0.9403 - val_loss: 0.3602 - val_acc: 0.8624\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1680 - acc: 0.9433 - val_loss: 0.3593 - val_acc: 0.8624\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1677 - acc: 0.9409 - val_loss: 0.3644 - val_acc: 0.8624\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1681 - acc: 0.9433 - val_loss: 0.3603 - val_acc: 0.8730\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1666 - acc: 0.9444 - val_loss: 0.3619 - val_acc: 0.8677\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1680 - acc: 0.9409 - val_loss: 0.3650 - val_acc: 0.8677\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1666 - acc: 0.9415 - val_loss: 0.3548 - val_acc: 0.8571\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1676 - acc: 0.9421 - val_loss: 0.3663 - val_acc: 0.8624\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1664 - acc: 0.9456 - val_loss: 0.3751 - val_acc: 0.8677\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1670 - acc: 0.9421 - val_loss: 0.3668 - val_acc: 0.8677\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1676 - acc: 0.9403 - val_loss: 0.3622 - val_acc: 0.8730\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1661 - acc: 0.9397 - val_loss: 0.3667 - val_acc: 0.8624\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1677 - acc: 0.9421 - val_loss: 0.3632 - val_acc: 0.8677\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1662 - acc: 0.9415 - val_loss: 0.3618 - val_acc: 0.8624\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1677 - acc: 0.9427 - val_loss: 0.3646 - val_acc: 0.8624\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1671 - acc: 0.9421 - val_loss: 0.3635 - val_acc: 0.8624\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1667 - acc: 0.9415 - val_loss: 0.3622 - val_acc: 0.8677\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1670 - acc: 0.9427 - val_loss: 0.3657 - val_acc: 0.8677\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1673 - acc: 0.9415 - val_loss: 0.3617 - val_acc: 0.8677\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1669 - acc: 0.9421 - val_loss: 0.3626 - val_acc: 0.8677\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1673 - acc: 0.9409 - val_loss: 0.3663 - val_acc: 0.8730\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1667 - acc: 0.9409 - val_loss: 0.3689 - val_acc: 0.8730\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1667 - acc: 0.9421 - val_loss: 0.3686 - val_acc: 0.8677\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1660 - acc: 0.9397 - val_loss: 0.3684 - val_acc: 0.8677\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1672 - acc: 0.9427 - val_loss: 0.3666 - val_acc: 0.8624\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1659 - acc: 0.9409 - val_loss: 0.3645 - val_acc: 0.8730\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1665 - acc: 0.9403 - val_loss: 0.3670 - val_acc: 0.8677\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1666 - acc: 0.9415 - val_loss: 0.3611 - val_acc: 0.8677\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1664 - acc: 0.9409 - val_loss: 0.3677 - val_acc: 0.8730\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1643 - acc: 0.9427 - val_loss: 0.3769 - val_acc: 0.8624\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1665 - acc: 0.9421 - val_loss: 0.3727 - val_acc: 0.8571\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1652 - acc: 0.9409 - val_loss: 0.3696 - val_acc: 0.8624\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1654 - acc: 0.9421 - val_loss: 0.3631 - val_acc: 0.8624\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1660 - acc: 0.9421 - val_loss: 0.3742 - val_acc: 0.8624\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1665 - acc: 0.9421 - val_loss: 0.3663 - val_acc: 0.8677\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1660 - acc: 0.9433 - val_loss: 0.3627 - val_acc: 0.8624\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1655 - acc: 0.9397 - val_loss: 0.3654 - val_acc: 0.8624\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1658 - acc: 0.9439 - val_loss: 0.3623 - val_acc: 0.8571\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1663 - acc: 0.9439 - val_loss: 0.3610 - val_acc: 0.8624\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.3799 - val_acc: 0.8624\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1657 - acc: 0.9433 - val_loss: 0.3759 - val_acc: 0.8677\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1660 - acc: 0.9409 - val_loss: 0.3730 - val_acc: 0.8730\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1663 - acc: 0.9439 - val_loss: 0.3671 - val_acc: 0.8571\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.3750 - val_acc: 0.8624\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1655 - acc: 0.9433 - val_loss: 0.3698 - val_acc: 0.8677\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1657 - acc: 0.9415 - val_loss: 0.3689 - val_acc: 0.8624\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.3752 - val_acc: 0.8677\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1656 - acc: 0.9391 - val_loss: 0.3665 - val_acc: 0.8571\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1651 - acc: 0.9427 - val_loss: 0.3717 - val_acc: 0.8519\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1653 - acc: 0.9409 - val_loss: 0.3653 - val_acc: 0.8624\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1651 - acc: 0.9444 - val_loss: 0.3723 - val_acc: 0.8571\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1652 - acc: 0.9421 - val_loss: 0.3644 - val_acc: 0.8677\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1646 - acc: 0.9433 - val_loss: 0.3674 - val_acc: 0.8677\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1654 - acc: 0.9403 - val_loss: 0.3647 - val_acc: 0.8624\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1649 - acc: 0.9444 - val_loss: 0.3663 - val_acc: 0.8571\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1644 - acc: 0.9421 - val_loss: 0.3680 - val_acc: 0.8571\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1649 - acc: 0.9409 - val_loss: 0.3628 - val_acc: 0.8624\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1648 - acc: 0.9444 - val_loss: 0.3627 - val_acc: 0.8624\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1648 - acc: 0.9456 - val_loss: 0.3637 - val_acc: 0.8677\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1644 - acc: 0.9433 - val_loss: 0.3725 - val_acc: 0.8624\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1649 - acc: 0.9403 - val_loss: 0.3654 - val_acc: 0.8571\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1656 - acc: 0.9439 - val_loss: 0.3637 - val_acc: 0.8624\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1645 - acc: 0.9385 - val_loss: 0.3654 - val_acc: 0.8571\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1640 - acc: 0.9433 - val_loss: 0.3630 - val_acc: 0.8677\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1646 - acc: 0.9427 - val_loss: 0.3639 - val_acc: 0.8730\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1656 - acc: 0.9421 - val_loss: 0.3667 - val_acc: 0.8677\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1645 - acc: 0.9444 - val_loss: 0.3690 - val_acc: 0.8624\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1646 - acc: 0.9409 - val_loss: 0.3629 - val_acc: 0.8624\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1646 - acc: 0.9427 - val_loss: 0.3636 - val_acc: 0.8677\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1642 - acc: 0.9444 - val_loss: 0.3670 - val_acc: 0.8730\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1646 - acc: 0.9450 - val_loss: 0.3669 - val_acc: 0.8571\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1643 - acc: 0.9409 - val_loss: 0.3699 - val_acc: 0.8677\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1646 - acc: 0.9427 - val_loss: 0.3685 - val_acc: 0.8571\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1645 - acc: 0.9403 - val_loss: 0.3669 - val_acc: 0.8677\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1642 - acc: 0.9427 - val_loss: 0.3670 - val_acc: 0.8677\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1637 - acc: 0.9427 - val_loss: 0.3642 - val_acc: 0.8624\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1647 - acc: 0.9409 - val_loss: 0.3623 - val_acc: 0.8624\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1630 - acc: 0.9444 - val_loss: 0.3722 - val_acc: 0.8677\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1647 - acc: 0.9427 - val_loss: 0.3676 - val_acc: 0.8677\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.1638 - acc: 0.9403 - val_loss: 0.3717 - val_acc: 0.8677\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1640 - acc: 0.9403 - val_loss: 0.3770 - val_acc: 0.8624\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1645 - acc: 0.9433 - val_loss: 0.3635 - val_acc: 0.8677\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1640 - acc: 0.9433 - val_loss: 0.3645 - val_acc: 0.8624\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1644 - acc: 0.9433 - val_loss: 0.3696 - val_acc: 0.8677\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1645 - acc: 0.9433 - val_loss: 0.3669 - val_acc: 0.8677\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1626 - acc: 0.9415 - val_loss: 0.3661 - val_acc: 0.8783\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1646 - acc: 0.9403 - val_loss: 0.3627 - val_acc: 0.8730\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1629 - acc: 0.9433 - val_loss: 0.3726 - val_acc: 0.8677\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1646 - acc: 0.9403 - val_loss: 0.3650 - val_acc: 0.8677\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1634 - acc: 0.9427 - val_loss: 0.3657 - val_acc: 0.8677\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1635 - acc: 0.9444 - val_loss: 0.3746 - val_acc: 0.8571\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1638 - acc: 0.9421 - val_loss: 0.3739 - val_acc: 0.8677\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1634 - acc: 0.9397 - val_loss: 0.3682 - val_acc: 0.8624\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1634 - acc: 0.9427 - val_loss: 0.3676 - val_acc: 0.8624\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1633 - acc: 0.9450 - val_loss: 0.3739 - val_acc: 0.8677\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1629 - acc: 0.9439 - val_loss: 0.3618 - val_acc: 0.8624\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1629 - acc: 0.9456 - val_loss: 0.3755 - val_acc: 0.8624\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1640 - acc: 0.9421 - val_loss: 0.3723 - val_acc: 0.8571\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1631 - acc: 0.9439 - val_loss: 0.3679 - val_acc: 0.8730\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1630 - acc: 0.9450 - val_loss: 0.3724 - val_acc: 0.8519\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1624 - acc: 0.9456 - val_loss: 0.3742 - val_acc: 0.8730\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1631 - acc: 0.9480 - val_loss: 0.3693 - val_acc: 0.8571\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1628 - acc: 0.9433 - val_loss: 0.3753 - val_acc: 0.8624\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1630 - acc: 0.9456 - val_loss: 0.3730 - val_acc: 0.8730\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1619 - acc: 0.9427 - val_loss: 0.3806 - val_acc: 0.8624\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1634 - acc: 0.9439 - val_loss: 0.3699 - val_acc: 0.8624\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1626 - acc: 0.9427 - val_loss: 0.3681 - val_acc: 0.8677\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1633 - acc: 0.9462 - val_loss: 0.3727 - val_acc: 0.8677\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1632 - acc: 0.9415 - val_loss: 0.3681 - val_acc: 0.8677\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1634 - acc: 0.9421 - val_loss: 0.3713 - val_acc: 0.8624\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1627 - acc: 0.9415 - val_loss: 0.3713 - val_acc: 0.8730\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1625 - acc: 0.9427 - val_loss: 0.3738 - val_acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1628 - acc: 0.9427 - val_loss: 0.3660 - val_acc: 0.8730\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.3706 - val_acc: 0.8677\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1629 - acc: 0.9439 - val_loss: 0.3704 - val_acc: 0.8624\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1621 - acc: 0.9444 - val_loss: 0.3722 - val_acc: 0.8624\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1633 - acc: 0.9433 - val_loss: 0.3705 - val_acc: 0.8624\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.3711 - val_acc: 0.8677\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1631 - acc: 0.9421 - val_loss: 0.3686 - val_acc: 0.8677\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1619 - acc: 0.9444 - val_loss: 0.3603 - val_acc: 0.8571\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1623 - acc: 0.9444 - val_loss: 0.3688 - val_acc: 0.8730\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1627 - acc: 0.9427 - val_loss: 0.3784 - val_acc: 0.8677\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1632 - acc: 0.9439 - val_loss: 0.3762 - val_acc: 0.8624\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1624 - acc: 0.9450 - val_loss: 0.3686 - val_acc: 0.8677\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1627 - acc: 0.9415 - val_loss: 0.3661 - val_acc: 0.8571\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1617 - acc: 0.9468 - val_loss: 0.3702 - val_acc: 0.8677\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1625 - acc: 0.9427 - val_loss: 0.3645 - val_acc: 0.8624\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1624 - acc: 0.9450 - val_loss: 0.3722 - val_acc: 0.8624\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1632 - acc: 0.9439 - val_loss: 0.3743 - val_acc: 0.8730\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1624 - acc: 0.9421 - val_loss: 0.3728 - val_acc: 0.8677\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1626 - acc: 0.9439 - val_loss: 0.3671 - val_acc: 0.8677\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1615 - acc: 0.9433 - val_loss: 0.3791 - val_acc: 0.8677\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1618 - acc: 0.9415 - val_loss: 0.3723 - val_acc: 0.8677\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1619 - acc: 0.9474 - val_loss: 0.3722 - val_acc: 0.8624\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1619 - acc: 0.9439 - val_loss: 0.3703 - val_acc: 0.8624\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1612 - acc: 0.9444 - val_loss: 0.3721 - val_acc: 0.8571\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.3739 - val_acc: 0.8730\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1614 - acc: 0.9444 - val_loss: 0.3806 - val_acc: 0.8624\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1619 - acc: 0.9415 - val_loss: 0.3678 - val_acc: 0.8624\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1621 - acc: 0.9415 - val_loss: 0.3710 - val_acc: 0.8624\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1621 - acc: 0.9415 - val_loss: 0.3702 - val_acc: 0.8624\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1624 - acc: 0.9439 - val_loss: 0.3694 - val_acc: 0.8677\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1617 - acc: 0.9456 - val_loss: 0.3714 - val_acc: 0.8624\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1621 - acc: 0.9439 - val_loss: 0.3744 - val_acc: 0.8571\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1623 - acc: 0.9439 - val_loss: 0.3632 - val_acc: 0.8624\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1617 - acc: 0.9433 - val_loss: 0.3741 - val_acc: 0.8624\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1616 - acc: 0.9439 - val_loss: 0.3731 - val_acc: 0.8624\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1620 - acc: 0.9421 - val_loss: 0.3660 - val_acc: 0.8624\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1616 - acc: 0.9468 - val_loss: 0.3666 - val_acc: 0.8624\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1613 - acc: 0.9433 - val_loss: 0.3726 - val_acc: 0.8624\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1620 - acc: 0.9439 - val_loss: 0.3768 - val_acc: 0.8730\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1622 - acc: 0.9439 - val_loss: 0.3692 - val_acc: 0.8624\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.1617 - acc: 0.9439 - val_loss: 0.3736 - val_acc: 0.8571\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1614 - acc: 0.9444 - val_loss: 0.3788 - val_acc: 0.8571\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1617 - acc: 0.9462 - val_loss: 0.3674 - val_acc: 0.8677\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1605 - acc: 0.9474 - val_loss: 0.3708 - val_acc: 0.8624\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1623 - acc: 0.9450 - val_loss: 0.3761 - val_acc: 0.8624\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1616 - acc: 0.9450 - val_loss: 0.3707 - val_acc: 0.8677\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1613 - acc: 0.9468 - val_loss: 0.3694 - val_acc: 0.8677\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1619 - acc: 0.9439 - val_loss: 0.3736 - val_acc: 0.8571\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1606 - acc: 0.9444 - val_loss: 0.3833 - val_acc: 0.8624\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1620 - acc: 0.9450 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1611 - acc: 0.9439 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1611 - acc: 0.9450 - val_loss: 0.3648 - val_acc: 0.8571\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1611 - acc: 0.9456 - val_loss: 0.3705 - val_acc: 0.8677\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1611 - acc: 0.9456 - val_loss: 0.3732 - val_acc: 0.8624\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1619 - acc: 0.9439 - val_loss: 0.3784 - val_acc: 0.8624\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1611 - acc: 0.9439 - val_loss: 0.3754 - val_acc: 0.8571\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1617 - acc: 0.9462 - val_loss: 0.3791 - val_acc: 0.8571\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1611 - acc: 0.9444 - val_loss: 0.3854 - val_acc: 0.8624\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1619 - acc: 0.9462 - val_loss: 0.3694 - val_acc: 0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1613 - acc: 0.9456 - val_loss: 0.3733 - val_acc: 0.8571\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1622 - acc: 0.946 - 0s 62us/step - loss: 0.1607 - acc: 0.9439 - val_loss: 0.3764 - val_acc: 0.8677\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1612 - acc: 0.9462 - val_loss: 0.3746 - val_acc: 0.8571\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1610 - acc: 0.9439 - val_loss: 0.3696 - val_acc: 0.8677\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1605 - acc: 0.9474 - val_loss: 0.3756 - val_acc: 0.8624\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1613 - acc: 0.9462 - val_loss: 0.3706 - val_acc: 0.8571\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1607 - acc: 0.9450 - val_loss: 0.3760 - val_acc: 0.8519\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1611 - acc: 0.9433 - val_loss: 0.3767 - val_acc: 0.8571\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1615 - acc: 0.9439 - val_loss: 0.3702 - val_acc: 0.8624\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1610 - acc: 0.9444 - val_loss: 0.3775 - val_acc: 0.8571\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1604 - acc: 0.9468 - val_loss: 0.3784 - val_acc: 0.8624\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1615 - acc: 0.9456 - val_loss: 0.3762 - val_acc: 0.8624\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1605 - acc: 0.9456 - val_loss: 0.3785 - val_acc: 0.8571\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1603 - acc: 0.9439 - val_loss: 0.3672 - val_acc: 0.8677\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1609 - acc: 0.9433 - val_loss: 0.3682 - val_acc: 0.8624\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1604 - acc: 0.9444 - val_loss: 0.3727 - val_acc: 0.8624\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1604 - acc: 0.9474 - val_loss: 0.3786 - val_acc: 0.8677\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1609 - acc: 0.9444 - val_loss: 0.3744 - val_acc: 0.8571\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1601 - acc: 0.9486 - val_loss: 0.3748 - val_acc: 0.8519\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1603 - acc: 0.9450 - val_loss: 0.3706 - val_acc: 0.8624\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1603 - acc: 0.9462 - val_loss: 0.3794 - val_acc: 0.8519\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1604 - acc: 0.9450 - val_loss: 0.3710 - val_acc: 0.8624\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1603 - acc: 0.9462 - val_loss: 0.3787 - val_acc: 0.8677\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1598 - acc: 0.9450 - val_loss: 0.3767 - val_acc: 0.8571\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1606 - acc: 0.9450 - val_loss: 0.3813 - val_acc: 0.8571\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1594 - acc: 0.9462 - val_loss: 0.3799 - val_acc: 0.8624\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1606 - acc: 0.9462 - val_loss: 0.3754 - val_acc: 0.8519\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1592 - acc: 0.9456 - val_loss: 0.3749 - val_acc: 0.8571\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1604 - acc: 0.9450 - val_loss: 0.3771 - val_acc: 0.8519\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1597 - acc: 0.9462 - val_loss: 0.3790 - val_acc: 0.8624\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.3695 - val_acc: 0.8624\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1596 - acc: 0.9474 - val_loss: 0.3848 - val_acc: 0.8571\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1602 - acc: 0.9456 - val_loss: 0.3795 - val_acc: 0.8519\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1601 - acc: 0.9450 - val_loss: 0.3814 - val_acc: 0.8519\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1601 - acc: 0.9439 - val_loss: 0.3768 - val_acc: 0.8519\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1597 - acc: 0.9480 - val_loss: 0.3791 - val_acc: 0.8519\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1596 - acc: 0.9456 - val_loss: 0.3724 - val_acc: 0.8571\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1598 - acc: 0.9456 - val_loss: 0.3797 - val_acc: 0.8519\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1599 - acc: 0.9433 - val_loss: 0.3754 - val_acc: 0.8571\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1593 - acc: 0.9444 - val_loss: 0.3805 - val_acc: 0.8624\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1602 - acc: 0.9450 - val_loss: 0.3810 - val_acc: 0.8519\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1592 - acc: 0.9462 - val_loss: 0.3873 - val_acc: 0.8519\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1604 - acc: 0.9456 - val_loss: 0.3804 - val_acc: 0.8624\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.1594 - acc: 0.9468 - val_loss: 0.3735 - val_acc: 0.8571\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1596 - acc: 0.9474 - val_loss: 0.3764 - val_acc: 0.8677\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1599 - acc: 0.9444 - val_loss: 0.3738 - val_acc: 0.8571\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1590 - acc: 0.9462 - val_loss: 0.3755 - val_acc: 0.8571\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1596 - acc: 0.9462 - val_loss: 0.3764 - val_acc: 0.8571\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1592 - acc: 0.9456 - val_loss: 0.3762 - val_acc: 0.8624\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1591 - acc: 0.9474 - val_loss: 0.3814 - val_acc: 0.8624\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1598 - acc: 0.9427 - val_loss: 0.3725 - val_acc: 0.8624\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1596 - acc: 0.9444 - val_loss: 0.3806 - val_acc: 0.8624\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1591 - acc: 0.9468 - val_loss: 0.3823 - val_acc: 0.8571\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1599 - acc: 0.9462 - val_loss: 0.3877 - val_acc: 0.8571\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9456 - val_loss: 0.3824 - val_acc: 0.8571\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1594 - acc: 0.9474 - val_loss: 0.3811 - val_acc: 0.8519\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1586 - acc: 0.9456 - val_loss: 0.3854 - val_acc: 0.8624\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1594 - acc: 0.9474 - val_loss: 0.3798 - val_acc: 0.8519\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.3753 - val_acc: 0.8571\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1591 - acc: 0.9439 - val_loss: 0.3786 - val_acc: 0.8571\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1591 - acc: 0.9433 - val_loss: 0.3834 - val_acc: 0.8571\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1593 - acc: 0.9468 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1594 - acc: 0.9439 - val_loss: 0.3844 - val_acc: 0.8571\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1598 - acc: 0.9456 - val_loss: 0.3723 - val_acc: 0.8677\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1594 - acc: 0.9480 - val_loss: 0.3773 - val_acc: 0.8571\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1590 - acc: 0.9468 - val_loss: 0.3785 - val_acc: 0.8519\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1594 - acc: 0.9462 - val_loss: 0.3782 - val_acc: 0.8519\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1575 - acc: 0.9462 - val_loss: 0.3733 - val_acc: 0.8624\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1588 - acc: 0.9480 - val_loss: 0.3803 - val_acc: 0.8519\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1586 - acc: 0.9456 - val_loss: 0.3729 - val_acc: 0.8624\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1590 - acc: 0.9509 - val_loss: 0.3771 - val_acc: 0.8571\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1592 - acc: 0.9456 - val_loss: 0.3765 - val_acc: 0.8677\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1591 - acc: 0.9486 - val_loss: 0.3836 - val_acc: 0.8571\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1590 - acc: 0.9486 - val_loss: 0.3873 - val_acc: 0.8571\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1595 - acc: 0.9468 - val_loss: 0.3858 - val_acc: 0.8519\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1589 - acc: 0.9462 - val_loss: 0.3817 - val_acc: 0.8677\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1594 - acc: 0.9456 - val_loss: 0.3784 - val_acc: 0.8571\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1590 - acc: 0.9444 - val_loss: 0.3798 - val_acc: 0.8571\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1589 - acc: 0.9450 - val_loss: 0.3760 - val_acc: 0.8624\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1584 - acc: 0.9444 - val_loss: 0.3803 - val_acc: 0.8571\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1590 - acc: 0.9456 - val_loss: 0.3846 - val_acc: 0.8571\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1587 - acc: 0.9468 - val_loss: 0.3816 - val_acc: 0.8571\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1588 - acc: 0.9480 - val_loss: 0.3820 - val_acc: 0.8571\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9474 - val_loss: 0.3860 - val_acc: 0.8571\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1588 - acc: 0.9480 - val_loss: 0.3868 - val_acc: 0.8571\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1583 - acc: 0.9468 - val_loss: 0.3859 - val_acc: 0.8624\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1587 - acc: 0.9492 - val_loss: 0.3789 - val_acc: 0.8571\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1587 - acc: 0.9444 - val_loss: 0.3766 - val_acc: 0.8677\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1583 - acc: 0.9456 - val_loss: 0.3863 - val_acc: 0.8571\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1586 - acc: 0.9474 - val_loss: 0.3845 - val_acc: 0.8571\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1576 - acc: 0.9468 - val_loss: 0.3815 - val_acc: 0.8624\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.1585 - acc: 0.9474 - val_loss: 0.3829 - val_acc: 0.8519\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1594 - acc: 0.9462 - val_loss: 0.3793 - val_acc: 0.8624\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1589 - acc: 0.9468 - val_loss: 0.3833 - val_acc: 0.8571\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1582 - acc: 0.9480 - val_loss: 0.3769 - val_acc: 0.8677\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1578 - acc: 0.9474 - val_loss: 0.3765 - val_acc: 0.8677\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.1587 - acc: 0.9468 - val_loss: 0.3763 - val_acc: 0.8677\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.1587 - acc: 0.9474 - val_loss: 0.3817 - val_acc: 0.8624\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.1581 - acc: 0.9456 - val_loss: 0.3788 - val_acc: 0.8571\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.1584 - acc: 0.9468 - val_loss: 0.3811 - val_acc: 0.8624\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 135us/step - loss: 0.1574 - acc: 0.9480 - val_loss: 0.3793 - val_acc: 0.8730\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1583 - acc: 0.9462 - val_loss: 0.3828 - val_acc: 0.8571\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.1582 - acc: 0.9462 - val_loss: 0.3802 - val_acc: 0.8571\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1593 - acc: 0.9480 - val_loss: 0.3769 - val_acc: 0.8571\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1581 - acc: 0.9480 - val_loss: 0.3754 - val_acc: 0.8571\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1576 - acc: 0.9480 - val_loss: 0.3855 - val_acc: 0.8466\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1579 - acc: 0.9468 - val_loss: 0.3756 - val_acc: 0.8519\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.1574 - acc: 0.9480 - val_loss: 0.3937 - val_acc: 0.8466\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 156us/step - loss: 0.1587 - acc: 0.9468 - val_loss: 0.3856 - val_acc: 0.8624\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.1582 - acc: 0.9444 - val_loss: 0.3828 - val_acc: 0.8624\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.1575 - acc: 0.9474 - val_loss: 0.3788 - val_acc: 0.8624\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1583 - acc: 0.9474 - val_loss: 0.3787 - val_acc: 0.8624\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1578 - acc: 0.9486 - val_loss: 0.3850 - val_acc: 0.8571\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.1588 - acc: 0.9462 - val_loss: 0.3801 - val_acc: 0.8624\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1580 - acc: 0.9468 - val_loss: 0.3820 - val_acc: 0.8519\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1573 - acc: 0.9480 - val_loss: 0.3887 - val_acc: 0.8571\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1586 - acc: 0.9468 - val_loss: 0.3824 - val_acc: 0.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1584 - acc: 0.9480 - val_loss: 0.3795 - val_acc: 0.8571\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1415 - acc: 0.950 - 0s 49us/step - loss: 0.1586 - acc: 0.9456 - val_loss: 0.3822 - val_acc: 0.8677\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1572 - acc: 0.9468 - val_loss: 0.3853 - val_acc: 0.8571\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1585 - acc: 0.9468 - val_loss: 0.3771 - val_acc: 0.8624\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1581 - acc: 0.9492 - val_loss: 0.3761 - val_acc: 0.8677\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.1572 - acc: 0.9462 - val_loss: 0.3702 - val_acc: 0.8677\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1584 - acc: 0.9462 - val_loss: 0.3815 - val_acc: 0.8466\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1578 - acc: 0.9462 - val_loss: 0.3790 - val_acc: 0.8571\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1575 - acc: 0.9480 - val_loss: 0.3771 - val_acc: 0.8624\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1582 - acc: 0.9474 - val_loss: 0.3814 - val_acc: 0.8624\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.1577 - acc: 0.9474 - val_loss: 0.3784 - val_acc: 0.8571\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.1569 - acc: 0.9450 - val_loss: 0.3888 - val_acc: 0.8519\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1578 - acc: 0.9444 - val_loss: 0.3825 - val_acc: 0.8519\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1577 - acc: 0.9480 - val_loss: 0.3878 - val_acc: 0.8519\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1571 - acc: 0.9450 - val_loss: 0.3870 - val_acc: 0.8571\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1584 - acc: 0.9474 - val_loss: 0.3803 - val_acc: 0.8624\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1581 - acc: 0.9486 - val_loss: 0.3856 - val_acc: 0.8571\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.1574 - acc: 0.9468 - val_loss: 0.3810 - val_acc: 0.8571\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.1579 - acc: 0.9498 - val_loss: 0.3817 - val_acc: 0.8571\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1579 - acc: 0.9462 - val_loss: 0.3808 - val_acc: 0.8571\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1563 - acc: 0.9462 - val_loss: 0.3763 - val_acc: 0.8571\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1585 - acc: 0.9474 - val_loss: 0.3799 - val_acc: 0.8571\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1572 - acc: 0.9486 - val_loss: 0.3817 - val_acc: 0.8624\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1577 - acc: 0.9480 - val_loss: 0.3832 - val_acc: 0.8519\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1577 - acc: 0.9462 - val_loss: 0.3828 - val_acc: 0.8571\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1576 - acc: 0.9480 - val_loss: 0.3763 - val_acc: 0.8677\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1566 - acc: 0.9474 - val_loss: 0.3840 - val_acc: 0.8571\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1574 - acc: 0.9450 - val_loss: 0.3832 - val_acc: 0.8624\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1576 - acc: 0.9474 - val_loss: 0.3803 - val_acc: 0.8519\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1571 - acc: 0.9486 - val_loss: 0.3861 - val_acc: 0.8677\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1574 - acc: 0.9456 - val_loss: 0.3804 - val_acc: 0.8624\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1580 - acc: 0.9468 - val_loss: 0.3799 - val_acc: 0.8677\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1578 - acc: 0.9468 - val_loss: 0.3795 - val_acc: 0.8624\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1575 - acc: 0.9474 - val_loss: 0.3783 - val_acc: 0.8624\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1579 - acc: 0.9456 - val_loss: 0.3777 - val_acc: 0.8571\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1568 - acc: 0.9480 - val_loss: 0.3810 - val_acc: 0.8571\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1569 - acc: 0.9480 - val_loss: 0.3831 - val_acc: 0.8677\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1575 - acc: 0.9486 - val_loss: 0.3808 - val_acc: 0.8624\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1577 - acc: 0.9468 - val_loss: 0.3757 - val_acc: 0.8624\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1577 - acc: 0.9462 - val_loss: 0.3815 - val_acc: 0.8571\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.3818 - val_acc: 0.8624\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1567 - acc: 0.9480 - val_loss: 0.3817 - val_acc: 0.8624\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1582 - acc: 0.9468 - val_loss: 0.3794 - val_acc: 0.8677\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1575 - acc: 0.9462 - val_loss: 0.3822 - val_acc: 0.8624\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1567 - acc: 0.9456 - val_loss: 0.3865 - val_acc: 0.8466\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1568 - acc: 0.9450 - val_loss: 0.3840 - val_acc: 0.8571\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1576 - acc: 0.9474 - val_loss: 0.3822 - val_acc: 0.8571\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1564 - acc: 0.9462 - val_loss: 0.3901 - val_acc: 0.8624\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1575 - acc: 0.9474 - val_loss: 0.3826 - val_acc: 0.8571\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1569 - acc: 0.9498 - val_loss: 0.3845 - val_acc: 0.8519\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1570 - acc: 0.9456 - val_loss: 0.3806 - val_acc: 0.8624\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1563 - acc: 0.9474 - val_loss: 0.3848 - val_acc: 0.8519\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.3831 - val_acc: 0.8571\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1577 - acc: 0.9468 - val_loss: 0.3850 - val_acc: 0.8571\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1563 - acc: 0.9486 - val_loss: 0.3902 - val_acc: 0.8519\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1575 - acc: 0.9480 - val_loss: 0.3891 - val_acc: 0.8519\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1573 - acc: 0.9462 - val_loss: 0.3859 - val_acc: 0.8624\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1570 - acc: 0.9468 - val_loss: 0.3864 - val_acc: 0.8624\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 715us/step - loss: 0.7981 - acc: 0.3753 - val_loss: 0.7328 - val_acc: 0.3915\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.7044 - acc: 0.4486 - val_loss: 0.6770 - val_acc: 0.5661\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.6576 - acc: 0.6673 - val_loss: 0.6547 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.6372 - acc: 0.6814 - val_loss: 0.6448 - val_acc: 0.6772\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.6238 - acc: 0.6915 - val_loss: 0.6369 - val_acc: 0.6878\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.6123 - acc: 0.7021 - val_loss: 0.6296 - val_acc: 0.6878\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.6010 - acc: 0.7210 - val_loss: 0.6226 - val_acc: 0.6878\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.5902 - acc: 0.7287 - val_loss: 0.6160 - val_acc: 0.6825\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.5800 - acc: 0.7400 - val_loss: 0.6099 - val_acc: 0.6720\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.5702 - acc: 0.7411 - val_loss: 0.6041 - val_acc: 0.6720\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.5613 - acc: 0.7512 - val_loss: 0.5993 - val_acc: 0.6720\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.5526 - acc: 0.7530 - val_loss: 0.5956 - val_acc: 0.6772\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.5450 - acc: 0.7553 - val_loss: 0.5918 - val_acc: 0.6667\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.5374 - acc: 0.7624 - val_loss: 0.5878 - val_acc: 0.6614\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.5303 - acc: 0.7707 - val_loss: 0.5850 - val_acc: 0.6720\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.5233 - acc: 0.7725 - val_loss: 0.5812 - val_acc: 0.6825\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.5166 - acc: 0.7778 - val_loss: 0.5778 - val_acc: 0.6825\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.5100 - acc: 0.7813 - val_loss: 0.5743 - val_acc: 0.6878\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.5038 - acc: 0.7843 - val_loss: 0.5707 - val_acc: 0.6825\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.4977 - acc: 0.7878 - val_loss: 0.5676 - val_acc: 0.6931\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.4911 - acc: 0.7914 - val_loss: 0.5638 - val_acc: 0.6931\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.4850 - acc: 0.7949 - val_loss: 0.5607 - val_acc: 0.7037\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4787 - acc: 0.7979 - val_loss: 0.5584 - val_acc: 0.7037\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.4727 - acc: 0.8073 - val_loss: 0.5548 - val_acc: 0.7143\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.4668 - acc: 0.8103 - val_loss: 0.5511 - val_acc: 0.7249\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.4613 - acc: 0.8138 - val_loss: 0.5468 - val_acc: 0.7249\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4556 - acc: 0.8162 - val_loss: 0.5425 - val_acc: 0.7302\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4501 - acc: 0.8186 - val_loss: 0.5410 - val_acc: 0.7302\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4446 - acc: 0.8209 - val_loss: 0.5379 - val_acc: 0.7302\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4394 - acc: 0.8262 - val_loss: 0.5337 - val_acc: 0.7354\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4342 - acc: 0.8292 - val_loss: 0.5290 - val_acc: 0.7407\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4288 - acc: 0.8339 - val_loss: 0.5268 - val_acc: 0.7302\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4237 - acc: 0.8381 - val_loss: 0.5217 - val_acc: 0.7460\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.4190 - acc: 0.8387 - val_loss: 0.5210 - val_acc: 0.7566\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4143 - acc: 0.8404 - val_loss: 0.5176 - val_acc: 0.7566\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.4095 - acc: 0.8428 - val_loss: 0.5156 - val_acc: 0.7566\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4048 - acc: 0.8452 - val_loss: 0.5104 - val_acc: 0.7619\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4001 - acc: 0.8457 - val_loss: 0.5064 - val_acc: 0.7566\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3957 - acc: 0.8475 - val_loss: 0.5043 - val_acc: 0.7619\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3909 - acc: 0.8499 - val_loss: 0.4984 - val_acc: 0.7566\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3869 - acc: 0.8570 - val_loss: 0.4949 - val_acc: 0.7619\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3826 - acc: 0.8522 - val_loss: 0.4911 - val_acc: 0.7672\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3784 - acc: 0.8540 - val_loss: 0.4896 - val_acc: 0.7672\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3743 - acc: 0.8534 - val_loss: 0.4845 - val_acc: 0.7672\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3701 - acc: 0.8540 - val_loss: 0.4842 - val_acc: 0.7672\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3662 - acc: 0.8558 - val_loss: 0.4783 - val_acc: 0.7725\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3625 - acc: 0.8558 - val_loss: 0.4747 - val_acc: 0.7725\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3584 - acc: 0.8593 - val_loss: 0.4735 - val_acc: 0.7725\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3554 - acc: 0.8593 - val_loss: 0.4671 - val_acc: 0.7725\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3522 - acc: 0.8617 - val_loss: 0.4664 - val_acc: 0.7725\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3487 - acc: 0.8623 - val_loss: 0.4652 - val_acc: 0.7725\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3453 - acc: 0.8635 - val_loss: 0.4611 - val_acc: 0.7778\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3421 - acc: 0.8647 - val_loss: 0.4556 - val_acc: 0.7831\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3386 - acc: 0.8670 - val_loss: 0.4603 - val_acc: 0.7831\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3361 - acc: 0.8670 - val_loss: 0.4542 - val_acc: 0.7884\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3329 - acc: 0.8717 - val_loss: 0.4493 - val_acc: 0.7884\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3302 - acc: 0.8706 - val_loss: 0.4475 - val_acc: 0.7884\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3273 - acc: 0.8753 - val_loss: 0.4455 - val_acc: 0.7884\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3243 - acc: 0.8735 - val_loss: 0.4434 - val_acc: 0.7937\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3215 - acc: 0.8765 - val_loss: 0.4386 - val_acc: 0.7937\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3184 - acc: 0.8806 - val_loss: 0.4372 - val_acc: 0.8042\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3156 - acc: 0.8771 - val_loss: 0.4290 - val_acc: 0.8095\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3131 - acc: 0.8794 - val_loss: 0.4299 - val_acc: 0.8095\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3105 - acc: 0.8824 - val_loss: 0.4268 - val_acc: 0.8095\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3078 - acc: 0.8830 - val_loss: 0.4238 - val_acc: 0.8095\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.3050 - acc: 0.8842 - val_loss: 0.4166 - val_acc: 0.8148\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3025 - acc: 0.8865 - val_loss: 0.4131 - val_acc: 0.8148\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3001 - acc: 0.8889 - val_loss: 0.4113 - val_acc: 0.8148\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2974 - acc: 0.8889 - val_loss: 0.4084 - val_acc: 0.8201\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2952 - acc: 0.8924 - val_loss: 0.4079 - val_acc: 0.8254\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2932 - acc: 0.8954 - val_loss: 0.4069 - val_acc: 0.8254\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2907 - acc: 0.8978 - val_loss: 0.4029 - val_acc: 0.8201\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2888 - acc: 0.8960 - val_loss: 0.4054 - val_acc: 0.8201\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2870 - acc: 0.8983 - val_loss: 0.4022 - val_acc: 0.8307\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2851 - acc: 0.9019 - val_loss: 0.4023 - val_acc: 0.8254\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2838 - acc: 0.9013 - val_loss: 0.3999 - val_acc: 0.8360\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2819 - acc: 0.9013 - val_loss: 0.4016 - val_acc: 0.8307\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2804 - acc: 0.9031 - val_loss: 0.3971 - val_acc: 0.8413\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2789 - acc: 0.9019 - val_loss: 0.3950 - val_acc: 0.8413\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.2775 - acc: 0.9037 - val_loss: 0.3953 - val_acc: 0.8413\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2760 - acc: 0.9007 - val_loss: 0.3930 - val_acc: 0.8413\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2745 - acc: 0.9025 - val_loss: 0.3945 - val_acc: 0.8413\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2731 - acc: 0.9013 - val_loss: 0.3904 - val_acc: 0.8413\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.2722 - acc: 0.9048 - val_loss: 0.3901 - val_acc: 0.8413\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2709 - acc: 0.9031 - val_loss: 0.3890 - val_acc: 0.8360\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2697 - acc: 0.9031 - val_loss: 0.3863 - val_acc: 0.8360\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2686 - acc: 0.9048 - val_loss: 0.3934 - val_acc: 0.8413\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2678 - acc: 0.9054 - val_loss: 0.3857 - val_acc: 0.8360\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2668 - acc: 0.9048 - val_loss: 0.3847 - val_acc: 0.8360\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2659 - acc: 0.9066 - val_loss: 0.3861 - val_acc: 0.8360\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2646 - acc: 0.9066 - val_loss: 0.3829 - val_acc: 0.8307\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2643 - acc: 0.9054 - val_loss: 0.3859 - val_acc: 0.8413\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2635 - acc: 0.9043 - val_loss: 0.3817 - val_acc: 0.8413\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2624 - acc: 0.9084 - val_loss: 0.3799 - val_acc: 0.8413\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2607 - acc: 0.9078 - val_loss: 0.3878 - val_acc: 0.8360\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2614 - acc: 0.9084 - val_loss: 0.3851 - val_acc: 0.8413\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2602 - acc: 0.9096 - val_loss: 0.3813 - val_acc: 0.8413\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2591 - acc: 0.9078 - val_loss: 0.3822 - val_acc: 0.8413\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2585 - acc: 0.9096 - val_loss: 0.3777 - val_acc: 0.8413\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2577 - acc: 0.9096 - val_loss: 0.3806 - val_acc: 0.8413\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2570 - acc: 0.9096 - val_loss: 0.3775 - val_acc: 0.8360\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2559 - acc: 0.9090 - val_loss: 0.3825 - val_acc: 0.8466\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2555 - acc: 0.9090 - val_loss: 0.3799 - val_acc: 0.8413\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2548 - acc: 0.9090 - val_loss: 0.3822 - val_acc: 0.8413\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2541 - acc: 0.9090 - val_loss: 0.3787 - val_acc: 0.8413\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2531 - acc: 0.9137 - val_loss: 0.3783 - val_acc: 0.8360\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2524 - acc: 0.9108 - val_loss: 0.3787 - val_acc: 0.8466\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2521 - acc: 0.9119 - val_loss: 0.3764 - val_acc: 0.8360\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2508 - acc: 0.9149 - val_loss: 0.3765 - val_acc: 0.8307\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2506 - acc: 0.9137 - val_loss: 0.3768 - val_acc: 0.8360\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2501 - acc: 0.9155 - val_loss: 0.3770 - val_acc: 0.8307\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2494 - acc: 0.9125 - val_loss: 0.3767 - val_acc: 0.8307\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2488 - acc: 0.9137 - val_loss: 0.3740 - val_acc: 0.8360\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2483 - acc: 0.9155 - val_loss: 0.3763 - val_acc: 0.8360\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2480 - acc: 0.9167 - val_loss: 0.3779 - val_acc: 0.8307\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2477 - acc: 0.9167 - val_loss: 0.3737 - val_acc: 0.8307\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2468 - acc: 0.9143 - val_loss: 0.3783 - val_acc: 0.8307\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2462 - acc: 0.9184 - val_loss: 0.3728 - val_acc: 0.8360\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2460 - acc: 0.9167 - val_loss: 0.3763 - val_acc: 0.8307\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2458 - acc: 0.9184 - val_loss: 0.3711 - val_acc: 0.8413\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2455 - acc: 0.9155 - val_loss: 0.3752 - val_acc: 0.8360\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2454 - acc: 0.9161 - val_loss: 0.3747 - val_acc: 0.8413\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2446 - acc: 0.9173 - val_loss: 0.3753 - val_acc: 0.8413\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2444 - acc: 0.9178 - val_loss: 0.3758 - val_acc: 0.8307\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2436 - acc: 0.9178 - val_loss: 0.3710 - val_acc: 0.8413\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2437 - acc: 0.9161 - val_loss: 0.3753 - val_acc: 0.8360\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2430 - acc: 0.9178 - val_loss: 0.3765 - val_acc: 0.8360\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2428 - acc: 0.9196 - val_loss: 0.3728 - val_acc: 0.8360\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2418 - acc: 0.9202 - val_loss: 0.3753 - val_acc: 0.8360\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2419 - acc: 0.9202 - val_loss: 0.3719 - val_acc: 0.8413\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2416 - acc: 0.9196 - val_loss: 0.3707 - val_acc: 0.8413\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2406 - acc: 0.9173 - val_loss: 0.3695 - val_acc: 0.8413\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2406 - acc: 0.9208 - val_loss: 0.3694 - val_acc: 0.8413\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2404 - acc: 0.9184 - val_loss: 0.3742 - val_acc: 0.8413\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2395 - acc: 0.9178 - val_loss: 0.3751 - val_acc: 0.8413\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2391 - acc: 0.9190 - val_loss: 0.3691 - val_acc: 0.8519\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2390 - acc: 0.9214 - val_loss: 0.3707 - val_acc: 0.8466\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2383 - acc: 0.9196 - val_loss: 0.3687 - val_acc: 0.8413\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2382 - acc: 0.9214 - val_loss: 0.3744 - val_acc: 0.8360\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2374 - acc: 0.9208 - val_loss: 0.3695 - val_acc: 0.8519\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2373 - acc: 0.9190 - val_loss: 0.3742 - val_acc: 0.8519\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2374 - acc: 0.9196 - val_loss: 0.3713 - val_acc: 0.8466\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2362 - acc: 0.9214 - val_loss: 0.3661 - val_acc: 0.8466\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2362 - acc: 0.9202 - val_loss: 0.3744 - val_acc: 0.8360\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2364 - acc: 0.9202 - val_loss: 0.3680 - val_acc: 0.8466\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2362 - acc: 0.9202 - val_loss: 0.3694 - val_acc: 0.8466\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2359 - acc: 0.9202 - val_loss: 0.3692 - val_acc: 0.8466\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2354 - acc: 0.9220 - val_loss: 0.3755 - val_acc: 0.8413\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2354 - acc: 0.9214 - val_loss: 0.3736 - val_acc: 0.8413\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2352 - acc: 0.9214 - val_loss: 0.3696 - val_acc: 0.8466\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2344 - acc: 0.9220 - val_loss: 0.3678 - val_acc: 0.8466\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2341 - acc: 0.9190 - val_loss: 0.3701 - val_acc: 0.8413\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2340 - acc: 0.9220 - val_loss: 0.3685 - val_acc: 0.8466\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2340 - acc: 0.9243 - val_loss: 0.3681 - val_acc: 0.8466\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2339 - acc: 0.9249 - val_loss: 0.3694 - val_acc: 0.8466\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2326 - acc: 0.9214 - val_loss: 0.3745 - val_acc: 0.8466\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2331 - acc: 0.9232 - val_loss: 0.3732 - val_acc: 0.8413\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2330 - acc: 0.9243 - val_loss: 0.3721 - val_acc: 0.8466\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2329 - acc: 0.9238 - val_loss: 0.3708 - val_acc: 0.8466\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2324 - acc: 0.9238 - val_loss: 0.3715 - val_acc: 0.8466\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2321 - acc: 0.9255 - val_loss: 0.3697 - val_acc: 0.8519\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2317 - acc: 0.9238 - val_loss: 0.3738 - val_acc: 0.8519\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2318 - acc: 0.9255 - val_loss: 0.3695 - val_acc: 0.8519\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2314 - acc: 0.9226 - val_loss: 0.3726 - val_acc: 0.8519\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2312 - acc: 0.9249 - val_loss: 0.3669 - val_acc: 0.8413\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2313 - acc: 0.9243 - val_loss: 0.3659 - val_acc: 0.8466\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2304 - acc: 0.9226 - val_loss: 0.3733 - val_acc: 0.8571\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2313 - acc: 0.9232 - val_loss: 0.3690 - val_acc: 0.8466\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2310 - acc: 0.9243 - val_loss: 0.3660 - val_acc: 0.8466\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2307 - acc: 0.9238 - val_loss: 0.3654 - val_acc: 0.8466\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2302 - acc: 0.9243 - val_loss: 0.3655 - val_acc: 0.8466\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2301 - acc: 0.9238 - val_loss: 0.3647 - val_acc: 0.8571\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2293 - acc: 0.9243 - val_loss: 0.3660 - val_acc: 0.8466\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2297 - acc: 0.9232 - val_loss: 0.3646 - val_acc: 0.8519\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2297 - acc: 0.9255 - val_loss: 0.3640 - val_acc: 0.8519\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2296 - acc: 0.9255 - val_loss: 0.3650 - val_acc: 0.8519\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2289 - acc: 0.9249 - val_loss: 0.3652 - val_acc: 0.8571\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2287 - acc: 0.9238 - val_loss: 0.3637 - val_acc: 0.8571\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2291 - acc: 0.9226 - val_loss: 0.3632 - val_acc: 0.8571\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2284 - acc: 0.9249 - val_loss: 0.3668 - val_acc: 0.8519\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2288 - acc: 0.9261 - val_loss: 0.3662 - val_acc: 0.8519\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2285 - acc: 0.9243 - val_loss: 0.3641 - val_acc: 0.8519\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2282 - acc: 0.9238 - val_loss: 0.3645 - val_acc: 0.8519\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2277 - acc: 0.9261 - val_loss: 0.3658 - val_acc: 0.8571\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2281 - acc: 0.9255 - val_loss: 0.3644 - val_acc: 0.8519\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2281 - acc: 0.9255 - val_loss: 0.3633 - val_acc: 0.8519\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2277 - acc: 0.9249 - val_loss: 0.3657 - val_acc: 0.8571\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2278 - acc: 0.9261 - val_loss: 0.3658 - val_acc: 0.8519\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2277 - acc: 0.9238 - val_loss: 0.3636 - val_acc: 0.8519\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2270 - acc: 0.9232 - val_loss: 0.3683 - val_acc: 0.8571\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2274 - acc: 0.9249 - val_loss: 0.3629 - val_acc: 0.8519\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2270 - acc: 0.9249 - val_loss: 0.3620 - val_acc: 0.8571\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2268 - acc: 0.9243 - val_loss: 0.3622 - val_acc: 0.8571\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2268 - acc: 0.9202 - val_loss: 0.3642 - val_acc: 0.8571\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2266 - acc: 0.9249 - val_loss: 0.3630 - val_acc: 0.8519\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2266 - acc: 0.9279 - val_loss: 0.3601 - val_acc: 0.8624\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2259 - acc: 0.9249 - val_loss: 0.3633 - val_acc: 0.8519\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2254 - acc: 0.9238 - val_loss: 0.3704 - val_acc: 0.8624\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2258 - acc: 0.9232 - val_loss: 0.3667 - val_acc: 0.8624\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2262 - acc: 0.9267 - val_loss: 0.3638 - val_acc: 0.8571\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2259 - acc: 0.9249 - val_loss: 0.3613 - val_acc: 0.8519\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.2257 - acc: 0.9226 - val_loss: 0.3638 - val_acc: 0.8519\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2256 - acc: 0.9255 - val_loss: 0.3609 - val_acc: 0.8519\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2248 - acc: 0.9267 - val_loss: 0.3640 - val_acc: 0.8571\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2255 - acc: 0.9249 - val_loss: 0.3627 - val_acc: 0.8519\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2251 - acc: 0.9249 - val_loss: 0.3616 - val_acc: 0.8571\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2244 - acc: 0.9255 - val_loss: 0.3651 - val_acc: 0.8624\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2248 - acc: 0.9249 - val_loss: 0.3596 - val_acc: 0.8519\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2247 - acc: 0.9273 - val_loss: 0.3609 - val_acc: 0.8519\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2246 - acc: 0.9261 - val_loss: 0.3614 - val_acc: 0.8519\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2245 - acc: 0.9249 - val_loss: 0.3588 - val_acc: 0.8519\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2241 - acc: 0.9261 - val_loss: 0.3631 - val_acc: 0.8571\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2246 - acc: 0.9243 - val_loss: 0.3619 - val_acc: 0.8571\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2241 - acc: 0.9273 - val_loss: 0.3587 - val_acc: 0.8571\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2245 - acc: 0.9261 - val_loss: 0.3584 - val_acc: 0.8519\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2235 - acc: 0.9267 - val_loss: 0.3621 - val_acc: 0.8624\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2238 - acc: 0.9249 - val_loss: 0.3583 - val_acc: 0.8519\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2237 - acc: 0.9273 - val_loss: 0.3581 - val_acc: 0.8519\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2236 - acc: 0.9267 - val_loss: 0.3588 - val_acc: 0.8519\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2235 - acc: 0.9243 - val_loss: 0.3596 - val_acc: 0.8519\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2226 - acc: 0.9273 - val_loss: 0.3621 - val_acc: 0.8571\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2235 - acc: 0.9232 - val_loss: 0.3611 - val_acc: 0.8519\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2234 - acc: 0.9255 - val_loss: 0.3598 - val_acc: 0.8519\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2232 - acc: 0.9249 - val_loss: 0.3588 - val_acc: 0.8519\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2226 - acc: 0.9249 - val_loss: 0.3602 - val_acc: 0.8519\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2229 - acc: 0.9291 - val_loss: 0.3622 - val_acc: 0.8571\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2229 - acc: 0.9232 - val_loss: 0.3591 - val_acc: 0.8519\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2229 - acc: 0.9267 - val_loss: 0.3619 - val_acc: 0.8519\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2227 - acc: 0.9255 - val_loss: 0.3638 - val_acc: 0.8624\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2226 - acc: 0.9255 - val_loss: 0.3613 - val_acc: 0.8519\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2225 - acc: 0.9249 - val_loss: 0.3589 - val_acc: 0.8519\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2222 - acc: 0.9255 - val_loss: 0.3572 - val_acc: 0.8519\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2226 - acc: 0.9261 - val_loss: 0.3588 - val_acc: 0.8519\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2221 - acc: 0.9261 - val_loss: 0.3572 - val_acc: 0.8571\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2221 - acc: 0.9285 - val_loss: 0.3569 - val_acc: 0.8571\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2223 - acc: 0.9226 - val_loss: 0.3575 - val_acc: 0.8519\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.3577 - val_acc: 0.8519\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2219 - acc: 0.9261 - val_loss: 0.3564 - val_acc: 0.8571\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2220 - acc: 0.9255 - val_loss: 0.3567 - val_acc: 0.8519\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2216 - acc: 0.9243 - val_loss: 0.3588 - val_acc: 0.8571\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2215 - acc: 0.9226 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2216 - acc: 0.9243 - val_loss: 0.3573 - val_acc: 0.8519\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2212 - acc: 0.9261 - val_loss: 0.3610 - val_acc: 0.8571\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2217 - acc: 0.9226 - val_loss: 0.3601 - val_acc: 0.8519\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2211 - acc: 0.9255 - val_loss: 0.3571 - val_acc: 0.8519\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2212 - acc: 0.9267 - val_loss: 0.3593 - val_acc: 0.8519\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2213 - acc: 0.9273 - val_loss: 0.3598 - val_acc: 0.8519\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2211 - acc: 0.9255 - val_loss: 0.3583 - val_acc: 0.8519\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2214 - acc: 0.9238 - val_loss: 0.3572 - val_acc: 0.8519\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2211 - acc: 0.9261 - val_loss: 0.3561 - val_acc: 0.8519\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2206 - acc: 0.9273 - val_loss: 0.3575 - val_acc: 0.8466\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2213 - acc: 0.9267 - val_loss: 0.3565 - val_acc: 0.8466\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2210 - acc: 0.9261 - val_loss: 0.3551 - val_acc: 0.8571\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2204 - acc: 0.9255 - val_loss: 0.3522 - val_acc: 0.8519\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2207 - acc: 0.9220 - val_loss: 0.3593 - val_acc: 0.8624\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2206 - acc: 0.9232 - val_loss: 0.3552 - val_acc: 0.8519\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2204 - acc: 0.9249 - val_loss: 0.3578 - val_acc: 0.8624\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2207 - acc: 0.9267 - val_loss: 0.3583 - val_acc: 0.8519\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2201 - acc: 0.9267 - val_loss: 0.3567 - val_acc: 0.8519\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2203 - acc: 0.9243 - val_loss: 0.3542 - val_acc: 0.8571\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2200 - acc: 0.9255 - val_loss: 0.3574 - val_acc: 0.8519\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2203 - acc: 0.9243 - val_loss: 0.3568 - val_acc: 0.8571\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2202 - acc: 0.9261 - val_loss: 0.3571 - val_acc: 0.8519\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2202 - acc: 0.9255 - val_loss: 0.3549 - val_acc: 0.8571\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2193 - acc: 0.9232 - val_loss: 0.3609 - val_acc: 0.8571\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2195 - acc: 0.9273 - val_loss: 0.3558 - val_acc: 0.8571\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2196 - acc: 0.9267 - val_loss: 0.3564 - val_acc: 0.8571\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2196 - acc: 0.9255 - val_loss: 0.3565 - val_acc: 0.8519\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2196 - acc: 0.9261 - val_loss: 0.3563 - val_acc: 0.8677\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2198 - acc: 0.9285 - val_loss: 0.3573 - val_acc: 0.8519\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2196 - acc: 0.9255 - val_loss: 0.3560 - val_acc: 0.8571\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2193 - acc: 0.9249 - val_loss: 0.3573 - val_acc: 0.8677\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2192 - acc: 0.9273 - val_loss: 0.3540 - val_acc: 0.8571\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2190 - acc: 0.9261 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2184 - acc: 0.9226 - val_loss: 0.3604 - val_acc: 0.8624\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2193 - acc: 0.9261 - val_loss: 0.3560 - val_acc: 0.8571\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2190 - acc: 0.9249 - val_loss: 0.3547 - val_acc: 0.8571\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2188 - acc: 0.9273 - val_loss: 0.3577 - val_acc: 0.8624\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2189 - acc: 0.9255 - val_loss: 0.3558 - val_acc: 0.8571\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2187 - acc: 0.9273 - val_loss: 0.3572 - val_acc: 0.8571\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2182 - acc: 0.9279 - val_loss: 0.3530 - val_acc: 0.8571\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2181 - acc: 0.9267 - val_loss: 0.3558 - val_acc: 0.8624\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2181 - acc: 0.9285 - val_loss: 0.3528 - val_acc: 0.8571\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2178 - acc: 0.9267 - val_loss: 0.3533 - val_acc: 0.8571\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2180 - acc: 0.9273 - val_loss: 0.3560 - val_acc: 0.8624\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2180 - acc: 0.9255 - val_loss: 0.3543 - val_acc: 0.8624\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2179 - acc: 0.9267 - val_loss: 0.3539 - val_acc: 0.8571\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2180 - acc: 0.9279 - val_loss: 0.3544 - val_acc: 0.8624\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2175 - acc: 0.9279 - val_loss: 0.3579 - val_acc: 0.8519\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2173 - acc: 0.9261 - val_loss: 0.3522 - val_acc: 0.8571\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2174 - acc: 0.9267 - val_loss: 0.3541 - val_acc: 0.8624\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2175 - acc: 0.9255 - val_loss: 0.3559 - val_acc: 0.8624\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2171 - acc: 0.9267 - val_loss: 0.3559 - val_acc: 0.8571\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2166 - acc: 0.9273 - val_loss: 0.3527 - val_acc: 0.8624\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2168 - acc: 0.9261 - val_loss: 0.3545 - val_acc: 0.8571\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2166 - acc: 0.9279 - val_loss: 0.3582 - val_acc: 0.8624\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2171 - acc: 0.9261 - val_loss: 0.3556 - val_acc: 0.8571\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2168 - acc: 0.9279 - val_loss: 0.3544 - val_acc: 0.8624\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2162 - acc: 0.9285 - val_loss: 0.3524 - val_acc: 0.8624\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2163 - acc: 0.9273 - val_loss: 0.3494 - val_acc: 0.8571\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2162 - acc: 0.9261 - val_loss: 0.3525 - val_acc: 0.8624\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2163 - acc: 0.9303 - val_loss: 0.3542 - val_acc: 0.8624\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2162 - acc: 0.9297 - val_loss: 0.3572 - val_acc: 0.8624\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2166 - acc: 0.9297 - val_loss: 0.3549 - val_acc: 0.8624\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2154 - acc: 0.9291 - val_loss: 0.3513 - val_acc: 0.8571\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2162 - acc: 0.9261 - val_loss: 0.3549 - val_acc: 0.8677\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2156 - acc: 0.9285 - val_loss: 0.3506 - val_acc: 0.8571\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2156 - acc: 0.9291 - val_loss: 0.3515 - val_acc: 0.8624\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2156 - acc: 0.9279 - val_loss: 0.3515 - val_acc: 0.8571\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2156 - acc: 0.9291 - val_loss: 0.3528 - val_acc: 0.8624\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2154 - acc: 0.9285 - val_loss: 0.3513 - val_acc: 0.8624\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2151 - acc: 0.9261 - val_loss: 0.3547 - val_acc: 0.8571\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2154 - acc: 0.9273 - val_loss: 0.3542 - val_acc: 0.8624\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2151 - acc: 0.9285 - val_loss: 0.3539 - val_acc: 0.8571\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2150 - acc: 0.9267 - val_loss: 0.3503 - val_acc: 0.8519\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2148 - acc: 0.9303 - val_loss: 0.3525 - val_acc: 0.8571\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2149 - acc: 0.9279 - val_loss: 0.3507 - val_acc: 0.8571\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2147 - acc: 0.9279 - val_loss: 0.3528 - val_acc: 0.8571\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2145 - acc: 0.9303 - val_loss: 0.3517 - val_acc: 0.8519\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2147 - acc: 0.9297 - val_loss: 0.3510 - val_acc: 0.8519\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.2143 - acc: 0.9261 - val_loss: 0.3522 - val_acc: 0.8571\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2148 - acc: 0.9267 - val_loss: 0.3526 - val_acc: 0.8519\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2130 - acc: 0.9261 - val_loss: 0.3586 - val_acc: 0.8624\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2144 - acc: 0.9291 - val_loss: 0.3540 - val_acc: 0.8571\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2144 - acc: 0.9267 - val_loss: 0.3532 - val_acc: 0.8519\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2142 - acc: 0.9297 - val_loss: 0.3544 - val_acc: 0.8677\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.2139 - acc: 0.9297 - val_loss: 0.3523 - val_acc: 0.8519\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2137 - acc: 0.9291 - val_loss: 0.3562 - val_acc: 0.8730\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2138 - acc: 0.9297 - val_loss: 0.3498 - val_acc: 0.8519\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.2142 - acc: 0.9261 - val_loss: 0.3520 - val_acc: 0.8519\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2134 - acc: 0.9279 - val_loss: 0.3523 - val_acc: 0.8677\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.2133 - acc: 0.9291 - val_loss: 0.3582 - val_acc: 0.8677\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2137 - acc: 0.9285 - val_loss: 0.3504 - val_acc: 0.8519\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2138 - acc: 0.9285 - val_loss: 0.3534 - val_acc: 0.8571\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2136 - acc: 0.9297 - val_loss: 0.3511 - val_acc: 0.8519\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.2133 - acc: 0.9297 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.2130 - acc: 0.9291 - val_loss: 0.3551 - val_acc: 0.8730\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.2132 - acc: 0.9279 - val_loss: 0.3518 - val_acc: 0.8519\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2132 - acc: 0.9255 - val_loss: 0.3544 - val_acc: 0.8571\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2134 - acc: 0.9285 - val_loss: 0.3529 - val_acc: 0.8519\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2131 - acc: 0.9291 - val_loss: 0.3516 - val_acc: 0.8571\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2128 - acc: 0.9285 - val_loss: 0.3546 - val_acc: 0.8677\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2127 - acc: 0.9279 - val_loss: 0.3518 - val_acc: 0.8571\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2126 - acc: 0.9279 - val_loss: 0.3563 - val_acc: 0.8730\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2126 - acc: 0.9291 - val_loss: 0.3549 - val_acc: 0.8624\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2122 - acc: 0.9285 - val_loss: 0.3524 - val_acc: 0.8571\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2125 - acc: 0.9285 - val_loss: 0.3518 - val_acc: 0.8624\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2124 - acc: 0.9303 - val_loss: 0.3545 - val_acc: 0.8730\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2122 - acc: 0.9297 - val_loss: 0.3555 - val_acc: 0.8730\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2126 - acc: 0.9291 - val_loss: 0.3510 - val_acc: 0.8624\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2123 - acc: 0.9303 - val_loss: 0.3509 - val_acc: 0.8677\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.2119 - acc: 0.9291 - val_loss: 0.3525 - val_acc: 0.8730\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2121 - acc: 0.9326 - val_loss: 0.3517 - val_acc: 0.8677\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2120 - acc: 0.9297 - val_loss: 0.3499 - val_acc: 0.8730\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2118 - acc: 0.9303 - val_loss: 0.3471 - val_acc: 0.8730\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2118 - acc: 0.9297 - val_loss: 0.3483 - val_acc: 0.8571\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2115 - acc: 0.9303 - val_loss: 0.3521 - val_acc: 0.8677\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2113 - acc: 0.9320 - val_loss: 0.3558 - val_acc: 0.8677\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2119 - acc: 0.9303 - val_loss: 0.3511 - val_acc: 0.8677\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2118 - acc: 0.9309 - val_loss: 0.3502 - val_acc: 0.8624\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2113 - acc: 0.9314 - val_loss: 0.3489 - val_acc: 0.8571\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2100 - acc: 0.9303 - val_loss: 0.3592 - val_acc: 0.8571\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2115 - acc: 0.9291 - val_loss: 0.3463 - val_acc: 0.8624\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2116 - acc: 0.9291 - val_loss: 0.3508 - val_acc: 0.8677\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2111 - acc: 0.9291 - val_loss: 0.3523 - val_acc: 0.8783\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2109 - acc: 0.9314 - val_loss: 0.3489 - val_acc: 0.8571\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2109 - acc: 0.9303 - val_loss: 0.3523 - val_acc: 0.8783\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2111 - acc: 0.9297 - val_loss: 0.3524 - val_acc: 0.8677\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2102 - acc: 0.9320 - val_loss: 0.3581 - val_acc: 0.8677\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2109 - acc: 0.9279 - val_loss: 0.3534 - val_acc: 0.8730\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2103 - acc: 0.9285 - val_loss: 0.3586 - val_acc: 0.8730\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2110 - acc: 0.9314 - val_loss: 0.3531 - val_acc: 0.8730\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2104 - acc: 0.9291 - val_loss: 0.3549 - val_acc: 0.8730\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2105 - acc: 0.9297 - val_loss: 0.3518 - val_acc: 0.8730\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2108 - acc: 0.9309 - val_loss: 0.3508 - val_acc: 0.8730\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.2105 - acc: 0.9326 - val_loss: 0.3500 - val_acc: 0.8730\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2098 - acc: 0.9314 - val_loss: 0.3505 - val_acc: 0.8730\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 136us/step - loss: 0.2101 - acc: 0.9326 - val_loss: 0.3530 - val_acc: 0.8677\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2104 - acc: 0.9314 - val_loss: 0.3504 - val_acc: 0.8730\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2100 - acc: 0.9326 - val_loss: 0.3503 - val_acc: 0.8730\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2098 - acc: 0.9291 - val_loss: 0.3501 - val_acc: 0.8730\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2099 - acc: 0.9314 - val_loss: 0.3516 - val_acc: 0.8783\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2095 - acc: 0.9332 - val_loss: 0.3487 - val_acc: 0.8677\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2094 - acc: 0.9344 - val_loss: 0.3502 - val_acc: 0.8677\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2094 - acc: 0.9320 - val_loss: 0.3502 - val_acc: 0.8677\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2094 - acc: 0.9332 - val_loss: 0.3491 - val_acc: 0.8677\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2090 - acc: 0.9314 - val_loss: 0.3504 - val_acc: 0.8730\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2090 - acc: 0.9332 - val_loss: 0.3514 - val_acc: 0.8730\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2092 - acc: 0.9320 - val_loss: 0.3499 - val_acc: 0.8677\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2089 - acc: 0.9314 - val_loss: 0.3478 - val_acc: 0.8677\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2082 - acc: 0.9279 - val_loss: 0.3506 - val_acc: 0.8677\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2086 - acc: 0.9350 - val_loss: 0.3535 - val_acc: 0.8677\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2086 - acc: 0.9320 - val_loss: 0.3549 - val_acc: 0.8730\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2087 - acc: 0.9309 - val_loss: 0.3494 - val_acc: 0.8677\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2086 - acc: 0.9332 - val_loss: 0.3486 - val_acc: 0.8677\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2078 - acc: 0.9356 - val_loss: 0.3529 - val_acc: 0.8677\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2078 - acc: 0.9332 - val_loss: 0.3569 - val_acc: 0.8783\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2082 - acc: 0.9326 - val_loss: 0.3535 - val_acc: 0.8677\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2075 - acc: 0.9309 - val_loss: 0.3579 - val_acc: 0.8783\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2078 - acc: 0.9338 - val_loss: 0.3573 - val_acc: 0.8730\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2080 - acc: 0.9344 - val_loss: 0.3542 - val_acc: 0.8730\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2076 - acc: 0.9326 - val_loss: 0.3553 - val_acc: 0.8730\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2074 - acc: 0.9356 - val_loss: 0.3535 - val_acc: 0.8730\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2076 - acc: 0.9332 - val_loss: 0.3543 - val_acc: 0.8730\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2075 - acc: 0.9338 - val_loss: 0.3543 - val_acc: 0.8730\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2071 - acc: 0.9344 - val_loss: 0.3536 - val_acc: 0.8677\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2072 - acc: 0.9338 - val_loss: 0.3554 - val_acc: 0.8730\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2072 - acc: 0.9338 - val_loss: 0.3550 - val_acc: 0.8677\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2068 - acc: 0.9356 - val_loss: 0.3573 - val_acc: 0.8730\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2069 - acc: 0.9338 - val_loss: 0.3578 - val_acc: 0.8730\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2075 - acc: 0.9332 - val_loss: 0.3553 - val_acc: 0.8730\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2065 - acc: 0.9350 - val_loss: 0.3532 - val_acc: 0.8677\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2067 - acc: 0.9338 - val_loss: 0.3525 - val_acc: 0.8677\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2071 - acc: 0.9320 - val_loss: 0.3550 - val_acc: 0.8730\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2067 - acc: 0.9332 - val_loss: 0.3564 - val_acc: 0.8730\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2064 - acc: 0.9350 - val_loss: 0.3513 - val_acc: 0.8677\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2065 - acc: 0.9344 - val_loss: 0.3529 - val_acc: 0.8783\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2067 - acc: 0.9344 - val_loss: 0.3525 - val_acc: 0.8730\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2064 - acc: 0.9338 - val_loss: 0.3566 - val_acc: 0.8730\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2061 - acc: 0.9356 - val_loss: 0.3606 - val_acc: 0.8730\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2068 - acc: 0.9314 - val_loss: 0.3537 - val_acc: 0.8783\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2057 - acc: 0.9344 - val_loss: 0.3564 - val_acc: 0.8730\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2059 - acc: 0.9356 - val_loss: 0.3521 - val_acc: 0.8730\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2057 - acc: 0.9332 - val_loss: 0.3586 - val_acc: 0.8730\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2056 - acc: 0.9350 - val_loss: 0.3555 - val_acc: 0.8730\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2057 - acc: 0.9356 - val_loss: 0.3561 - val_acc: 0.8730\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2056 - acc: 0.9326 - val_loss: 0.3518 - val_acc: 0.8730\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2060 - acc: 0.9332 - val_loss: 0.3555 - val_acc: 0.8730\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2049 - acc: 0.9362 - val_loss: 0.3594 - val_acc: 0.8730\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2062 - acc: 0.9338 - val_loss: 0.3546 - val_acc: 0.8730\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2052 - acc: 0.9350 - val_loss: 0.3518 - val_acc: 0.8677\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2052 - acc: 0.9332 - val_loss: 0.3558 - val_acc: 0.8730\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2057 - acc: 0.9350 - val_loss: 0.3564 - val_acc: 0.8730\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2058 - acc: 0.9356 - val_loss: 0.3561 - val_acc: 0.8730\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2049 - acc: 0.9338 - val_loss: 0.3593 - val_acc: 0.8783\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2052 - acc: 0.9362 - val_loss: 0.3604 - val_acc: 0.8783\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2047 - acc: 0.9356 - val_loss: 0.3529 - val_acc: 0.8730\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2049 - acc: 0.9356 - val_loss: 0.3513 - val_acc: 0.8730\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2050 - acc: 0.9362 - val_loss: 0.3545 - val_acc: 0.8730\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2045 - acc: 0.9368 - val_loss: 0.3557 - val_acc: 0.8783\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2050 - acc: 0.9332 - val_loss: 0.3518 - val_acc: 0.8730\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2044 - acc: 0.9356 - val_loss: 0.3555 - val_acc: 0.8783\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2046 - acc: 0.9344 - val_loss: 0.3525 - val_acc: 0.8677\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2046 - acc: 0.9362 - val_loss: 0.3555 - val_acc: 0.8783\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2040 - acc: 0.9344 - val_loss: 0.3507 - val_acc: 0.8730\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2038 - acc: 0.9362 - val_loss: 0.3548 - val_acc: 0.8783\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2043 - acc: 0.9368 - val_loss: 0.3537 - val_acc: 0.8783\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2041 - acc: 0.9356 - val_loss: 0.3511 - val_acc: 0.8783\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2035 - acc: 0.9368 - val_loss: 0.3508 - val_acc: 0.8836\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2038 - acc: 0.9356 - val_loss: 0.3517 - val_acc: 0.8783\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2035 - acc: 0.9362 - val_loss: 0.3542 - val_acc: 0.8783\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2036 - acc: 0.9368 - val_loss: 0.3515 - val_acc: 0.8783\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2034 - acc: 0.9350 - val_loss: 0.3533 - val_acc: 0.8783\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2034 - acc: 0.9368 - val_loss: 0.3512 - val_acc: 0.8783\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2030 - acc: 0.9350 - val_loss: 0.3517 - val_acc: 0.8783\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2029 - acc: 0.9368 - val_loss: 0.3525 - val_acc: 0.8836\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2037 - acc: 0.9356 - val_loss: 0.3477 - val_acc: 0.8783\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2026 - acc: 0.9356 - val_loss: 0.3496 - val_acc: 0.8836\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2031 - acc: 0.9344 - val_loss: 0.3517 - val_acc: 0.8836\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2030 - acc: 0.9356 - val_loss: 0.3514 - val_acc: 0.8836\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2027 - acc: 0.9356 - val_loss: 0.3517 - val_acc: 0.8836\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2029 - acc: 0.9362 - val_loss: 0.3472 - val_acc: 0.8783\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2027 - acc: 0.9356 - val_loss: 0.3507 - val_acc: 0.8836\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2027 - acc: 0.9362 - val_loss: 0.3455 - val_acc: 0.8783\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2025 - acc: 0.9403 - val_loss: 0.3482 - val_acc: 0.8836\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2028 - acc: 0.9350 - val_loss: 0.3504 - val_acc: 0.8730\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2024 - acc: 0.9350 - val_loss: 0.3514 - val_acc: 0.8836\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2026 - acc: 0.9344 - val_loss: 0.3523 - val_acc: 0.8783\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2022 - acc: 0.9350 - val_loss: 0.3525 - val_acc: 0.8783\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2019 - acc: 0.9379 - val_loss: 0.3498 - val_acc: 0.8836\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2021 - acc: 0.9362 - val_loss: 0.3466 - val_acc: 0.8783\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2023 - acc: 0.9374 - val_loss: 0.3500 - val_acc: 0.8836\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2027 - acc: 0.9350 - val_loss: 0.3492 - val_acc: 0.8836\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2022 - acc: 0.9362 - val_loss: 0.3472 - val_acc: 0.8783\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2012 - acc: 0.9379 - val_loss: 0.3550 - val_acc: 0.8783\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2021 - acc: 0.9344 - val_loss: 0.3549 - val_acc: 0.8836\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2023 - acc: 0.9385 - val_loss: 0.3472 - val_acc: 0.8783\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2018 - acc: 0.9356 - val_loss: 0.3469 - val_acc: 0.8836\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2017 - acc: 0.9362 - val_loss: 0.3527 - val_acc: 0.8836\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2020 - acc: 0.9350 - val_loss: 0.3465 - val_acc: 0.8730\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2014 - acc: 0.9362 - val_loss: 0.3497 - val_acc: 0.8783\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2015 - acc: 0.9350 - val_loss: 0.3490 - val_acc: 0.8783\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2010 - acc: 0.9350 - val_loss: 0.3486 - val_acc: 0.8783\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2020 - acc: 0.9362 - val_loss: 0.3502 - val_acc: 0.8730\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2016 - acc: 0.9374 - val_loss: 0.3486 - val_acc: 0.8836\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2015 - acc: 0.9374 - val_loss: 0.3489 - val_acc: 0.8783\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2014 - acc: 0.9350 - val_loss: 0.3481 - val_acc: 0.8730\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2018 - acc: 0.9368 - val_loss: 0.3500 - val_acc: 0.8783\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2011 - acc: 0.9344 - val_loss: 0.3499 - val_acc: 0.8783\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2014 - acc: 0.9368 - val_loss: 0.3475 - val_acc: 0.8783\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2006 - acc: 0.9368 - val_loss: 0.3537 - val_acc: 0.8783\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2008 - acc: 0.9379 - val_loss: 0.3525 - val_acc: 0.8836\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2015 - acc: 0.9368 - val_loss: 0.3477 - val_acc: 0.8783\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2012 - acc: 0.9368 - val_loss: 0.3483 - val_acc: 0.8783\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2013 - acc: 0.9368 - val_loss: 0.3466 - val_acc: 0.8783\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2009 - acc: 0.9379 - val_loss: 0.3487 - val_acc: 0.8783\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2009 - acc: 0.9362 - val_loss: 0.3502 - val_acc: 0.8783\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2012 - acc: 0.9344 - val_loss: 0.3474 - val_acc: 0.8783\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2005 - acc: 0.9356 - val_loss: 0.3450 - val_acc: 0.8730\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2009 - acc: 0.9362 - val_loss: 0.3491 - val_acc: 0.8783\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2003 - acc: 0.9379 - val_loss: 0.3512 - val_acc: 0.8836\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2007 - acc: 0.9385 - val_loss: 0.3463 - val_acc: 0.8783\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1998 - acc: 0.9374 - val_loss: 0.3487 - val_acc: 0.8783\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2005 - acc: 0.9362 - val_loss: 0.3450 - val_acc: 0.8783\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2007 - acc: 0.9362 - val_loss: 0.3462 - val_acc: 0.8730\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2005 - acc: 0.9362 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2004 - acc: 0.9338 - val_loss: 0.3506 - val_acc: 0.8783\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2004 - acc: 0.9385 - val_loss: 0.3512 - val_acc: 0.8836\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2003 - acc: 0.9374 - val_loss: 0.3485 - val_acc: 0.8783\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1999 - acc: 0.9374 - val_loss: 0.3495 - val_acc: 0.8836\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2005 - acc: 0.9362 - val_loss: 0.3483 - val_acc: 0.8783\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2003 - acc: 0.9356 - val_loss: 0.3511 - val_acc: 0.8836\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2003 - acc: 0.9403 - val_loss: 0.3494 - val_acc: 0.8730\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2004 - acc: 0.9374 - val_loss: 0.3487 - val_acc: 0.8730\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1996 - acc: 0.9362 - val_loss: 0.3463 - val_acc: 0.8730\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1998 - acc: 0.9368 - val_loss: 0.3472 - val_acc: 0.8730\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1996 - acc: 0.9379 - val_loss: 0.3470 - val_acc: 0.8730\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1993 - acc: 0.9391 - val_loss: 0.3486 - val_acc: 0.8730\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1993 - acc: 0.9374 - val_loss: 0.3486 - val_acc: 0.8783\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1992 - acc: 0.9356 - val_loss: 0.3436 - val_acc: 0.8783\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1991 - acc: 0.9379 - val_loss: 0.3489 - val_acc: 0.8783\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1989 - acc: 0.9385 - val_loss: 0.3467 - val_acc: 0.8730\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1989 - acc: 0.9385 - val_loss: 0.3491 - val_acc: 0.8730\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1993 - acc: 0.9391 - val_loss: 0.3467 - val_acc: 0.8783\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1993 - acc: 0.9374 - val_loss: 0.3445 - val_acc: 0.8730\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1989 - acc: 0.9362 - val_loss: 0.3421 - val_acc: 0.8783\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1979 - acc: 0.9362 - val_loss: 0.3425 - val_acc: 0.8783\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1986 - acc: 0.9362 - val_loss: 0.3441 - val_acc: 0.8783\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1982 - acc: 0.9362 - val_loss: 0.3484 - val_acc: 0.8836\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1992 - acc: 0.935 - 0s 59us/step - loss: 0.1984 - acc: 0.9362 - val_loss: 0.3427 - val_acc: 0.8783\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1981 - acc: 0.9374 - val_loss: 0.3459 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1981 - acc: 0.9374 - val_loss: 0.3452 - val_acc: 0.8783\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1982 - acc: 0.9385 - val_loss: 0.3477 - val_acc: 0.8783\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1984 - acc: 0.9374 - val_loss: 0.3431 - val_acc: 0.8783\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1988 - acc: 0.9374 - val_loss: 0.3432 - val_acc: 0.8783\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1980 - acc: 0.9362 - val_loss: 0.3447 - val_acc: 0.8783\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1973 - acc: 0.9385 - val_loss: 0.3427 - val_acc: 0.8783\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1977 - acc: 0.9362 - val_loss: 0.3417 - val_acc: 0.8783\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1980 - acc: 0.9391 - val_loss: 0.3427 - val_acc: 0.8783\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1980 - acc: 0.9379 - val_loss: 0.3425 - val_acc: 0.8783\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1979 - acc: 0.9397 - val_loss: 0.3420 - val_acc: 0.8836\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1977 - acc: 0.9374 - val_loss: 0.3437 - val_acc: 0.8783\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1975 - acc: 0.9374 - val_loss: 0.3408 - val_acc: 0.8783\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1975 - acc: 0.9374 - val_loss: 0.3443 - val_acc: 0.8783\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1973 - acc: 0.9379 - val_loss: 0.3438 - val_acc: 0.8783\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1979 - acc: 0.9374 - val_loss: 0.3429 - val_acc: 0.8783\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1976 - acc: 0.9379 - val_loss: 0.3438 - val_acc: 0.8783\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1967 - acc: 0.9379 - val_loss: 0.3514 - val_acc: 0.8836\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1970 - acc: 0.9368 - val_loss: 0.3444 - val_acc: 0.8836\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1973 - acc: 0.9391 - val_loss: 0.3439 - val_acc: 0.8783\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1974 - acc: 0.9374 - val_loss: 0.3459 - val_acc: 0.8783\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1969 - acc: 0.9368 - val_loss: 0.3474 - val_acc: 0.8783\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1972 - acc: 0.9356 - val_loss: 0.3459 - val_acc: 0.8783\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1973 - acc: 0.9374 - val_loss: 0.3429 - val_acc: 0.8783\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1971 - acc: 0.9379 - val_loss: 0.3453 - val_acc: 0.8783\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1975 - acc: 0.9368 - val_loss: 0.3444 - val_acc: 0.8783\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1967 - acc: 0.9356 - val_loss: 0.3437 - val_acc: 0.8783\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1970 - acc: 0.9350 - val_loss: 0.3462 - val_acc: 0.8783\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1967 - acc: 0.9374 - val_loss: 0.3470 - val_acc: 0.8783\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1969 - acc: 0.9374 - val_loss: 0.3486 - val_acc: 0.8783\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1965 - acc: 0.9362 - val_loss: 0.3495 - val_acc: 0.8730\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1968 - acc: 0.9379 - val_loss: 0.3439 - val_acc: 0.8783\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1970 - acc: 0.9368 - val_loss: 0.3461 - val_acc: 0.8783\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1967 - acc: 0.9379 - val_loss: 0.3447 - val_acc: 0.8783\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1968 - acc: 0.9368 - val_loss: 0.3464 - val_acc: 0.8836\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1963 - acc: 0.9362 - val_loss: 0.3453 - val_acc: 0.8836\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1963 - acc: 0.9362 - val_loss: 0.3480 - val_acc: 0.8783\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1968 - acc: 0.9391 - val_loss: 0.3441 - val_acc: 0.8783\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1965 - acc: 0.9374 - val_loss: 0.3474 - val_acc: 0.8783\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1961 - acc: 0.9397 - val_loss: 0.3436 - val_acc: 0.8783\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1964 - acc: 0.9368 - val_loss: 0.3494 - val_acc: 0.8783\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1963 - acc: 0.9374 - val_loss: 0.3482 - val_acc: 0.8783\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1958 - acc: 0.9362 - val_loss: 0.3432 - val_acc: 0.8783\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1962 - acc: 0.9374 - val_loss: 0.3464 - val_acc: 0.8783\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1966 - acc: 0.9350 - val_loss: 0.3448 - val_acc: 0.8783\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1958 - acc: 0.9356 - val_loss: 0.3437 - val_acc: 0.8836\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1956 - acc: 0.9379 - val_loss: 0.3463 - val_acc: 0.8730\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1958 - acc: 0.9356 - val_loss: 0.3429 - val_acc: 0.8783\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1959 - acc: 0.9368 - val_loss: 0.3442 - val_acc: 0.8783\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1960 - acc: 0.9362 - val_loss: 0.3469 - val_acc: 0.8783\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1956 - acc: 0.9368 - val_loss: 0.3471 - val_acc: 0.8783\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1959 - acc: 0.9374 - val_loss: 0.3456 - val_acc: 0.8783\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1956 - acc: 0.9385 - val_loss: 0.3447 - val_acc: 0.8783\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1952 - acc: 0.9374 - val_loss: 0.3476 - val_acc: 0.8730\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1958 - acc: 0.9374 - val_loss: 0.3447 - val_acc: 0.8836\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1957 - acc: 0.9374 - val_loss: 0.3441 - val_acc: 0.8783\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1955 - acc: 0.9368 - val_loss: 0.3445 - val_acc: 0.8783\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1952 - acc: 0.9391 - val_loss: 0.3445 - val_acc: 0.8783\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1948 - acc: 0.9362 - val_loss: 0.3405 - val_acc: 0.8836\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1953 - acc: 0.9379 - val_loss: 0.3391 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1953 - acc: 0.9385 - val_loss: 0.3410 - val_acc: 0.8836\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1950 - acc: 0.9391 - val_loss: 0.3394 - val_acc: 0.8836\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1949 - acc: 0.9374 - val_loss: 0.3411 - val_acc: 0.8836\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1949 - acc: 0.9374 - val_loss: 0.3410 - val_acc: 0.8836\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1949 - acc: 0.9391 - val_loss: 0.3409 - val_acc: 0.8836\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1941 - acc: 0.9385 - val_loss: 0.3362 - val_acc: 0.8836\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1946 - acc: 0.9385 - val_loss: 0.3399 - val_acc: 0.8889\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1943 - acc: 0.9379 - val_loss: 0.3388 - val_acc: 0.8836\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1939 - acc: 0.9409 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1941 - acc: 0.9397 - val_loss: 0.3392 - val_acc: 0.8836\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1941 - acc: 0.9385 - val_loss: 0.3354 - val_acc: 0.8836\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1938 - acc: 0.9385 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1936 - acc: 0.9391 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1933 - acc: 0.9409 - val_loss: 0.3363 - val_acc: 0.8836\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1936 - acc: 0.9374 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1932 - acc: 0.9374 - val_loss: 0.3374 - val_acc: 0.8836\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1937 - acc: 0.9379 - val_loss: 0.3374 - val_acc: 0.8836\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1932 - acc: 0.9397 - val_loss: 0.3419 - val_acc: 0.8783\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1931 - acc: 0.9379 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1925 - acc: 0.9415 - val_loss: 0.3444 - val_acc: 0.8836\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1934 - acc: 0.9385 - val_loss: 0.3436 - val_acc: 0.8783\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1935 - acc: 0.9379 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1928 - acc: 0.9397 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1929 - acc: 0.9368 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1926 - acc: 0.9391 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1935 - acc: 0.9397 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1922 - acc: 0.9374 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1928 - acc: 0.9379 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1922 - acc: 0.9379 - val_loss: 0.3385 - val_acc: 0.8836\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1923 - acc: 0.9374 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1927 - acc: 0.9403 - val_loss: 0.3344 - val_acc: 0.8836\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1927 - acc: 0.9385 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1921 - acc: 0.9391 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1923 - acc: 0.9379 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1919 - acc: 0.9403 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1925 - acc: 0.9385 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1918 - acc: 0.9397 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1922 - acc: 0.9391 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1919 - acc: 0.9397 - val_loss: 0.3366 - val_acc: 0.8836\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1922 - acc: 0.9379 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1922 - acc: 0.9391 - val_loss: 0.3359 - val_acc: 0.8836\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1924 - acc: 0.9391 - val_loss: 0.3354 - val_acc: 0.8836\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1924 - acc: 0.9374 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1917 - acc: 0.9391 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1918 - acc: 0.9385 - val_loss: 0.3341 - val_acc: 0.8836\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1916 - acc: 0.9403 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1917 - acc: 0.9379 - val_loss: 0.3323 - val_acc: 0.8836\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1918 - acc: 0.9403 - val_loss: 0.3387 - val_acc: 0.8836\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1922 - acc: 0.9379 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1913 - acc: 0.9385 - val_loss: 0.3382 - val_acc: 0.8836\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1916 - acc: 0.9379 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1918 - acc: 0.9397 - val_loss: 0.3375 - val_acc: 0.8836\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1915 - acc: 0.9391 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1916 - acc: 0.9391 - val_loss: 0.3404 - val_acc: 0.8836\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1915 - acc: 0.9409 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1917 - acc: 0.9385 - val_loss: 0.3391 - val_acc: 0.8836\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1915 - acc: 0.9397 - val_loss: 0.3405 - val_acc: 0.8836\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1912 - acc: 0.9397 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1912 - acc: 0.9385 - val_loss: 0.3409 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1911 - acc: 0.9385 - val_loss: 0.3406 - val_acc: 0.8836\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1914 - acc: 0.9391 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1912 - acc: 0.9385 - val_loss: 0.3414 - val_acc: 0.8836\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1913 - acc: 0.9409 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1913 - acc: 0.9421 - val_loss: 0.3357 - val_acc: 0.8836\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1907 - acc: 0.9415 - val_loss: 0.3347 - val_acc: 0.8836\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1909 - acc: 0.9385 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1910 - acc: 0.9415 - val_loss: 0.3359 - val_acc: 0.8836\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1908 - acc: 0.9397 - val_loss: 0.3366 - val_acc: 0.8836\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1904 - acc: 0.9415 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1911 - acc: 0.9397 - val_loss: 0.3387 - val_acc: 0.8836\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1906 - acc: 0.9409 - val_loss: 0.3357 - val_acc: 0.8836\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1913 - acc: 0.9391 - val_loss: 0.3378 - val_acc: 0.8836\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1912 - acc: 0.9374 - val_loss: 0.3352 - val_acc: 0.8836\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1899 - acc: 0.9403 - val_loss: 0.3412 - val_acc: 0.8836\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1913 - acc: 0.9374 - val_loss: 0.3373 - val_acc: 0.8836\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1909 - acc: 0.9397 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1909 - acc: 0.9403 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1903 - acc: 0.9379 - val_loss: 0.3363 - val_acc: 0.8836\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1901 - acc: 0.9409 - val_loss: 0.3433 - val_acc: 0.8836\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1909 - acc: 0.9421 - val_loss: 0.3379 - val_acc: 0.8836\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1905 - acc: 0.9379 - val_loss: 0.3367 - val_acc: 0.8836\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1904 - acc: 0.9409 - val_loss: 0.3393 - val_acc: 0.8836\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1903 - acc: 0.9397 - val_loss: 0.3370 - val_acc: 0.8836\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1905 - acc: 0.9415 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1904 - acc: 0.9403 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1904 - acc: 0.9433 - val_loss: 0.3370 - val_acc: 0.8836\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1905 - acc: 0.9385 - val_loss: 0.3383 - val_acc: 0.8836\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1903 - acc: 0.9403 - val_loss: 0.3396 - val_acc: 0.8836\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1904 - acc: 0.9409 - val_loss: 0.3379 - val_acc: 0.8836\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1906 - acc: 0.9409 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1901 - acc: 0.9397 - val_loss: 0.3351 - val_acc: 0.8836\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1903 - acc: 0.9385 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1897 - acc: 0.9415 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1899 - acc: 0.9409 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1902 - acc: 0.9397 - val_loss: 0.3370 - val_acc: 0.8836\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1898 - acc: 0.9403 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1896 - acc: 0.9415 - val_loss: 0.3372 - val_acc: 0.8836\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1892 - acc: 0.9403 - val_loss: 0.3390 - val_acc: 0.8836\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1898 - acc: 0.9403 - val_loss: 0.3390 - val_acc: 0.8836\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1897 - acc: 0.9409 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1897 - acc: 0.9409 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1896 - acc: 0.9415 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1897 - acc: 0.9421 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1899 - acc: 0.9403 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1892 - acc: 0.9409 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1895 - acc: 0.9409 - val_loss: 0.3373 - val_acc: 0.8836\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1896 - acc: 0.9415 - val_loss: 0.3394 - val_acc: 0.8836\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1889 - acc: 0.9421 - val_loss: 0.3370 - val_acc: 0.8836\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1893 - acc: 0.9403 - val_loss: 0.3345 - val_acc: 0.8836\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1893 - acc: 0.9409 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1887 - acc: 0.9421 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1890 - acc: 0.9379 - val_loss: 0.3366 - val_acc: 0.8836\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1893 - acc: 0.9409 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1890 - acc: 0.9427 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1891 - acc: 0.9421 - val_loss: 0.3363 - val_acc: 0.8836\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1889 - acc: 0.9427 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1891 - acc: 0.9415 - val_loss: 0.3388 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1889 - acc: 0.9427 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1888 - acc: 0.9415 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1891 - acc: 0.9433 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1890 - acc: 0.9415 - val_loss: 0.3360 - val_acc: 0.8836\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1888 - acc: 0.9415 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1889 - acc: 0.9409 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1887 - acc: 0.9403 - val_loss: 0.3357 - val_acc: 0.8836\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1886 - acc: 0.9415 - val_loss: 0.3351 - val_acc: 0.8836\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1885 - acc: 0.9415 - val_loss: 0.3385 - val_acc: 0.8836\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1885 - acc: 0.9415 - val_loss: 0.3420 - val_acc: 0.8836\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1887 - acc: 0.9391 - val_loss: 0.3409 - val_acc: 0.8836\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1886 - acc: 0.9391 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1889 - acc: 0.9415 - val_loss: 0.3362 - val_acc: 0.8836\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1888 - acc: 0.9409 - val_loss: 0.3366 - val_acc: 0.8836\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1881 - acc: 0.9397 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1887 - acc: 0.9415 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1886 - acc: 0.9415 - val_loss: 0.3409 - val_acc: 0.8836\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1887 - acc: 0.9397 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1880 - acc: 0.9409 - val_loss: 0.3411 - val_acc: 0.8836\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1882 - acc: 0.9421 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1884 - acc: 0.9415 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1882 - acc: 0.9397 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1880 - acc: 0.9409 - val_loss: 0.3386 - val_acc: 0.8836\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1885 - acc: 0.9385 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1877 - acc: 0.9421 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1881 - acc: 0.9427 - val_loss: 0.3409 - val_acc: 0.8836\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1881 - acc: 0.9415 - val_loss: 0.3386 - val_acc: 0.8836\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1872 - acc: 0.9433 - val_loss: 0.3410 - val_acc: 0.8836\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1883 - acc: 0.9403 - val_loss: 0.3403 - val_acc: 0.8836\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1877 - acc: 0.9415 - val_loss: 0.3363 - val_acc: 0.8836\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1876 - acc: 0.9427 - val_loss: 0.3347 - val_acc: 0.8836\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1880 - acc: 0.9415 - val_loss: 0.3371 - val_acc: 0.8836\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1884 - acc: 0.9391 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1872 - acc: 0.9409 - val_loss: 0.3420 - val_acc: 0.8836\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1884 - acc: 0.9403 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1878 - acc: 0.9391 - val_loss: 0.3379 - val_acc: 0.8836\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1869 - acc: 0.9397 - val_loss: 0.3427 - val_acc: 0.8836\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1876 - acc: 0.9433 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1875 - acc: 0.9415 - val_loss: 0.3367 - val_acc: 0.8836\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1874 - acc: 0.9379 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1874 - acc: 0.9409 - val_loss: 0.3392 - val_acc: 0.8836\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1875 - acc: 0.9415 - val_loss: 0.3407 - val_acc: 0.8836\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1874 - acc: 0.9421 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1869 - acc: 0.9421 - val_loss: 0.3396 - val_acc: 0.8836\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1874 - acc: 0.9385 - val_loss: 0.3390 - val_acc: 0.8836\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1873 - acc: 0.9403 - val_loss: 0.3385 - val_acc: 0.8836\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1871 - acc: 0.9409 - val_loss: 0.3376 - val_acc: 0.8836\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1868 - acc: 0.9415 - val_loss: 0.3422 - val_acc: 0.8836\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1874 - acc: 0.9391 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1869 - acc: 0.9403 - val_loss: 0.3385 - val_acc: 0.8836\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1866 - acc: 0.9421 - val_loss: 0.3402 - val_acc: 0.8836\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1869 - acc: 0.9403 - val_loss: 0.3363 - val_acc: 0.8836\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1867 - acc: 0.9385 - val_loss: 0.3372 - val_acc: 0.8836\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1860 - acc: 0.9415 - val_loss: 0.3436 - val_acc: 0.8836\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1872 - acc: 0.9403 - val_loss: 0.3400 - val_acc: 0.8836\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1868 - acc: 0.9403 - val_loss: 0.3417 - val_acc: 0.8836\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1863 - acc: 0.9397 - val_loss: 0.3429 - val_acc: 0.8836\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1866 - acc: 0.9409 - val_loss: 0.3411 - val_acc: 0.8836\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1862 - acc: 0.9409 - val_loss: 0.3404 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1866 - acc: 0.9397 - val_loss: 0.3376 - val_acc: 0.8836\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1864 - acc: 0.9397 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1857 - acc: 0.9391 - val_loss: 0.3393 - val_acc: 0.8836\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1863 - acc: 0.9409 - val_loss: 0.3394 - val_acc: 0.8836\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1860 - acc: 0.9385 - val_loss: 0.3414 - val_acc: 0.8836\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1862 - acc: 0.9409 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1858 - acc: 0.9415 - val_loss: 0.3374 - val_acc: 0.8836\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1856 - acc: 0.9427 - val_loss: 0.3411 - val_acc: 0.8836\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1863 - acc: 0.9427 - val_loss: 0.3404 - val_acc: 0.8836\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1858 - acc: 0.9415 - val_loss: 0.3388 - val_acc: 0.8836\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1860 - acc: 0.9427 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1863 - acc: 0.9397 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1854 - acc: 0.9421 - val_loss: 0.3396 - val_acc: 0.8836\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1858 - acc: 0.9403 - val_loss: 0.3386 - val_acc: 0.8836\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1856 - acc: 0.9421 - val_loss: 0.3405 - val_acc: 0.8836\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1859 - acc: 0.9433 - val_loss: 0.3406 - val_acc: 0.8836\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1853 - acc: 0.9403 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1856 - acc: 0.9421 - val_loss: 0.3399 - val_acc: 0.8836\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1858 - acc: 0.9403 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1853 - acc: 0.9427 - val_loss: 0.3425 - val_acc: 0.8836\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1854 - acc: 0.9415 - val_loss: 0.3391 - val_acc: 0.8836\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1855 - acc: 0.9421 - val_loss: 0.3423 - val_acc: 0.8783\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1854 - acc: 0.9391 - val_loss: 0.3383 - val_acc: 0.8836\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1861 - acc: 0.9397 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1852 - acc: 0.9427 - val_loss: 0.3394 - val_acc: 0.8836\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1851 - acc: 0.9397 - val_loss: 0.3413 - val_acc: 0.8836\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1852 - acc: 0.9379 - val_loss: 0.3431 - val_acc: 0.8836\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1854 - acc: 0.9409 - val_loss: 0.3370 - val_acc: 0.8836\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1854 - acc: 0.9427 - val_loss: 0.3393 - val_acc: 0.8836\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1853 - acc: 0.9403 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1851 - acc: 0.9403 - val_loss: 0.3375 - val_acc: 0.8836\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1846 - acc: 0.9421 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1850 - acc: 0.9409 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1848 - acc: 0.9427 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1854 - acc: 0.9397 - val_loss: 0.3374 - val_acc: 0.8836\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1849 - acc: 0.9397 - val_loss: 0.3408 - val_acc: 0.8836\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1851 - acc: 0.9427 - val_loss: 0.3403 - val_acc: 0.8836\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1850 - acc: 0.9415 - val_loss: 0.3377 - val_acc: 0.8836\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1841 - acc: 0.9427 - val_loss: 0.3407 - val_acc: 0.8836\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1849 - acc: 0.9391 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1848 - acc: 0.9421 - val_loss: 0.3371 - val_acc: 0.8836\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1844 - acc: 0.9403 - val_loss: 0.3388 - val_acc: 0.8836\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1848 - acc: 0.9433 - val_loss: 0.3381 - val_acc: 0.8836\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1849 - acc: 0.9415 - val_loss: 0.3383 - val_acc: 0.8836\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1848 - acc: 0.9391 - val_loss: 0.3410 - val_acc: 0.8836\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1839 - acc: 0.9439 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1843 - acc: 0.9415 - val_loss: 0.3398 - val_acc: 0.8836\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1847 - acc: 0.9421 - val_loss: 0.3399 - val_acc: 0.8836\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1842 - acc: 0.9415 - val_loss: 0.3417 - val_acc: 0.8836\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1848 - acc: 0.9415 - val_loss: 0.3376 - val_acc: 0.8836\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1844 - acc: 0.9403 - val_loss: 0.3375 - val_acc: 0.8836\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1846 - acc: 0.9409 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1839 - acc: 0.9427 - val_loss: 0.3433 - val_acc: 0.8889\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1848 - acc: 0.9379 - val_loss: 0.3408 - val_acc: 0.8836\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1846 - acc: 0.9403 - val_loss: 0.3385 - val_acc: 0.8836\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1846 - acc: 0.9421 - val_loss: 0.3378 - val_acc: 0.8836\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1846 - acc: 0.9427 - val_loss: 0.3378 - val_acc: 0.8889\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1843 - acc: 0.9403 - val_loss: 0.3379 - val_acc: 0.8836\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1836 - acc: 0.9403 - val_loss: 0.3402 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1840 - acc: 0.9444 - val_loss: 0.3378 - val_acc: 0.8836\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1841 - acc: 0.9409 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1842 - acc: 0.9415 - val_loss: 0.3394 - val_acc: 0.8836\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1838 - acc: 0.9427 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1845 - acc: 0.9415 - val_loss: 0.3374 - val_acc: 0.8836\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1833 - acc: 0.9462 - val_loss: 0.3380 - val_acc: 0.8836\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1842 - acc: 0.9427 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1836 - acc: 0.9439 - val_loss: 0.3383 - val_acc: 0.8836\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1838 - acc: 0.9409 - val_loss: 0.3378 - val_acc: 0.8836\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1837 - acc: 0.9421 - val_loss: 0.3369 - val_acc: 0.8836\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1839 - acc: 0.9439 - val_loss: 0.3345 - val_acc: 0.8836\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1839 - acc: 0.9444 - val_loss: 0.3391 - val_acc: 0.8836\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1843 - acc: 0.9421 - val_loss: 0.3354 - val_acc: 0.8836\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1830 - acc: 0.9450 - val_loss: 0.3344 - val_acc: 0.8836\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1825 - acc: 0.9444 - val_loss: 0.3410 - val_acc: 0.8889\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1840 - acc: 0.9403 - val_loss: 0.3402 - val_acc: 0.8836\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1834 - acc: 0.9439 - val_loss: 0.3370 - val_acc: 0.8889\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1836 - acc: 0.9409 - val_loss: 0.3368 - val_acc: 0.8889\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1830 - acc: 0.9433 - val_loss: 0.3302 - val_acc: 0.8836\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1836 - acc: 0.9439 - val_loss: 0.3337 - val_acc: 0.8836\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1823 - acc: 0.9433 - val_loss: 0.3376 - val_acc: 0.8889\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1828 - acc: 0.9415 - val_loss: 0.3324 - val_acc: 0.8836\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1830 - acc: 0.9403 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1834 - acc: 0.9409 - val_loss: 0.3353 - val_acc: 0.8836\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1835 - acc: 0.9415 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1832 - acc: 0.9433 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1830 - acc: 0.9421 - val_loss: 0.3353 - val_acc: 0.8836\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1828 - acc: 0.9439 - val_loss: 0.3340 - val_acc: 0.8836\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1834 - acc: 0.9421 - val_loss: 0.3342 - val_acc: 0.8836\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1825 - acc: 0.9427 - val_loss: 0.3323 - val_acc: 0.8889\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1827 - acc: 0.9385 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1827 - acc: 0.9427 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1827 - acc: 0.9397 - val_loss: 0.3317 - val_acc: 0.8889\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1831 - acc: 0.9439 - val_loss: 0.3341 - val_acc: 0.8836\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1828 - acc: 0.9427 - val_loss: 0.3340 - val_acc: 0.8889\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1824 - acc: 0.9439 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1824 - acc: 0.9415 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1825 - acc: 0.9427 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1824 - acc: 0.9415 - val_loss: 0.3323 - val_acc: 0.8836\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1826 - acc: 0.9409 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1820 - acc: 0.9433 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1822 - acc: 0.9427 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1822 - acc: 0.9421 - val_loss: 0.3346 - val_acc: 0.8836\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1826 - acc: 0.9409 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1822 - acc: 0.9433 - val_loss: 0.3334 - val_acc: 0.8836\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1823 - acc: 0.9444 - val_loss: 0.3359 - val_acc: 0.8836\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1825 - acc: 0.9415 - val_loss: 0.3344 - val_acc: 0.8836\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1822 - acc: 0.9433 - val_loss: 0.3303 - val_acc: 0.8889\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1820 - acc: 0.9391 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1824 - acc: 0.9409 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1819 - acc: 0.9439 - val_loss: 0.3313 - val_acc: 0.8942\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1822 - acc: 0.9433 - val_loss: 0.3314 - val_acc: 0.8836\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1811 - acc: 0.9444 - val_loss: 0.3302 - val_acc: 0.8836\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1813 - acc: 0.9439 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1815 - acc: 0.9433 - val_loss: 0.3350 - val_acc: 0.8889\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1822 - acc: 0.9409 - val_loss: 0.3302 - val_acc: 0.8836\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1814 - acc: 0.9403 - val_loss: 0.3336 - val_acc: 0.8836\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1818 - acc: 0.9415 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1814 - acc: 0.9403 - val_loss: 0.3322 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1812 - acc: 0.9421 - val_loss: 0.3323 - val_acc: 0.8836\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1816 - acc: 0.9415 - val_loss: 0.3288 - val_acc: 0.8836\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1811 - acc: 0.9439 - val_loss: 0.3347 - val_acc: 0.8836\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1815 - acc: 0.9409 - val_loss: 0.3305 - val_acc: 0.8889\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1813 - acc: 0.9421 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1814 - acc: 0.9427 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1810 - acc: 0.9421 - val_loss: 0.3307 - val_acc: 0.8836\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1810 - acc: 0.9409 - val_loss: 0.3303 - val_acc: 0.8836\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1819 - acc: 0.9403 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1807 - acc: 0.9421 - val_loss: 0.3381 - val_acc: 0.8783\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1813 - acc: 0.9427 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1805 - acc: 0.9456 - val_loss: 0.3264 - val_acc: 0.8889\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1816 - acc: 0.9409 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1807 - acc: 0.9433 - val_loss: 0.3301 - val_acc: 0.8942\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1806 - acc: 0.9409 - val_loss: 0.3327 - val_acc: 0.8836\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1813 - acc: 0.9409 - val_loss: 0.3300 - val_acc: 0.8836\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1808 - acc: 0.9433 - val_loss: 0.3318 - val_acc: 0.8836\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1802 - acc: 0.9439 - val_loss: 0.3289 - val_acc: 0.8836\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1812 - acc: 0.9427 - val_loss: 0.3330 - val_acc: 0.8836\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1807 - acc: 0.9427 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1805 - acc: 0.9427 - val_loss: 0.3320 - val_acc: 0.8889\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1808 - acc: 0.9421 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1808 - acc: 0.9433 - val_loss: 0.3290 - val_acc: 0.8889\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1801 - acc: 0.9439 - val_loss: 0.3298 - val_acc: 0.8836\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1808 - acc: 0.9456 - val_loss: 0.3326 - val_acc: 0.8836\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1810 - acc: 0.9421 - val_loss: 0.3301 - val_acc: 0.8942\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1803 - acc: 0.9427 - val_loss: 0.3324 - val_acc: 0.8889\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1806 - acc: 0.9439 - val_loss: 0.3296 - val_acc: 0.8836\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1804 - acc: 0.9433 - val_loss: 0.3322 - val_acc: 0.8889\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1809 - acc: 0.9439 - val_loss: 0.3299 - val_acc: 0.8889\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1803 - acc: 0.9433 - val_loss: 0.3309 - val_acc: 0.8836\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1803 - acc: 0.9444 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1807 - acc: 0.9439 - val_loss: 0.3311 - val_acc: 0.8889\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1802 - acc: 0.9415 - val_loss: 0.3325 - val_acc: 0.8889\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1802 - acc: 0.9439 - val_loss: 0.3312 - val_acc: 0.8836\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1804 - acc: 0.9433 - val_loss: 0.3291 - val_acc: 0.8889\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1800 - acc: 0.9415 - val_loss: 0.3305 - val_acc: 0.8783\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1806 - acc: 0.9421 - val_loss: 0.3301 - val_acc: 0.8836\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1795 - acc: 0.9444 - val_loss: 0.3256 - val_acc: 0.8836\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1800 - acc: 0.9444 - val_loss: 0.3303 - val_acc: 0.8942\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1805 - acc: 0.9421 - val_loss: 0.3310 - val_acc: 0.8889\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1800 - acc: 0.9450 - val_loss: 0.3313 - val_acc: 0.8942\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1796 - acc: 0.9450 - val_loss: 0.3358 - val_acc: 0.8889\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1801 - acc: 0.9427 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1805 - acc: 0.9444 - val_loss: 0.3295 - val_acc: 0.8889\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1800 - acc: 0.9433 - val_loss: 0.3272 - val_acc: 0.8836\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1794 - acc: 0.9427 - val_loss: 0.3317 - val_acc: 0.8942\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1795 - acc: 0.9415 - val_loss: 0.3280 - val_acc: 0.8889\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1794 - acc: 0.9450 - val_loss: 0.3277 - val_acc: 0.8889\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1796 - acc: 0.9439 - val_loss: 0.3301 - val_acc: 0.8889\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1792 - acc: 0.9450 - val_loss: 0.3259 - val_acc: 0.8889\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1802 - acc: 0.9427 - val_loss: 0.3269 - val_acc: 0.8836\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1795 - acc: 0.9427 - val_loss: 0.3292 - val_acc: 0.8889\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1794 - acc: 0.9444 - val_loss: 0.3314 - val_acc: 0.8942\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1794 - acc: 0.9439 - val_loss: 0.3301 - val_acc: 0.8942\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1801 - acc: 0.9439 - val_loss: 0.3282 - val_acc: 0.8836\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1795 - acc: 0.9450 - val_loss: 0.3260 - val_acc: 0.8836\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1791 - acc: 0.9444 - val_loss: 0.3321 - val_acc: 0.8889\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1792 - acc: 0.9450 - val_loss: 0.3304 - val_acc: 0.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1793 - acc: 0.9421 - val_loss: 0.3281 - val_acc: 0.8889\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1792 - acc: 0.9415 - val_loss: 0.3283 - val_acc: 0.8889\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1793 - acc: 0.9433 - val_loss: 0.3299 - val_acc: 0.8942\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1787 - acc: 0.9444 - val_loss: 0.3314 - val_acc: 0.8889\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1794 - acc: 0.9427 - val_loss: 0.3279 - val_acc: 0.8942\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1795 - acc: 0.9421 - val_loss: 0.3276 - val_acc: 0.8942\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1791 - acc: 0.9427 - val_loss: 0.3258 - val_acc: 0.8889\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1790 - acc: 0.9444 - val_loss: 0.3242 - val_acc: 0.8889\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1794 - acc: 0.9439 - val_loss: 0.3245 - val_acc: 0.8942\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 140us/step - loss: 0.1788 - acc: 0.9421 - val_loss: 0.3263 - val_acc: 0.8942\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1788 - acc: 0.9444 - val_loss: 0.3277 - val_acc: 0.8836\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1794 - acc: 0.9433 - val_loss: 0.3262 - val_acc: 0.8942\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1786 - acc: 0.9439 - val_loss: 0.3277 - val_acc: 0.8889\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1788 - acc: 0.9439 - val_loss: 0.3331 - val_acc: 0.8889\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1789 - acc: 0.9462 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1787 - acc: 0.9421 - val_loss: 0.3290 - val_acc: 0.8942\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1786 - acc: 0.9468 - val_loss: 0.3296 - val_acc: 0.8889\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1791 - acc: 0.9427 - val_loss: 0.3267 - val_acc: 0.8836\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1786 - acc: 0.9444 - val_loss: 0.3272 - val_acc: 0.8942\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1785 - acc: 0.9444 - val_loss: 0.3298 - val_acc: 0.8942\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1780 - acc: 0.9468 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1783 - acc: 0.9444 - val_loss: 0.3284 - val_acc: 0.8836\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1786 - acc: 0.9450 - val_loss: 0.3312 - val_acc: 0.8889\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1784 - acc: 0.9433 - val_loss: 0.3249 - val_acc: 0.8889\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1783 - acc: 0.9444 - val_loss: 0.3276 - val_acc: 0.8889\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1788 - acc: 0.9427 - val_loss: 0.3292 - val_acc: 0.8836\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1790 - acc: 0.9427 - val_loss: 0.3286 - val_acc: 0.8889\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1779 - acc: 0.9456 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1784 - acc: 0.9439 - val_loss: 0.3289 - val_acc: 0.8889\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1781 - acc: 0.9433 - val_loss: 0.3268 - val_acc: 0.8836\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1785 - acc: 0.9450 - val_loss: 0.3276 - val_acc: 0.8889\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1766 - acc: 0.9444 - val_loss: 0.3334 - val_acc: 0.8889\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1784 - acc: 0.9450 - val_loss: 0.3293 - val_acc: 0.8889\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1782 - acc: 0.9444 - val_loss: 0.3342 - val_acc: 0.8836\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1781 - acc: 0.9444 - val_loss: 0.3364 - val_acc: 0.8836\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1784 - acc: 0.9403 - val_loss: 0.3315 - val_acc: 0.8836\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1781 - acc: 0.9456 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1776 - acc: 0.9456 - val_loss: 0.3291 - val_acc: 0.8836\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1782 - acc: 0.9439 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1780 - acc: 0.9439 - val_loss: 0.3262 - val_acc: 0.8836\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1778 - acc: 0.9439 - val_loss: 0.3301 - val_acc: 0.8889\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1781 - acc: 0.9439 - val_loss: 0.3292 - val_acc: 0.8942\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1778 - acc: 0.9450 - val_loss: 0.3285 - val_acc: 0.8836\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1776 - acc: 0.9439 - val_loss: 0.3302 - val_acc: 0.8836\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1782 - acc: 0.9439 - val_loss: 0.3294 - val_acc: 0.8836\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1777 - acc: 0.9415 - val_loss: 0.3287 - val_acc: 0.8836\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1782 - acc: 0.9433 - val_loss: 0.3309 - val_acc: 0.8783\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1775 - acc: 0.9433 - val_loss: 0.3336 - val_acc: 0.8836\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1780 - acc: 0.9444 - val_loss: 0.3290 - val_acc: 0.8836\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1781 - acc: 0.9427 - val_loss: 0.3324 - val_acc: 0.8836\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1780 - acc: 0.9439 - val_loss: 0.3312 - val_acc: 0.8836\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1774 - acc: 0.9450 - val_loss: 0.3284 - val_acc: 0.8783\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1779 - acc: 0.9427 - val_loss: 0.3272 - val_acc: 0.8783\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1772 - acc: 0.9450 - val_loss: 0.3291 - val_acc: 0.8783\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1776 - acc: 0.9450 - val_loss: 0.3294 - val_acc: 0.8783\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1784 - acc: 0.9439 - val_loss: 0.3286 - val_acc: 0.8836\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 466us/step - loss: 0.6718 - acc: 0.6448 - val_loss: 0.6631 - val_acc: 0.6349\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.6455 - acc: 0.6667 - val_loss: 0.6493 - val_acc: 0.6720\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.6270 - acc: 0.6832 - val_loss: 0.6395 - val_acc: 0.6614\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.6125 - acc: 0.6939 - val_loss: 0.6317 - val_acc: 0.6667\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.5998 - acc: 0.7039 - val_loss: 0.6258 - val_acc: 0.6614\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.5880 - acc: 0.7145 - val_loss: 0.6196 - val_acc: 0.6720\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5773 - acc: 0.7234 - val_loss: 0.6127 - val_acc: 0.6720\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.5665 - acc: 0.7293 - val_loss: 0.6047 - val_acc: 0.6772\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5558 - acc: 0.7329 - val_loss: 0.5989 - val_acc: 0.6878\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.5447 - acc: 0.7459 - val_loss: 0.5922 - val_acc: 0.6878\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5335 - acc: 0.7571 - val_loss: 0.5828 - val_acc: 0.6878\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.5220 - acc: 0.7636 - val_loss: 0.5772 - val_acc: 0.6931\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5102 - acc: 0.7701 - val_loss: 0.5671 - val_acc: 0.7090\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4983 - acc: 0.7825 - val_loss: 0.5576 - val_acc: 0.7143\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4858 - acc: 0.7908 - val_loss: 0.5479 - val_acc: 0.7302\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4734 - acc: 0.7985 - val_loss: 0.5386 - val_acc: 0.7407\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4605 - acc: 0.8085 - val_loss: 0.5287 - val_acc: 0.7460\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4476 - acc: 0.8150 - val_loss: 0.5163 - val_acc: 0.7566\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4349 - acc: 0.8203 - val_loss: 0.5044 - val_acc: 0.7513\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4231 - acc: 0.8268 - val_loss: 0.4939 - val_acc: 0.7619\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4118 - acc: 0.8345 - val_loss: 0.4837 - val_acc: 0.7672\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4010 - acc: 0.8363 - val_loss: 0.4752 - val_acc: 0.7725\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3904 - acc: 0.8452 - val_loss: 0.4689 - val_acc: 0.7831\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3814 - acc: 0.8487 - val_loss: 0.4576 - val_acc: 0.7831\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3726 - acc: 0.8582 - val_loss: 0.4544 - val_acc: 0.7937\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3647 - acc: 0.8605 - val_loss: 0.4440 - val_acc: 0.7884\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3569 - acc: 0.8629 - val_loss: 0.4402 - val_acc: 0.8042\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3501 - acc: 0.8664 - val_loss: 0.4359 - val_acc: 0.8095\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3429 - acc: 0.8717 - val_loss: 0.4341 - val_acc: 0.8042\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3378 - acc: 0.8723 - val_loss: 0.4266 - val_acc: 0.8042\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3325 - acc: 0.8735 - val_loss: 0.4215 - val_acc: 0.8095\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3274 - acc: 0.8753 - val_loss: 0.4208 - val_acc: 0.8095\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3236 - acc: 0.8783 - val_loss: 0.4178 - val_acc: 0.8095\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3200 - acc: 0.8800 - val_loss: 0.4112 - val_acc: 0.8201\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3155 - acc: 0.8836 - val_loss: 0.4107 - val_acc: 0.8095\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3125 - acc: 0.8848 - val_loss: 0.4158 - val_acc: 0.8095\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3097 - acc: 0.8818 - val_loss: 0.4045 - val_acc: 0.8254\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3069 - acc: 0.8871 - val_loss: 0.4029 - val_acc: 0.8307\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3041 - acc: 0.8895 - val_loss: 0.3978 - val_acc: 0.8360\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3014 - acc: 0.8871 - val_loss: 0.4032 - val_acc: 0.8254\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2995 - acc: 0.8889 - val_loss: 0.3951 - val_acc: 0.8466\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2972 - acc: 0.8883 - val_loss: 0.3951 - val_acc: 0.8413\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2952 - acc: 0.8913 - val_loss: 0.3937 - val_acc: 0.8466\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2929 - acc: 0.8966 - val_loss: 0.3934 - val_acc: 0.8413\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2906 - acc: 0.8960 - val_loss: 0.3937 - val_acc: 0.8413\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2889 - acc: 0.8983 - val_loss: 0.3912 - val_acc: 0.8466\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2870 - acc: 0.8989 - val_loss: 0.3902 - val_acc: 0.8466\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2844 - acc: 0.8954 - val_loss: 0.3813 - val_acc: 0.8519\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2834 - acc: 0.8989 - val_loss: 0.3826 - val_acc: 0.8413\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2816 - acc: 0.8989 - val_loss: 0.3836 - val_acc: 0.8466\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2795 - acc: 0.9019 - val_loss: 0.3800 - val_acc: 0.8519\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2786 - acc: 0.9054 - val_loss: 0.3815 - val_acc: 0.8519\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2770 - acc: 0.9037 - val_loss: 0.3820 - val_acc: 0.8466\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2753 - acc: 0.9037 - val_loss: 0.3748 - val_acc: 0.8466\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2733 - acc: 0.9037 - val_loss: 0.3692 - val_acc: 0.8624\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2732 - acc: 0.9054 - val_loss: 0.3745 - val_acc: 0.8571\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2706 - acc: 0.9054 - val_loss: 0.3712 - val_acc: 0.8571\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2704 - acc: 0.9054 - val_loss: 0.3675 - val_acc: 0.8624\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2685 - acc: 0.9054 - val_loss: 0.3791 - val_acc: 0.8571\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2684 - acc: 0.9072 - val_loss: 0.3689 - val_acc: 0.8571\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2672 - acc: 0.9043 - val_loss: 0.3670 - val_acc: 0.8624\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2665 - acc: 0.9066 - val_loss: 0.3643 - val_acc: 0.8624\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2650 - acc: 0.9054 - val_loss: 0.3621 - val_acc: 0.8624\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2648 - acc: 0.9060 - val_loss: 0.3627 - val_acc: 0.8624\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2636 - acc: 0.9060 - val_loss: 0.3600 - val_acc: 0.8571\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2630 - acc: 0.9078 - val_loss: 0.3609 - val_acc: 0.8624\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2620 - acc: 0.9072 - val_loss: 0.3634 - val_acc: 0.8624\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2605 - acc: 0.9072 - val_loss: 0.3610 - val_acc: 0.8624\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2604 - acc: 0.9084 - val_loss: 0.3585 - val_acc: 0.8677\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2599 - acc: 0.9113 - val_loss: 0.3593 - val_acc: 0.8624\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2580 - acc: 0.9119 - val_loss: 0.3671 - val_acc: 0.8519\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2585 - acc: 0.9119 - val_loss: 0.3581 - val_acc: 0.8677\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2564 - acc: 0.9125 - val_loss: 0.3625 - val_acc: 0.8624\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2567 - acc: 0.9119 - val_loss: 0.3631 - val_acc: 0.8624\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2555 - acc: 0.9137 - val_loss: 0.3635 - val_acc: 0.8571\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2551 - acc: 0.9113 - val_loss: 0.3579 - val_acc: 0.8730\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2538 - acc: 0.9143 - val_loss: 0.3548 - val_acc: 0.8783\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2528 - acc: 0.9155 - val_loss: 0.3594 - val_acc: 0.8624\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2526 - acc: 0.9149 - val_loss: 0.3610 - val_acc: 0.8571\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2523 - acc: 0.9155 - val_loss: 0.3591 - val_acc: 0.8677\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2516 - acc: 0.9167 - val_loss: 0.3593 - val_acc: 0.8624\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2507 - acc: 0.9155 - val_loss: 0.3554 - val_acc: 0.8783\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2505 - acc: 0.9161 - val_loss: 0.3548 - val_acc: 0.8730\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2496 - acc: 0.9173 - val_loss: 0.3607 - val_acc: 0.8677\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2491 - acc: 0.9173 - val_loss: 0.3553 - val_acc: 0.8730\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2487 - acc: 0.9178 - val_loss: 0.3550 - val_acc: 0.8730\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2475 - acc: 0.9190 - val_loss: 0.3524 - val_acc: 0.8836\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2473 - acc: 0.9202 - val_loss: 0.3569 - val_acc: 0.8783\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2467 - acc: 0.9208 - val_loss: 0.3552 - val_acc: 0.8730\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2459 - acc: 0.9202 - val_loss: 0.3523 - val_acc: 0.8783\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2455 - acc: 0.9202 - val_loss: 0.3553 - val_acc: 0.8836\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2457 - acc: 0.9202 - val_loss: 0.3559 - val_acc: 0.8730\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2447 - acc: 0.9202 - val_loss: 0.3543 - val_acc: 0.8783\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2442 - acc: 0.9196 - val_loss: 0.3513 - val_acc: 0.8677\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2434 - acc: 0.9214 - val_loss: 0.3503 - val_acc: 0.8730\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2439 - acc: 0.9196 - val_loss: 0.3513 - val_acc: 0.8730\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2422 - acc: 0.9196 - val_loss: 0.3553 - val_acc: 0.8730\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2419 - acc: 0.9208 - val_loss: 0.3560 - val_acc: 0.8730\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2413 - acc: 0.9214 - val_loss: 0.3555 - val_acc: 0.8677\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2410 - acc: 0.9232 - val_loss: 0.3563 - val_acc: 0.8677\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2408 - acc: 0.9214 - val_loss: 0.3533 - val_acc: 0.8730\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2403 - acc: 0.9261 - val_loss: 0.3497 - val_acc: 0.8677\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2391 - acc: 0.9214 - val_loss: 0.3507 - val_acc: 0.8624\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2393 - acc: 0.9202 - val_loss: 0.3516 - val_acc: 0.8730\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2384 - acc: 0.9273 - val_loss: 0.3501 - val_acc: 0.8677\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2371 - acc: 0.9249 - val_loss: 0.3465 - val_acc: 0.8677\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2371 - acc: 0.9226 - val_loss: 0.3542 - val_acc: 0.8730\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2372 - acc: 0.9261 - val_loss: 0.3468 - val_acc: 0.8677\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2360 - acc: 0.9249 - val_loss: 0.3524 - val_acc: 0.8783\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2363 - acc: 0.9249 - val_loss: 0.3439 - val_acc: 0.8677\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2357 - acc: 0.9238 - val_loss: 0.3413 - val_acc: 0.8783\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2350 - acc: 0.9261 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2346 - acc: 0.9249 - val_loss: 0.3474 - val_acc: 0.8836\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2330 - acc: 0.9267 - val_loss: 0.3407 - val_acc: 0.8677\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2317 - acc: 0.9273 - val_loss: 0.3446 - val_acc: 0.8783\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2326 - acc: 0.9249 - val_loss: 0.3431 - val_acc: 0.8730\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2318 - acc: 0.9297 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2320 - acc: 0.9261 - val_loss: 0.3409 - val_acc: 0.8624\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2302 - acc: 0.9291 - val_loss: 0.3407 - val_acc: 0.8730\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2297 - acc: 0.9297 - val_loss: 0.3409 - val_acc: 0.8730\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2292 - acc: 0.9285 - val_loss: 0.3393 - val_acc: 0.8624\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2286 - acc: 0.9255 - val_loss: 0.3403 - val_acc: 0.8677\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2276 - acc: 0.9291 - val_loss: 0.3371 - val_acc: 0.8677\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2276 - acc: 0.9285 - val_loss: 0.3389 - val_acc: 0.8571\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2263 - acc: 0.9291 - val_loss: 0.3408 - val_acc: 0.8783\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2255 - acc: 0.9309 - val_loss: 0.3373 - val_acc: 0.8783\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2258 - acc: 0.9320 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2245 - acc: 0.9273 - val_loss: 0.3433 - val_acc: 0.8730\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2243 - acc: 0.9309 - val_loss: 0.3423 - val_acc: 0.8677\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2232 - acc: 0.9297 - val_loss: 0.3361 - val_acc: 0.8677\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2234 - acc: 0.9291 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2222 - acc: 0.9314 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2221 - acc: 0.9303 - val_loss: 0.3348 - val_acc: 0.8730\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2217 - acc: 0.9309 - val_loss: 0.3395 - val_acc: 0.8677\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2208 - acc: 0.9303 - val_loss: 0.3397 - val_acc: 0.8677\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2199 - acc: 0.9326 - val_loss: 0.3360 - val_acc: 0.8889\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2206 - acc: 0.9314 - val_loss: 0.3386 - val_acc: 0.8783\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2197 - acc: 0.9297 - val_loss: 0.3356 - val_acc: 0.8730\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2194 - acc: 0.9309 - val_loss: 0.3344 - val_acc: 0.8677\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2184 - acc: 0.9314 - val_loss: 0.3382 - val_acc: 0.8730\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.2179 - acc: 0.9332 - val_loss: 0.3399 - val_acc: 0.8783\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2173 - acc: 0.9344 - val_loss: 0.3387 - val_acc: 0.8783\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2174 - acc: 0.9320 - val_loss: 0.3404 - val_acc: 0.8730\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2163 - acc: 0.9320 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2147 - acc: 0.9297 - val_loss: 0.3348 - val_acc: 0.8677\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2153 - acc: 0.9368 - val_loss: 0.3412 - val_acc: 0.8783\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2148 - acc: 0.9314 - val_loss: 0.3420 - val_acc: 0.8783\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2148 - acc: 0.9338 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2142 - acc: 0.9338 - val_loss: 0.3368 - val_acc: 0.8889\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2139 - acc: 0.9362 - val_loss: 0.3333 - val_acc: 0.8836\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2128 - acc: 0.9344 - val_loss: 0.3351 - val_acc: 0.8730\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2130 - acc: 0.9332 - val_loss: 0.3364 - val_acc: 0.8836\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2124 - acc: 0.9326 - val_loss: 0.3308 - val_acc: 0.8783\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2120 - acc: 0.9326 - val_loss: 0.3346 - val_acc: 0.8836\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2113 - acc: 0.9314 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2111 - acc: 0.9356 - val_loss: 0.3387 - val_acc: 0.8836\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2112 - acc: 0.9356 - val_loss: 0.3312 - val_acc: 0.8730\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2103 - acc: 0.9344 - val_loss: 0.3325 - val_acc: 0.8783\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2098 - acc: 0.9332 - val_loss: 0.3297 - val_acc: 0.8783\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2089 - acc: 0.9344 - val_loss: 0.3405 - val_acc: 0.8836\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2090 - acc: 0.9338 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2091 - acc: 0.9350 - val_loss: 0.3389 - val_acc: 0.8836\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2087 - acc: 0.9332 - val_loss: 0.3384 - val_acc: 0.8836\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2087 - acc: 0.9362 - val_loss: 0.3347 - val_acc: 0.8783\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2084 - acc: 0.9350 - val_loss: 0.3316 - val_acc: 0.8889\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2082 - acc: 0.9356 - val_loss: 0.3288 - val_acc: 0.8783\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2068 - acc: 0.9356 - val_loss: 0.3312 - val_acc: 0.8783\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2073 - acc: 0.9362 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2067 - acc: 0.9362 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2063 - acc: 0.9374 - val_loss: 0.3357 - val_acc: 0.8783\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2068 - acc: 0.9350 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2066 - acc: 0.9350 - val_loss: 0.3338 - val_acc: 0.8783\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2062 - acc: 0.9356 - val_loss: 0.3280 - val_acc: 0.8783\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2057 - acc: 0.9356 - val_loss: 0.3378 - val_acc: 0.8836\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2055 - acc: 0.9332 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2052 - acc: 0.9338 - val_loss: 0.3363 - val_acc: 0.8783\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.2053 - acc: 0.9338 - val_loss: 0.3276 - val_acc: 0.8783\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2040 - acc: 0.9344 - val_loss: 0.3276 - val_acc: 0.8783\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2048 - acc: 0.9344 - val_loss: 0.3317 - val_acc: 0.8889\n",
      "Epoch 180/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2044 - acc: 0.9362 - val_loss: 0.3362 - val_acc: 0.8836\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2043 - acc: 0.9368 - val_loss: 0.3392 - val_acc: 0.8836\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2043 - acc: 0.9356 - val_loss: 0.3333 - val_acc: 0.8783\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2035 - acc: 0.9338 - val_loss: 0.3351 - val_acc: 0.8836\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2037 - acc: 0.9368 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2033 - acc: 0.9362 - val_loss: 0.3331 - val_acc: 0.8836\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2029 - acc: 0.9356 - val_loss: 0.3303 - val_acc: 0.8730\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2021 - acc: 0.9362 - val_loss: 0.3391 - val_acc: 0.8836\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2031 - acc: 0.9344 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2024 - acc: 0.9344 - val_loss: 0.3337 - val_acc: 0.8836\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2024 - acc: 0.9368 - val_loss: 0.3322 - val_acc: 0.8783\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2020 - acc: 0.9344 - val_loss: 0.3346 - val_acc: 0.8836\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2016 - acc: 0.9344 - val_loss: 0.3301 - val_acc: 0.8783\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2021 - acc: 0.9344 - val_loss: 0.3330 - val_acc: 0.8836\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2012 - acc: 0.9326 - val_loss: 0.3361 - val_acc: 0.8783\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2013 - acc: 0.9368 - val_loss: 0.3288 - val_acc: 0.8836\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2007 - acc: 0.9356 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2006 - acc: 0.9374 - val_loss: 0.3348 - val_acc: 0.8836\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2014 - acc: 0.9338 - val_loss: 0.3311 - val_acc: 0.8783\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1995 - acc: 0.9344 - val_loss: 0.3348 - val_acc: 0.8783\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2001 - acc: 0.9362 - val_loss: 0.3282 - val_acc: 0.8836\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1996 - acc: 0.9338 - val_loss: 0.3261 - val_acc: 0.8836\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1995 - acc: 0.9338 - val_loss: 0.3252 - val_acc: 0.8836\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1996 - acc: 0.9374 - val_loss: 0.3288 - val_acc: 0.8889\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1993 - acc: 0.9332 - val_loss: 0.3250 - val_acc: 0.8836\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1987 - acc: 0.9350 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1993 - acc: 0.9362 - val_loss: 0.3294 - val_acc: 0.8836\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1983 - acc: 0.9356 - val_loss: 0.3312 - val_acc: 0.8889\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1989 - acc: 0.9362 - val_loss: 0.3321 - val_acc: 0.8889\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1987 - acc: 0.9356 - val_loss: 0.3287 - val_acc: 0.8942\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1983 - acc: 0.9350 - val_loss: 0.3291 - val_acc: 0.8889\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1980 - acc: 0.9362 - val_loss: 0.3301 - val_acc: 0.8889\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1978 - acc: 0.9368 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1968 - acc: 0.9344 - val_loss: 0.3405 - val_acc: 0.8836\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1973 - acc: 0.9379 - val_loss: 0.3314 - val_acc: 0.8836\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1973 - acc: 0.9350 - val_loss: 0.3355 - val_acc: 0.8836\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1965 - acc: 0.9350 - val_loss: 0.3311 - val_acc: 0.8889\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1967 - acc: 0.9379 - val_loss: 0.3325 - val_acc: 0.8889\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1965 - acc: 0.9338 - val_loss: 0.3236 - val_acc: 0.8889\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1959 - acc: 0.9368 - val_loss: 0.3244 - val_acc: 0.8889\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1955 - acc: 0.9356 - val_loss: 0.3236 - val_acc: 0.8942\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1956 - acc: 0.9379 - val_loss: 0.3266 - val_acc: 0.8889\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1951 - acc: 0.9368 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1951 - acc: 0.9356 - val_loss: 0.3301 - val_acc: 0.8836\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1947 - acc: 0.9362 - val_loss: 0.3286 - val_acc: 0.8889\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1944 - acc: 0.9362 - val_loss: 0.3225 - val_acc: 0.8889\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1942 - acc: 0.9362 - val_loss: 0.3291 - val_acc: 0.8889\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1939 - acc: 0.9374 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1940 - acc: 0.9362 - val_loss: 0.3262 - val_acc: 0.8836\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1941 - acc: 0.9368 - val_loss: 0.3290 - val_acc: 0.8836\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1938 - acc: 0.9338 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1936 - acc: 0.9385 - val_loss: 0.3260 - val_acc: 0.8889\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1931 - acc: 0.9379 - val_loss: 0.3276 - val_acc: 0.8836\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1933 - acc: 0.9368 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1933 - acc: 0.9385 - val_loss: 0.3272 - val_acc: 0.8889\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1923 - acc: 0.9385 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1926 - acc: 0.9362 - val_loss: 0.3251 - val_acc: 0.8889\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1925 - acc: 0.9356 - val_loss: 0.3277 - val_acc: 0.8836\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1931 - acc: 0.9379 - val_loss: 0.3265 - val_acc: 0.8889\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1920 - acc: 0.9374 - val_loss: 0.3227 - val_acc: 0.8889\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1920 - acc: 0.9368 - val_loss: 0.3251 - val_acc: 0.8889\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1918 - acc: 0.9344 - val_loss: 0.3241 - val_acc: 0.8836\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1916 - acc: 0.9385 - val_loss: 0.3218 - val_acc: 0.8889\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1910 - acc: 0.9368 - val_loss: 0.3305 - val_acc: 0.8889\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1915 - acc: 0.9379 - val_loss: 0.3276 - val_acc: 0.8889\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1907 - acc: 0.9374 - val_loss: 0.3268 - val_acc: 0.8836\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1910 - acc: 0.9374 - val_loss: 0.3280 - val_acc: 0.8889\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1903 - acc: 0.9403 - val_loss: 0.3205 - val_acc: 0.8889\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1904 - acc: 0.9385 - val_loss: 0.3206 - val_acc: 0.8889\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.3223 - val_acc: 0.8836\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1902 - acc: 0.9397 - val_loss: 0.3219 - val_acc: 0.8942\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1900 - acc: 0.9356 - val_loss: 0.3292 - val_acc: 0.8836\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1902 - acc: 0.9397 - val_loss: 0.3220 - val_acc: 0.8889\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.3265 - val_acc: 0.8836\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1888 - acc: 0.9374 - val_loss: 0.3193 - val_acc: 0.8942\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1893 - acc: 0.9385 - val_loss: 0.3311 - val_acc: 0.8836\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1890 - acc: 0.9403 - val_loss: 0.3282 - val_acc: 0.8836\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1886 - acc: 0.9385 - val_loss: 0.3328 - val_acc: 0.8889\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1895 - acc: 0.9385 - val_loss: 0.3243 - val_acc: 0.8942\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1886 - acc: 0.9368 - val_loss: 0.3298 - val_acc: 0.8889\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1880 - acc: 0.9391 - val_loss: 0.3211 - val_acc: 0.8942\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1888 - acc: 0.9391 - val_loss: 0.3280 - val_acc: 0.8836\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1873 - acc: 0.9409 - val_loss: 0.3292 - val_acc: 0.8942\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1886 - acc: 0.9379 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1879 - acc: 0.9391 - val_loss: 0.3218 - val_acc: 0.8942\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1879 - acc: 0.9379 - val_loss: 0.3214 - val_acc: 0.8942\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1880 - acc: 0.9397 - val_loss: 0.3243 - val_acc: 0.8942\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.1877 - acc: 0.9391 - val_loss: 0.3261 - val_acc: 0.8889\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1876 - acc: 0.9350 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1872 - acc: 0.9397 - val_loss: 0.3246 - val_acc: 0.8942\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1869 - acc: 0.9391 - val_loss: 0.3279 - val_acc: 0.8889\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1871 - acc: 0.9391 - val_loss: 0.3317 - val_acc: 0.8942\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1868 - acc: 0.9427 - val_loss: 0.3318 - val_acc: 0.8942\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1870 - acc: 0.9391 - val_loss: 0.3299 - val_acc: 0.8889\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1866 - acc: 0.9397 - val_loss: 0.3230 - val_acc: 0.8889\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1860 - acc: 0.9362 - val_loss: 0.3348 - val_acc: 0.8836\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1863 - acc: 0.9433 - val_loss: 0.3251 - val_acc: 0.8942\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1863 - acc: 0.9374 - val_loss: 0.3252 - val_acc: 0.8889\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1855 - acc: 0.9397 - val_loss: 0.3231 - val_acc: 0.8942\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1847 - acc: 0.9391 - val_loss: 0.3300 - val_acc: 0.8836\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1857 - acc: 0.9362 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1852 - acc: 0.9403 - val_loss: 0.3277 - val_acc: 0.8836\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1855 - acc: 0.9427 - val_loss: 0.3288 - val_acc: 0.8942\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1850 - acc: 0.9403 - val_loss: 0.3283 - val_acc: 0.8942\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1850 - acc: 0.9397 - val_loss: 0.3333 - val_acc: 0.8942\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1848 - acc: 0.9403 - val_loss: 0.3271 - val_acc: 0.8942\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1843 - acc: 0.9415 - val_loss: 0.3237 - val_acc: 0.8942\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1847 - acc: 0.9397 - val_loss: 0.3311 - val_acc: 0.8836\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1841 - acc: 0.9409 - val_loss: 0.3305 - val_acc: 0.8889\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1838 - acc: 0.9403 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1847 - acc: 0.9415 - val_loss: 0.3309 - val_acc: 0.8836\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1842 - acc: 0.9403 - val_loss: 0.3314 - val_acc: 0.8889\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1843 - acc: 0.9444 - val_loss: 0.3313 - val_acc: 0.8889\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1845 - acc: 0.9421 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1843 - acc: 0.9421 - val_loss: 0.3234 - val_acc: 0.8942\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1839 - acc: 0.9415 - val_loss: 0.3298 - val_acc: 0.8942\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1838 - acc: 0.9391 - val_loss: 0.3295 - val_acc: 0.8942\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1835 - acc: 0.9450 - val_loss: 0.3197 - val_acc: 0.8889\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1827 - acc: 0.9415 - val_loss: 0.3357 - val_acc: 0.8836\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1833 - acc: 0.9397 - val_loss: 0.3211 - val_acc: 0.8889\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1828 - acc: 0.9427 - val_loss: 0.3295 - val_acc: 0.8889\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1823 - acc: 0.9421 - val_loss: 0.3227 - val_acc: 0.8889\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1834 - acc: 0.9374 - val_loss: 0.3300 - val_acc: 0.8942\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1829 - acc: 0.9415 - val_loss: 0.3277 - val_acc: 0.8942\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1825 - acc: 0.9391 - val_loss: 0.3330 - val_acc: 0.8942\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1829 - acc: 0.9415 - val_loss: 0.3278 - val_acc: 0.8942\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1828 - acc: 0.9415 - val_loss: 0.3268 - val_acc: 0.8942\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1819 - acc: 0.9379 - val_loss: 0.3321 - val_acc: 0.8836\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1823 - acc: 0.9415 - val_loss: 0.3306 - val_acc: 0.8942\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1828 - acc: 0.9403 - val_loss: 0.3251 - val_acc: 0.8942\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1823 - acc: 0.9421 - val_loss: 0.3236 - val_acc: 0.8942\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1818 - acc: 0.9415 - val_loss: 0.3310 - val_acc: 0.8942\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1820 - acc: 0.9403 - val_loss: 0.3335 - val_acc: 0.8942\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1821 - acc: 0.9397 - val_loss: 0.3345 - val_acc: 0.8942\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1819 - acc: 0.9409 - val_loss: 0.3310 - val_acc: 0.8942\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1811 - acc: 0.9415 - val_loss: 0.3341 - val_acc: 0.8942\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1816 - acc: 0.9409 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1818 - acc: 0.9397 - val_loss: 0.3251 - val_acc: 0.8942\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1816 - acc: 0.9397 - val_loss: 0.3298 - val_acc: 0.8942\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1806 - acc: 0.9415 - val_loss: 0.3315 - val_acc: 0.8889\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1812 - acc: 0.9421 - val_loss: 0.3211 - val_acc: 0.8942\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1810 - acc: 0.9415 - val_loss: 0.3284 - val_acc: 0.8942\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1813 - acc: 0.9415 - val_loss: 0.3262 - val_acc: 0.8942\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1800 - acc: 0.9433 - val_loss: 0.3419 - val_acc: 0.8942\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1817 - acc: 0.9397 - val_loss: 0.3291 - val_acc: 0.8942\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1808 - acc: 0.9421 - val_loss: 0.3231 - val_acc: 0.8942\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1803 - acc: 0.9433 - val_loss: 0.3285 - val_acc: 0.8942\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1802 - acc: 0.9427 - val_loss: 0.3209 - val_acc: 0.8942\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1805 - acc: 0.9427 - val_loss: 0.3255 - val_acc: 0.8942\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1797 - acc: 0.9427 - val_loss: 0.3287 - val_acc: 0.8942\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1799 - acc: 0.9415 - val_loss: 0.3333 - val_acc: 0.8942\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1803 - acc: 0.9427 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1795 - acc: 0.9415 - val_loss: 0.3310 - val_acc: 0.8942\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1804 - acc: 0.9421 - val_loss: 0.3262 - val_acc: 0.8942\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1796 - acc: 0.9444 - val_loss: 0.3243 - val_acc: 0.8942\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1797 - acc: 0.9421 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1798 - acc: 0.9421 - val_loss: 0.3247 - val_acc: 0.8942\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1794 - acc: 0.9433 - val_loss: 0.3270 - val_acc: 0.8942\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1794 - acc: 0.9415 - val_loss: 0.3277 - val_acc: 0.8942\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1784 - acc: 0.9433 - val_loss: 0.3391 - val_acc: 0.8836\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1789 - acc: 0.9409 - val_loss: 0.3327 - val_acc: 0.8942\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1800 - acc: 0.9397 - val_loss: 0.3256 - val_acc: 0.8942\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1788 - acc: 0.9421 - val_loss: 0.3272 - val_acc: 0.8942\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1790 - acc: 0.9415 - val_loss: 0.3278 - val_acc: 0.8942\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1788 - acc: 0.9397 - val_loss: 0.3303 - val_acc: 0.8942\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1771 - acc: 0.9391 - val_loss: 0.3398 - val_acc: 0.8942\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1788 - acc: 0.9374 - val_loss: 0.3279 - val_acc: 0.8942\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1777 - acc: 0.9450 - val_loss: 0.3228 - val_acc: 0.8889\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1781 - acc: 0.9427 - val_loss: 0.3226 - val_acc: 0.8889\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1780 - acc: 0.9450 - val_loss: 0.3280 - val_acc: 0.8942\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1775 - acc: 0.9409 - val_loss: 0.3292 - val_acc: 0.8889\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1773 - acc: 0.9409 - val_loss: 0.3206 - val_acc: 0.8942\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1770 - acc: 0.9403 - val_loss: 0.3186 - val_acc: 0.8889\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1776 - acc: 0.9421 - val_loss: 0.3198 - val_acc: 0.8942\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1766 - acc: 0.9427 - val_loss: 0.3259 - val_acc: 0.8942\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1762 - acc: 0.9421 - val_loss: 0.3194 - val_acc: 0.8889\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1761 - acc: 0.9421 - val_loss: 0.3311 - val_acc: 0.8889\n",
      "Epoch 357/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1760 - acc: 0.9391 - val_loss: 0.3277 - val_acc: 0.8942\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1767 - acc: 0.9427 - val_loss: 0.3313 - val_acc: 0.8942\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1767 - acc: 0.9409 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1765 - acc: 0.9439 - val_loss: 0.3372 - val_acc: 0.8942\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1759 - acc: 0.9409 - val_loss: 0.3215 - val_acc: 0.8942\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1765 - acc: 0.9444 - val_loss: 0.3308 - val_acc: 0.8942\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1762 - acc: 0.9427 - val_loss: 0.3266 - val_acc: 0.8942\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1759 - acc: 0.9433 - val_loss: 0.3311 - val_acc: 0.8942\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1753 - acc: 0.9421 - val_loss: 0.3345 - val_acc: 0.8942\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1767 - acc: 0.9409 - val_loss: 0.3232 - val_acc: 0.8942\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1757 - acc: 0.9450 - val_loss: 0.3185 - val_acc: 0.8942\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1754 - acc: 0.9421 - val_loss: 0.3329 - val_acc: 0.8942\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1756 - acc: 0.9439 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1758 - acc: 0.9409 - val_loss: 0.3283 - val_acc: 0.8942\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1753 - acc: 0.9433 - val_loss: 0.3180 - val_acc: 0.8942\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1751 - acc: 0.9427 - val_loss: 0.3292 - val_acc: 0.8942\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1749 - acc: 0.9450 - val_loss: 0.3282 - val_acc: 0.8942\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1750 - acc: 0.9415 - val_loss: 0.3232 - val_acc: 0.8942\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1749 - acc: 0.9439 - val_loss: 0.3202 - val_acc: 0.8942\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1752 - acc: 0.9427 - val_loss: 0.3228 - val_acc: 0.8942\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1749 - acc: 0.9450 - val_loss: 0.3253 - val_acc: 0.8942\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1752 - acc: 0.9433 - val_loss: 0.3233 - val_acc: 0.8942\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1747 - acc: 0.9450 - val_loss: 0.3292 - val_acc: 0.8942\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1745 - acc: 0.9433 - val_loss: 0.3246 - val_acc: 0.8942\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1738 - acc: 0.9439 - val_loss: 0.3359 - val_acc: 0.8942\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1733 - acc: 0.9439 - val_loss: 0.3384 - val_acc: 0.8942\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1745 - acc: 0.9427 - val_loss: 0.3315 - val_acc: 0.8942\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1743 - acc: 0.9462 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1745 - acc: 0.9439 - val_loss: 0.3317 - val_acc: 0.8942\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1740 - acc: 0.9427 - val_loss: 0.3280 - val_acc: 0.8942\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1738 - acc: 0.9433 - val_loss: 0.3176 - val_acc: 0.8942\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1735 - acc: 0.9439 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1735 - acc: 0.9427 - val_loss: 0.3207 - val_acc: 0.8942\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1740 - acc: 0.9450 - val_loss: 0.3279 - val_acc: 0.8942\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1733 - acc: 0.9462 - val_loss: 0.3206 - val_acc: 0.8942\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1725 - acc: 0.9421 - val_loss: 0.3232 - val_acc: 0.8942\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1734 - acc: 0.9439 - val_loss: 0.3250 - val_acc: 0.8942\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1730 - acc: 0.9427 - val_loss: 0.3248 - val_acc: 0.8942\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1735 - acc: 0.9439 - val_loss: 0.3216 - val_acc: 0.8942\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1737 - acc: 0.9439 - val_loss: 0.3207 - val_acc: 0.8942\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1731 - acc: 0.9433 - val_loss: 0.3248 - val_acc: 0.8942\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1724 - acc: 0.9450 - val_loss: 0.3276 - val_acc: 0.8942\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1725 - acc: 0.9462 - val_loss: 0.3346 - val_acc: 0.8942\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1729 - acc: 0.9456 - val_loss: 0.3224 - val_acc: 0.8942\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1727 - acc: 0.9444 - val_loss: 0.3259 - val_acc: 0.8942\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1717 - acc: 0.9433 - val_loss: 0.3207 - val_acc: 0.8942\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1722 - acc: 0.9456 - val_loss: 0.3327 - val_acc: 0.8942\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1729 - acc: 0.9439 - val_loss: 0.3240 - val_acc: 0.8942\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1721 - acc: 0.9480 - val_loss: 0.3186 - val_acc: 0.8995\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1726 - acc: 0.9433 - val_loss: 0.3192 - val_acc: 0.8942\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1716 - acc: 0.9450 - val_loss: 0.3270 - val_acc: 0.8942\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1723 - acc: 0.9456 - val_loss: 0.3291 - val_acc: 0.8942\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1717 - acc: 0.9456 - val_loss: 0.3277 - val_acc: 0.8995\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1724 - acc: 0.9456 - val_loss: 0.3262 - val_acc: 0.8942\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1720 - acc: 0.9450 - val_loss: 0.3311 - val_acc: 0.8942\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1720 - acc: 0.9427 - val_loss: 0.3276 - val_acc: 0.8942\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1719 - acc: 0.9444 - val_loss: 0.3272 - val_acc: 0.8995\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1715 - acc: 0.9450 - val_loss: 0.3360 - val_acc: 0.8942\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1718 - acc: 0.9450 - val_loss: 0.3241 - val_acc: 0.8995\n",
      "Epoch 416/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1706 - acc: 0.9462 - val_loss: 0.3212 - val_acc: 0.8942\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1718 - acc: 0.9456 - val_loss: 0.3189 - val_acc: 0.8942\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1704 - acc: 0.9403 - val_loss: 0.3286 - val_acc: 0.8942\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1716 - acc: 0.9450 - val_loss: 0.3285 - val_acc: 0.8942\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1712 - acc: 0.9427 - val_loss: 0.3298 - val_acc: 0.8942\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1710 - acc: 0.9456 - val_loss: 0.3220 - val_acc: 0.8995\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1708 - acc: 0.9468 - val_loss: 0.3259 - val_acc: 0.8942\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1712 - acc: 0.9433 - val_loss: 0.3179 - val_acc: 0.8995\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1713 - acc: 0.9433 - val_loss: 0.3179 - val_acc: 0.8995\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1705 - acc: 0.9450 - val_loss: 0.3170 - val_acc: 0.8942\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1701 - acc: 0.9456 - val_loss: 0.3230 - val_acc: 0.8995\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1696 - acc: 0.9468 - val_loss: 0.3164 - val_acc: 0.8995\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1705 - acc: 0.9456 - val_loss: 0.3203 - val_acc: 0.8995\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1709 - acc: 0.9456 - val_loss: 0.3174 - val_acc: 0.8995\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1700 - acc: 0.9456 - val_loss: 0.3225 - val_acc: 0.8995\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1701 - acc: 0.9444 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1701 - acc: 0.9456 - val_loss: 0.3149 - val_acc: 0.8942\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1703 - acc: 0.9433 - val_loss: 0.3130 - val_acc: 0.8942\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1699 - acc: 0.9468 - val_loss: 0.3231 - val_acc: 0.8995\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1704 - acc: 0.9456 - val_loss: 0.3171 - val_acc: 0.8942\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1703 - acc: 0.9468 - val_loss: 0.3235 - val_acc: 0.8942\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1698 - acc: 0.9433 - val_loss: 0.3218 - val_acc: 0.8942\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1692 - acc: 0.9462 - val_loss: 0.3170 - val_acc: 0.8995\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1691 - acc: 0.9486 - val_loss: 0.3265 - val_acc: 0.8942\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1699 - acc: 0.9474 - val_loss: 0.3216 - val_acc: 0.8995\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1700 - acc: 0.9421 - val_loss: 0.3188 - val_acc: 0.8995\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1698 - acc: 0.9480 - val_loss: 0.3132 - val_acc: 0.8995\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1697 - acc: 0.9468 - val_loss: 0.3191 - val_acc: 0.8942\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1687 - acc: 0.9480 - val_loss: 0.3259 - val_acc: 0.8942\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1692 - acc: 0.9444 - val_loss: 0.3308 - val_acc: 0.8942\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1695 - acc: 0.9474 - val_loss: 0.3235 - val_acc: 0.8995\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1696 - acc: 0.9474 - val_loss: 0.3147 - val_acc: 0.8995\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1687 - acc: 0.9486 - val_loss: 0.3269 - val_acc: 0.8942\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1691 - acc: 0.9480 - val_loss: 0.3163 - val_acc: 0.8995\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1686 - acc: 0.9456 - val_loss: 0.3242 - val_acc: 0.8942\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1687 - acc: 0.9492 - val_loss: 0.3142 - val_acc: 0.8995\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1686 - acc: 0.9468 - val_loss: 0.3224 - val_acc: 0.8995\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1698 - acc: 0.9444 - val_loss: 0.3134 - val_acc: 0.8995\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1679 - acc: 0.9474 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1696 - acc: 0.9462 - val_loss: 0.3136 - val_acc: 0.8995\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1678 - acc: 0.9492 - val_loss: 0.3257 - val_acc: 0.8995\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1691 - acc: 0.9474 - val_loss: 0.3203 - val_acc: 0.8995\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1686 - acc: 0.9486 - val_loss: 0.3258 - val_acc: 0.8995\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1690 - acc: 0.9474 - val_loss: 0.3195 - val_acc: 0.8995\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1688 - acc: 0.9474 - val_loss: 0.3223 - val_acc: 0.8942\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1679 - acc: 0.9468 - val_loss: 0.3177 - val_acc: 0.8995\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1674 - acc: 0.9480 - val_loss: 0.3204 - val_acc: 0.8995\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1682 - acc: 0.9444 - val_loss: 0.3261 - val_acc: 0.8942\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1681 - acc: 0.9486 - val_loss: 0.3275 - val_acc: 0.8995\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1667 - acc: 0.9462 - val_loss: 0.3357 - val_acc: 0.8942\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1688 - acc: 0.9468 - val_loss: 0.3239 - val_acc: 0.8995\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1678 - acc: 0.9456 - val_loss: 0.3110 - val_acc: 0.8995\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1676 - acc: 0.9486 - val_loss: 0.3194 - val_acc: 0.8995\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1684 - acc: 0.9450 - val_loss: 0.3150 - val_acc: 0.8995\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1678 - acc: 0.9450 - val_loss: 0.3234 - val_acc: 0.8995\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1675 - acc: 0.9486 - val_loss: 0.3222 - val_acc: 0.8942\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1680 - acc: 0.9474 - val_loss: 0.3178 - val_acc: 0.8995\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1675 - acc: 0.9474 - val_loss: 0.3138 - val_acc: 0.8995\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1678 - acc: 0.9486 - val_loss: 0.3209 - val_acc: 0.8995\n",
      "Epoch 475/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1674 - acc: 0.9474 - val_loss: 0.3226 - val_acc: 0.8942\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1675 - acc: 0.9504 - val_loss: 0.3182 - val_acc: 0.8995\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1677 - acc: 0.9486 - val_loss: 0.3236 - val_acc: 0.8995\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1672 - acc: 0.9450 - val_loss: 0.3268 - val_acc: 0.8995\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1673 - acc: 0.9480 - val_loss: 0.3196 - val_acc: 0.8995\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1675 - acc: 0.9468 - val_loss: 0.3147 - val_acc: 0.8995\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1677 - acc: 0.9468 - val_loss: 0.3158 - val_acc: 0.8995\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1671 - acc: 0.9450 - val_loss: 0.3148 - val_acc: 0.8995\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1674 - acc: 0.9444 - val_loss: 0.3170 - val_acc: 0.8995\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1674 - acc: 0.9474 - val_loss: 0.3208 - val_acc: 0.8942\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1661 - acc: 0.9492 - val_loss: 0.3240 - val_acc: 0.8995\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1669 - acc: 0.9492 - val_loss: 0.3216 - val_acc: 0.8995\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1654 - acc: 0.9504 - val_loss: 0.3126 - val_acc: 0.8995\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1672 - acc: 0.9480 - val_loss: 0.3168 - val_acc: 0.8995\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1659 - acc: 0.9468 - val_loss: 0.3109 - val_acc: 0.8995\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1672 - acc: 0.9456 - val_loss: 0.3135 - val_acc: 0.8995\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1663 - acc: 0.9498 - val_loss: 0.3194 - val_acc: 0.8995\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1663 - acc: 0.9474 - val_loss: 0.3116 - val_acc: 0.8995\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1659 - acc: 0.9480 - val_loss: 0.3208 - val_acc: 0.8942\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1660 - acc: 0.9486 - val_loss: 0.3248 - val_acc: 0.8995\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1662 - acc: 0.9504 - val_loss: 0.3148 - val_acc: 0.8995\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1662 - acc: 0.9462 - val_loss: 0.3191 - val_acc: 0.8995\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1658 - acc: 0.9515 - val_loss: 0.3230 - val_acc: 0.8995\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1656 - acc: 0.9486 - val_loss: 0.3137 - val_acc: 0.8995\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1664 - acc: 0.9486 - val_loss: 0.3132 - val_acc: 0.8995\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1662 - acc: 0.9468 - val_loss: 0.3093 - val_acc: 0.8995\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1657 - acc: 0.9492 - val_loss: 0.3175 - val_acc: 0.8942\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1660 - acc: 0.9474 - val_loss: 0.3187 - val_acc: 0.8889\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1650 - acc: 0.9492 - val_loss: 0.3304 - val_acc: 0.8942\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1651 - acc: 0.9474 - val_loss: 0.3207 - val_acc: 0.8942\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1655 - acc: 0.9462 - val_loss: 0.3173 - val_acc: 0.8995\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1655 - acc: 0.9468 - val_loss: 0.3146 - val_acc: 0.8995\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1651 - acc: 0.9486 - val_loss: 0.3168 - val_acc: 0.8889\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1653 - acc: 0.9480 - val_loss: 0.3169 - val_acc: 0.8942\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1647 - acc: 0.9468 - val_loss: 0.3232 - val_acc: 0.8889\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1652 - acc: 0.9480 - val_loss: 0.3132 - val_acc: 0.8995\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1650 - acc: 0.9474 - val_loss: 0.3156 - val_acc: 0.8942\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1648 - acc: 0.9468 - val_loss: 0.3155 - val_acc: 0.8995\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1648 - acc: 0.9492 - val_loss: 0.3150 - val_acc: 0.8995\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1650 - acc: 0.9504 - val_loss: 0.3191 - val_acc: 0.8889\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1643 - acc: 0.9456 - val_loss: 0.3198 - val_acc: 0.8889\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1652 - acc: 0.9486 - val_loss: 0.3160 - val_acc: 0.8942\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1650 - acc: 0.9468 - val_loss: 0.3144 - val_acc: 0.8995\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1649 - acc: 0.9474 - val_loss: 0.3141 - val_acc: 0.8995\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1645 - acc: 0.9474 - val_loss: 0.3165 - val_acc: 0.8995\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1649 - acc: 0.9456 - val_loss: 0.3138 - val_acc: 0.8995\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1640 - acc: 0.9498 - val_loss: 0.3223 - val_acc: 0.8995\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1646 - acc: 0.9492 - val_loss: 0.3143 - val_acc: 0.8995\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1641 - acc: 0.9492 - val_loss: 0.3192 - val_acc: 0.8889\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1645 - acc: 0.9492 - val_loss: 0.3172 - val_acc: 0.8942\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1638 - acc: 0.9498 - val_loss: 0.3189 - val_acc: 0.8995\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1641 - acc: 0.9480 - val_loss: 0.3251 - val_acc: 0.8889\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1646 - acc: 0.9468 - val_loss: 0.3169 - val_acc: 0.8942\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1645 - acc: 0.9492 - val_loss: 0.3123 - val_acc: 0.8995\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1644 - acc: 0.9486 - val_loss: 0.3081 - val_acc: 0.8995\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1636 - acc: 0.9480 - val_loss: 0.3201 - val_acc: 0.8942\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1645 - acc: 0.9456 - val_loss: 0.3203 - val_acc: 0.8942\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1644 - acc: 0.9480 - val_loss: 0.3168 - val_acc: 0.8942\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1642 - acc: 0.9474 - val_loss: 0.3217 - val_acc: 0.8942\n",
      "Epoch 534/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1645 - acc: 0.9474 - val_loss: 0.3150 - val_acc: 0.8942\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1636 - acc: 0.9486 - val_loss: 0.3234 - val_acc: 0.8889\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1634 - acc: 0.9474 - val_loss: 0.3135 - val_acc: 0.8995\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1636 - acc: 0.9474 - val_loss: 0.3121 - val_acc: 0.8942\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1624 - acc: 0.9486 - val_loss: 0.3294 - val_acc: 0.8942\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1642 - acc: 0.9486 - val_loss: 0.3150 - val_acc: 0.8942\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1641 - acc: 0.9504 - val_loss: 0.3165 - val_acc: 0.8942\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1623 - acc: 0.9492 - val_loss: 0.3270 - val_acc: 0.8942\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1640 - acc: 0.9486 - val_loss: 0.3120 - val_acc: 0.8995\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1633 - acc: 0.9480 - val_loss: 0.3147 - val_acc: 0.8942\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9486 - val_loss: 0.3211 - val_acc: 0.8942\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1631 - acc: 0.9504 - val_loss: 0.3207 - val_acc: 0.8942\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1628 - acc: 0.9480 - val_loss: 0.3123 - val_acc: 0.8942\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1628 - acc: 0.9492 - val_loss: 0.3137 - val_acc: 0.8942\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1635 - acc: 0.9480 - val_loss: 0.3183 - val_acc: 0.8995\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1631 - acc: 0.9480 - val_loss: 0.3155 - val_acc: 0.8995\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1637 - acc: 0.9462 - val_loss: 0.3163 - val_acc: 0.8995\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1633 - acc: 0.9468 - val_loss: 0.3139 - val_acc: 0.8995\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1627 - acc: 0.9498 - val_loss: 0.3114 - val_acc: 0.8995\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1632 - acc: 0.9480 - val_loss: 0.3151 - val_acc: 0.8995\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1629 - acc: 0.9480 - val_loss: 0.3138 - val_acc: 0.8942\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1628 - acc: 0.9492 - val_loss: 0.3100 - val_acc: 0.8995\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1627 - acc: 0.9509 - val_loss: 0.3148 - val_acc: 0.8942\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1622 - acc: 0.9480 - val_loss: 0.3146 - val_acc: 0.8942\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1627 - acc: 0.9486 - val_loss: 0.3169 - val_acc: 0.8942\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1626 - acc: 0.9486 - val_loss: 0.3147 - val_acc: 0.8942\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1612 - acc: 0.9492 - val_loss: 0.3043 - val_acc: 0.8995\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1629 - acc: 0.9462 - val_loss: 0.3162 - val_acc: 0.8995\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1620 - acc: 0.9474 - val_loss: 0.3141 - val_acc: 0.8942\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1619 - acc: 0.9486 - val_loss: 0.3184 - val_acc: 0.8942\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1621 - acc: 0.9486 - val_loss: 0.3169 - val_acc: 0.8995\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1620 - acc: 0.9486 - val_loss: 0.3203 - val_acc: 0.8942\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1617 - acc: 0.9480 - val_loss: 0.3181 - val_acc: 0.8942\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1623 - acc: 0.9468 - val_loss: 0.3163 - val_acc: 0.8942\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1616 - acc: 0.9486 - val_loss: 0.3200 - val_acc: 0.8889\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1613 - acc: 0.9486 - val_loss: 0.3102 - val_acc: 0.8995\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1614 - acc: 0.9480 - val_loss: 0.3119 - val_acc: 0.8995\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1610 - acc: 0.9509 - val_loss: 0.3129 - val_acc: 0.8995\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1620 - acc: 0.9468 - val_loss: 0.3115 - val_acc: 0.8995\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1614 - acc: 0.9480 - val_loss: 0.3183 - val_acc: 0.8942\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1616 - acc: 0.9486 - val_loss: 0.3135 - val_acc: 0.8995\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1615 - acc: 0.9474 - val_loss: 0.3150 - val_acc: 0.8942\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1608 - acc: 0.9509 - val_loss: 0.3088 - val_acc: 0.8995\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1620 - acc: 0.9480 - val_loss: 0.3181 - val_acc: 0.8942\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1616 - acc: 0.9486 - val_loss: 0.3117 - val_acc: 0.8995\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1619 - acc: 0.9486 - val_loss: 0.3097 - val_acc: 0.8995\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1607 - acc: 0.9486 - val_loss: 0.3138 - val_acc: 0.8942\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1612 - acc: 0.9486 - val_loss: 0.3131 - val_acc: 0.8995\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1611 - acc: 0.9498 - val_loss: 0.3065 - val_acc: 0.8995\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1609 - acc: 0.9480 - val_loss: 0.3157 - val_acc: 0.8889\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1609 - acc: 0.9492 - val_loss: 0.3167 - val_acc: 0.8995\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1618 - acc: 0.9474 - val_loss: 0.3139 - val_acc: 0.8995\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1596 - acc: 0.9498 - val_loss: 0.3263 - val_acc: 0.8942\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1617 - acc: 0.9468 - val_loss: 0.3190 - val_acc: 0.8942\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1607 - acc: 0.9504 - val_loss: 0.3137 - val_acc: 0.8995\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1608 - acc: 0.9498 - val_loss: 0.3185 - val_acc: 0.8942\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1610 - acc: 0.9509 - val_loss: 0.3069 - val_acc: 0.8995\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1609 - acc: 0.9498 - val_loss: 0.3092 - val_acc: 0.8995\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1608 - acc: 0.9492 - val_loss: 0.3144 - val_acc: 0.8942\n",
      "Epoch 593/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1601 - acc: 0.9492 - val_loss: 0.3215 - val_acc: 0.8995\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1607 - acc: 0.9492 - val_loss: 0.3169 - val_acc: 0.8995\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1608 - acc: 0.9492 - val_loss: 0.3152 - val_acc: 0.8942\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1607 - acc: 0.9486 - val_loss: 0.3185 - val_acc: 0.8942\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1594 - acc: 0.9498 - val_loss: 0.3310 - val_acc: 0.8942\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1597 - acc: 0.9509 - val_loss: 0.3024 - val_acc: 0.8942\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1608 - acc: 0.9504 - val_loss: 0.3087 - val_acc: 0.8995\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1599 - acc: 0.9504 - val_loss: 0.3232 - val_acc: 0.8995\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1606 - acc: 0.9486 - val_loss: 0.3080 - val_acc: 0.8995\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1606 - acc: 0.9474 - val_loss: 0.3065 - val_acc: 0.8995\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1608 - acc: 0.9486 - val_loss: 0.3068 - val_acc: 0.8995\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1597 - acc: 0.9480 - val_loss: 0.3082 - val_acc: 0.8995\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1605 - acc: 0.9492 - val_loss: 0.3106 - val_acc: 0.8995\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1608 - acc: 0.9468 - val_loss: 0.3189 - val_acc: 0.8942\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1601 - acc: 0.9480 - val_loss: 0.3129 - val_acc: 0.8995\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3155 - val_acc: 0.8942\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1603 - acc: 0.9498 - val_loss: 0.3118 - val_acc: 0.8995\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1597 - acc: 0.9492 - val_loss: 0.3151 - val_acc: 0.8995\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3058 - val_acc: 0.8995\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1593 - acc: 0.9498 - val_loss: 0.3109 - val_acc: 0.8995\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1594 - acc: 0.9515 - val_loss: 0.3157 - val_acc: 0.8995\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1595 - acc: 0.9509 - val_loss: 0.3176 - val_acc: 0.8995\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1598 - acc: 0.9486 - val_loss: 0.3099 - val_acc: 0.8995\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1582 - acc: 0.9492 - val_loss: 0.3164 - val_acc: 0.8995\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1599 - acc: 0.9486 - val_loss: 0.3160 - val_acc: 0.8995\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1601 - acc: 0.9480 - val_loss: 0.3089 - val_acc: 0.8995\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1586 - acc: 0.9480 - val_loss: 0.3100 - val_acc: 0.8995\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1592 - acc: 0.9498 - val_loss: 0.3210 - val_acc: 0.8942\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1587 - acc: 0.9492 - val_loss: 0.3181 - val_acc: 0.8942\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1592 - acc: 0.9492 - val_loss: 0.3104 - val_acc: 0.8995\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1591 - acc: 0.9492 - val_loss: 0.3138 - val_acc: 0.8942\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1589 - acc: 0.9515 - val_loss: 0.3145 - val_acc: 0.8995\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1590 - acc: 0.9521 - val_loss: 0.3140 - val_acc: 0.8995\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1590 - acc: 0.9486 - val_loss: 0.3118 - val_acc: 0.8995\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1594 - acc: 0.9474 - val_loss: 0.3143 - val_acc: 0.8995\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1595 - acc: 0.9504 - val_loss: 0.3144 - val_acc: 0.8942\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1589 - acc: 0.9468 - val_loss: 0.3094 - val_acc: 0.8995\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1594 - acc: 0.9492 - val_loss: 0.3153 - val_acc: 0.8942\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1591 - acc: 0.9504 - val_loss: 0.3144 - val_acc: 0.8942\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1589 - acc: 0.9480 - val_loss: 0.3174 - val_acc: 0.8995\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1582 - acc: 0.9504 - val_loss: 0.3200 - val_acc: 0.8942\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1587 - acc: 0.9486 - val_loss: 0.3172 - val_acc: 0.8942\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1597 - acc: 0.9498 - val_loss: 0.3132 - val_acc: 0.8995\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1577 - acc: 0.9509 - val_loss: 0.3254 - val_acc: 0.8995\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1599 - acc: 0.9492 - val_loss: 0.3225 - val_acc: 0.8995\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1586 - acc: 0.9486 - val_loss: 0.3098 - val_acc: 0.8995\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1592 - acc: 0.9498 - val_loss: 0.3155 - val_acc: 0.8995\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1587 - acc: 0.9480 - val_loss: 0.3127 - val_acc: 0.8995\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1590 - acc: 0.9492 - val_loss: 0.3132 - val_acc: 0.8942\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1572 - acc: 0.9486 - val_loss: 0.3192 - val_acc: 0.8942\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1591 - acc: 0.9498 - val_loss: 0.3196 - val_acc: 0.8942\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1574 - acc: 0.9498 - val_loss: 0.3247 - val_acc: 0.8942\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1585 - acc: 0.9486 - val_loss: 0.3156 - val_acc: 0.8942\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1581 - acc: 0.9504 - val_loss: 0.3247 - val_acc: 0.8942\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1589 - acc: 0.9498 - val_loss: 0.3081 - val_acc: 0.8995\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1579 - acc: 0.9504 - val_loss: 0.3136 - val_acc: 0.8995\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1579 - acc: 0.9480 - val_loss: 0.3145 - val_acc: 0.8995\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1580 - acc: 0.9504 - val_loss: 0.3116 - val_acc: 0.8995\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1586 - acc: 0.9498 - val_loss: 0.3122 - val_acc: 0.8942\n",
      "Epoch 652/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1575 - acc: 0.9509 - val_loss: 0.3139 - val_acc: 0.8942\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1578 - acc: 0.9498 - val_loss: 0.3112 - val_acc: 0.8995\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1574 - acc: 0.9498 - val_loss: 0.3182 - val_acc: 0.8942\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1576 - acc: 0.9492 - val_loss: 0.3125 - val_acc: 0.8995\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1578 - acc: 0.9504 - val_loss: 0.3103 - val_acc: 0.9048\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1578 - acc: 0.9504 - val_loss: 0.3102 - val_acc: 0.8995\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1574 - acc: 0.9504 - val_loss: 0.3152 - val_acc: 0.8942\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1573 - acc: 0.9480 - val_loss: 0.3159 - val_acc: 0.8995\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1572 - acc: 0.9509 - val_loss: 0.3101 - val_acc: 0.9048\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1578 - acc: 0.9486 - val_loss: 0.3163 - val_acc: 0.8995\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1574 - acc: 0.9515 - val_loss: 0.3136 - val_acc: 0.8995\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1577 - acc: 0.9504 - val_loss: 0.3222 - val_acc: 0.8995\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1568 - acc: 0.9527 - val_loss: 0.3227 - val_acc: 0.8942\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9521 - val_loss: 0.3346 - val_acc: 0.8942\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1575 - acc: 0.9504 - val_loss: 0.3261 - val_acc: 0.8889\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1575 - acc: 0.9498 - val_loss: 0.3194 - val_acc: 0.8942\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1568 - acc: 0.9504 - val_loss: 0.3267 - val_acc: 0.8942\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1575 - acc: 0.9509 - val_loss: 0.3170 - val_acc: 0.8995\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1566 - acc: 0.9504 - val_loss: 0.3303 - val_acc: 0.8889\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1570 - acc: 0.9515 - val_loss: 0.3292 - val_acc: 0.8889\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1569 - acc: 0.9504 - val_loss: 0.3302 - val_acc: 0.8889\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1570 - acc: 0.9504 - val_loss: 0.3151 - val_acc: 0.8995\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1564 - acc: 0.9509 - val_loss: 0.3262 - val_acc: 0.8942\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1568 - acc: 0.9492 - val_loss: 0.3291 - val_acc: 0.8942\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1577 - acc: 0.9504 - val_loss: 0.3149 - val_acc: 0.8995\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1563 - acc: 0.9492 - val_loss: 0.3262 - val_acc: 0.8995\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1560 - acc: 0.9492 - val_loss: 0.3324 - val_acc: 0.8995\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1571 - acc: 0.9504 - val_loss: 0.3289 - val_acc: 0.8995\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1562 - acc: 0.9504 - val_loss: 0.3215 - val_acc: 0.8995\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1566 - acc: 0.9492 - val_loss: 0.3171 - val_acc: 0.8995\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1561 - acc: 0.9533 - val_loss: 0.3155 - val_acc: 0.8995\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1559 - acc: 0.9492 - val_loss: 0.3180 - val_acc: 0.8995\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9509 - val_loss: 0.3185 - val_acc: 0.8995\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1571 - acc: 0.9492 - val_loss: 0.3200 - val_acc: 0.8995\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1555 - acc: 0.9533 - val_loss: 0.3198 - val_acc: 0.8995\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1563 - acc: 0.9486 - val_loss: 0.3165 - val_acc: 0.8995\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1551 - acc: 0.9521 - val_loss: 0.3245 - val_acc: 0.8995\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1560 - acc: 0.9498 - val_loss: 0.3225 - val_acc: 0.8995\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1563 - acc: 0.9492 - val_loss: 0.3275 - val_acc: 0.8995\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1552 - acc: 0.9492 - val_loss: 0.3311 - val_acc: 0.8889\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1552 - acc: 0.9509 - val_loss: 0.3173 - val_acc: 0.8995\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1552 - acc: 0.9498 - val_loss: 0.3231 - val_acc: 0.8889\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1560 - acc: 0.9498 - val_loss: 0.3238 - val_acc: 0.8995\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1563 - acc: 0.9509 - val_loss: 0.3231 - val_acc: 0.8995\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1557 - acc: 0.9492 - val_loss: 0.3234 - val_acc: 0.8995\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1563 - acc: 0.9515 - val_loss: 0.3199 - val_acc: 0.8995\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1557 - acc: 0.9504 - val_loss: 0.3216 - val_acc: 0.8995\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1556 - acc: 0.9509 - val_loss: 0.3206 - val_acc: 0.8995\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1554 - acc: 0.9498 - val_loss: 0.3181 - val_acc: 0.8995\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1558 - acc: 0.9515 - val_loss: 0.3156 - val_acc: 0.8995\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1554 - acc: 0.9498 - val_loss: 0.3183 - val_acc: 0.8995\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1553 - acc: 0.9498 - val_loss: 0.3135 - val_acc: 0.8995\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1560 - acc: 0.9509 - val_loss: 0.3220 - val_acc: 0.8995\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1556 - acc: 0.9480 - val_loss: 0.3150 - val_acc: 0.9048\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1558 - acc: 0.9515 - val_loss: 0.3183 - val_acc: 0.8995\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1550 - acc: 0.9521 - val_loss: 0.3134 - val_acc: 0.8995\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1553 - acc: 0.9474 - val_loss: 0.3173 - val_acc: 0.9048\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1547 - acc: 0.9527 - val_loss: 0.3136 - val_acc: 0.9048\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1550 - acc: 0.9486 - val_loss: 0.3109 - val_acc: 0.8995\n",
      "Epoch 711/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1553 - acc: 0.9521 - val_loss: 0.3146 - val_acc: 0.8995\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1552 - acc: 0.9515 - val_loss: 0.3178 - val_acc: 0.8995\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1550 - acc: 0.9515 - val_loss: 0.3252 - val_acc: 0.8889\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1542 - acc: 0.9515 - val_loss: 0.3215 - val_acc: 0.8995\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1550 - acc: 0.9515 - val_loss: 0.3240 - val_acc: 0.8889\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1552 - acc: 0.9504 - val_loss: 0.3157 - val_acc: 0.8995\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1552 - acc: 0.9509 - val_loss: 0.3169 - val_acc: 0.8995\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1549 - acc: 0.9492 - val_loss: 0.3159 - val_acc: 0.8995\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1545 - acc: 0.9509 - val_loss: 0.3194 - val_acc: 0.8995\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1555 - acc: 0.9504 - val_loss: 0.3220 - val_acc: 0.8995\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1543 - acc: 0.9521 - val_loss: 0.3293 - val_acc: 0.8889\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1549 - acc: 0.9492 - val_loss: 0.3170 - val_acc: 0.8995\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1546 - acc: 0.9527 - val_loss: 0.3197 - val_acc: 0.8995\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1544 - acc: 0.9509 - val_loss: 0.3207 - val_acc: 0.8995\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1541 - acc: 0.9509 - val_loss: 0.3217 - val_acc: 0.8995\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1537 - acc: 0.9504 - val_loss: 0.3192 - val_acc: 0.8995\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1537 - acc: 0.9527 - val_loss: 0.3223 - val_acc: 0.8995\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1543 - acc: 0.9515 - val_loss: 0.3311 - val_acc: 0.8995\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1537 - acc: 0.9509 - val_loss: 0.3339 - val_acc: 0.8889\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1547 - acc: 0.9504 - val_loss: 0.3203 - val_acc: 0.8995\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1539 - acc: 0.9509 - val_loss: 0.3301 - val_acc: 0.8889\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1548 - acc: 0.9527 - val_loss: 0.3216 - val_acc: 0.8942\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1540 - acc: 0.9527 - val_loss: 0.3258 - val_acc: 0.8942\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1541 - acc: 0.9515 - val_loss: 0.3200 - val_acc: 0.8995\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1542 - acc: 0.9492 - val_loss: 0.3174 - val_acc: 0.8995\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1531 - acc: 0.9492 - val_loss: 0.3218 - val_acc: 0.8995\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1542 - acc: 0.9509 - val_loss: 0.3227 - val_acc: 0.8995\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1439 - acc: 0.957 - 0s 49us/step - loss: 0.1539 - acc: 0.9515 - val_loss: 0.3202 - val_acc: 0.8995\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1538 - acc: 0.9527 - val_loss: 0.3237 - val_acc: 0.8942\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1522 - acc: 0.9504 - val_loss: 0.3184 - val_acc: 0.8995\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1542 - acc: 0.9498 - val_loss: 0.3167 - val_acc: 0.8995\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1536 - acc: 0.9527 - val_loss: 0.3220 - val_acc: 0.8889\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1528 - acc: 0.9509 - val_loss: 0.3274 - val_acc: 0.8889\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1535 - acc: 0.9509 - val_loss: 0.3250 - val_acc: 0.8995\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1527 - acc: 0.9498 - val_loss: 0.3257 - val_acc: 0.8995\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1530 - acc: 0.9515 - val_loss: 0.3185 - val_acc: 0.8995\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1532 - acc: 0.9509 - val_loss: 0.3252 - val_acc: 0.8995\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1534 - acc: 0.9515 - val_loss: 0.3219 - val_acc: 0.8995\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1532 - acc: 0.9527 - val_loss: 0.3170 - val_acc: 0.8995\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1535 - acc: 0.9515 - val_loss: 0.3252 - val_acc: 0.8995\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1533 - acc: 0.9527 - val_loss: 0.3214 - val_acc: 0.8942\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1522 - acc: 0.9504 - val_loss: 0.3135 - val_acc: 0.8995\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1521 - acc: 0.9504 - val_loss: 0.3232 - val_acc: 0.8995\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1530 - acc: 0.9509 - val_loss: 0.3297 - val_acc: 0.8995\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1537 - acc: 0.9509 - val_loss: 0.3197 - val_acc: 0.8995\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1523 - acc: 0.9498 - val_loss: 0.3211 - val_acc: 0.8995\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1520 - acc: 0.9515 - val_loss: 0.3203 - val_acc: 0.8995\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1528 - acc: 0.9492 - val_loss: 0.3208 - val_acc: 0.8995\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1523 - acc: 0.9504 - val_loss: 0.3101 - val_acc: 0.9048\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1524 - acc: 0.9521 - val_loss: 0.3145 - val_acc: 0.8995\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1523 - acc: 0.9504 - val_loss: 0.3157 - val_acc: 0.8995\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1531 - acc: 0.9498 - val_loss: 0.3209 - val_acc: 0.8995\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1517 - acc: 0.9521 - val_loss: 0.3119 - val_acc: 0.8942\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1526 - acc: 0.9527 - val_loss: 0.3244 - val_acc: 0.8889\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1529 - acc: 0.9492 - val_loss: 0.3149 - val_acc: 0.9048\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1517 - acc: 0.9515 - val_loss: 0.3213 - val_acc: 0.8995\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1522 - acc: 0.9492 - val_loss: 0.3158 - val_acc: 0.8995\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1516 - acc: 0.9509 - val_loss: 0.3292 - val_acc: 0.8889\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1533 - acc: 0.9504 - val_loss: 0.3165 - val_acc: 0.8995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1520 - acc: 0.9521 - val_loss: 0.3281 - val_acc: 0.8995\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1513 - acc: 0.9515 - val_loss: 0.3339 - val_acc: 0.8995\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1525 - acc: 0.9504 - val_loss: 0.3164 - val_acc: 0.8995\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1522 - acc: 0.9533 - val_loss: 0.3284 - val_acc: 0.8995\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1521 - acc: 0.9527 - val_loss: 0.3300 - val_acc: 0.8995\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1521 - acc: 0.9527 - val_loss: 0.3119 - val_acc: 0.8995\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1513 - acc: 0.9515 - val_loss: 0.3221 - val_acc: 0.8995\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1522 - acc: 0.9504 - val_loss: 0.3315 - val_acc: 0.8889\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1521 - acc: 0.9504 - val_loss: 0.3142 - val_acc: 0.8995\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1519 - acc: 0.9504 - val_loss: 0.3192 - val_acc: 0.8995\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1504 - acc: 0.9527 - val_loss: 0.3213 - val_acc: 0.8942\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1527 - acc: 0.9527 - val_loss: 0.3148 - val_acc: 0.8995\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1522 - acc: 0.9515 - val_loss: 0.3184 - val_acc: 0.8942\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1522 - acc: 0.9509 - val_loss: 0.3216 - val_acc: 0.8942\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1512 - acc: 0.9504 - val_loss: 0.3232 - val_acc: 0.8995\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1520 - acc: 0.9527 - val_loss: 0.3183 - val_acc: 0.8995\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1522 - acc: 0.9498 - val_loss: 0.3084 - val_acc: 0.9048\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1516 - acc: 0.9527 - val_loss: 0.3224 - val_acc: 0.8995\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1513 - acc: 0.9515 - val_loss: 0.3222 - val_acc: 0.8995\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1514 - acc: 0.9533 - val_loss: 0.3140 - val_acc: 0.8995\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1517 - acc: 0.9533 - val_loss: 0.3205 - val_acc: 0.9048\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1521 - acc: 0.9509 - val_loss: 0.3236 - val_acc: 0.8995\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1514 - acc: 0.9492 - val_loss: 0.3268 - val_acc: 0.8942\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1521 - acc: 0.9515 - val_loss: 0.3227 - val_acc: 0.8995\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1518 - acc: 0.9521 - val_loss: 0.3181 - val_acc: 0.8995\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1513 - acc: 0.9509 - val_loss: 0.3254 - val_acc: 0.8942\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1521 - acc: 0.9521 - val_loss: 0.3166 - val_acc: 0.8995\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1502 - acc: 0.9527 - val_loss: 0.3372 - val_acc: 0.8889\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1517 - acc: 0.9515 - val_loss: 0.3185 - val_acc: 0.8995\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1512 - acc: 0.9509 - val_loss: 0.3189 - val_acc: 0.9048\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1504 - acc: 0.9533 - val_loss: 0.3120 - val_acc: 0.8942\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1507 - acc: 0.9539 - val_loss: 0.3243 - val_acc: 0.8995\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1511 - acc: 0.9509 - val_loss: 0.3213 - val_acc: 0.8995\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1508 - acc: 0.9509 - val_loss: 0.3165 - val_acc: 0.8995\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1504 - acc: 0.9539 - val_loss: 0.3156 - val_acc: 0.8995\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1509 - acc: 0.9515 - val_loss: 0.3209 - val_acc: 0.8995\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1503 - acc: 0.9521 - val_loss: 0.3208 - val_acc: 0.9048\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1505 - acc: 0.9539 - val_loss: 0.3208 - val_acc: 0.8995\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1508 - acc: 0.9533 - val_loss: 0.3237 - val_acc: 0.8995\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1513 - acc: 0.9515 - val_loss: 0.3250 - val_acc: 0.8995\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1506 - acc: 0.9504 - val_loss: 0.3156 - val_acc: 0.8995\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1512 - acc: 0.9498 - val_loss: 0.3236 - val_acc: 0.8995\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1515 - acc: 0.9498 - val_loss: 0.3211 - val_acc: 0.8995\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1504 - acc: 0.9515 - val_loss: 0.3237 - val_acc: 0.8995\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1508 - acc: 0.9521 - val_loss: 0.3173 - val_acc: 0.8995\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1505 - acc: 0.9533 - val_loss: 0.3258 - val_acc: 0.8889\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1503 - acc: 0.9527 - val_loss: 0.3228 - val_acc: 0.8942\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1506 - acc: 0.9498 - val_loss: 0.3241 - val_acc: 0.8942\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1514 - acc: 0.9527 - val_loss: 0.3209 - val_acc: 0.8995\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1496 - acc: 0.9521 - val_loss: 0.3239 - val_acc: 0.8995\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1509 - acc: 0.9515 - val_loss: 0.3201 - val_acc: 0.8995\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1504 - acc: 0.9533 - val_loss: 0.3181 - val_acc: 0.8995\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1506 - acc: 0.9509 - val_loss: 0.3194 - val_acc: 0.8995\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1498 - acc: 0.9498 - val_loss: 0.3308 - val_acc: 0.8995\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1507 - acc: 0.9527 - val_loss: 0.3179 - val_acc: 0.8995\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1502 - acc: 0.9504 - val_loss: 0.3283 - val_acc: 0.8942\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1505 - acc: 0.9533 - val_loss: 0.3242 - val_acc: 0.8995\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1500 - acc: 0.9533 - val_loss: 0.3321 - val_acc: 0.8942\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1502 - acc: 0.9521 - val_loss: 0.3201 - val_acc: 0.9048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1512 - acc: 0.9509 - val_loss: 0.3198 - val_acc: 0.8995\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1502 - acc: 0.9533 - val_loss: 0.3247 - val_acc: 0.8995\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1503 - acc: 0.9521 - val_loss: 0.3174 - val_acc: 0.8995\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1497 - acc: 0.9515 - val_loss: 0.3183 - val_acc: 0.9048\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1491 - acc: 0.9545 - val_loss: 0.3206 - val_acc: 0.8995\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1494 - acc: 0.9533 - val_loss: 0.3182 - val_acc: 0.8942\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1501 - acc: 0.9545 - val_loss: 0.3211 - val_acc: 0.8995\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1507 - acc: 0.9515 - val_loss: 0.3232 - val_acc: 0.8942\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1503 - acc: 0.9509 - val_loss: 0.3209 - val_acc: 0.8995\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1490 - acc: 0.9504 - val_loss: 0.3262 - val_acc: 0.8995\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1504 - acc: 0.9504 - val_loss: 0.3207 - val_acc: 0.8995\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1490 - acc: 0.9509 - val_loss: 0.3363 - val_acc: 0.8995\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1509 - acc: 0.9504 - val_loss: 0.3192 - val_acc: 0.8995\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1498 - acc: 0.9527 - val_loss: 0.3210 - val_acc: 0.8942\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1499 - acc: 0.9545 - val_loss: 0.3165 - val_acc: 0.8995\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1503 - acc: 0.9486 - val_loss: 0.3281 - val_acc: 0.8995\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1484 - acc: 0.9515 - val_loss: 0.3197 - val_acc: 0.8995\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1503 - acc: 0.9515 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1488 - acc: 0.9527 - val_loss: 0.3426 - val_acc: 0.8889\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1503 - acc: 0.9498 - val_loss: 0.3253 - val_acc: 0.8995\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1502 - acc: 0.9515 - val_loss: 0.3206 - val_acc: 0.8995\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1491 - acc: 0.9509 - val_loss: 0.3210 - val_acc: 0.8995\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1495 - acc: 0.9545 - val_loss: 0.3175 - val_acc: 0.8995\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1495 - acc: 0.9533 - val_loss: 0.3227 - val_acc: 0.8995\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1499 - acc: 0.9521 - val_loss: 0.3279 - val_acc: 0.8942\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1493 - acc: 0.9515 - val_loss: 0.3296 - val_acc: 0.8942\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1498 - acc: 0.9515 - val_loss: 0.3357 - val_acc: 0.8942\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1491 - acc: 0.9539 - val_loss: 0.3210 - val_acc: 0.8942\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1487 - acc: 0.9521 - val_loss: 0.3285 - val_acc: 0.8889\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1494 - acc: 0.9527 - val_loss: 0.3207 - val_acc: 0.8995\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1493 - acc: 0.9492 - val_loss: 0.3267 - val_acc: 0.8942\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1492 - acc: 0.9527 - val_loss: 0.3223 - val_acc: 0.8995\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1495 - acc: 0.9504 - val_loss: 0.3219 - val_acc: 0.8995\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1487 - acc: 0.9533 - val_loss: 0.3349 - val_acc: 0.8995\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1484 - acc: 0.9527 - val_loss: 0.3157 - val_acc: 0.8995\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1494 - acc: 0.9509 - val_loss: 0.3195 - val_acc: 0.8995\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1490 - acc: 0.9527 - val_loss: 0.3213 - val_acc: 0.8995\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1487 - acc: 0.9533 - val_loss: 0.3243 - val_acc: 0.8942\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1491 - acc: 0.9515 - val_loss: 0.3257 - val_acc: 0.8995\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1486 - acc: 0.9533 - val_loss: 0.3244 - val_acc: 0.8995\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1487 - acc: 0.9533 - val_loss: 0.3186 - val_acc: 0.8995\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1486 - acc: 0.9521 - val_loss: 0.3215 - val_acc: 0.8995\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1484 - acc: 0.9539 - val_loss: 0.3268 - val_acc: 0.8942\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.1487 - acc: 0.9515 - val_loss: 0.3189 - val_acc: 0.8995\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1495 - acc: 0.9551 - val_loss: 0.3202 - val_acc: 0.8995\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1482 - acc: 0.9533 - val_loss: 0.3388 - val_acc: 0.8942\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1491 - acc: 0.9527 - val_loss: 0.3258 - val_acc: 0.8995\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.1494 - acc: 0.9527 - val_loss: 0.3289 - val_acc: 0.8942\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.1485 - acc: 0.9527 - val_loss: 0.3238 - val_acc: 0.8995\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.1481 - acc: 0.9533 - val_loss: 0.3336 - val_acc: 0.8942\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1490 - acc: 0.9521 - val_loss: 0.3237 - val_acc: 0.8942\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1486 - acc: 0.9515 - val_loss: 0.3270 - val_acc: 0.8995\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.1482 - acc: 0.9563 - val_loss: 0.3322 - val_acc: 0.8942\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1483 - acc: 0.9527 - val_loss: 0.3351 - val_acc: 0.8942\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1484 - acc: 0.9509 - val_loss: 0.3265 - val_acc: 0.8995\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1478 - acc: 0.9515 - val_loss: 0.3205 - val_acc: 0.8942\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1480 - acc: 0.9515 - val_loss: 0.3196 - val_acc: 0.8995\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1479 - acc: 0.9557 - val_loss: 0.3280 - val_acc: 0.8942\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1478 - acc: 0.9527 - val_loss: 0.3225 - val_acc: 0.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1473 - acc: 0.9539 - val_loss: 0.3212 - val_acc: 0.8995\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1473 - acc: 0.9545 - val_loss: 0.3342 - val_acc: 0.8942\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1486 - acc: 0.9521 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1469 - acc: 0.9521 - val_loss: 0.3306 - val_acc: 0.8942\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1485 - acc: 0.9533 - val_loss: 0.3256 - val_acc: 0.8942\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1483 - acc: 0.9515 - val_loss: 0.3320 - val_acc: 0.8942\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1481 - acc: 0.9521 - val_loss: 0.3307 - val_acc: 0.8942\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1475 - acc: 0.9539 - val_loss: 0.3339 - val_acc: 0.8889\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1479 - acc: 0.9521 - val_loss: 0.3258 - val_acc: 0.8995\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1479 - acc: 0.9545 - val_loss: 0.3271 - val_acc: 0.8995\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1468 - acc: 0.9539 - val_loss: 0.3200 - val_acc: 0.8942\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1480 - acc: 0.9551 - val_loss: 0.3307 - val_acc: 0.8942\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1477 - acc: 0.9515 - val_loss: 0.3258 - val_acc: 0.8995\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1472 - acc: 0.9557 - val_loss: 0.3325 - val_acc: 0.8942\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1474 - acc: 0.9545 - val_loss: 0.3335 - val_acc: 0.8995\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1479 - acc: 0.9521 - val_loss: 0.3262 - val_acc: 0.8995\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1461 - acc: 0.9527 - val_loss: 0.3454 - val_acc: 0.8889\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1478 - acc: 0.9527 - val_loss: 0.3242 - val_acc: 0.9048\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1479 - acc: 0.9527 - val_loss: 0.3305 - val_acc: 0.8942\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1477 - acc: 0.9539 - val_loss: 0.3284 - val_acc: 0.8995\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1470 - acc: 0.9539 - val_loss: 0.3341 - val_acc: 0.8942\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1471 - acc: 0.9539 - val_loss: 0.3260 - val_acc: 0.8889\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1473 - acc: 0.9521 - val_loss: 0.3230 - val_acc: 0.8995\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1472 - acc: 0.9527 - val_loss: 0.3364 - val_acc: 0.8942\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1473 - acc: 0.9557 - val_loss: 0.3207 - val_acc: 0.8995\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1470 - acc: 0.9545 - val_loss: 0.3232 - val_acc: 0.8995\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1471 - acc: 0.9527 - val_loss: 0.3348 - val_acc: 0.8942\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1470 - acc: 0.9539 - val_loss: 0.3289 - val_acc: 0.8942\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1477 - acc: 0.9509 - val_loss: 0.3211 - val_acc: 0.8995\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1471 - acc: 0.9521 - val_loss: 0.3183 - val_acc: 0.8995\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1464 - acc: 0.9509 - val_loss: 0.3233 - val_acc: 0.8889\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1475 - acc: 0.9557 - val_loss: 0.3264 - val_acc: 0.8942\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1473 - acc: 0.9527 - val_loss: 0.3233 - val_acc: 0.8995\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1478 - acc: 0.9504 - val_loss: 0.3253 - val_acc: 0.8995\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1465 - acc: 0.9557 - val_loss: 0.3250 - val_acc: 0.8995\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1469 - acc: 0.9521 - val_loss: 0.3314 - val_acc: 0.8889\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1470 - acc: 0.9515 - val_loss: 0.3343 - val_acc: 0.8942\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1474 - acc: 0.9521 - val_loss: 0.3249 - val_acc: 0.8995\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1471 - acc: 0.9539 - val_loss: 0.3323 - val_acc: 0.8995\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1476 - acc: 0.9545 - val_loss: 0.3208 - val_acc: 0.8942\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1470 - acc: 0.9539 - val_loss: 0.3238 - val_acc: 0.8995\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1466 - acc: 0.9551 - val_loss: 0.3303 - val_acc: 0.8995\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1468 - acc: 0.9533 - val_loss: 0.3286 - val_acc: 0.8942\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1454 - acc: 0.9545 - val_loss: 0.3399 - val_acc: 0.8942\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1472 - acc: 0.9533 - val_loss: 0.3396 - val_acc: 0.8889\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1471 - acc: 0.9521 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1468 - acc: 0.9557 - val_loss: 0.3354 - val_acc: 0.8942\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1475 - acc: 0.9521 - val_loss: 0.3273 - val_acc: 0.8942\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1460 - acc: 0.9533 - val_loss: 0.3374 - val_acc: 0.8942\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1464 - acc: 0.9539 - val_loss: 0.3360 - val_acc: 0.8889\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1465 - acc: 0.9563 - val_loss: 0.3145 - val_acc: 0.9048\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1460 - acc: 0.9580 - val_loss: 0.3196 - val_acc: 0.9048\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1463 - acc: 0.9498 - val_loss: 0.3126 - val_acc: 0.9048\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1461 - acc: 0.9551 - val_loss: 0.3209 - val_acc: 0.9048\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1465 - acc: 0.9539 - val_loss: 0.3345 - val_acc: 0.8942\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1461 - acc: 0.9551 - val_loss: 0.3269 - val_acc: 0.8995\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1460 - acc: 0.9527 - val_loss: 0.3293 - val_acc: 0.8995\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1464 - acc: 0.9545 - val_loss: 0.3324 - val_acc: 0.8942\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1462 - acc: 0.9539 - val_loss: 0.3379 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1460 - acc: 0.9533 - val_loss: 0.3327 - val_acc: 0.8942\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1453 - acc: 0.9545 - val_loss: 0.3381 - val_acc: 0.8889\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1462 - acc: 0.9539 - val_loss: 0.3430 - val_acc: 0.8942\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1461 - acc: 0.9533 - val_loss: 0.3210 - val_acc: 0.8995\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1461 - acc: 0.9551 - val_loss: 0.3292 - val_acc: 0.8995\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1457 - acc: 0.9545 - val_loss: 0.3326 - val_acc: 0.8995\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1459 - acc: 0.9545 - val_loss: 0.3362 - val_acc: 0.8889\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1454 - acc: 0.9557 - val_loss: 0.3457 - val_acc: 0.8836\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1462 - acc: 0.9551 - val_loss: 0.3231 - val_acc: 0.9048\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1469 - acc: 0.9509 - val_loss: 0.3299 - val_acc: 0.8942\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1456 - acc: 0.9551 - val_loss: 0.3248 - val_acc: 0.8995\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1468 - acc: 0.9545 - val_loss: 0.3299 - val_acc: 0.8942\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1456 - acc: 0.9551 - val_loss: 0.3273 - val_acc: 0.8995\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1454 - acc: 0.9563 - val_loss: 0.3350 - val_acc: 0.8889\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1457 - acc: 0.9551 - val_loss: 0.3284 - val_acc: 0.8995\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1458 - acc: 0.9545 - val_loss: 0.3182 - val_acc: 0.8995\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1456 - acc: 0.9586 - val_loss: 0.3307 - val_acc: 0.8942\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1456 - acc: 0.9551 - val_loss: 0.3357 - val_acc: 0.8942\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1449 - acc: 0.9533 - val_loss: 0.3238 - val_acc: 0.8942\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1443 - acc: 0.9545 - val_loss: 0.3425 - val_acc: 0.8889\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1459 - acc: 0.9521 - val_loss: 0.3349 - val_acc: 0.8889\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1449 - acc: 0.9533 - val_loss: 0.3265 - val_acc: 0.8942\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1454 - acc: 0.9563 - val_loss: 0.3370 - val_acc: 0.8942\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1448 - acc: 0.9521 - val_loss: 0.3413 - val_acc: 0.8889\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1452 - acc: 0.9557 - val_loss: 0.3386 - val_acc: 0.8889\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1443 - acc: 0.9539 - val_loss: 0.3260 - val_acc: 0.8995\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1461 - acc: 0.9521 - val_loss: 0.3311 - val_acc: 0.8995\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1456 - acc: 0.9515 - val_loss: 0.3282 - val_acc: 0.8942\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1461 - acc: 0.9533 - val_loss: 0.3329 - val_acc: 0.8942\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1452 - acc: 0.9539 - val_loss: 0.3275 - val_acc: 0.8995\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1458 - acc: 0.9539 - val_loss: 0.3338 - val_acc: 0.8942\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1449 - acc: 0.9557 - val_loss: 0.3370 - val_acc: 0.8889\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1451 - acc: 0.9533 - val_loss: 0.3405 - val_acc: 0.8889\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1454 - acc: 0.9539 - val_loss: 0.3377 - val_acc: 0.8889\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1449 - acc: 0.9533 - val_loss: 0.3339 - val_acc: 0.8995\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1449 - acc: 0.9515 - val_loss: 0.3353 - val_acc: 0.8942\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1463 - acc: 0.9551 - val_loss: 0.3317 - val_acc: 0.8995\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1451 - acc: 0.9533 - val_loss: 0.3222 - val_acc: 0.8942\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1446 - acc: 0.9539 - val_loss: 0.3257 - val_acc: 0.8995\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1449 - acc: 0.9551 - val_loss: 0.3387 - val_acc: 0.8942\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1458 - acc: 0.9527 - val_loss: 0.3291 - val_acc: 0.8995\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1449 - acc: 0.9539 - val_loss: 0.3357 - val_acc: 0.8942\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1455 - acc: 0.9551 - val_loss: 0.3378 - val_acc: 0.8889\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1452 - acc: 0.9557 - val_loss: 0.3352 - val_acc: 0.8889\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1445 - acc: 0.9545 - val_loss: 0.3270 - val_acc: 0.8995\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1439 - acc: 0.9563 - val_loss: 0.3244 - val_acc: 0.9101\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1463 - acc: 0.9551 - val_loss: 0.3275 - val_acc: 0.9048\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1440 - acc: 0.9551 - val_loss: 0.3427 - val_acc: 0.8836\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1449 - acc: 0.9563 - val_loss: 0.3430 - val_acc: 0.8889\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1458 - acc: 0.9545 - val_loss: 0.3279 - val_acc: 0.8995\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1441 - acc: 0.9533 - val_loss: 0.3358 - val_acc: 0.8942\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1444 - acc: 0.9527 - val_loss: 0.3474 - val_acc: 0.8889\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1445 - acc: 0.9533 - val_loss: 0.3410 - val_acc: 0.8942\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1446 - acc: 0.9551 - val_loss: 0.3402 - val_acc: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Model1 Definition\n",
    "\n",
    "activation_function = ['relu', 'sigmoid', 'tanh']\n",
    "hum_sub_act_accuracy = []\n",
    "for i in range(len(activation_function)):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(12, input_dim=9, activation='relu'))\n",
    "    model1.add(Dense(8, activation= activation_function[i]))\n",
    "    model1.add(Dense(1, activation='sigmoid'))\n",
    "    model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model1, iterating on the data in batches of 32 samples\n",
    "    summary = model1.fit(x_train_human_sub, y_train_human_sub, validation_split = 0.1, nb_epoch=1000, batch_size=32)\n",
    "    scores1 = model1.evaluate(x=x_test_human_sub, y=y_test_human_sub, verbose=0)\n",
    "    scores_acc1 = scores1[1]\n",
    "    hum_sub_act_accuracy.append(scores_acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Activation Function Vs Accuracy')\n",
    "plt.plot(activation_function,hum_sub_act_accuracy,'cs-', label='Accuracy')\n",
    "plt.axis([min(activation_function), max(activation_function), min(hum_sub_act_accuracy)-0.1, max(hum_sub_act_accuracy)+0.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('activation_function')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1b3a2802e8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b3a688dd8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b3a75ecf8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b3a789eb8>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating plots for accuracy and loss using matplotlib package\n",
    "%matplotlib inline\n",
    "df1 = pd.DataFrame(summary.history)\n",
    "df1.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model Accuracy\n",
    "scores1 = model1.evaluate(x=x_test_human_sub, y=y_test_human_sub, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for Human Observed Dataset with Feature Subtraction \n",
      "\n",
      "Test Accuracy =  95%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for Human Observed Dataset with Feature Subtraction \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 656us/step - loss: 0.7767 - acc: 0.4728 - val_loss: 0.7266 - val_acc: 0.5238\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.6900 - acc: 0.6017 - val_loss: 0.6808 - val_acc: 0.6508\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.6464 - acc: 0.6862 - val_loss: 0.6538 - val_acc: 0.6825\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.6171 - acc: 0.7193 - val_loss: 0.6311 - val_acc: 0.6984\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.5923 - acc: 0.7358 - val_loss: 0.6131 - val_acc: 0.6931\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.5704 - acc: 0.7459 - val_loss: 0.5967 - val_acc: 0.6984\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.5510 - acc: 0.7595 - val_loss: 0.5819 - val_acc: 0.7090\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.5340 - acc: 0.7654 - val_loss: 0.5690 - val_acc: 0.7302\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.5186 - acc: 0.7754 - val_loss: 0.5577 - val_acc: 0.7249\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.5042 - acc: 0.7855 - val_loss: 0.5462 - val_acc: 0.7196\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.4913 - acc: 0.7926 - val_loss: 0.5367 - val_acc: 0.7143\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.4788 - acc: 0.7996 - val_loss: 0.5252 - val_acc: 0.7143\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4674 - acc: 0.8067 - val_loss: 0.5173 - val_acc: 0.7143\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.4559 - acc: 0.8091 - val_loss: 0.5085 - val_acc: 0.7143\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.4446 - acc: 0.8180 - val_loss: 0.4988 - val_acc: 0.7249\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.4346 - acc: 0.8209 - val_loss: 0.4924 - val_acc: 0.7302\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.4246 - acc: 0.8298 - val_loss: 0.4842 - val_acc: 0.7354\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.4154 - acc: 0.8339 - val_loss: 0.4751 - val_acc: 0.7407\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.4065 - acc: 0.8357 - val_loss: 0.4706 - val_acc: 0.7460\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3982 - acc: 0.8428 - val_loss: 0.4646 - val_acc: 0.7619\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3896 - acc: 0.8469 - val_loss: 0.4586 - val_acc: 0.7778\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3824 - acc: 0.8493 - val_loss: 0.4483 - val_acc: 0.7778\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3749 - acc: 0.8517 - val_loss: 0.4424 - val_acc: 0.7937\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3678 - acc: 0.8576 - val_loss: 0.4359 - val_acc: 0.8042\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3609 - acc: 0.8617 - val_loss: 0.4298 - val_acc: 0.7937\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3547 - acc: 0.8611 - val_loss: 0.4253 - val_acc: 0.8042\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3475 - acc: 0.8682 - val_loss: 0.4228 - val_acc: 0.8095\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3433 - acc: 0.8688 - val_loss: 0.4181 - val_acc: 0.8095\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3376 - acc: 0.8717 - val_loss: 0.4127 - val_acc: 0.8095\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3327 - acc: 0.8735 - val_loss: 0.4149 - val_acc: 0.8148\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3285 - acc: 0.8759 - val_loss: 0.4098 - val_acc: 0.8148\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3246 - acc: 0.8729 - val_loss: 0.4080 - val_acc: 0.8148\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3207 - acc: 0.8759 - val_loss: 0.4024 - val_acc: 0.8148\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.3166 - acc: 0.8818 - val_loss: 0.4017 - val_acc: 0.8148\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.3137 - acc: 0.8865 - val_loss: 0.3979 - val_acc: 0.8095\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.3094 - acc: 0.8842 - val_loss: 0.3910 - val_acc: 0.8254\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3061 - acc: 0.8824 - val_loss: 0.3995 - val_acc: 0.8148\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3029 - acc: 0.8865 - val_loss: 0.3918 - val_acc: 0.8201\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2984 - acc: 0.8930 - val_loss: 0.3889 - val_acc: 0.8254\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2966 - acc: 0.8924 - val_loss: 0.3932 - val_acc: 0.8201\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2934 - acc: 0.8936 - val_loss: 0.3857 - val_acc: 0.8307\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2905 - acc: 0.8978 - val_loss: 0.3888 - val_acc: 0.8254\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2877 - acc: 0.8924 - val_loss: 0.3804 - val_acc: 0.8413\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2852 - acc: 0.8948 - val_loss: 0.3858 - val_acc: 0.8307\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2828 - acc: 0.8960 - val_loss: 0.3863 - val_acc: 0.8307\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2809 - acc: 0.8966 - val_loss: 0.3787 - val_acc: 0.8360\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2781 - acc: 0.8930 - val_loss: 0.3809 - val_acc: 0.8360\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2758 - acc: 0.8966 - val_loss: 0.3808 - val_acc: 0.8413\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2734 - acc: 0.8972 - val_loss: 0.3746 - val_acc: 0.8360\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2718 - acc: 0.8983 - val_loss: 0.3797 - val_acc: 0.8413\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2697 - acc: 0.8995 - val_loss: 0.3742 - val_acc: 0.8413\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2676 - acc: 0.9001 - val_loss: 0.3758 - val_acc: 0.8413\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2657 - acc: 0.9007 - val_loss: 0.3720 - val_acc: 0.8466\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2640 - acc: 0.9037 - val_loss: 0.3688 - val_acc: 0.8519\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.2617 - acc: 0.9043 - val_loss: 0.3726 - val_acc: 0.8519\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2597 - acc: 0.9043 - val_loss: 0.3733 - val_acc: 0.8413\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2585 - acc: 0.9048 - val_loss: 0.3688 - val_acc: 0.8519\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2560 - acc: 0.9084 - val_loss: 0.3628 - val_acc: 0.8624\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.2558 - acc: 0.9054 - val_loss: 0.3646 - val_acc: 0.8466\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.2541 - acc: 0.9078 - val_loss: 0.3665 - val_acc: 0.8360\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 147us/step - loss: 0.2524 - acc: 0.9072 - val_loss: 0.3645 - val_acc: 0.8519\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.2515 - acc: 0.9096 - val_loss: 0.3627 - val_acc: 0.8571\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2499 - acc: 0.9096 - val_loss: 0.3637 - val_acc: 0.8360\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2484 - acc: 0.9113 - val_loss: 0.3682 - val_acc: 0.8466\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2475 - acc: 0.9125 - val_loss: 0.3618 - val_acc: 0.8624\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2460 - acc: 0.9143 - val_loss: 0.3687 - val_acc: 0.8466\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2455 - acc: 0.9119 - val_loss: 0.3603 - val_acc: 0.8624\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2440 - acc: 0.9125 - val_loss: 0.3586 - val_acc: 0.8677\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2429 - acc: 0.9125 - val_loss: 0.3640 - val_acc: 0.8519\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2414 - acc: 0.9137 - val_loss: 0.3541 - val_acc: 0.8677\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2408 - acc: 0.9131 - val_loss: 0.3573 - val_acc: 0.8624\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.2385 - acc: 0.9137 - val_loss: 0.3673 - val_acc: 0.8519\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.2394 - acc: 0.9167 - val_loss: 0.3598 - val_acc: 0.8571\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2383 - acc: 0.9161 - val_loss: 0.3591 - val_acc: 0.8677\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2371 - acc: 0.9149 - val_loss: 0.3637 - val_acc: 0.8571\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2361 - acc: 0.9149 - val_loss: 0.3580 - val_acc: 0.8677\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2359 - acc: 0.9155 - val_loss: 0.3624 - val_acc: 0.8571\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2353 - acc: 0.9173 - val_loss: 0.3619 - val_acc: 0.8730\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2341 - acc: 0.9173 - val_loss: 0.3646 - val_acc: 0.8624\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2336 - acc: 0.9178 - val_loss: 0.3624 - val_acc: 0.8624\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2327 - acc: 0.9155 - val_loss: 0.3582 - val_acc: 0.8730\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2324 - acc: 0.9167 - val_loss: 0.3581 - val_acc: 0.8730\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2310 - acc: 0.9190 - val_loss: 0.3574 - val_acc: 0.8677\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2305 - acc: 0.9202 - val_loss: 0.3543 - val_acc: 0.8677\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2298 - acc: 0.9196 - val_loss: 0.3601 - val_acc: 0.8677\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2289 - acc: 0.9214 - val_loss: 0.3671 - val_acc: 0.8677\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2287 - acc: 0.9214 - val_loss: 0.3622 - val_acc: 0.8624\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2281 - acc: 0.9208 - val_loss: 0.3606 - val_acc: 0.8624\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2274 - acc: 0.9214 - val_loss: 0.3638 - val_acc: 0.8624\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2268 - acc: 0.9190 - val_loss: 0.3629 - val_acc: 0.8677\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2263 - acc: 0.9184 - val_loss: 0.3580 - val_acc: 0.8677\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2256 - acc: 0.9184 - val_loss: 0.3576 - val_acc: 0.8624\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2254 - acc: 0.9202 - val_loss: 0.3576 - val_acc: 0.8677\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2241 - acc: 0.9202 - val_loss: 0.3603 - val_acc: 0.8677\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2242 - acc: 0.9184 - val_loss: 0.3590 - val_acc: 0.8624\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.2233 - acc: 0.9238 - val_loss: 0.3573 - val_acc: 0.8624\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2228 - acc: 0.9196 - val_loss: 0.3660 - val_acc: 0.8730\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.3625 - val_acc: 0.8624\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2223 - acc: 0.9226 - val_loss: 0.3608 - val_acc: 0.8624\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2215 - acc: 0.9220 - val_loss: 0.3599 - val_acc: 0.8677\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2212 - acc: 0.9232 - val_loss: 0.3591 - val_acc: 0.8624\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2209 - acc: 0.9238 - val_loss: 0.3645 - val_acc: 0.8677\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2205 - acc: 0.9243 - val_loss: 0.3536 - val_acc: 0.8624\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2198 - acc: 0.9232 - val_loss: 0.3537 - val_acc: 0.8677\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2199 - acc: 0.9232 - val_loss: 0.3601 - val_acc: 0.8624\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2195 - acc: 0.9232 - val_loss: 0.3593 - val_acc: 0.8624\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2193 - acc: 0.9249 - val_loss: 0.3613 - val_acc: 0.8624\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2188 - acc: 0.9232 - val_loss: 0.3619 - val_acc: 0.8624\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2177 - acc: 0.9273 - val_loss: 0.3571 - val_acc: 0.8624\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2176 - acc: 0.9249 - val_loss: 0.3573 - val_acc: 0.8624\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2169 - acc: 0.9267 - val_loss: 0.3657 - val_acc: 0.8624\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2167 - acc: 0.9261 - val_loss: 0.3689 - val_acc: 0.8677\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2165 - acc: 0.9273 - val_loss: 0.3591 - val_acc: 0.8677\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2156 - acc: 0.9249 - val_loss: 0.3723 - val_acc: 0.8677\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2163 - acc: 0.9243 - val_loss: 0.3628 - val_acc: 0.8677\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2160 - acc: 0.9261 - val_loss: 0.3586 - val_acc: 0.8624\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2149 - acc: 0.9273 - val_loss: 0.3553 - val_acc: 0.8677\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2146 - acc: 0.9297 - val_loss: 0.3561 - val_acc: 0.8624\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2143 - acc: 0.9267 - val_loss: 0.3620 - val_acc: 0.8624\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2134 - acc: 0.9279 - val_loss: 0.3564 - val_acc: 0.8624\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2137 - acc: 0.9267 - val_loss: 0.3582 - val_acc: 0.8624\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2135 - acc: 0.9285 - val_loss: 0.3634 - val_acc: 0.8677\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2127 - acc: 0.9303 - val_loss: 0.3588 - val_acc: 0.8624\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2118 - acc: 0.9285 - val_loss: 0.3583 - val_acc: 0.8677\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2116 - acc: 0.9267 - val_loss: 0.3625 - val_acc: 0.8677\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2117 - acc: 0.9297 - val_loss: 0.3597 - val_acc: 0.8624\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2106 - acc: 0.9291 - val_loss: 0.3678 - val_acc: 0.8624\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2106 - acc: 0.9285 - val_loss: 0.3637 - val_acc: 0.8677\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2103 - acc: 0.9267 - val_loss: 0.3553 - val_acc: 0.8677\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2100 - acc: 0.9267 - val_loss: 0.3576 - val_acc: 0.8624\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2097 - acc: 0.9309 - val_loss: 0.3612 - val_acc: 0.8624\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2092 - acc: 0.9273 - val_loss: 0.3570 - val_acc: 0.8677\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2092 - acc: 0.9303 - val_loss: 0.3573 - val_acc: 0.8624\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2088 - acc: 0.9297 - val_loss: 0.3605 - val_acc: 0.8624\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2082 - acc: 0.9309 - val_loss: 0.3618 - val_acc: 0.8624\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2080 - acc: 0.9303 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2076 - acc: 0.9320 - val_loss: 0.3663 - val_acc: 0.8571\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2069 - acc: 0.9309 - val_loss: 0.3686 - val_acc: 0.8677\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2067 - acc: 0.9291 - val_loss: 0.3683 - val_acc: 0.8571\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2067 - acc: 0.9326 - val_loss: 0.3672 - val_acc: 0.8624\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2062 - acc: 0.9326 - val_loss: 0.3630 - val_acc: 0.8677\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2063 - acc: 0.9303 - val_loss: 0.3628 - val_acc: 0.8677\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2057 - acc: 0.9320 - val_loss: 0.3640 - val_acc: 0.8624\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2055 - acc: 0.9291 - val_loss: 0.3635 - val_acc: 0.8677\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2049 - acc: 0.9332 - val_loss: 0.3617 - val_acc: 0.8624\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2052 - acc: 0.9338 - val_loss: 0.3554 - val_acc: 0.8624\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2047 - acc: 0.9309 - val_loss: 0.3571 - val_acc: 0.8571\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2042 - acc: 0.9344 - val_loss: 0.3590 - val_acc: 0.8624\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2044 - acc: 0.9297 - val_loss: 0.3624 - val_acc: 0.8624\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2038 - acc: 0.9314 - val_loss: 0.3578 - val_acc: 0.8624\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2028 - acc: 0.9297 - val_loss: 0.3608 - val_acc: 0.8624\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2028 - acc: 0.9314 - val_loss: 0.3619 - val_acc: 0.8624\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2023 - acc: 0.9326 - val_loss: 0.3528 - val_acc: 0.8730\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2030 - acc: 0.9344 - val_loss: 0.3560 - val_acc: 0.8624\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2015 - acc: 0.9332 - val_loss: 0.3620 - val_acc: 0.8677\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2018 - acc: 0.9338 - val_loss: 0.3579 - val_acc: 0.8677\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2017 - acc: 0.9356 - val_loss: 0.3533 - val_acc: 0.8571\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2017 - acc: 0.9344 - val_loss: 0.3533 - val_acc: 0.8624\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2015 - acc: 0.9332 - val_loss: 0.3551 - val_acc: 0.8624\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2009 - acc: 0.9326 - val_loss: 0.3575 - val_acc: 0.8677\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2006 - acc: 0.9338 - val_loss: 0.3611 - val_acc: 0.8624\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1996 - acc: 0.9338 - val_loss: 0.3582 - val_acc: 0.8571\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1998 - acc: 0.9350 - val_loss: 0.3507 - val_acc: 0.8730\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1996 - acc: 0.9326 - val_loss: 0.3672 - val_acc: 0.8624\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1991 - acc: 0.9338 - val_loss: 0.3498 - val_acc: 0.8624\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1992 - acc: 0.9344 - val_loss: 0.3611 - val_acc: 0.8624\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1995 - acc: 0.9344 - val_loss: 0.3537 - val_acc: 0.8730\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1997 - acc: 0.9332 - val_loss: 0.3533 - val_acc: 0.8677\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1985 - acc: 0.9356 - val_loss: 0.3572 - val_acc: 0.8677\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1989 - acc: 0.9356 - val_loss: 0.3534 - val_acc: 0.8677\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1980 - acc: 0.9362 - val_loss: 0.3477 - val_acc: 0.8783\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.1974 - acc: 0.9338 - val_loss: 0.3552 - val_acc: 0.8624\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1978 - acc: 0.9338 - val_loss: 0.3564 - val_acc: 0.8677\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1968 - acc: 0.9332 - val_loss: 0.3578 - val_acc: 0.8677\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 136us/step - loss: 0.1972 - acc: 0.9350 - val_loss: 0.3490 - val_acc: 0.8730\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1977 - acc: 0.9338 - val_loss: 0.3493 - val_acc: 0.8783\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1970 - acc: 0.9332 - val_loss: 0.3477 - val_acc: 0.8783\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1965 - acc: 0.9350 - val_loss: 0.3525 - val_acc: 0.8677\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1976 - acc: 0.9338 - val_loss: 0.3532 - val_acc: 0.8677\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1957 - acc: 0.9362 - val_loss: 0.3570 - val_acc: 0.8677\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1966 - acc: 0.9356 - val_loss: 0.3478 - val_acc: 0.8730\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1952 - acc: 0.9368 - val_loss: 0.3463 - val_acc: 0.8783\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1950 - acc: 0.9338 - val_loss: 0.3512 - val_acc: 0.8783\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1950 - acc: 0.9374 - val_loss: 0.3484 - val_acc: 0.8783\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.1949 - acc: 0.9362 - val_loss: 0.3597 - val_acc: 0.8730\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1946 - acc: 0.9368 - val_loss: 0.3494 - val_acc: 0.8836\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1947 - acc: 0.9374 - val_loss: 0.3479 - val_acc: 0.8730\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1946 - acc: 0.9356 - val_loss: 0.3484 - val_acc: 0.8836\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1937 - acc: 0.9350 - val_loss: 0.3512 - val_acc: 0.8836\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1933 - acc: 0.9379 - val_loss: 0.3457 - val_acc: 0.8783\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1938 - acc: 0.9356 - val_loss: 0.3485 - val_acc: 0.8889\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1930 - acc: 0.9362 - val_loss: 0.3452 - val_acc: 0.8889\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1932 - acc: 0.9385 - val_loss: 0.3452 - val_acc: 0.8942\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1919 - acc: 0.9350 - val_loss: 0.3534 - val_acc: 0.8889\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1931 - acc: 0.9379 - val_loss: 0.3414 - val_acc: 0.8942\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1922 - acc: 0.9350 - val_loss: 0.3474 - val_acc: 0.8783\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1930 - acc: 0.9397 - val_loss: 0.3458 - val_acc: 0.8942\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1920 - acc: 0.9362 - val_loss: 0.3454 - val_acc: 0.8942\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1918 - acc: 0.9391 - val_loss: 0.3447 - val_acc: 0.8889\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1916 - acc: 0.9368 - val_loss: 0.3430 - val_acc: 0.8836\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1918 - acc: 0.9368 - val_loss: 0.3483 - val_acc: 0.8942\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1918 - acc: 0.9368 - val_loss: 0.3483 - val_acc: 0.8942\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1911 - acc: 0.9391 - val_loss: 0.3381 - val_acc: 0.8942\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.3487 - val_acc: 0.8889\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1899 - acc: 0.9356 - val_loss: 0.3518 - val_acc: 0.8889\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1906 - acc: 0.9385 - val_loss: 0.3509 - val_acc: 0.8836\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1907 - acc: 0.9403 - val_loss: 0.3442 - val_acc: 0.8889\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1895 - acc: 0.9409 - val_loss: 0.3485 - val_acc: 0.8942\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1888 - acc: 0.9362 - val_loss: 0.3553 - val_acc: 0.8889\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1888 - acc: 0.9397 - val_loss: 0.3422 - val_acc: 0.8730\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1889 - acc: 0.9397 - val_loss: 0.3527 - val_acc: 0.8889\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1893 - acc: 0.9374 - val_loss: 0.3476 - val_acc: 0.8942\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1884 - acc: 0.9403 - val_loss: 0.3569 - val_acc: 0.8836\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1892 - acc: 0.9409 - val_loss: 0.3463 - val_acc: 0.8942\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1877 - acc: 0.9397 - val_loss: 0.3528 - val_acc: 0.8942\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1881 - acc: 0.9409 - val_loss: 0.3573 - val_acc: 0.8836\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1881 - acc: 0.9385 - val_loss: 0.3475 - val_acc: 0.8942\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1877 - acc: 0.9409 - val_loss: 0.3559 - val_acc: 0.8836\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1879 - acc: 0.9403 - val_loss: 0.3472 - val_acc: 0.8889\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1872 - acc: 0.9403 - val_loss: 0.3452 - val_acc: 0.8942\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1868 - acc: 0.9403 - val_loss: 0.3420 - val_acc: 0.8889\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1866 - acc: 0.9397 - val_loss: 0.3494 - val_acc: 0.8942\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1870 - acc: 0.9409 - val_loss: 0.3469 - val_acc: 0.8889\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1865 - acc: 0.9403 - val_loss: 0.3505 - val_acc: 0.8836\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1865 - acc: 0.9409 - val_loss: 0.3452 - val_acc: 0.8836\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1859 - acc: 0.9397 - val_loss: 0.3491 - val_acc: 0.8889\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1860 - acc: 0.9397 - val_loss: 0.3422 - val_acc: 0.8889\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1850 - acc: 0.9391 - val_loss: 0.3525 - val_acc: 0.8889\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1861 - acc: 0.9409 - val_loss: 0.3446 - val_acc: 0.8942\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1852 - acc: 0.9391 - val_loss: 0.3476 - val_acc: 0.8942\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1838 - acc: 0.9421 - val_loss: 0.3414 - val_acc: 0.8836\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1853 - acc: 0.9415 - val_loss: 0.3569 - val_acc: 0.8889\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1846 - acc: 0.9421 - val_loss: 0.3455 - val_acc: 0.8783\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1847 - acc: 0.9421 - val_loss: 0.3508 - val_acc: 0.8889\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1854 - acc: 0.9409 - val_loss: 0.3502 - val_acc: 0.8889\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1841 - acc: 0.9403 - val_loss: 0.3546 - val_acc: 0.8836\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1845 - acc: 0.9403 - val_loss: 0.3503 - val_acc: 0.8783\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1845 - acc: 0.9421 - val_loss: 0.3518 - val_acc: 0.8889\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1838 - acc: 0.9409 - val_loss: 0.3513 - val_acc: 0.8889\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1831 - acc: 0.9409 - val_loss: 0.3518 - val_acc: 0.8836\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1843 - acc: 0.9397 - val_loss: 0.3515 - val_acc: 0.8889\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1832 - acc: 0.9403 - val_loss: 0.3569 - val_acc: 0.8889\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1835 - acc: 0.9397 - val_loss: 0.3499 - val_acc: 0.8889\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1835 - acc: 0.9421 - val_loss: 0.3472 - val_acc: 0.8942\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1827 - acc: 0.9415 - val_loss: 0.3509 - val_acc: 0.8889\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1835 - acc: 0.9427 - val_loss: 0.3491 - val_acc: 0.8889\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1822 - acc: 0.9409 - val_loss: 0.3421 - val_acc: 0.8836\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1814 - acc: 0.9397 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1824 - acc: 0.9397 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1818 - acc: 0.9403 - val_loss: 0.3480 - val_acc: 0.8889\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1824 - acc: 0.9409 - val_loss: 0.3490 - val_acc: 0.8889\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1817 - acc: 0.9409 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1816 - acc: 0.9409 - val_loss: 0.3518 - val_acc: 0.8889\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1817 - acc: 0.9415 - val_loss: 0.3506 - val_acc: 0.8942\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1809 - acc: 0.9409 - val_loss: 0.3519 - val_acc: 0.8889\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1814 - acc: 0.9415 - val_loss: 0.3550 - val_acc: 0.8783\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1813 - acc: 0.9427 - val_loss: 0.3451 - val_acc: 0.8889\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1819 - acc: 0.9409 - val_loss: 0.3584 - val_acc: 0.8836\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1810 - acc: 0.9415 - val_loss: 0.3507 - val_acc: 0.8942\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1809 - acc: 0.9415 - val_loss: 0.3452 - val_acc: 0.8836\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1804 - acc: 0.9415 - val_loss: 0.3479 - val_acc: 0.8836\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1803 - acc: 0.9403 - val_loss: 0.3417 - val_acc: 0.8836\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1801 - acc: 0.9421 - val_loss: 0.3503 - val_acc: 0.8889\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1813 - acc: 0.9415 - val_loss: 0.3481 - val_acc: 0.8889\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1804 - acc: 0.9409 - val_loss: 0.3566 - val_acc: 0.8942\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1803 - acc: 0.9421 - val_loss: 0.3506 - val_acc: 0.8889\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1800 - acc: 0.9409 - val_loss: 0.3559 - val_acc: 0.8942\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1810 - acc: 0.9409 - val_loss: 0.3530 - val_acc: 0.8942\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1806 - acc: 0.9415 - val_loss: 0.3460 - val_acc: 0.8889\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1802 - acc: 0.9409 - val_loss: 0.3484 - val_acc: 0.8942\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1805 - acc: 0.9421 - val_loss: 0.3507 - val_acc: 0.8942\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1798 - acc: 0.9409 - val_loss: 0.3539 - val_acc: 0.8889\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1797 - acc: 0.9415 - val_loss: 0.3486 - val_acc: 0.8942\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1796 - acc: 0.9403 - val_loss: 0.3488 - val_acc: 0.8836\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1805 - acc: 0.9415 - val_loss: 0.3514 - val_acc: 0.8889\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1797 - acc: 0.9427 - val_loss: 0.3534 - val_acc: 0.8836\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1800 - acc: 0.9397 - val_loss: 0.3502 - val_acc: 0.8889\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1792 - acc: 0.9409 - val_loss: 0.3469 - val_acc: 0.8836\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1798 - acc: 0.9403 - val_loss: 0.3514 - val_acc: 0.8889\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1790 - acc: 0.9415 - val_loss: 0.3559 - val_acc: 0.8942\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.1788 - acc: 0.9427 - val_loss: 0.3605 - val_acc: 0.8942\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1791 - acc: 0.9409 - val_loss: 0.3496 - val_acc: 0.8889\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1789 - acc: 0.9415 - val_loss: 0.3449 - val_acc: 0.8889\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1791 - acc: 0.9403 - val_loss: 0.3479 - val_acc: 0.8836\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1783 - acc: 0.9415 - val_loss: 0.3618 - val_acc: 0.8889\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1791 - acc: 0.9409 - val_loss: 0.3495 - val_acc: 0.8942\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1780 - acc: 0.9421 - val_loss: 0.3542 - val_acc: 0.8942\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1780 - acc: 0.9403 - val_loss: 0.3544 - val_acc: 0.8889\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1787 - acc: 0.9415 - val_loss: 0.3521 - val_acc: 0.8942\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1784 - acc: 0.9427 - val_loss: 0.3496 - val_acc: 0.8942\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1784 - acc: 0.9421 - val_loss: 0.3500 - val_acc: 0.8942\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1767 - acc: 0.9421 - val_loss: 0.3515 - val_acc: 0.8836\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1785 - acc: 0.9415 - val_loss: 0.3547 - val_acc: 0.8942\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1769 - acc: 0.9415 - val_loss: 0.3549 - val_acc: 0.8889\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1773 - acc: 0.9409 - val_loss: 0.3496 - val_acc: 0.8783\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1777 - acc: 0.9415 - val_loss: 0.3508 - val_acc: 0.8889\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1777 - acc: 0.9415 - val_loss: 0.3505 - val_acc: 0.8836\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1774 - acc: 0.9397 - val_loss: 0.3524 - val_acc: 0.8836\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1771 - acc: 0.9397 - val_loss: 0.3550 - val_acc: 0.8889\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.1777 - acc: 0.9433 - val_loss: 0.3522 - val_acc: 0.8942\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1766 - acc: 0.9409 - val_loss: 0.3499 - val_acc: 0.8889\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1774 - acc: 0.9403 - val_loss: 0.3546 - val_acc: 0.8889\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1770 - acc: 0.9415 - val_loss: 0.3508 - val_acc: 0.8889\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1777 - acc: 0.9421 - val_loss: 0.3583 - val_acc: 0.8889\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1771 - acc: 0.9433 - val_loss: 0.3641 - val_acc: 0.8836\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 145us/step - loss: 0.1765 - acc: 0.9427 - val_loss: 0.3539 - val_acc: 0.8942\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1776 - acc: 0.9409 - val_loss: 0.3545 - val_acc: 0.8889\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.1767 - acc: 0.9427 - val_loss: 0.3573 - val_acc: 0.8889\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 157us/step - loss: 0.1767 - acc: 0.9391 - val_loss: 0.3604 - val_acc: 0.8942\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1766 - acc: 0.9415 - val_loss: 0.3601 - val_acc: 0.8889\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1767 - acc: 0.9415 - val_loss: 0.3515 - val_acc: 0.8889\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1757 - acc: 0.9421 - val_loss: 0.3549 - val_acc: 0.8942\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1769 - acc: 0.9397 - val_loss: 0.3578 - val_acc: 0.8942\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1758 - acc: 0.9409 - val_loss: 0.3540 - val_acc: 0.8889\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1766 - acc: 0.9409 - val_loss: 0.3550 - val_acc: 0.8942\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1754 - acc: 0.9409 - val_loss: 0.3606 - val_acc: 0.8942\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1753 - acc: 0.9433 - val_loss: 0.3513 - val_acc: 0.8942\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1752 - acc: 0.9427 - val_loss: 0.3530 - val_acc: 0.8889\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1758 - acc: 0.9439 - val_loss: 0.3524 - val_acc: 0.8889\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1757 - acc: 0.9409 - val_loss: 0.3546 - val_acc: 0.8889\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1757 - acc: 0.9409 - val_loss: 0.3574 - val_acc: 0.8889\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1754 - acc: 0.9427 - val_loss: 0.3547 - val_acc: 0.8942\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1741 - acc: 0.9433 - val_loss: 0.3504 - val_acc: 0.8889\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1765 - acc: 0.9409 - val_loss: 0.3540 - val_acc: 0.8942\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1752 - acc: 0.9427 - val_loss: 0.3610 - val_acc: 0.8889\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1751 - acc: 0.9421 - val_loss: 0.3647 - val_acc: 0.8942\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 102us/step - loss: 0.1759 - acc: 0.9415 - val_loss: 0.3554 - val_acc: 0.8942\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 147us/step - loss: 0.1746 - acc: 0.9421 - val_loss: 0.3553 - val_acc: 0.8942\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 182us/step - loss: 0.1749 - acc: 0.9421 - val_loss: 0.3550 - val_acc: 0.8942\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 137us/step - loss: 0.1745 - acc: 0.9439 - val_loss: 0.3536 - val_acc: 0.8942\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1743 - acc: 0.9427 - val_loss: 0.3550 - val_acc: 0.8889\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1742 - acc: 0.9415 - val_loss: 0.3569 - val_acc: 0.8942\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1740 - acc: 0.9427 - val_loss: 0.3608 - val_acc: 0.8942\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.1739 - acc: 0.9439 - val_loss: 0.3516 - val_acc: 0.8889\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1740 - acc: 0.9439 - val_loss: 0.3604 - val_acc: 0.8995\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1734 - acc: 0.9427 - val_loss: 0.3609 - val_acc: 0.8889\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1731 - acc: 0.9433 - val_loss: 0.3605 - val_acc: 0.8942\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1740 - acc: 0.9403 - val_loss: 0.3642 - val_acc: 0.8942\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1739 - acc: 0.9433 - val_loss: 0.3562 - val_acc: 0.8889\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1735 - acc: 0.9439 - val_loss: 0.3575 - val_acc: 0.8942\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1733 - acc: 0.9427 - val_loss: 0.3577 - val_acc: 0.8942\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1732 - acc: 0.9421 - val_loss: 0.3624 - val_acc: 0.8889\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1733 - acc: 0.9433 - val_loss: 0.3642 - val_acc: 0.8889\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1734 - acc: 0.9427 - val_loss: 0.3651 - val_acc: 0.8889\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1733 - acc: 0.9427 - val_loss: 0.3570 - val_acc: 0.8889\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1723 - acc: 0.9421 - val_loss: 0.3613 - val_acc: 0.8942\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1733 - acc: 0.9433 - val_loss: 0.3571 - val_acc: 0.8942\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1732 - acc: 0.9439 - val_loss: 0.3577 - val_acc: 0.8942\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1725 - acc: 0.9421 - val_loss: 0.3610 - val_acc: 0.8942\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1726 - acc: 0.9433 - val_loss: 0.3593 - val_acc: 0.8889\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1724 - acc: 0.9433 - val_loss: 0.3590 - val_acc: 0.8942\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1723 - acc: 0.9403 - val_loss: 0.3717 - val_acc: 0.8889\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1726 - acc: 0.9433 - val_loss: 0.3609 - val_acc: 0.8942\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1721 - acc: 0.9439 - val_loss: 0.3634 - val_acc: 0.8942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1717 - acc: 0.9444 - val_loss: 0.3563 - val_acc: 0.8889\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1717 - acc: 0.9427 - val_loss: 0.3624 - val_acc: 0.8942\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1716 - acc: 0.9450 - val_loss: 0.3646 - val_acc: 0.8942\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1728 - acc: 0.947 - 0s 55us/step - loss: 0.1712 - acc: 0.9433 - val_loss: 0.3622 - val_acc: 0.8942\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1718 - acc: 0.9427 - val_loss: 0.3618 - val_acc: 0.8995\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1715 - acc: 0.9421 - val_loss: 0.3594 - val_acc: 0.8995\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1716 - acc: 0.9421 - val_loss: 0.3624 - val_acc: 0.8995\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1713 - acc: 0.9439 - val_loss: 0.3658 - val_acc: 0.8995\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1713 - acc: 0.9427 - val_loss: 0.3638 - val_acc: 0.8889\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1710 - acc: 0.9450 - val_loss: 0.3702 - val_acc: 0.8942\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1703 - acc: 0.9450 - val_loss: 0.3685 - val_acc: 0.8995\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1720 - acc: 0.9427 - val_loss: 0.3641 - val_acc: 0.8889\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1714 - acc: 0.9421 - val_loss: 0.3613 - val_acc: 0.8889\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1711 - acc: 0.9421 - val_loss: 0.3623 - val_acc: 0.8783\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1711 - acc: 0.9415 - val_loss: 0.3684 - val_acc: 0.8889\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1700 - acc: 0.9433 - val_loss: 0.3709 - val_acc: 0.8942\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1707 - acc: 0.9439 - val_loss: 0.3704 - val_acc: 0.8889\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1711 - acc: 0.9433 - val_loss: 0.3674 - val_acc: 0.8889\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1708 - acc: 0.9439 - val_loss: 0.3705 - val_acc: 0.8942\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1701 - acc: 0.9433 - val_loss: 0.3772 - val_acc: 0.8942\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1705 - acc: 0.9439 - val_loss: 0.3647 - val_acc: 0.8942\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1694 - acc: 0.9427 - val_loss: 0.3806 - val_acc: 0.8995\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1705 - acc: 0.9427 - val_loss: 0.3667 - val_acc: 0.8836\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1702 - acc: 0.9444 - val_loss: 0.3709 - val_acc: 0.8942\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1703 - acc: 0.9439 - val_loss: 0.3738 - val_acc: 0.8889\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1703 - acc: 0.9433 - val_loss: 0.3751 - val_acc: 0.8942\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1690 - acc: 0.9427 - val_loss: 0.3648 - val_acc: 0.8836\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1701 - acc: 0.9427 - val_loss: 0.3782 - val_acc: 0.8836\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1699 - acc: 0.9450 - val_loss: 0.3743 - val_acc: 0.8836\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1698 - acc: 0.9421 - val_loss: 0.3750 - val_acc: 0.8942\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1694 - acc: 0.9427 - val_loss: 0.3690 - val_acc: 0.8783\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1698 - acc: 0.9439 - val_loss: 0.3709 - val_acc: 0.8836\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1696 - acc: 0.9427 - val_loss: 0.3726 - val_acc: 0.8889\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1692 - acc: 0.9439 - val_loss: 0.3754 - val_acc: 0.8889\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1690 - acc: 0.9427 - val_loss: 0.3829 - val_acc: 0.8889\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1696 - acc: 0.9433 - val_loss: 0.3733 - val_acc: 0.8889\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1691 - acc: 0.9433 - val_loss: 0.3779 - val_acc: 0.8889\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1692 - acc: 0.9427 - val_loss: 0.3706 - val_acc: 0.8889\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1687 - acc: 0.9450 - val_loss: 0.3795 - val_acc: 0.8889\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1679 - acc: 0.9439 - val_loss: 0.3673 - val_acc: 0.8889\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1686 - acc: 0.9439 - val_loss: 0.3790 - val_acc: 0.8942\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1688 - acc: 0.9439 - val_loss: 0.3825 - val_acc: 0.8889\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1688 - acc: 0.9439 - val_loss: 0.3788 - val_acc: 0.8889\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1687 - acc: 0.9450 - val_loss: 0.3767 - val_acc: 0.8836\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1689 - acc: 0.9433 - val_loss: 0.3755 - val_acc: 0.8783\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1685 - acc: 0.9427 - val_loss: 0.3764 - val_acc: 0.8889\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1683 - acc: 0.9433 - val_loss: 0.3779 - val_acc: 0.8942\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1686 - acc: 0.9444 - val_loss: 0.3757 - val_acc: 0.8836\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1684 - acc: 0.9433 - val_loss: 0.3765 - val_acc: 0.8889\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1676 - acc: 0.9439 - val_loss: 0.3751 - val_acc: 0.8836\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1675 - acc: 0.9439 - val_loss: 0.3873 - val_acc: 0.8995\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1677 - acc: 0.9433 - val_loss: 0.3792 - val_acc: 0.8942\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1679 - acc: 0.9450 - val_loss: 0.3824 - val_acc: 0.8942\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1678 - acc: 0.9450 - val_loss: 0.3848 - val_acc: 0.8942\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1688 - acc: 0.9427 - val_loss: 0.3817 - val_acc: 0.8942\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1680 - acc: 0.9433 - val_loss: 0.3768 - val_acc: 0.8942\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1673 - acc: 0.9450 - val_loss: 0.3734 - val_acc: 0.8783\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1684 - acc: 0.9433 - val_loss: 0.3776 - val_acc: 0.8942\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1669 - acc: 0.9427 - val_loss: 0.3714 - val_acc: 0.8942\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1671 - acc: 0.9421 - val_loss: 0.3807 - val_acc: 0.8889\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1671 - acc: 0.9444 - val_loss: 0.3790 - val_acc: 0.8942\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1669 - acc: 0.9450 - val_loss: 0.3783 - val_acc: 0.8942\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1680 - acc: 0.9433 - val_loss: 0.3753 - val_acc: 0.8889\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1675 - acc: 0.9439 - val_loss: 0.3739 - val_acc: 0.8942\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1673 - acc: 0.9462 - val_loss: 0.3805 - val_acc: 0.8889\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1676 - acc: 0.9433 - val_loss: 0.3768 - val_acc: 0.8942\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1676 - acc: 0.9444 - val_loss: 0.3798 - val_acc: 0.8942\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1668 - acc: 0.9439 - val_loss: 0.3808 - val_acc: 0.8942\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1669 - acc: 0.9433 - val_loss: 0.3855 - val_acc: 0.8942\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1670 - acc: 0.9439 - val_loss: 0.3788 - val_acc: 0.8889\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1663 - acc: 0.9439 - val_loss: 0.3794 - val_acc: 0.8942\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1668 - acc: 0.9444 - val_loss: 0.3765 - val_acc: 0.8889\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1669 - acc: 0.9450 - val_loss: 0.3770 - val_acc: 0.8942\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1663 - acc: 0.9427 - val_loss: 0.3762 - val_acc: 0.8889\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1664 - acc: 0.9450 - val_loss: 0.3786 - val_acc: 0.8995\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1660 - acc: 0.9439 - val_loss: 0.3834 - val_acc: 0.8942\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1667 - acc: 0.9439 - val_loss: 0.3806 - val_acc: 0.8889\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1664 - acc: 0.9427 - val_loss: 0.3837 - val_acc: 0.8942\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1672 - acc: 0.9450 - val_loss: 0.3772 - val_acc: 0.8889\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1667 - acc: 0.9427 - val_loss: 0.3765 - val_acc: 0.8995\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1652 - acc: 0.9450 - val_loss: 0.3763 - val_acc: 0.8889\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1662 - acc: 0.9480 - val_loss: 0.3852 - val_acc: 0.8942\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1661 - acc: 0.9439 - val_loss: 0.3824 - val_acc: 0.8995\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1662 - acc: 0.9444 - val_loss: 0.3812 - val_acc: 0.8995\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1662 - acc: 0.9444 - val_loss: 0.3844 - val_acc: 0.8889\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1664 - acc: 0.9439 - val_loss: 0.3802 - val_acc: 0.8889\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1661 - acc: 0.9444 - val_loss: 0.3806 - val_acc: 0.8995\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1660 - acc: 0.9421 - val_loss: 0.3761 - val_acc: 0.8889\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1647 - acc: 0.9444 - val_loss: 0.3905 - val_acc: 0.8995\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1664 - acc: 0.9450 - val_loss: 0.3804 - val_acc: 0.8995\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1654 - acc: 0.9450 - val_loss: 0.3851 - val_acc: 0.8942\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1661 - acc: 0.9409 - val_loss: 0.3864 - val_acc: 0.8942\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1639 - acc: 0.9456 - val_loss: 0.3828 - val_acc: 0.8995\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1650 - acc: 0.9444 - val_loss: 0.3796 - val_acc: 0.8836\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1657 - acc: 0.9444 - val_loss: 0.3798 - val_acc: 0.8889\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1662 - acc: 0.9450 - val_loss: 0.3835 - val_acc: 0.8995\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1645 - acc: 0.9409 - val_loss: 0.3935 - val_acc: 0.8889\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1656 - acc: 0.9415 - val_loss: 0.3854 - val_acc: 0.8995\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1652 - acc: 0.9433 - val_loss: 0.3827 - val_acc: 0.8995\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1652 - acc: 0.9444 - val_loss: 0.3921 - val_acc: 0.8942\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1653 - acc: 0.9439 - val_loss: 0.3895 - val_acc: 0.8889\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1644 - acc: 0.9439 - val_loss: 0.3830 - val_acc: 0.8836\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1647 - acc: 0.9450 - val_loss: 0.3866 - val_acc: 0.8889\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1645 - acc: 0.9433 - val_loss: 0.4014 - val_acc: 0.8836\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1656 - acc: 0.9427 - val_loss: 0.3914 - val_acc: 0.8942\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1646 - acc: 0.9439 - val_loss: 0.3871 - val_acc: 0.8836\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1637 - acc: 0.9456 - val_loss: 0.4011 - val_acc: 0.8889\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1653 - acc: 0.9439 - val_loss: 0.3929 - val_acc: 0.8942\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1651 - acc: 0.9427 - val_loss: 0.3859 - val_acc: 0.8942\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1652 - acc: 0.9433 - val_loss: 0.3848 - val_acc: 0.8942\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1643 - acc: 0.9433 - val_loss: 0.3957 - val_acc: 0.8836\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1645 - acc: 0.9439 - val_loss: 0.3873 - val_acc: 0.8995\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1653 - acc: 0.9439 - val_loss: 0.3889 - val_acc: 0.8942\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1643 - acc: 0.9439 - val_loss: 0.3867 - val_acc: 0.8889\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1636 - acc: 0.9456 - val_loss: 0.3924 - val_acc: 0.8942\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1643 - acc: 0.9415 - val_loss: 0.3779 - val_acc: 0.8836\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1631 - acc: 0.9456 - val_loss: 0.3803 - val_acc: 0.8836\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1643 - acc: 0.9439 - val_loss: 0.3932 - val_acc: 0.8942\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1644 - acc: 0.9433 - val_loss: 0.3912 - val_acc: 0.8942\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1638 - acc: 0.9456 - val_loss: 0.3980 - val_acc: 0.8942\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1644 - acc: 0.9403 - val_loss: 0.3809 - val_acc: 0.8889\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1636 - acc: 0.9456 - val_loss: 0.3850 - val_acc: 0.8889\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1639 - acc: 0.9439 - val_loss: 0.3950 - val_acc: 0.8889\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1630 - acc: 0.9444 - val_loss: 0.3905 - val_acc: 0.8889\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1638 - acc: 0.9450 - val_loss: 0.3821 - val_acc: 0.8836\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1622 - acc: 0.9462 - val_loss: 0.3992 - val_acc: 0.8942\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1633 - acc: 0.9456 - val_loss: 0.3922 - val_acc: 0.8942\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1634 - acc: 0.9450 - val_loss: 0.3898 - val_acc: 0.8942\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1630 - acc: 0.9444 - val_loss: 0.3831 - val_acc: 0.8942\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1640 - acc: 0.9450 - val_loss: 0.3894 - val_acc: 0.8783\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1632 - acc: 0.9415 - val_loss: 0.3876 - val_acc: 0.8836\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1627 - acc: 0.9450 - val_loss: 0.3949 - val_acc: 0.8783\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1631 - acc: 0.9439 - val_loss: 0.3814 - val_acc: 0.8836\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1634 - acc: 0.9450 - val_loss: 0.3931 - val_acc: 0.8836\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1628 - acc: 0.9444 - val_loss: 0.3932 - val_acc: 0.8889\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1631 - acc: 0.9444 - val_loss: 0.3925 - val_acc: 0.8889\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1627 - acc: 0.9456 - val_loss: 0.3898 - val_acc: 0.8836\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1631 - acc: 0.9439 - val_loss: 0.3866 - val_acc: 0.8889\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1624 - acc: 0.9462 - val_loss: 0.3841 - val_acc: 0.8836\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1634 - acc: 0.9439 - val_loss: 0.3872 - val_acc: 0.8783\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1619 - acc: 0.9450 - val_loss: 0.3865 - val_acc: 0.8836\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1623 - acc: 0.9474 - val_loss: 0.3885 - val_acc: 0.8836\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1625 - acc: 0.9439 - val_loss: 0.3917 - val_acc: 0.8783\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1622 - acc: 0.9456 - val_loss: 0.3898 - val_acc: 0.8783\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1621 - acc: 0.9456 - val_loss: 0.3900 - val_acc: 0.8836\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9474 - val_loss: 0.3847 - val_acc: 0.8783\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1619 - acc: 0.9433 - val_loss: 0.3977 - val_acc: 0.8730\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1631 - acc: 0.9439 - val_loss: 0.3958 - val_acc: 0.8836\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1623 - acc: 0.9450 - val_loss: 0.3935 - val_acc: 0.8836\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1618 - acc: 0.9480 - val_loss: 0.3919 - val_acc: 0.8836\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1620 - acc: 0.9450 - val_loss: 0.3928 - val_acc: 0.8836\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1620 - acc: 0.9439 - val_loss: 0.3880 - val_acc: 0.8730\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1609 - acc: 0.9462 - val_loss: 0.3955 - val_acc: 0.8836\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1625 - acc: 0.9450 - val_loss: 0.3809 - val_acc: 0.8730\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1622 - acc: 0.9439 - val_loss: 0.3837 - val_acc: 0.8836\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1618 - acc: 0.9444 - val_loss: 0.3884 - val_acc: 0.8889\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1618 - acc: 0.9462 - val_loss: 0.3892 - val_acc: 0.8836\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1604 - acc: 0.9468 - val_loss: 0.3876 - val_acc: 0.8783\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1614 - acc: 0.9462 - val_loss: 0.4002 - val_acc: 0.8783\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1613 - acc: 0.9433 - val_loss: 0.3938 - val_acc: 0.8836\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1615 - acc: 0.9456 - val_loss: 0.3998 - val_acc: 0.8783\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1624 - acc: 0.9456 - val_loss: 0.3890 - val_acc: 0.8730\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1626 - acc: 0.9433 - val_loss: 0.3870 - val_acc: 0.8730\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1601 - acc: 0.9474 - val_loss: 0.3880 - val_acc: 0.8730\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1621 - acc: 0.9450 - val_loss: 0.3950 - val_acc: 0.8730\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1616 - acc: 0.9450 - val_loss: 0.3906 - val_acc: 0.8783\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1615 - acc: 0.9474 - val_loss: 0.3870 - val_acc: 0.8783\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.1596 - acc: 0.9462 - val_loss: 0.4097 - val_acc: 0.8836\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1621 - acc: 0.9456 - val_loss: 0.3882 - val_acc: 0.8783\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1616 - acc: 0.9450 - val_loss: 0.3921 - val_acc: 0.8836\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1609 - acc: 0.9456 - val_loss: 0.3911 - val_acc: 0.8730\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1613 - acc: 0.9439 - val_loss: 0.3893 - val_acc: 0.8783\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1608 - acc: 0.9450 - val_loss: 0.3881 - val_acc: 0.8836\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1604 - acc: 0.9409 - val_loss: 0.3900 - val_acc: 0.8783\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1606 - acc: 0.9450 - val_loss: 0.3947 - val_acc: 0.8730\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1607 - acc: 0.9462 - val_loss: 0.4020 - val_acc: 0.8783\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1615 - acc: 0.9474 - val_loss: 0.3941 - val_acc: 0.8783\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.1612 - acc: 0.9462 - val_loss: 0.3920 - val_acc: 0.8783\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1604 - acc: 0.9468 - val_loss: 0.3918 - val_acc: 0.8730\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1608 - acc: 0.9439 - val_loss: 0.3893 - val_acc: 0.8783\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1609 - acc: 0.9456 - val_loss: 0.4035 - val_acc: 0.8730\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1615 - acc: 0.9450 - val_loss: 0.3968 - val_acc: 0.8783\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1596 - acc: 0.9468 - val_loss: 0.3947 - val_acc: 0.8783\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1615 - acc: 0.9456 - val_loss: 0.3930 - val_acc: 0.8783\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1601 - acc: 0.9468 - val_loss: 0.3913 - val_acc: 0.8783\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1617 - acc: 0.9468 - val_loss: 0.3929 - val_acc: 0.8783\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1597 - acc: 0.9468 - val_loss: 0.4000 - val_acc: 0.8730\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1610 - acc: 0.9427 - val_loss: 0.3930 - val_acc: 0.8783\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1602 - acc: 0.9456 - val_loss: 0.3845 - val_acc: 0.8730\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1609 - acc: 0.9468 - val_loss: 0.3941 - val_acc: 0.8783\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1597 - acc: 0.9427 - val_loss: 0.3894 - val_acc: 0.8730\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1607 - acc: 0.9462 - val_loss: 0.4005 - val_acc: 0.8836\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1592 - acc: 0.9468 - val_loss: 0.3893 - val_acc: 0.8730\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1598 - acc: 0.9468 - val_loss: 0.3933 - val_acc: 0.8730\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1608 - acc: 0.9450 - val_loss: 0.3969 - val_acc: 0.8836\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1602 - acc: 0.9462 - val_loss: 0.4012 - val_acc: 0.8836\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1601 - acc: 0.9480 - val_loss: 0.3946 - val_acc: 0.8783\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1602 - acc: 0.9456 - val_loss: 0.3918 - val_acc: 0.8783\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1603 - acc: 0.9456 - val_loss: 0.3882 - val_acc: 0.8836\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1599 - acc: 0.9474 - val_loss: 0.3928 - val_acc: 0.8783\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9474 - val_loss: 0.4078 - val_acc: 0.8836\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1593 - acc: 0.9468 - val_loss: 0.3936 - val_acc: 0.8783\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1600 - acc: 0.9474 - val_loss: 0.3877 - val_acc: 0.8836\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1594 - acc: 0.9450 - val_loss: 0.3993 - val_acc: 0.8783\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1599 - acc: 0.9439 - val_loss: 0.3852 - val_acc: 0.8836\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1590 - acc: 0.9480 - val_loss: 0.3993 - val_acc: 0.8783\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1591 - acc: 0.9462 - val_loss: 0.3906 - val_acc: 0.8836\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1596 - acc: 0.9439 - val_loss: 0.3813 - val_acc: 0.8836\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1589 - acc: 0.9456 - val_loss: 0.3832 - val_acc: 0.8836\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1595 - acc: 0.9468 - val_loss: 0.3869 - val_acc: 0.8783\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.3977 - val_acc: 0.8836\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1585 - acc: 0.9474 - val_loss: 0.4036 - val_acc: 0.8783\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.3908 - val_acc: 0.8783\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1583 - acc: 0.9480 - val_loss: 0.3834 - val_acc: 0.8889\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1598 - acc: 0.9468 - val_loss: 0.3819 - val_acc: 0.8889\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1584 - acc: 0.9474 - val_loss: 0.3990 - val_acc: 0.8889\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1602 - acc: 0.9462 - val_loss: 0.3889 - val_acc: 0.8836\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1587 - acc: 0.9468 - val_loss: 0.3945 - val_acc: 0.8889\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1585 - acc: 0.9486 - val_loss: 0.3983 - val_acc: 0.8783\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1584 - acc: 0.9468 - val_loss: 0.3935 - val_acc: 0.8783\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1594 - acc: 0.9439 - val_loss: 0.3930 - val_acc: 0.8783\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1576 - acc: 0.9456 - val_loss: 0.4026 - val_acc: 0.8836\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1587 - acc: 0.9474 - val_loss: 0.3950 - val_acc: 0.8836\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1587 - acc: 0.9468 - val_loss: 0.3950 - val_acc: 0.8783\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1591 - acc: 0.9468 - val_loss: 0.3927 - val_acc: 0.8836\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1587 - acc: 0.9456 - val_loss: 0.3928 - val_acc: 0.8836\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1583 - acc: 0.9462 - val_loss: 0.4017 - val_acc: 0.8677\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1585 - acc: 0.9486 - val_loss: 0.3885 - val_acc: 0.8783\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1597 - acc: 0.9468 - val_loss: 0.3864 - val_acc: 0.8889\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1580 - acc: 0.9486 - val_loss: 0.3838 - val_acc: 0.8783\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1584 - acc: 0.9474 - val_loss: 0.3996 - val_acc: 0.8783\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1587 - acc: 0.9462 - val_loss: 0.3911 - val_acc: 0.8783\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1583 - acc: 0.9468 - val_loss: 0.3933 - val_acc: 0.8783\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1575 - acc: 0.9480 - val_loss: 0.3869 - val_acc: 0.8836\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1587 - acc: 0.9474 - val_loss: 0.3939 - val_acc: 0.8783\n",
      "Epoch 590/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1568 - acc: 0.9468 - val_loss: 0.3935 - val_acc: 0.8836\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1580 - acc: 0.9456 - val_loss: 0.3968 - val_acc: 0.8783\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1582 - acc: 0.9468 - val_loss: 0.3865 - val_acc: 0.8889\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1578 - acc: 0.9468 - val_loss: 0.3882 - val_acc: 0.8889\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1574 - acc: 0.9468 - val_loss: 0.3907 - val_acc: 0.8836\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1582 - acc: 0.9462 - val_loss: 0.3936 - val_acc: 0.8783\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1581 - acc: 0.9462 - val_loss: 0.3921 - val_acc: 0.8836\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1574 - acc: 0.9462 - val_loss: 0.4117 - val_acc: 0.8783\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1573 - acc: 0.9492 - val_loss: 0.3914 - val_acc: 0.8836\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1583 - acc: 0.9444 - val_loss: 0.3912 - val_acc: 0.8889\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1574 - acc: 0.9474 - val_loss: 0.4000 - val_acc: 0.8836\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1578 - acc: 0.9480 - val_loss: 0.3960 - val_acc: 0.8889\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1577 - acc: 0.9462 - val_loss: 0.3927 - val_acc: 0.8836\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1582 - acc: 0.9474 - val_loss: 0.3899 - val_acc: 0.8783\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1568 - acc: 0.9462 - val_loss: 0.3981 - val_acc: 0.8783\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1572 - acc: 0.9439 - val_loss: 0.4005 - val_acc: 0.8889\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1579 - acc: 0.9480 - val_loss: 0.3966 - val_acc: 0.8889\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1580 - acc: 0.9486 - val_loss: 0.4019 - val_acc: 0.8836\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1571 - acc: 0.9474 - val_loss: 0.4116 - val_acc: 0.8783\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1584 - acc: 0.9450 - val_loss: 0.3907 - val_acc: 0.8889\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1573 - acc: 0.9456 - val_loss: 0.3925 - val_acc: 0.8783\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1583 - acc: 0.9474 - val_loss: 0.3926 - val_acc: 0.8836\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1582 - acc: 0.9456 - val_loss: 0.3988 - val_acc: 0.8783\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1571 - acc: 0.9456 - val_loss: 0.4005 - val_acc: 0.8836\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1577 - acc: 0.9462 - val_loss: 0.3985 - val_acc: 0.8836\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1566 - acc: 0.9456 - val_loss: 0.3941 - val_acc: 0.8783\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1575 - acc: 0.9433 - val_loss: 0.4003 - val_acc: 0.8783\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1571 - acc: 0.9456 - val_loss: 0.3993 - val_acc: 0.8783\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.1568 - acc: 0.9450 - val_loss: 0.3896 - val_acc: 0.8783\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.1571 - acc: 0.9462 - val_loss: 0.3906 - val_acc: 0.8836\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1558 - acc: 0.9474 - val_loss: 0.3994 - val_acc: 0.8836\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1579 - acc: 0.9468 - val_loss: 0.4020 - val_acc: 0.8783\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1566 - acc: 0.9480 - val_loss: 0.4070 - val_acc: 0.8783\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1573 - acc: 0.9462 - val_loss: 0.3968 - val_acc: 0.8836\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1573 - acc: 0.9456 - val_loss: 0.4003 - val_acc: 0.8836\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1572 - acc: 0.9456 - val_loss: 0.4114 - val_acc: 0.8783\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1569 - acc: 0.9492 - val_loss: 0.4004 - val_acc: 0.8730\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1578 - acc: 0.9439 - val_loss: 0.4010 - val_acc: 0.8783\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1568 - acc: 0.9462 - val_loss: 0.4022 - val_acc: 0.8836\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1570 - acc: 0.9462 - val_loss: 0.3974 - val_acc: 0.8783\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1563 - acc: 0.9456 - val_loss: 0.4019 - val_acc: 0.8836\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1579 - acc: 0.9450 - val_loss: 0.3945 - val_acc: 0.8889\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.3984 - val_acc: 0.8889\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1571 - acc: 0.9444 - val_loss: 0.4056 - val_acc: 0.8783\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1576 - acc: 0.9462 - val_loss: 0.4087 - val_acc: 0.8836\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1571 - acc: 0.9456 - val_loss: 0.3991 - val_acc: 0.8730\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1577 - acc: 0.9468 - val_loss: 0.4035 - val_acc: 0.8730\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 127us/step - loss: 0.1567 - acc: 0.9456 - val_loss: 0.3990 - val_acc: 0.8730\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.1557 - acc: 0.9474 - val_loss: 0.4068 - val_acc: 0.8783\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1577 - acc: 0.9444 - val_loss: 0.4013 - val_acc: 0.8730\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.1565 - acc: 0.9439 - val_loss: 0.4001 - val_acc: 0.8783\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.1572 - acc: 0.9468 - val_loss: 0.4045 - val_acc: 0.8783\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.3979 - val_acc: 0.8730\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.1568 - acc: 0.9462 - val_loss: 0.4008 - val_acc: 0.8836\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.1557 - acc: 0.9492 - val_loss: 0.3952 - val_acc: 0.8836\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.3932 - val_acc: 0.8836\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.1572 - acc: 0.9486 - val_loss: 0.4002 - val_acc: 0.8783\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.1567 - acc: 0.9456 - val_loss: 0.4022 - val_acc: 0.8836\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.1571 - acc: 0.9468 - val_loss: 0.4039 - val_acc: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1572 - acc: 0.9444 - val_loss: 0.3972 - val_acc: 0.8783\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1561 - acc: 0.9468 - val_loss: 0.3987 - val_acc: 0.8889\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.4106 - val_acc: 0.8836\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1564 - acc: 0.9462 - val_loss: 0.3887 - val_acc: 0.8836\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1565 - acc: 0.9474 - val_loss: 0.4031 - val_acc: 0.8783\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1566 - acc: 0.9474 - val_loss: 0.4188 - val_acc: 0.8836\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1570 - acc: 0.9456 - val_loss: 0.4028 - val_acc: 0.8783\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1559 - acc: 0.9444 - val_loss: 0.4031 - val_acc: 0.8889\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1562 - acc: 0.9480 - val_loss: 0.4103 - val_acc: 0.8836\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1567 - acc: 0.9450 - val_loss: 0.4033 - val_acc: 0.8783\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1564 - acc: 0.9468 - val_loss: 0.4040 - val_acc: 0.8783\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1564 - acc: 0.9456 - val_loss: 0.4102 - val_acc: 0.8836\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1563 - acc: 0.9468 - val_loss: 0.4034 - val_acc: 0.8836\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1567 - acc: 0.9468 - val_loss: 0.3975 - val_acc: 0.8836\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1560 - acc: 0.9486 - val_loss: 0.4003 - val_acc: 0.8836\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1562 - acc: 0.9492 - val_loss: 0.4074 - val_acc: 0.8836\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1557 - acc: 0.9480 - val_loss: 0.4167 - val_acc: 0.8836\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1569 - acc: 0.9456 - val_loss: 0.4033 - val_acc: 0.8783\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1565 - acc: 0.9468 - val_loss: 0.4052 - val_acc: 0.8836\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1561 - acc: 0.9456 - val_loss: 0.4148 - val_acc: 0.8783\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1562 - acc: 0.9462 - val_loss: 0.3916 - val_acc: 0.8783\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1563 - acc: 0.9480 - val_loss: 0.4012 - val_acc: 0.8836\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1563 - acc: 0.9456 - val_loss: 0.3990 - val_acc: 0.8836\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1566 - acc: 0.9468 - val_loss: 0.4062 - val_acc: 0.8836\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1560 - acc: 0.9498 - val_loss: 0.4168 - val_acc: 0.8783\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1549 - acc: 0.9474 - val_loss: 0.4213 - val_acc: 0.8836\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1557 - acc: 0.9468 - val_loss: 0.4184 - val_acc: 0.8836\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1560 - acc: 0.9492 - val_loss: 0.4195 - val_acc: 0.8783\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1566 - acc: 0.9444 - val_loss: 0.4058 - val_acc: 0.8783\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1560 - acc: 0.9474 - val_loss: 0.4095 - val_acc: 0.8836\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.4135 - val_acc: 0.8783\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1560 - acc: 0.9474 - val_loss: 0.4046 - val_acc: 0.8783\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1568 - acc: 0.9468 - val_loss: 0.3983 - val_acc: 0.8836\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1559 - acc: 0.9480 - val_loss: 0.4029 - val_acc: 0.8730\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1550 - acc: 0.9504 - val_loss: 0.4001 - val_acc: 0.8783\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1559 - acc: 0.9462 - val_loss: 0.4000 - val_acc: 0.8836\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1552 - acc: 0.9480 - val_loss: 0.4143 - val_acc: 0.8836\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1563 - acc: 0.9462 - val_loss: 0.4130 - val_acc: 0.8783\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1545 - acc: 0.9504 - val_loss: 0.4161 - val_acc: 0.8783\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.3974 - val_acc: 0.8836\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1558 - acc: 0.9474 - val_loss: 0.3956 - val_acc: 0.8836\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1568 - acc: 0.9450 - val_loss: 0.4012 - val_acc: 0.8783\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1560 - acc: 0.9450 - val_loss: 0.4037 - val_acc: 0.8836\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1554 - acc: 0.9492 - val_loss: 0.4011 - val_acc: 0.8836\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1567 - acc: 0.9450 - val_loss: 0.4088 - val_acc: 0.8783\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1555 - acc: 0.9468 - val_loss: 0.3983 - val_acc: 0.8836\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1556 - acc: 0.9450 - val_loss: 0.3931 - val_acc: 0.8836\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.1541 - acc: 0.9468 - val_loss: 0.4115 - val_acc: 0.8783\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1562 - acc: 0.9462 - val_loss: 0.4014 - val_acc: 0.8889\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.1567 - acc: 0.9456 - val_loss: 0.3958 - val_acc: 0.8836\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.1551 - acc: 0.9474 - val_loss: 0.4008 - val_acc: 0.8783\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1558 - acc: 0.9486 - val_loss: 0.4065 - val_acc: 0.8783\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1554 - acc: 0.9456 - val_loss: 0.4043 - val_acc: 0.8783\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1559 - acc: 0.9498 - val_loss: 0.4099 - val_acc: 0.8783\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1548 - acc: 0.9462 - val_loss: 0.4181 - val_acc: 0.8836\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1572 - acc: 0.9468 - val_loss: 0.3935 - val_acc: 0.8836\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1564 - acc: 0.9462 - val_loss: 0.3962 - val_acc: 0.8836\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1568 - acc: 0.9462 - val_loss: 0.3930 - val_acc: 0.8836\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1550 - acc: 0.9474 - val_loss: 0.3913 - val_acc: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.1559 - acc: 0.9480 - val_loss: 0.3906 - val_acc: 0.8836\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9492 - val_loss: 0.3990 - val_acc: 0.8836\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1552 - acc: 0.9444 - val_loss: 0.4143 - val_acc: 0.8889\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1563 - acc: 0.9450 - val_loss: 0.3910 - val_acc: 0.8783\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.1555 - acc: 0.9468 - val_loss: 0.3995 - val_acc: 0.8836\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1555 - acc: 0.9486 - val_loss: 0.3980 - val_acc: 0.8783\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.3965 - val_acc: 0.8783\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1548 - acc: 0.9474 - val_loss: 0.4099 - val_acc: 0.8889\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1567 - acc: 0.9474 - val_loss: 0.3989 - val_acc: 0.8783\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1560 - acc: 0.9498 - val_loss: 0.3994 - val_acc: 0.8836\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9468 - val_loss: 0.4042 - val_acc: 0.8836\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1555 - acc: 0.9456 - val_loss: 0.4023 - val_acc: 0.8836\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1559 - acc: 0.9462 - val_loss: 0.4026 - val_acc: 0.8783\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1562 - acc: 0.9474 - val_loss: 0.4013 - val_acc: 0.8783\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1554 - acc: 0.9492 - val_loss: 0.4008 - val_acc: 0.8730\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1557 - acc: 0.9468 - val_loss: 0.4050 - val_acc: 0.8730\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1541 - acc: 0.9474 - val_loss: 0.4159 - val_acc: 0.8889\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1564 - acc: 0.9450 - val_loss: 0.3929 - val_acc: 0.8836\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1557 - acc: 0.9468 - val_loss: 0.4013 - val_acc: 0.8783\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1560 - acc: 0.9444 - val_loss: 0.3965 - val_acc: 0.8783\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1556 - acc: 0.9444 - val_loss: 0.4043 - val_acc: 0.8783\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1550 - acc: 0.9468 - val_loss: 0.4068 - val_acc: 0.8783\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1559 - acc: 0.9462 - val_loss: 0.4059 - val_acc: 0.8783\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1549 - acc: 0.9468 - val_loss: 0.4048 - val_acc: 0.8836\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1551 - acc: 0.9498 - val_loss: 0.4013 - val_acc: 0.8783\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1557 - acc: 0.9480 - val_loss: 0.3983 - val_acc: 0.8783\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1550 - acc: 0.9427 - val_loss: 0.4125 - val_acc: 0.8783\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1555 - acc: 0.9427 - val_loss: 0.3975 - val_acc: 0.8836\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1553 - acc: 0.9450 - val_loss: 0.4047 - val_acc: 0.8836\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1550 - acc: 0.9462 - val_loss: 0.4058 - val_acc: 0.8783\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1554 - acc: 0.9456 - val_loss: 0.4145 - val_acc: 0.8836\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.4031 - val_acc: 0.8783\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1553 - acc: 0.9462 - val_loss: 0.4017 - val_acc: 0.8783\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1554 - acc: 0.9474 - val_loss: 0.4022 - val_acc: 0.8783\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1550 - acc: 0.9474 - val_loss: 0.4027 - val_acc: 0.8783\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1551 - acc: 0.9486 - val_loss: 0.3933 - val_acc: 0.8836\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1550 - acc: 0.9474 - val_loss: 0.4178 - val_acc: 0.8783\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1553 - acc: 0.9456 - val_loss: 0.4023 - val_acc: 0.8836\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1564 - acc: 0.9474 - val_loss: 0.3970 - val_acc: 0.8783\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1549 - acc: 0.9468 - val_loss: 0.4124 - val_acc: 0.8783\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1559 - acc: 0.9450 - val_loss: 0.4118 - val_acc: 0.8889\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1552 - acc: 0.9462 - val_loss: 0.4055 - val_acc: 0.8783\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1555 - acc: 0.9462 - val_loss: 0.4127 - val_acc: 0.8836\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1551 - acc: 0.9486 - val_loss: 0.4125 - val_acc: 0.8730\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1547 - acc: 0.9492 - val_loss: 0.4000 - val_acc: 0.8730\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1548 - acc: 0.9450 - val_loss: 0.4073 - val_acc: 0.8730\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1550 - acc: 0.9456 - val_loss: 0.4081 - val_acc: 0.8836\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1551 - acc: 0.9468 - val_loss: 0.4042 - val_acc: 0.8730\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1562 - acc: 0.9456 - val_loss: 0.4133 - val_acc: 0.8783\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1546 - acc: 0.9480 - val_loss: 0.4150 - val_acc: 0.8783\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1560 - acc: 0.9462 - val_loss: 0.4014 - val_acc: 0.8783\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1540 - acc: 0.9456 - val_loss: 0.3981 - val_acc: 0.8730\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1554 - acc: 0.9456 - val_loss: 0.4042 - val_acc: 0.8783\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1545 - acc: 0.9450 - val_loss: 0.4029 - val_acc: 0.8783\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1556 - acc: 0.9462 - val_loss: 0.4130 - val_acc: 0.8783\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1539 - acc: 0.9486 - val_loss: 0.4015 - val_acc: 0.8783\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.4098 - val_acc: 0.8783\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1550 - acc: 0.9480 - val_loss: 0.4105 - val_acc: 0.8783\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1548 - acc: 0.9444 - val_loss: 0.4002 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1552 - acc: 0.9480 - val_loss: 0.4011 - val_acc: 0.8783\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1548 - acc: 0.9468 - val_loss: 0.4051 - val_acc: 0.8836\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1543 - acc: 0.9474 - val_loss: 0.3980 - val_acc: 0.8836\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1546 - acc: 0.9498 - val_loss: 0.4094 - val_acc: 0.8836\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1555 - acc: 0.9480 - val_loss: 0.3968 - val_acc: 0.8783\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1556 - acc: 0.9480 - val_loss: 0.4040 - val_acc: 0.8730\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1543 - acc: 0.9444 - val_loss: 0.4093 - val_acc: 0.8783\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1547 - acc: 0.9480 - val_loss: 0.3995 - val_acc: 0.8783\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1538 - acc: 0.9456 - val_loss: 0.3906 - val_acc: 0.8783\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1556 - acc: 0.9433 - val_loss: 0.4026 - val_acc: 0.8783\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1545 - acc: 0.9468 - val_loss: 0.4064 - val_acc: 0.8783\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1555 - acc: 0.9474 - val_loss: 0.4093 - val_acc: 0.8836\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1555 - acc: 0.9462 - val_loss: 0.4040 - val_acc: 0.8836\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1543 - acc: 0.9456 - val_loss: 0.4197 - val_acc: 0.8783\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1556 - acc: 0.9456 - val_loss: 0.4073 - val_acc: 0.8836\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1550 - acc: 0.9468 - val_loss: 0.4213 - val_acc: 0.8836\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1557 - acc: 0.9450 - val_loss: 0.4089 - val_acc: 0.8836\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1543 - acc: 0.9462 - val_loss: 0.4178 - val_acc: 0.8836\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1537 - acc: 0.9486 - val_loss: 0.4034 - val_acc: 0.8783\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1558 - acc: 0.9456 - val_loss: 0.4060 - val_acc: 0.8783\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1543 - acc: 0.9474 - val_loss: 0.4138 - val_acc: 0.8836\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1541 - acc: 0.9439 - val_loss: 0.4225 - val_acc: 0.8836\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1553 - acc: 0.9486 - val_loss: 0.4116 - val_acc: 0.8836\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1545 - acc: 0.9492 - val_loss: 0.4056 - val_acc: 0.8783\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1555 - acc: 0.9486 - val_loss: 0.4098 - val_acc: 0.8836\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1540 - acc: 0.9468 - val_loss: 0.4067 - val_acc: 0.8836\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1553 - acc: 0.9468 - val_loss: 0.3962 - val_acc: 0.8783\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1548 - acc: 0.9462 - val_loss: 0.4051 - val_acc: 0.8783\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1559 - acc: 0.9480 - val_loss: 0.4047 - val_acc: 0.8836\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1537 - acc: 0.9474 - val_loss: 0.4022 - val_acc: 0.8889\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1546 - acc: 0.9498 - val_loss: 0.4099 - val_acc: 0.8836\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1554 - acc: 0.9474 - val_loss: 0.4069 - val_acc: 0.8836\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1536 - acc: 0.9480 - val_loss: 0.3874 - val_acc: 0.8889\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1554 - acc: 0.9444 - val_loss: 0.4091 - val_acc: 0.8836\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1529 - acc: 0.9486 - val_loss: 0.4103 - val_acc: 0.8836\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1545 - acc: 0.9456 - val_loss: 0.4034 - val_acc: 0.8783\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1532 - acc: 0.9492 - val_loss: 0.3968 - val_acc: 0.8836\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1550 - acc: 0.9450 - val_loss: 0.3976 - val_acc: 0.8836\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1544 - acc: 0.9439 - val_loss: 0.3872 - val_acc: 0.8836\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1537 - acc: 0.9468 - val_loss: 0.4024 - val_acc: 0.8836\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1535 - acc: 0.9480 - val_loss: 0.3988 - val_acc: 0.8836\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1544 - acc: 0.9462 - val_loss: 0.4024 - val_acc: 0.8836\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1545 - acc: 0.9486 - val_loss: 0.4009 - val_acc: 0.8836\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1524 - acc: 0.9486 - val_loss: 0.4003 - val_acc: 0.8783\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1526 - acc: 0.9480 - val_loss: 0.4067 - val_acc: 0.8783\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1540 - acc: 0.9462 - val_loss: 0.4060 - val_acc: 0.8836\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1521 - acc: 0.9474 - val_loss: 0.3980 - val_acc: 0.8836\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1537 - acc: 0.9468 - val_loss: 0.4053 - val_acc: 0.8783\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1535 - acc: 0.9468 - val_loss: 0.4043 - val_acc: 0.8730\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1535 - acc: 0.9468 - val_loss: 0.3929 - val_acc: 0.8783\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1527 - acc: 0.9486 - val_loss: 0.4171 - val_acc: 0.8836\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1536 - acc: 0.9498 - val_loss: 0.4012 - val_acc: 0.8836\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1528 - acc: 0.9468 - val_loss: 0.4077 - val_acc: 0.8783\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1522 - acc: 0.9468 - val_loss: 0.4059 - val_acc: 0.8836\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1544 - acc: 0.9480 - val_loss: 0.4048 - val_acc: 0.8783\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1527 - acc: 0.9444 - val_loss: 0.4102 - val_acc: 0.8783\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1536 - acc: 0.9468 - val_loss: 0.3959 - val_acc: 0.8783\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1524 - acc: 0.9444 - val_loss: 0.4109 - val_acc: 0.8836\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.4079 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1526 - acc: 0.9480 - val_loss: 0.4102 - val_acc: 0.8836\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1541 - acc: 0.9468 - val_loss: 0.4039 - val_acc: 0.8836\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1520 - acc: 0.9468 - val_loss: 0.4172 - val_acc: 0.8836\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1529 - acc: 0.9468 - val_loss: 0.4197 - val_acc: 0.8889\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1530 - acc: 0.9462 - val_loss: 0.4001 - val_acc: 0.8889\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1529 - acc: 0.9474 - val_loss: 0.4051 - val_acc: 0.8889\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1528 - acc: 0.9462 - val_loss: 0.4140 - val_acc: 0.8836\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1535 - acc: 0.9439 - val_loss: 0.4025 - val_acc: 0.8836\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1527 - acc: 0.9468 - val_loss: 0.4256 - val_acc: 0.8836\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1536 - acc: 0.9468 - val_loss: 0.4072 - val_acc: 0.8889\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1524 - acc: 0.9498 - val_loss: 0.3987 - val_acc: 0.8889\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1533 - acc: 0.9480 - val_loss: 0.4002 - val_acc: 0.8889\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1517 - acc: 0.9486 - val_loss: 0.4062 - val_acc: 0.8836\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1527 - acc: 0.9492 - val_loss: 0.4079 - val_acc: 0.8836\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1534 - acc: 0.9456 - val_loss: 0.4109 - val_acc: 0.8836\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1521 - acc: 0.9492 - val_loss: 0.4080 - val_acc: 0.8836\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1536 - acc: 0.9480 - val_loss: 0.4021 - val_acc: 0.8889\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1520 - acc: 0.9486 - val_loss: 0.4010 - val_acc: 0.8783\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1533 - acc: 0.9444 - val_loss: 0.4059 - val_acc: 0.8836\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1518 - acc: 0.9480 - val_loss: 0.4125 - val_acc: 0.8836\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1532 - acc: 0.9462 - val_loss: 0.4098 - val_acc: 0.8836\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1524 - acc: 0.9486 - val_loss: 0.3943 - val_acc: 0.8889\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1523 - acc: 0.9474 - val_loss: 0.4207 - val_acc: 0.8836\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1530 - acc: 0.9462 - val_loss: 0.4011 - val_acc: 0.8889\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1523 - acc: 0.9468 - val_loss: 0.4121 - val_acc: 0.8889\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1508 - acc: 0.9474 - val_loss: 0.4121 - val_acc: 0.8836\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.4077 - val_acc: 0.8836\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1521 - acc: 0.9474 - val_loss: 0.4020 - val_acc: 0.8836\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1518 - acc: 0.9474 - val_loss: 0.4041 - val_acc: 0.8889\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1525 - acc: 0.9480 - val_loss: 0.4028 - val_acc: 0.8889\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1528 - acc: 0.9480 - val_loss: 0.3965 - val_acc: 0.8889\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1520 - acc: 0.9480 - val_loss: 0.3984 - val_acc: 0.8889\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1521 - acc: 0.9456 - val_loss: 0.3940 - val_acc: 0.8836\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1517 - acc: 0.9468 - val_loss: 0.4058 - val_acc: 0.8836\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1523 - acc: 0.9480 - val_loss: 0.4022 - val_acc: 0.8889\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1527 - acc: 0.9468 - val_loss: 0.3977 - val_acc: 0.8836\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1514 - acc: 0.9492 - val_loss: 0.4027 - val_acc: 0.8836\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1517 - acc: 0.9450 - val_loss: 0.4086 - val_acc: 0.8836\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1522 - acc: 0.9474 - val_loss: 0.3936 - val_acc: 0.8889\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1521 - acc: 0.9456 - val_loss: 0.4013 - val_acc: 0.8836\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1521 - acc: 0.9468 - val_loss: 0.4052 - val_acc: 0.8836\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1518 - acc: 0.9468 - val_loss: 0.4183 - val_acc: 0.8836\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1517 - acc: 0.9468 - val_loss: 0.4026 - val_acc: 0.8836\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1519 - acc: 0.9444 - val_loss: 0.4138 - val_acc: 0.8836\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1513 - acc: 0.9480 - val_loss: 0.3928 - val_acc: 0.8836\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1522 - acc: 0.9462 - val_loss: 0.3998 - val_acc: 0.8836\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1503 - acc: 0.9486 - val_loss: 0.3910 - val_acc: 0.8889\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1522 - acc: 0.9456 - val_loss: 0.4030 - val_acc: 0.8836\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1526 - acc: 0.9468 - val_loss: 0.4078 - val_acc: 0.8836\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1517 - acc: 0.9468 - val_loss: 0.4146 - val_acc: 0.8836\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1521 - acc: 0.9462 - val_loss: 0.3980 - val_acc: 0.8889\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1520 - acc: 0.9480 - val_loss: 0.4073 - val_acc: 0.8889\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1526 - acc: 0.9474 - val_loss: 0.4100 - val_acc: 0.8836\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1509 - acc: 0.9480 - val_loss: 0.4103 - val_acc: 0.8889\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1512 - acc: 0.9486 - val_loss: 0.3904 - val_acc: 0.8836\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1520 - acc: 0.9468 - val_loss: 0.3997 - val_acc: 0.8836\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1518 - acc: 0.9456 - val_loss: 0.3921 - val_acc: 0.8889\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1513 - acc: 0.9468 - val_loss: 0.3984 - val_acc: 0.8836\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1519 - acc: 0.9480 - val_loss: 0.3978 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1519 - acc: 0.9492 - val_loss: 0.4052 - val_acc: 0.8889\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1516 - acc: 0.9462 - val_loss: 0.3997 - val_acc: 0.8889\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1504 - acc: 0.9492 - val_loss: 0.4101 - val_acc: 0.8889\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1510 - acc: 0.9492 - val_loss: 0.4116 - val_acc: 0.8836\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1518 - acc: 0.9468 - val_loss: 0.4080 - val_acc: 0.8889\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1508 - acc: 0.9462 - val_loss: 0.3959 - val_acc: 0.8836\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1520 - acc: 0.9444 - val_loss: 0.4067 - val_acc: 0.8836\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1511 - acc: 0.9462 - val_loss: 0.4014 - val_acc: 0.8889\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1524 - acc: 0.9480 - val_loss: 0.3998 - val_acc: 0.8889\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1512 - acc: 0.9492 - val_loss: 0.4021 - val_acc: 0.8836\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1517 - acc: 0.9462 - val_loss: 0.3960 - val_acc: 0.8889\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1504 - acc: 0.9492 - val_loss: 0.4025 - val_acc: 0.8836\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1516 - acc: 0.9474 - val_loss: 0.3966 - val_acc: 0.8889\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1510 - acc: 0.9462 - val_loss: 0.4006 - val_acc: 0.8889\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1513 - acc: 0.9468 - val_loss: 0.3946 - val_acc: 0.8889\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1520 - acc: 0.9462 - val_loss: 0.4074 - val_acc: 0.8836\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1510 - acc: 0.9474 - val_loss: 0.4021 - val_acc: 0.8836\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1508 - acc: 0.9486 - val_loss: 0.4069 - val_acc: 0.8836\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1515 - acc: 0.9462 - val_loss: 0.4045 - val_acc: 0.8836\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1506 - acc: 0.9468 - val_loss: 0.4034 - val_acc: 0.8889\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1511 - acc: 0.9480 - val_loss: 0.4035 - val_acc: 0.8836\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1506 - acc: 0.9504 - val_loss: 0.3959 - val_acc: 0.8889\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1512 - acc: 0.9474 - val_loss: 0.3853 - val_acc: 0.8942\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1511 - acc: 0.9474 - val_loss: 0.3984 - val_acc: 0.8836\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1508 - acc: 0.9474 - val_loss: 0.4084 - val_acc: 0.8836\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1520 - acc: 0.9474 - val_loss: 0.4031 - val_acc: 0.8889\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1514 - acc: 0.9474 - val_loss: 0.4005 - val_acc: 0.8836\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1500 - acc: 0.9509 - val_loss: 0.4135 - val_acc: 0.8836\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1517 - acc: 0.9450 - val_loss: 0.4002 - val_acc: 0.8889\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1514 - acc: 0.9480 - val_loss: 0.4073 - val_acc: 0.8836\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.4119 - val_acc: 0.8836\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1509 - acc: 0.9444 - val_loss: 0.4000 - val_acc: 0.8889\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1508 - acc: 0.9498 - val_loss: 0.4030 - val_acc: 0.8889\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1512 - acc: 0.9486 - val_loss: 0.4053 - val_acc: 0.8836\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1505 - acc: 0.9480 - val_loss: 0.4133 - val_acc: 0.8836\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1500 - acc: 0.9468 - val_loss: 0.4100 - val_acc: 0.8836\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1502 - acc: 0.9474 - val_loss: 0.3988 - val_acc: 0.8836\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1512 - acc: 0.9462 - val_loss: 0.4204 - val_acc: 0.8836\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1514 - acc: 0.9468 - val_loss: 0.3993 - val_acc: 0.8889\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1509 - acc: 0.9444 - val_loss: 0.4124 - val_acc: 0.8836\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1509 - acc: 0.9462 - val_loss: 0.3996 - val_acc: 0.8836\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1500 - acc: 0.9480 - val_loss: 0.4019 - val_acc: 0.8836\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1496 - acc: 0.9498 - val_loss: 0.4264 - val_acc: 0.8836\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1508 - acc: 0.9492 - val_loss: 0.3979 - val_acc: 0.8836\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1512 - acc: 0.9492 - val_loss: 0.4126 - val_acc: 0.8836\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.3907 - val_acc: 0.8889\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1499 - acc: 0.9509 - val_loss: 0.3981 - val_acc: 0.8836\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1504 - acc: 0.9462 - val_loss: 0.3924 - val_acc: 0.8942\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.3987 - val_acc: 0.8942\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1501 - acc: 0.9486 - val_loss: 0.4000 - val_acc: 0.8889\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1504 - acc: 0.9468 - val_loss: 0.4228 - val_acc: 0.8836\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1507 - acc: 0.9480 - val_loss: 0.4145 - val_acc: 0.8836\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1506 - acc: 0.9492 - val_loss: 0.4038 - val_acc: 0.8836\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.4193 - val_acc: 0.8836\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1507 - acc: 0.9468 - val_loss: 0.4068 - val_acc: 0.8783\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1503 - acc: 0.9480 - val_loss: 0.4017 - val_acc: 0.8836\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1504 - acc: 0.9456 - val_loss: 0.4080 - val_acc: 0.8836\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1504 - acc: 0.9462 - val_loss: 0.4132 - val_acc: 0.8836\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1508 - acc: 0.9486 - val_loss: 0.4031 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1512 - acc: 0.9498 - val_loss: 0.4066 - val_acc: 0.8836\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1506 - acc: 0.9468 - val_loss: 0.4122 - val_acc: 0.8836\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1490 - acc: 0.9474 - val_loss: 0.4204 - val_acc: 0.8836\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1499 - acc: 0.9474 - val_loss: 0.4094 - val_acc: 0.8836\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1486 - acc: 0.9480 - val_loss: 0.4284 - val_acc: 0.8889\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1502 - acc: 0.9498 - val_loss: 0.4031 - val_acc: 0.8889\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1509 - acc: 0.9498 - val_loss: 0.4075 - val_acc: 0.8889\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1485 - acc: 0.9486 - val_loss: 0.4143 - val_acc: 0.8889\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1499 - acc: 0.9492 - val_loss: 0.3988 - val_acc: 0.8889\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1495 - acc: 0.9492 - val_loss: 0.3957 - val_acc: 0.8889\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1504 - acc: 0.9504 - val_loss: 0.4208 - val_acc: 0.8836\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1502 - acc: 0.9474 - val_loss: 0.3938 - val_acc: 0.8942\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1511 - acc: 0.9486 - val_loss: 0.4123 - val_acc: 0.8836\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1492 - acc: 0.9480 - val_loss: 0.4123 - val_acc: 0.8836\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1496 - acc: 0.9498 - val_loss: 0.4247 - val_acc: 0.8889\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1481 - acc: 0.9480 - val_loss: 0.3958 - val_acc: 0.8889\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1500 - acc: 0.9456 - val_loss: 0.4097 - val_acc: 0.8889\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1496 - acc: 0.9456 - val_loss: 0.4046 - val_acc: 0.8889\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1500 - acc: 0.9474 - val_loss: 0.4039 - val_acc: 0.8836\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1494 - acc: 0.9486 - val_loss: 0.4024 - val_acc: 0.8889\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1500 - acc: 0.9492 - val_loss: 0.4041 - val_acc: 0.8889\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1492 - acc: 0.9486 - val_loss: 0.4162 - val_acc: 0.8889\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1513 - acc: 0.9444 - val_loss: 0.4087 - val_acc: 0.8836\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.4102 - val_acc: 0.8836\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1492 - acc: 0.9480 - val_loss: 0.3952 - val_acc: 0.8889\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1494 - acc: 0.9480 - val_loss: 0.4111 - val_acc: 0.8836\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1508 - acc: 0.9468 - val_loss: 0.4068 - val_acc: 0.8836\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1498 - acc: 0.9492 - val_loss: 0.4098 - val_acc: 0.8889\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1509 - acc: 0.9462 - val_loss: 0.3984 - val_acc: 0.8942\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1500 - acc: 0.9498 - val_loss: 0.4136 - val_acc: 0.8836\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1495 - acc: 0.9492 - val_loss: 0.4066 - val_acc: 0.8836\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1488 - acc: 0.9468 - val_loss: 0.4021 - val_acc: 0.8889\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1503 - acc: 0.9480 - val_loss: 0.3972 - val_acc: 0.8889\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1487 - acc: 0.9486 - val_loss: 0.4050 - val_acc: 0.8942\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1497 - acc: 0.9474 - val_loss: 0.4042 - val_acc: 0.8889\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1490 - acc: 0.9480 - val_loss: 0.4190 - val_acc: 0.8783\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1503 - acc: 0.9474 - val_loss: 0.4019 - val_acc: 0.8942\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1499 - acc: 0.9474 - val_loss: 0.4069 - val_acc: 0.8836\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1498 - acc: 0.9492 - val_loss: 0.4067 - val_acc: 0.8836\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1494 - acc: 0.9504 - val_loss: 0.4010 - val_acc: 0.8889\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1504 - acc: 0.9486 - val_loss: 0.4172 - val_acc: 0.8836\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1491 - acc: 0.9474 - val_loss: 0.4152 - val_acc: 0.8836\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1497 - acc: 0.9480 - val_loss: 0.4103 - val_acc: 0.8836\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1491 - acc: 0.9498 - val_loss: 0.4168 - val_acc: 0.8836\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1507 - acc: 0.9474 - val_loss: 0.4181 - val_acc: 0.8836\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1499 - acc: 0.9474 - val_loss: 0.4098 - val_acc: 0.8836\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1488 - acc: 0.9486 - val_loss: 0.4201 - val_acc: 0.8836\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1503 - acc: 0.9486 - val_loss: 0.4126 - val_acc: 0.8836\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1501 - acc: 0.9492 - val_loss: 0.4113 - val_acc: 0.8836\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1496 - acc: 0.9468 - val_loss: 0.4098 - val_acc: 0.8836\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1490 - acc: 0.9498 - val_loss: 0.4162 - val_acc: 0.8836\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1496 - acc: 0.9462 - val_loss: 0.4133 - val_acc: 0.8889\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1496 - acc: 0.9480 - val_loss: 0.4115 - val_acc: 0.8836\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1487 - acc: 0.9492 - val_loss: 0.4140 - val_acc: 0.8836\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1501 - acc: 0.9474 - val_loss: 0.4083 - val_acc: 0.8836\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1498 - acc: 0.9474 - val_loss: 0.4017 - val_acc: 0.8889\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1496 - acc: 0.9486 - val_loss: 0.4100 - val_acc: 0.8836\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 528us/step - loss: 0.6851 - acc: 0.5414 - val_loss: 0.6635 - val_acc: 0.6190\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.6331 - acc: 0.6927 - val_loss: 0.6404 - val_acc: 0.6508\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.6085 - acc: 0.7228 - val_loss: 0.6262 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.5902 - acc: 0.7382 - val_loss: 0.6155 - val_acc: 0.6825\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.5757 - acc: 0.7488 - val_loss: 0.6081 - val_acc: 0.6825\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.5642 - acc: 0.7583 - val_loss: 0.6013 - val_acc: 0.6931\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.5542 - acc: 0.7636 - val_loss: 0.5954 - val_acc: 0.6825\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.5457 - acc: 0.7654 - val_loss: 0.5909 - val_acc: 0.6825\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.5379 - acc: 0.7683 - val_loss: 0.5856 - val_acc: 0.6931\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.5310 - acc: 0.7742 - val_loss: 0.5813 - val_acc: 0.6931\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.5246 - acc: 0.7796 - val_loss: 0.5772 - val_acc: 0.7090\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.5188 - acc: 0.7843 - val_loss: 0.5728 - val_acc: 0.7090\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.5134 - acc: 0.7872 - val_loss: 0.5687 - val_acc: 0.7249\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.5083 - acc: 0.7890 - val_loss: 0.5650 - val_acc: 0.7302\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.5037 - acc: 0.7896 - val_loss: 0.5615 - val_acc: 0.7460\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.4994 - acc: 0.7931 - val_loss: 0.5579 - val_acc: 0.7460\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.4952 - acc: 0.7943 - val_loss: 0.5549 - val_acc: 0.7513\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.4914 - acc: 0.7955 - val_loss: 0.5519 - val_acc: 0.7513\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.4875 - acc: 0.7979 - val_loss: 0.5493 - val_acc: 0.7513\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.4839 - acc: 0.7985 - val_loss: 0.5464 - val_acc: 0.7460\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.4804 - acc: 0.7985 - val_loss: 0.5439 - val_acc: 0.7460\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.4770 - acc: 0.8020 - val_loss: 0.5418 - val_acc: 0.7460\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4737 - acc: 0.8002 - val_loss: 0.5394 - val_acc: 0.7460\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4705 - acc: 0.8044 - val_loss: 0.5373 - val_acc: 0.7460\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4676 - acc: 0.8056 - val_loss: 0.5354 - val_acc: 0.7407\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4645 - acc: 0.8050 - val_loss: 0.5334 - val_acc: 0.7407\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4615 - acc: 0.8067 - val_loss: 0.5316 - val_acc: 0.7460\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4586 - acc: 0.8103 - val_loss: 0.5300 - val_acc: 0.7460\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4557 - acc: 0.8156 - val_loss: 0.5284 - val_acc: 0.7566\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4531 - acc: 0.8162 - val_loss: 0.5270 - val_acc: 0.7619\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4504 - acc: 0.8174 - val_loss: 0.5255 - val_acc: 0.7513\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4479 - acc: 0.8197 - val_loss: 0.5244 - val_acc: 0.7513\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4453 - acc: 0.8209 - val_loss: 0.5231 - val_acc: 0.7513\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4429 - acc: 0.8215 - val_loss: 0.5221 - val_acc: 0.7513\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4405 - acc: 0.8227 - val_loss: 0.5210 - val_acc: 0.7566\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4383 - acc: 0.8239 - val_loss: 0.5197 - val_acc: 0.7513\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4361 - acc: 0.8245 - val_loss: 0.5186 - val_acc: 0.7513\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4340 - acc: 0.8286 - val_loss: 0.5174 - val_acc: 0.7513\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4319 - acc: 0.8292 - val_loss: 0.5166 - val_acc: 0.7619\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4299 - acc: 0.8327 - val_loss: 0.5156 - val_acc: 0.7619\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4280 - acc: 0.8327 - val_loss: 0.5145 - val_acc: 0.7619\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4260 - acc: 0.8357 - val_loss: 0.5134 - val_acc: 0.7566\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4241 - acc: 0.8357 - val_loss: 0.5122 - val_acc: 0.7566\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4221 - acc: 0.8369 - val_loss: 0.5112 - val_acc: 0.7566\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4202 - acc: 0.8363 - val_loss: 0.5102 - val_acc: 0.7619\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4183 - acc: 0.8381 - val_loss: 0.5092 - val_acc: 0.7619\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4164 - acc: 0.8392 - val_loss: 0.5081 - val_acc: 0.7619\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4145 - acc: 0.8392 - val_loss: 0.5070 - val_acc: 0.7619\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4126 - acc: 0.8404 - val_loss: 0.5062 - val_acc: 0.7672\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4107 - acc: 0.8440 - val_loss: 0.5051 - val_acc: 0.7725\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4088 - acc: 0.8446 - val_loss: 0.5038 - val_acc: 0.7725\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4071 - acc: 0.8440 - val_loss: 0.5027 - val_acc: 0.7672\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4053 - acc: 0.8446 - val_loss: 0.5019 - val_acc: 0.7619\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4036 - acc: 0.8475 - val_loss: 0.5010 - val_acc: 0.7619\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4018 - acc: 0.8446 - val_loss: 0.5005 - val_acc: 0.7672\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4002 - acc: 0.8469 - val_loss: 0.4999 - val_acc: 0.7778\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3986 - acc: 0.8469 - val_loss: 0.4992 - val_acc: 0.7778\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3971 - acc: 0.8487 - val_loss: 0.4979 - val_acc: 0.7778\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3956 - acc: 0.8511 - val_loss: 0.4971 - val_acc: 0.7778\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3939 - acc: 0.8511 - val_loss: 0.4962 - val_acc: 0.7725\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3923 - acc: 0.8522 - val_loss: 0.4956 - val_acc: 0.7725\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3908 - acc: 0.8511 - val_loss: 0.4948 - val_acc: 0.7725\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3892 - acc: 0.8517 - val_loss: 0.4938 - val_acc: 0.7725\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3877 - acc: 0.8517 - val_loss: 0.4933 - val_acc: 0.7725\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3863 - acc: 0.8522 - val_loss: 0.4923 - val_acc: 0.7778\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3848 - acc: 0.8522 - val_loss: 0.4912 - val_acc: 0.7778\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3834 - acc: 0.8534 - val_loss: 0.4904 - val_acc: 0.7778\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3821 - acc: 0.8528 - val_loss: 0.4892 - val_acc: 0.7725\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3806 - acc: 0.8534 - val_loss: 0.4882 - val_acc: 0.7725\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3793 - acc: 0.8534 - val_loss: 0.4874 - val_acc: 0.7831\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3780 - acc: 0.8552 - val_loss: 0.4868 - val_acc: 0.7831\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3768 - acc: 0.8570 - val_loss: 0.4862 - val_acc: 0.7831\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3755 - acc: 0.8564 - val_loss: 0.4855 - val_acc: 0.7831\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3743 - acc: 0.8564 - val_loss: 0.4848 - val_acc: 0.7831\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3732 - acc: 0.8552 - val_loss: 0.4844 - val_acc: 0.7778\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3720 - acc: 0.8540 - val_loss: 0.4837 - val_acc: 0.7831\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3709 - acc: 0.8564 - val_loss: 0.4830 - val_acc: 0.7778\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3697 - acc: 0.8564 - val_loss: 0.4826 - val_acc: 0.7831\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3687 - acc: 0.8570 - val_loss: 0.4819 - val_acc: 0.7831\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3675 - acc: 0.8558 - val_loss: 0.4812 - val_acc: 0.7884\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3664 - acc: 0.8576 - val_loss: 0.4808 - val_acc: 0.7884\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3654 - acc: 0.8576 - val_loss: 0.4801 - val_acc: 0.7884\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3643 - acc: 0.8582 - val_loss: 0.4796 - val_acc: 0.7884\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3633 - acc: 0.8593 - val_loss: 0.4788 - val_acc: 0.7884\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3622 - acc: 0.8593 - val_loss: 0.4784 - val_acc: 0.7884\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3613 - acc: 0.8593 - val_loss: 0.4778 - val_acc: 0.7884\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3603 - acc: 0.8611 - val_loss: 0.4773 - val_acc: 0.7884\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3594 - acc: 0.8611 - val_loss: 0.4768 - val_acc: 0.7884\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3583 - acc: 0.8617 - val_loss: 0.4764 - val_acc: 0.7884\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3574 - acc: 0.8617 - val_loss: 0.4757 - val_acc: 0.7884\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3565 - acc: 0.8617 - val_loss: 0.4754 - val_acc: 0.7884\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3556 - acc: 0.8605 - val_loss: 0.4750 - val_acc: 0.7884\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3548 - acc: 0.8629 - val_loss: 0.4746 - val_acc: 0.7884\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3539 - acc: 0.8629 - val_loss: 0.4744 - val_acc: 0.7884\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3530 - acc: 0.8635 - val_loss: 0.4740 - val_acc: 0.7884\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3522 - acc: 0.8617 - val_loss: 0.4735 - val_acc: 0.7884\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3513 - acc: 0.8623 - val_loss: 0.4729 - val_acc: 0.7884\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3505 - acc: 0.8641 - val_loss: 0.4723 - val_acc: 0.7831\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3496 - acc: 0.8641 - val_loss: 0.4721 - val_acc: 0.7831\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3488 - acc: 0.8641 - val_loss: 0.4715 - val_acc: 0.7884\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.3479 - acc: 0.8658 - val_loss: 0.4712 - val_acc: 0.7884\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.3471 - acc: 0.8652 - val_loss: 0.4708 - val_acc: 0.7884\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.3463 - acc: 0.8652 - val_loss: 0.4703 - val_acc: 0.7937\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.3453 - acc: 0.8658 - val_loss: 0.4701 - val_acc: 0.7937\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.3446 - acc: 0.8658 - val_loss: 0.4696 - val_acc: 0.7937\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.3438 - acc: 0.8658 - val_loss: 0.4693 - val_acc: 0.7937\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.3430 - acc: 0.8670 - val_loss: 0.4689 - val_acc: 0.7937\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.3422 - acc: 0.8664 - val_loss: 0.4688 - val_acc: 0.7989\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3414 - acc: 0.8664 - val_loss: 0.4683 - val_acc: 0.7937\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.3406 - acc: 0.8682 - val_loss: 0.4679 - val_acc: 0.7989\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.3399 - acc: 0.8676 - val_loss: 0.4676 - val_acc: 0.7989\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3392 - acc: 0.8676 - val_loss: 0.4673 - val_acc: 0.7989\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3384 - acc: 0.8676 - val_loss: 0.4670 - val_acc: 0.7989\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3377 - acc: 0.8688 - val_loss: 0.4665 - val_acc: 0.7989\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3370 - acc: 0.8688 - val_loss: 0.4662 - val_acc: 0.7989\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3363 - acc: 0.8694 - val_loss: 0.4659 - val_acc: 0.8042\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3355 - acc: 0.8700 - val_loss: 0.4652 - val_acc: 0.8042\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3347 - acc: 0.8717 - val_loss: 0.4648 - val_acc: 0.8042\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3340 - acc: 0.8723 - val_loss: 0.4643 - val_acc: 0.8042\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3333 - acc: 0.8729 - val_loss: 0.4639 - val_acc: 0.8042\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.3326 - acc: 0.8723 - val_loss: 0.4635 - val_acc: 0.8042\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3320 - acc: 0.8717 - val_loss: 0.4632 - val_acc: 0.8042\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3313 - acc: 0.8735 - val_loss: 0.4628 - val_acc: 0.8042\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3305 - acc: 0.8741 - val_loss: 0.4625 - val_acc: 0.8042\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3299 - acc: 0.8741 - val_loss: 0.4620 - val_acc: 0.8042\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3293 - acc: 0.8747 - val_loss: 0.4619 - val_acc: 0.8042\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3286 - acc: 0.8735 - val_loss: 0.4614 - val_acc: 0.8042\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3280 - acc: 0.8753 - val_loss: 0.4610 - val_acc: 0.8042\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3273 - acc: 0.8747 - val_loss: 0.4606 - val_acc: 0.8042\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3268 - acc: 0.8759 - val_loss: 0.4604 - val_acc: 0.8042\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3261 - acc: 0.8753 - val_loss: 0.4601 - val_acc: 0.8042\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3255 - acc: 0.8765 - val_loss: 0.4596 - val_acc: 0.8042\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3249 - acc: 0.8771 - val_loss: 0.4595 - val_acc: 0.8042\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3243 - acc: 0.8771 - val_loss: 0.4591 - val_acc: 0.8042\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3237 - acc: 0.8771 - val_loss: 0.4589 - val_acc: 0.8042\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3231 - acc: 0.8771 - val_loss: 0.4585 - val_acc: 0.8042\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3225 - acc: 0.8794 - val_loss: 0.4582 - val_acc: 0.7989\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.3219 - acc: 0.8794 - val_loss: 0.4581 - val_acc: 0.7989\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.3213 - acc: 0.8788 - val_loss: 0.4578 - val_acc: 0.7989\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.3207 - acc: 0.8777 - val_loss: 0.4574 - val_acc: 0.7989\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3201 - acc: 0.8788 - val_loss: 0.4574 - val_acc: 0.7989\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3194 - acc: 0.8783 - val_loss: 0.4569 - val_acc: 0.7989\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3188 - acc: 0.8783 - val_loss: 0.4568 - val_acc: 0.7989\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.3182 - acc: 0.8794 - val_loss: 0.4561 - val_acc: 0.7989\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3176 - acc: 0.8783 - val_loss: 0.4561 - val_acc: 0.7989\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3170 - acc: 0.8788 - val_loss: 0.4558 - val_acc: 0.7989\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3164 - acc: 0.8806 - val_loss: 0.4554 - val_acc: 0.7989\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3158 - acc: 0.8794 - val_loss: 0.4553 - val_acc: 0.7989\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3153 - acc: 0.8800 - val_loss: 0.4549 - val_acc: 0.7989\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3147 - acc: 0.8806 - val_loss: 0.4549 - val_acc: 0.7989\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3141 - acc: 0.8812 - val_loss: 0.4550 - val_acc: 0.7989\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3136 - acc: 0.8800 - val_loss: 0.4548 - val_acc: 0.7989\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3130 - acc: 0.8794 - val_loss: 0.4546 - val_acc: 0.7989\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3124 - acc: 0.8806 - val_loss: 0.4544 - val_acc: 0.7989\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.3119 - acc: 0.8794 - val_loss: 0.4544 - val_acc: 0.7989\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.3113 - acc: 0.8800 - val_loss: 0.4541 - val_acc: 0.7989\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.3108 - acc: 0.8824 - val_loss: 0.4540 - val_acc: 0.7989\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.3102 - acc: 0.8818 - val_loss: 0.4537 - val_acc: 0.7989\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3097 - acc: 0.8818 - val_loss: 0.4538 - val_acc: 0.7989\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3091 - acc: 0.8806 - val_loss: 0.4537 - val_acc: 0.7989\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3085 - acc: 0.8818 - val_loss: 0.4538 - val_acc: 0.7989\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3080 - acc: 0.8818 - val_loss: 0.4535 - val_acc: 0.7989\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3075 - acc: 0.8824 - val_loss: 0.4536 - val_acc: 0.7989\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3070 - acc: 0.8830 - val_loss: 0.4535 - val_acc: 0.7989\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3065 - acc: 0.8818 - val_loss: 0.4532 - val_acc: 0.7989\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3059 - acc: 0.8836 - val_loss: 0.4530 - val_acc: 0.7989\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3055 - acc: 0.8836 - val_loss: 0.4532 - val_acc: 0.7989\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3050 - acc: 0.8818 - val_loss: 0.4530 - val_acc: 0.7989\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3045 - acc: 0.8824 - val_loss: 0.4528 - val_acc: 0.7989\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3040 - acc: 0.8836 - val_loss: 0.4529 - val_acc: 0.7989\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3036 - acc: 0.8824 - val_loss: 0.4528 - val_acc: 0.7989\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3032 - acc: 0.8842 - val_loss: 0.4528 - val_acc: 0.7989\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3027 - acc: 0.8836 - val_loss: 0.4529 - val_acc: 0.7989\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3022 - acc: 0.8842 - val_loss: 0.4529 - val_acc: 0.7989\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3018 - acc: 0.8842 - val_loss: 0.4529 - val_acc: 0.7989\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3014 - acc: 0.8836 - val_loss: 0.4526 - val_acc: 0.7989\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3009 - acc: 0.8842 - val_loss: 0.4524 - val_acc: 0.7989\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3004 - acc: 0.8848 - val_loss: 0.4522 - val_acc: 0.7989\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2999 - acc: 0.8853 - val_loss: 0.4520 - val_acc: 0.7989\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2994 - acc: 0.8877 - val_loss: 0.4516 - val_acc: 0.7989\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2989 - acc: 0.8871 - val_loss: 0.4517 - val_acc: 0.7989\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2985 - acc: 0.8859 - val_loss: 0.4517 - val_acc: 0.7989\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2981 - acc: 0.8865 - val_loss: 0.4516 - val_acc: 0.8042\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2976 - acc: 0.8883 - val_loss: 0.4514 - val_acc: 0.8042\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2972 - acc: 0.8865 - val_loss: 0.4517 - val_acc: 0.8042\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2967 - acc: 0.8859 - val_loss: 0.4515 - val_acc: 0.8042\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2963 - acc: 0.8883 - val_loss: 0.4516 - val_acc: 0.8042\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2959 - acc: 0.8871 - val_loss: 0.4515 - val_acc: 0.8042\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2955 - acc: 0.8901 - val_loss: 0.4514 - val_acc: 0.8042\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2950 - acc: 0.8901 - val_loss: 0.4512 - val_acc: 0.8042\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2946 - acc: 0.8895 - val_loss: 0.4510 - val_acc: 0.8042\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2943 - acc: 0.8889 - val_loss: 0.4508 - val_acc: 0.8042\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2938 - acc: 0.8913 - val_loss: 0.4506 - val_acc: 0.8042\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2934 - acc: 0.8907 - val_loss: 0.4508 - val_acc: 0.8042\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2930 - acc: 0.8918 - val_loss: 0.4505 - val_acc: 0.8042\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2926 - acc: 0.8907 - val_loss: 0.4503 - val_acc: 0.8042\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2923 - acc: 0.8913 - val_loss: 0.4506 - val_acc: 0.8042\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2919 - acc: 0.8913 - val_loss: 0.4504 - val_acc: 0.8042\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2915 - acc: 0.8901 - val_loss: 0.4502 - val_acc: 0.8042\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2911 - acc: 0.8924 - val_loss: 0.4503 - val_acc: 0.8042\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2907 - acc: 0.8918 - val_loss: 0.4504 - val_acc: 0.8042\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2904 - acc: 0.8924 - val_loss: 0.4502 - val_acc: 0.8042\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2900 - acc: 0.8924 - val_loss: 0.4502 - val_acc: 0.8042\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2896 - acc: 0.8924 - val_loss: 0.4503 - val_acc: 0.8042\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2893 - acc: 0.8924 - val_loss: 0.4501 - val_acc: 0.8042\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2889 - acc: 0.8913 - val_loss: 0.4502 - val_acc: 0.8042\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2886 - acc: 0.8924 - val_loss: 0.4501 - val_acc: 0.8042\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2882 - acc: 0.8924 - val_loss: 0.4499 - val_acc: 0.8095\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2878 - acc: 0.8918 - val_loss: 0.4500 - val_acc: 0.8095\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2875 - acc: 0.8936 - val_loss: 0.4501 - val_acc: 0.8095\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2872 - acc: 0.8930 - val_loss: 0.4501 - val_acc: 0.8095\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2869 - acc: 0.8930 - val_loss: 0.4500 - val_acc: 0.8095\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2866 - acc: 0.8936 - val_loss: 0.4499 - val_acc: 0.8095\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2863 - acc: 0.8936 - val_loss: 0.4498 - val_acc: 0.8095\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2859 - acc: 0.8930 - val_loss: 0.4499 - val_acc: 0.8095\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2856 - acc: 0.8936 - val_loss: 0.4495 - val_acc: 0.8095\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2853 - acc: 0.8936 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2850 - acc: 0.8936 - val_loss: 0.4494 - val_acc: 0.8095\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2847 - acc: 0.8942 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2844 - acc: 0.8948 - val_loss: 0.4495 - val_acc: 0.8095\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2842 - acc: 0.8936 - val_loss: 0.4494 - val_acc: 0.8095\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2838 - acc: 0.8954 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2835 - acc: 0.8942 - val_loss: 0.4497 - val_acc: 0.8095\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2833 - acc: 0.8936 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2830 - acc: 0.8936 - val_loss: 0.4498 - val_acc: 0.8095\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2827 - acc: 0.8936 - val_loss: 0.4498 - val_acc: 0.8095\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2824 - acc: 0.8942 - val_loss: 0.4497 - val_acc: 0.8095\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2821 - acc: 0.8942 - val_loss: 0.4495 - val_acc: 0.8095\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2819 - acc: 0.8942 - val_loss: 0.4496 - val_acc: 0.8095\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2816 - acc: 0.8942 - val_loss: 0.4494 - val_acc: 0.8095\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2813 - acc: 0.8942 - val_loss: 0.4493 - val_acc: 0.8095\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2811 - acc: 0.8954 - val_loss: 0.4493 - val_acc: 0.8095\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2808 - acc: 0.8948 - val_loss: 0.4492 - val_acc: 0.8095\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2806 - acc: 0.8948 - val_loss: 0.4492 - val_acc: 0.8095\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2803 - acc: 0.8972 - val_loss: 0.4491 - val_acc: 0.8095\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2800 - acc: 0.8972 - val_loss: 0.4490 - val_acc: 0.8095\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2797 - acc: 0.8960 - val_loss: 0.4489 - val_acc: 0.8095\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2795 - acc: 0.8989 - val_loss: 0.4490 - val_acc: 0.8095\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2793 - acc: 0.8989 - val_loss: 0.4488 - val_acc: 0.8095\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2790 - acc: 0.8978 - val_loss: 0.4489 - val_acc: 0.8095\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2788 - acc: 0.8995 - val_loss: 0.4490 - val_acc: 0.8095\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2786 - acc: 0.8989 - val_loss: 0.4490 - val_acc: 0.8095\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2784 - acc: 0.8989 - val_loss: 0.4491 - val_acc: 0.8095\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2781 - acc: 0.8989 - val_loss: 0.4492 - val_acc: 0.8095\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2779 - acc: 0.8989 - val_loss: 0.4490 - val_acc: 0.8095\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2776 - acc: 0.8995 - val_loss: 0.4489 - val_acc: 0.8095\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2774 - acc: 0.9001 - val_loss: 0.4488 - val_acc: 0.8148\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2772 - acc: 0.8995 - val_loss: 0.4487 - val_acc: 0.8148\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2770 - acc: 0.8995 - val_loss: 0.4487 - val_acc: 0.8148\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2767 - acc: 0.8995 - val_loss: 0.4489 - val_acc: 0.8148\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2765 - acc: 0.8995 - val_loss: 0.4489 - val_acc: 0.8148\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2763 - acc: 0.9001 - val_loss: 0.4488 - val_acc: 0.8148\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2760 - acc: 0.8989 - val_loss: 0.4489 - val_acc: 0.8148\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2758 - acc: 0.8995 - val_loss: 0.4486 - val_acc: 0.8148\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2756 - acc: 0.9007 - val_loss: 0.4486 - val_acc: 0.8148\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2754 - acc: 0.8995 - val_loss: 0.4484 - val_acc: 0.8148\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2751 - acc: 0.9007 - val_loss: 0.4484 - val_acc: 0.8148\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2750 - acc: 0.9007 - val_loss: 0.4483 - val_acc: 0.8148\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2747 - acc: 0.9007 - val_loss: 0.4481 - val_acc: 0.8148\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2745 - acc: 0.9007 - val_loss: 0.4481 - val_acc: 0.8148\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2743 - acc: 0.9007 - val_loss: 0.4480 - val_acc: 0.8148\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2741 - acc: 0.9013 - val_loss: 0.4478 - val_acc: 0.8148\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2739 - acc: 0.9013 - val_loss: 0.4479 - val_acc: 0.8148\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2737 - acc: 0.9013 - val_loss: 0.4478 - val_acc: 0.8148\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2734 - acc: 0.9013 - val_loss: 0.4476 - val_acc: 0.8148\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2733 - acc: 0.9007 - val_loss: 0.4476 - val_acc: 0.8148\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2731 - acc: 0.9013 - val_loss: 0.4476 - val_acc: 0.8148\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2728 - acc: 0.9013 - val_loss: 0.4476 - val_acc: 0.8148\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2727 - acc: 0.9025 - val_loss: 0.4476 - val_acc: 0.8148\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2725 - acc: 0.9013 - val_loss: 0.4475 - val_acc: 0.8148\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2723 - acc: 0.9013 - val_loss: 0.4472 - val_acc: 0.8148\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2720 - acc: 0.9031 - val_loss: 0.4471 - val_acc: 0.8148\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2719 - acc: 0.9025 - val_loss: 0.4472 - val_acc: 0.8148\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2717 - acc: 0.9031 - val_loss: 0.4471 - val_acc: 0.8148\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2715 - acc: 0.9025 - val_loss: 0.4469 - val_acc: 0.8148\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2713 - acc: 0.9025 - val_loss: 0.4468 - val_acc: 0.8148\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2711 - acc: 0.9031 - val_loss: 0.4468 - val_acc: 0.8148\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2709 - acc: 0.9031 - val_loss: 0.4467 - val_acc: 0.8148\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2707 - acc: 0.9025 - val_loss: 0.4467 - val_acc: 0.8148\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2706 - acc: 0.9031 - val_loss: 0.4465 - val_acc: 0.8148\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2703 - acc: 0.9031 - val_loss: 0.4465 - val_acc: 0.8148\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2702 - acc: 0.9031 - val_loss: 0.4463 - val_acc: 0.8148\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2700 - acc: 0.9031 - val_loss: 0.4464 - val_acc: 0.8148\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2698 - acc: 0.9031 - val_loss: 0.4462 - val_acc: 0.8148\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2696 - acc: 0.9025 - val_loss: 0.4463 - val_acc: 0.8148\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2694 - acc: 0.9031 - val_loss: 0.4461 - val_acc: 0.8148\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2693 - acc: 0.9037 - val_loss: 0.4461 - val_acc: 0.8148\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2691 - acc: 0.9031 - val_loss: 0.4461 - val_acc: 0.8148\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2689 - acc: 0.9037 - val_loss: 0.4459 - val_acc: 0.8148\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2687 - acc: 0.9037 - val_loss: 0.4457 - val_acc: 0.8148\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2686 - acc: 0.9043 - val_loss: 0.4457 - val_acc: 0.8148\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2684 - acc: 0.9031 - val_loss: 0.4456 - val_acc: 0.8148\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2682 - acc: 0.9043 - val_loss: 0.4454 - val_acc: 0.8148\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2680 - acc: 0.9043 - val_loss: 0.4455 - val_acc: 0.8148\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2679 - acc: 0.9037 - val_loss: 0.4455 - val_acc: 0.8148\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2677 - acc: 0.9043 - val_loss: 0.4454 - val_acc: 0.8148\n",
      "Epoch 297/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2676 - acc: 0.9043 - val_loss: 0.4452 - val_acc: 0.8148\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2673 - acc: 0.9043 - val_loss: 0.4451 - val_acc: 0.8148\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2672 - acc: 0.9037 - val_loss: 0.4450 - val_acc: 0.8148\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2670 - acc: 0.9054 - val_loss: 0.4449 - val_acc: 0.8148\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2668 - acc: 0.9048 - val_loss: 0.4450 - val_acc: 0.8148\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2667 - acc: 0.9054 - val_loss: 0.4449 - val_acc: 0.8148\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2665 - acc: 0.9054 - val_loss: 0.4449 - val_acc: 0.8148\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2663 - acc: 0.9060 - val_loss: 0.4446 - val_acc: 0.8148\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2662 - acc: 0.9060 - val_loss: 0.4445 - val_acc: 0.8148\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2660 - acc: 0.9060 - val_loss: 0.4444 - val_acc: 0.8148\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2658 - acc: 0.9060 - val_loss: 0.4446 - val_acc: 0.8148\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2657 - acc: 0.9054 - val_loss: 0.4445 - val_acc: 0.8148\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2655 - acc: 0.9060 - val_loss: 0.4444 - val_acc: 0.8148\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2653 - acc: 0.9066 - val_loss: 0.4443 - val_acc: 0.8148\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2651 - acc: 0.9066 - val_loss: 0.4442 - val_acc: 0.8148\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2650 - acc: 0.9078 - val_loss: 0.4441 - val_acc: 0.8148\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2648 - acc: 0.9060 - val_loss: 0.4441 - val_acc: 0.8148\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2646 - acc: 0.9072 - val_loss: 0.4439 - val_acc: 0.8148\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2645 - acc: 0.9078 - val_loss: 0.4440 - val_acc: 0.8148\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2643 - acc: 0.9072 - val_loss: 0.4438 - val_acc: 0.8148\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2642 - acc: 0.9078 - val_loss: 0.4436 - val_acc: 0.8148\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2640 - acc: 0.9084 - val_loss: 0.4435 - val_acc: 0.8148\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2638 - acc: 0.9084 - val_loss: 0.4436 - val_acc: 0.8148\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2637 - acc: 0.9078 - val_loss: 0.4435 - val_acc: 0.8148\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2635 - acc: 0.9090 - val_loss: 0.4432 - val_acc: 0.8148\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2633 - acc: 0.9084 - val_loss: 0.4430 - val_acc: 0.8148\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2631 - acc: 0.9096 - val_loss: 0.4429 - val_acc: 0.8148\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2630 - acc: 0.9090 - val_loss: 0.4430 - val_acc: 0.8148\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2628 - acc: 0.9096 - val_loss: 0.4431 - val_acc: 0.8148\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2626 - acc: 0.9096 - val_loss: 0.4428 - val_acc: 0.8148\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2625 - acc: 0.9096 - val_loss: 0.4425 - val_acc: 0.8201\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2623 - acc: 0.9119 - val_loss: 0.4426 - val_acc: 0.8201\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2621 - acc: 0.9113 - val_loss: 0.4427 - val_acc: 0.8201\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2620 - acc: 0.9102 - val_loss: 0.4423 - val_acc: 0.8201\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2618 - acc: 0.9113 - val_loss: 0.4424 - val_acc: 0.8201\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2617 - acc: 0.9113 - val_loss: 0.4423 - val_acc: 0.8201\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2615 - acc: 0.9119 - val_loss: 0.4422 - val_acc: 0.8201\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2613 - acc: 0.9113 - val_loss: 0.4422 - val_acc: 0.8201\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2612 - acc: 0.9119 - val_loss: 0.4422 - val_acc: 0.8201\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2610 - acc: 0.9119 - val_loss: 0.4420 - val_acc: 0.8201\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2608 - acc: 0.9119 - val_loss: 0.4419 - val_acc: 0.8201\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2607 - acc: 0.9119 - val_loss: 0.4417 - val_acc: 0.8201\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2605 - acc: 0.9119 - val_loss: 0.4416 - val_acc: 0.8201\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2604 - acc: 0.9113 - val_loss: 0.4417 - val_acc: 0.8201\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2602 - acc: 0.9119 - val_loss: 0.4416 - val_acc: 0.8201\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2601 - acc: 0.9113 - val_loss: 0.4417 - val_acc: 0.8201\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2599 - acc: 0.9119 - val_loss: 0.4417 - val_acc: 0.8201\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2598 - acc: 0.9113 - val_loss: 0.4418 - val_acc: 0.8201\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2596 - acc: 0.9113 - val_loss: 0.4416 - val_acc: 0.8201\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2595 - acc: 0.9119 - val_loss: 0.4416 - val_acc: 0.8201\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2593 - acc: 0.9119 - val_loss: 0.4414 - val_acc: 0.8201\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2592 - acc: 0.9113 - val_loss: 0.4414 - val_acc: 0.8201\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2591 - acc: 0.9113 - val_loss: 0.4413 - val_acc: 0.8201\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2589 - acc: 0.9119 - val_loss: 0.4411 - val_acc: 0.8201\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2588 - acc: 0.9119 - val_loss: 0.4410 - val_acc: 0.8201\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2586 - acc: 0.9119 - val_loss: 0.4409 - val_acc: 0.8201\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2585 - acc: 0.9119 - val_loss: 0.4408 - val_acc: 0.8201\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2583 - acc: 0.9119 - val_loss: 0.4405 - val_acc: 0.8201\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2582 - acc: 0.9125 - val_loss: 0.4405 - val_acc: 0.8201\n",
      "Epoch 356/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2580 - acc: 0.9119 - val_loss: 0.4403 - val_acc: 0.8201\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2579 - acc: 0.9125 - val_loss: 0.4403 - val_acc: 0.8201\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2578 - acc: 0.9125 - val_loss: 0.4403 - val_acc: 0.8201\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2576 - acc: 0.9125 - val_loss: 0.4403 - val_acc: 0.8201\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2575 - acc: 0.9125 - val_loss: 0.4401 - val_acc: 0.8201\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2574 - acc: 0.9119 - val_loss: 0.4400 - val_acc: 0.8201\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2573 - acc: 0.9125 - val_loss: 0.4400 - val_acc: 0.8201\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2571 - acc: 0.9131 - val_loss: 0.4399 - val_acc: 0.8201\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2570 - acc: 0.9131 - val_loss: 0.4400 - val_acc: 0.8201\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2569 - acc: 0.9137 - val_loss: 0.4400 - val_acc: 0.8201\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2567 - acc: 0.9125 - val_loss: 0.4399 - val_acc: 0.8201\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2566 - acc: 0.9125 - val_loss: 0.4396 - val_acc: 0.8201\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2565 - acc: 0.9131 - val_loss: 0.4396 - val_acc: 0.8201\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2564 - acc: 0.9131 - val_loss: 0.4395 - val_acc: 0.8201\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2562 - acc: 0.9131 - val_loss: 0.4393 - val_acc: 0.8201\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2561 - acc: 0.9125 - val_loss: 0.4393 - val_acc: 0.8201\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2560 - acc: 0.9131 - val_loss: 0.4392 - val_acc: 0.8201\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2558 - acc: 0.9137 - val_loss: 0.4392 - val_acc: 0.8201\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2557 - acc: 0.9131 - val_loss: 0.4392 - val_acc: 0.8201\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2556 - acc: 0.9131 - val_loss: 0.4393 - val_acc: 0.8201\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2555 - acc: 0.9137 - val_loss: 0.4391 - val_acc: 0.8201\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2553 - acc: 0.9137 - val_loss: 0.4390 - val_acc: 0.8201\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2552 - acc: 0.9125 - val_loss: 0.4388 - val_acc: 0.8201\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2551 - acc: 0.9131 - val_loss: 0.4387 - val_acc: 0.8201\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2550 - acc: 0.9137 - val_loss: 0.4386 - val_acc: 0.8201\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2549 - acc: 0.9149 - val_loss: 0.4385 - val_acc: 0.8201\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2548 - acc: 0.9143 - val_loss: 0.4386 - val_acc: 0.8201\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2546 - acc: 0.9143 - val_loss: 0.4385 - val_acc: 0.8201\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2545 - acc: 0.9137 - val_loss: 0.4383 - val_acc: 0.8201\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2544 - acc: 0.9137 - val_loss: 0.4383 - val_acc: 0.8201\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2543 - acc: 0.9143 - val_loss: 0.4381 - val_acc: 0.8201\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2542 - acc: 0.9143 - val_loss: 0.4381 - val_acc: 0.8201\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2541 - acc: 0.9137 - val_loss: 0.4380 - val_acc: 0.8201\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2540 - acc: 0.9143 - val_loss: 0.4377 - val_acc: 0.8201\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2539 - acc: 0.9137 - val_loss: 0.4378 - val_acc: 0.8201\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2538 - acc: 0.9131 - val_loss: 0.4378 - val_acc: 0.8201\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2537 - acc: 0.9137 - val_loss: 0.4377 - val_acc: 0.8201\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2536 - acc: 0.9131 - val_loss: 0.4375 - val_acc: 0.8201\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2535 - acc: 0.9137 - val_loss: 0.4374 - val_acc: 0.8201\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2534 - acc: 0.9137 - val_loss: 0.4373 - val_acc: 0.8201\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2533 - acc: 0.9137 - val_loss: 0.4372 - val_acc: 0.8201\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2532 - acc: 0.9137 - val_loss: 0.4373 - val_acc: 0.8201\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2530 - acc: 0.9143 - val_loss: 0.4372 - val_acc: 0.8201\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2529 - acc: 0.9137 - val_loss: 0.4372 - val_acc: 0.8201\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2528 - acc: 0.9137 - val_loss: 0.4370 - val_acc: 0.8201\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2528 - acc: 0.9137 - val_loss: 0.4370 - val_acc: 0.8201\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2526 - acc: 0.9137 - val_loss: 0.4370 - val_acc: 0.8201\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2526 - acc: 0.9137 - val_loss: 0.4369 - val_acc: 0.8201\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2524 - acc: 0.9137 - val_loss: 0.4368 - val_acc: 0.8201\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2523 - acc: 0.9137 - val_loss: 0.4366 - val_acc: 0.8201\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2522 - acc: 0.9143 - val_loss: 0.4365 - val_acc: 0.8254\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2521 - acc: 0.9137 - val_loss: 0.4363 - val_acc: 0.8254\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2520 - acc: 0.9137 - val_loss: 0.4362 - val_acc: 0.8254\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2519 - acc: 0.9137 - val_loss: 0.4361 - val_acc: 0.8254\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2518 - acc: 0.9137 - val_loss: 0.4360 - val_acc: 0.8254\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2517 - acc: 0.9137 - val_loss: 0.4360 - val_acc: 0.8254\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2516 - acc: 0.9137 - val_loss: 0.4359 - val_acc: 0.8254\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2515 - acc: 0.9137 - val_loss: 0.4357 - val_acc: 0.8254\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2514 - acc: 0.9143 - val_loss: 0.4355 - val_acc: 0.8254\n",
      "Epoch 415/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2513 - acc: 0.9137 - val_loss: 0.4356 - val_acc: 0.8254\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2512 - acc: 0.9137 - val_loss: 0.4355 - val_acc: 0.8254\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2511 - acc: 0.9131 - val_loss: 0.4355 - val_acc: 0.8307\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2510 - acc: 0.9137 - val_loss: 0.4353 - val_acc: 0.8307\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2509 - acc: 0.9131 - val_loss: 0.4352 - val_acc: 0.8307\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2508 - acc: 0.9137 - val_loss: 0.4351 - val_acc: 0.8307\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2507 - acc: 0.9137 - val_loss: 0.4349 - val_acc: 0.8307\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2506 - acc: 0.9131 - val_loss: 0.4351 - val_acc: 0.8307\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2505 - acc: 0.9125 - val_loss: 0.4349 - val_acc: 0.8307\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2504 - acc: 0.9131 - val_loss: 0.4349 - val_acc: 0.8307\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2503 - acc: 0.9137 - val_loss: 0.4348 - val_acc: 0.8307\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2502 - acc: 0.9137 - val_loss: 0.4347 - val_acc: 0.8307\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2501 - acc: 0.9137 - val_loss: 0.4345 - val_acc: 0.8307\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2500 - acc: 0.9131 - val_loss: 0.4342 - val_acc: 0.8307\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2499 - acc: 0.9131 - val_loss: 0.4340 - val_acc: 0.8307\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2498 - acc: 0.9131 - val_loss: 0.4339 - val_acc: 0.8307\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2497 - acc: 0.9137 - val_loss: 0.4338 - val_acc: 0.8307\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2496 - acc: 0.9131 - val_loss: 0.4336 - val_acc: 0.8360\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2495 - acc: 0.9125 - val_loss: 0.4335 - val_acc: 0.8360\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2494 - acc: 0.9131 - val_loss: 0.4335 - val_acc: 0.8360\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2493 - acc: 0.9131 - val_loss: 0.4336 - val_acc: 0.8307\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2492 - acc: 0.9137 - val_loss: 0.4335 - val_acc: 0.8307\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2491 - acc: 0.9137 - val_loss: 0.4334 - val_acc: 0.8360\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2491 - acc: 0.9137 - val_loss: 0.4333 - val_acc: 0.8307\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2490 - acc: 0.9137 - val_loss: 0.4333 - val_acc: 0.8307\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2489 - acc: 0.9131 - val_loss: 0.4332 - val_acc: 0.8307\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2488 - acc: 0.9137 - val_loss: 0.4330 - val_acc: 0.8360\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2487 - acc: 0.9137 - val_loss: 0.4330 - val_acc: 0.8360\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2486 - acc: 0.9131 - val_loss: 0.4329 - val_acc: 0.8360\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2485 - acc: 0.9137 - val_loss: 0.4329 - val_acc: 0.8360\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2484 - acc: 0.9137 - val_loss: 0.4328 - val_acc: 0.8360\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2483 - acc: 0.9131 - val_loss: 0.4327 - val_acc: 0.8360\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2483 - acc: 0.9137 - val_loss: 0.4326 - val_acc: 0.8360\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2481 - acc: 0.9137 - val_loss: 0.4325 - val_acc: 0.8360\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2481 - acc: 0.9137 - val_loss: 0.4326 - val_acc: 0.8307\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2480 - acc: 0.9137 - val_loss: 0.4325 - val_acc: 0.8307\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2479 - acc: 0.9143 - val_loss: 0.4323 - val_acc: 0.8360\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2478 - acc: 0.9143 - val_loss: 0.4323 - val_acc: 0.8307\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2477 - acc: 0.9131 - val_loss: 0.4323 - val_acc: 0.8307\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2476 - acc: 0.9149 - val_loss: 0.4322 - val_acc: 0.8307\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2475 - acc: 0.9143 - val_loss: 0.4319 - val_acc: 0.8307\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2475 - acc: 0.9149 - val_loss: 0.4320 - val_acc: 0.8307\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2473 - acc: 0.9143 - val_loss: 0.4320 - val_acc: 0.8307\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2473 - acc: 0.9149 - val_loss: 0.4320 - val_acc: 0.8307\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2472 - acc: 0.9143 - val_loss: 0.4320 - val_acc: 0.8307\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2471 - acc: 0.9149 - val_loss: 0.4318 - val_acc: 0.8307\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2470 - acc: 0.9149 - val_loss: 0.4317 - val_acc: 0.8307\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2470 - acc: 0.9149 - val_loss: 0.4317 - val_acc: 0.8307\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2469 - acc: 0.9155 - val_loss: 0.4315 - val_acc: 0.8307\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2468 - acc: 0.9149 - val_loss: 0.4316 - val_acc: 0.8307\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2467 - acc: 0.9149 - val_loss: 0.4316 - val_acc: 0.8307\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2466 - acc: 0.9149 - val_loss: 0.4314 - val_acc: 0.8307\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2465 - acc: 0.9149 - val_loss: 0.4313 - val_acc: 0.8307\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2464 - acc: 0.9149 - val_loss: 0.4312 - val_acc: 0.8307\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2464 - acc: 0.9149 - val_loss: 0.4312 - val_acc: 0.8307\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2463 - acc: 0.9149 - val_loss: 0.4312 - val_acc: 0.8307\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2462 - acc: 0.9149 - val_loss: 0.4312 - val_acc: 0.8307\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2461 - acc: 0.9149 - val_loss: 0.4311 - val_acc: 0.8307\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2460 - acc: 0.9149 - val_loss: 0.4310 - val_acc: 0.8307\n",
      "Epoch 474/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2459 - acc: 0.9149 - val_loss: 0.4309 - val_acc: 0.8307\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2459 - acc: 0.9149 - val_loss: 0.4308 - val_acc: 0.8307\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2458 - acc: 0.9155 - val_loss: 0.4308 - val_acc: 0.8307\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2457 - acc: 0.9149 - val_loss: 0.4308 - val_acc: 0.8307\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2456 - acc: 0.9149 - val_loss: 0.4308 - val_acc: 0.8307\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2456 - acc: 0.9155 - val_loss: 0.4308 - val_acc: 0.8254\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2455 - acc: 0.9155 - val_loss: 0.4307 - val_acc: 0.8254\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2454 - acc: 0.9161 - val_loss: 0.4305 - val_acc: 0.8307\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2453 - acc: 0.9155 - val_loss: 0.4304 - val_acc: 0.8307\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2452 - acc: 0.9161 - val_loss: 0.4304 - val_acc: 0.8360\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2451 - acc: 0.9155 - val_loss: 0.4303 - val_acc: 0.8360\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2451 - acc: 0.9161 - val_loss: 0.4304 - val_acc: 0.8307\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2450 - acc: 0.9161 - val_loss: 0.4305 - val_acc: 0.8307\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.2449 - acc: 0.9161 - val_loss: 0.4304 - val_acc: 0.8307\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.2448 - acc: 0.9161 - val_loss: 0.4305 - val_acc: 0.8307\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 117us/step - loss: 0.2448 - acc: 0.9161 - val_loss: 0.4303 - val_acc: 0.8307\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.2447 - acc: 0.9161 - val_loss: 0.4302 - val_acc: 0.8307\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.2470 - acc: 0.915 - 0s 117us/step - loss: 0.2446 - acc: 0.9161 - val_loss: 0.4299 - val_acc: 0.8413\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.2445 - acc: 0.9161 - val_loss: 0.4298 - val_acc: 0.8413\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.2445 - acc: 0.9161 - val_loss: 0.4298 - val_acc: 0.8413\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2443 - acc: 0.9167 - val_loss: 0.4299 - val_acc: 0.8360\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 203us/step - loss: 0.2443 - acc: 0.9161 - val_loss: 0.4297 - val_acc: 0.8413\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.2442 - acc: 0.9161 - val_loss: 0.4296 - val_acc: 0.8413\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2441 - acc: 0.9161 - val_loss: 0.4297 - val_acc: 0.8307\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.2441 - acc: 0.9161 - val_loss: 0.4296 - val_acc: 0.8307\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.2440 - acc: 0.9161 - val_loss: 0.4293 - val_acc: 0.8360\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 134us/step - loss: 0.2439 - acc: 0.9167 - val_loss: 0.4293 - val_acc: 0.8307\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 145us/step - loss: 0.2438 - acc: 0.9161 - val_loss: 0.4291 - val_acc: 0.8413\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 179us/step - loss: 0.2438 - acc: 0.9167 - val_loss: 0.4290 - val_acc: 0.8413\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 149us/step - loss: 0.2437 - acc: 0.9161 - val_loss: 0.4290 - val_acc: 0.8413\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 152us/step - loss: 0.2436 - acc: 0.9161 - val_loss: 0.4289 - val_acc: 0.8413\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 159us/step - loss: 0.2435 - acc: 0.9167 - val_loss: 0.4289 - val_acc: 0.8413\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 147us/step - loss: 0.2434 - acc: 0.9161 - val_loss: 0.4287 - val_acc: 0.8413\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.2434 - acc: 0.9167 - val_loss: 0.4287 - val_acc: 0.8413\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 130us/step - loss: 0.2433 - acc: 0.9161 - val_loss: 0.4287 - val_acc: 0.8413\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.2432 - acc: 0.9161 - val_loss: 0.4286 - val_acc: 0.8413\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.2432 - acc: 0.9167 - val_loss: 0.4285 - val_acc: 0.8413\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 103us/step - loss: 0.2431 - acc: 0.9167 - val_loss: 0.4286 - val_acc: 0.8413\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2430 - acc: 0.9161 - val_loss: 0.4284 - val_acc: 0.8413\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.2430 - acc: 0.9161 - val_loss: 0.4285 - val_acc: 0.8413\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 135us/step - loss: 0.2429 - acc: 0.9167 - val_loss: 0.4284 - val_acc: 0.8413\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2428 - acc: 0.9167 - val_loss: 0.4283 - val_acc: 0.8413\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.2427 - acc: 0.9161 - val_loss: 0.4282 - val_acc: 0.8413\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.2427 - acc: 0.9161 - val_loss: 0.4279 - val_acc: 0.8413\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.2426 - acc: 0.9167 - val_loss: 0.4279 - val_acc: 0.8413\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.2425 - acc: 0.9178 - val_loss: 0.4280 - val_acc: 0.8413\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.2424 - acc: 0.9173 - val_loss: 0.4278 - val_acc: 0.8413\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2424 - acc: 0.9173 - val_loss: 0.4278 - val_acc: 0.8413\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2423 - acc: 0.9167 - val_loss: 0.4277 - val_acc: 0.8413\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2422 - acc: 0.9173 - val_loss: 0.4275 - val_acc: 0.8413\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2422 - acc: 0.9173 - val_loss: 0.4276 - val_acc: 0.8413\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2421 - acc: 0.9167 - val_loss: 0.4274 - val_acc: 0.8413\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2421 - acc: 0.9173 - val_loss: 0.4275 - val_acc: 0.8413\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2420 - acc: 0.9178 - val_loss: 0.4275 - val_acc: 0.8413\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2419 - acc: 0.9178 - val_loss: 0.4274 - val_acc: 0.8413\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2418 - acc: 0.9173 - val_loss: 0.4272 - val_acc: 0.8413\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2418 - acc: 0.9167 - val_loss: 0.4270 - val_acc: 0.8413\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2417 - acc: 0.9173 - val_loss: 0.4269 - val_acc: 0.8413\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2416 - acc: 0.9178 - val_loss: 0.4269 - val_acc: 0.8413\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2415 - acc: 0.9178 - val_loss: 0.4269 - val_acc: 0.8413\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2415 - acc: 0.9178 - val_loss: 0.4268 - val_acc: 0.8413\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2414 - acc: 0.9178 - val_loss: 0.4267 - val_acc: 0.8413\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2414 - acc: 0.9173 - val_loss: 0.4268 - val_acc: 0.8413\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2413 - acc: 0.9173 - val_loss: 0.4266 - val_acc: 0.8413\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2412 - acc: 0.9178 - val_loss: 0.4267 - val_acc: 0.8413\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2412 - acc: 0.9184 - val_loss: 0.4267 - val_acc: 0.8413\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2411 - acc: 0.9178 - val_loss: 0.4267 - val_acc: 0.8360\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2410 - acc: 0.9173 - val_loss: 0.4264 - val_acc: 0.8413\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2410 - acc: 0.9184 - val_loss: 0.4263 - val_acc: 0.8413\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2409 - acc: 0.9178 - val_loss: 0.4260 - val_acc: 0.8413\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2408 - acc: 0.9190 - val_loss: 0.4260 - val_acc: 0.8413\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2407 - acc: 0.9184 - val_loss: 0.4259 - val_acc: 0.8413\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2407 - acc: 0.9184 - val_loss: 0.4258 - val_acc: 0.8413\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2406 - acc: 0.9184 - val_loss: 0.4257 - val_acc: 0.8413\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2405 - acc: 0.9184 - val_loss: 0.4256 - val_acc: 0.8413\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2405 - acc: 0.9184 - val_loss: 0.4257 - val_acc: 0.8413\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2404 - acc: 0.9184 - val_loss: 0.4256 - val_acc: 0.8413\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2403 - acc: 0.9178 - val_loss: 0.4257 - val_acc: 0.8413\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2403 - acc: 0.9184 - val_loss: 0.4256 - val_acc: 0.8413\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2402 - acc: 0.9184 - val_loss: 0.4254 - val_acc: 0.8413\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2401 - acc: 0.9184 - val_loss: 0.4254 - val_acc: 0.8413\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2401 - acc: 0.9184 - val_loss: 0.4253 - val_acc: 0.8413\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2400 - acc: 0.9184 - val_loss: 0.4251 - val_acc: 0.8413\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2400 - acc: 0.9184 - val_loss: 0.4251 - val_acc: 0.8413\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2399 - acc: 0.9184 - val_loss: 0.4250 - val_acc: 0.8413\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2398 - acc: 0.9184 - val_loss: 0.4250 - val_acc: 0.8413\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2398 - acc: 0.9184 - val_loss: 0.4249 - val_acc: 0.8413\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2397 - acc: 0.9184 - val_loss: 0.4247 - val_acc: 0.8413\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2396 - acc: 0.9184 - val_loss: 0.4245 - val_acc: 0.8413\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2395 - acc: 0.9178 - val_loss: 0.4247 - val_acc: 0.8413\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2395 - acc: 0.9184 - val_loss: 0.4247 - val_acc: 0.8413\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2394 - acc: 0.9184 - val_loss: 0.4246 - val_acc: 0.8413\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2393 - acc: 0.9184 - val_loss: 0.4246 - val_acc: 0.8413\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2393 - acc: 0.9190 - val_loss: 0.4244 - val_acc: 0.8413\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2392 - acc: 0.9190 - val_loss: 0.4243 - val_acc: 0.8413\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2391 - acc: 0.9184 - val_loss: 0.4242 - val_acc: 0.8413\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2391 - acc: 0.9190 - val_loss: 0.4241 - val_acc: 0.8413\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2390 - acc: 0.9190 - val_loss: 0.4239 - val_acc: 0.8413\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2389 - acc: 0.9190 - val_loss: 0.4239 - val_acc: 0.8413\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2389 - acc: 0.9196 - val_loss: 0.4237 - val_acc: 0.8413\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2388 - acc: 0.9190 - val_loss: 0.4236 - val_acc: 0.8413\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2387 - acc: 0.9196 - val_loss: 0.4237 - val_acc: 0.8413\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2387 - acc: 0.9184 - val_loss: 0.4236 - val_acc: 0.8413\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2386 - acc: 0.9184 - val_loss: 0.4236 - val_acc: 0.8413\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2385 - acc: 0.9184 - val_loss: 0.4236 - val_acc: 0.8413\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2385 - acc: 0.9190 - val_loss: 0.4237 - val_acc: 0.8413\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2384 - acc: 0.9190 - val_loss: 0.4237 - val_acc: 0.8413\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2384 - acc: 0.9196 - val_loss: 0.4236 - val_acc: 0.8413\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2383 - acc: 0.9202 - val_loss: 0.4234 - val_acc: 0.8413\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2382 - acc: 0.9196 - val_loss: 0.4235 - val_acc: 0.8413\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2382 - acc: 0.9202 - val_loss: 0.4235 - val_acc: 0.8413\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2381 - acc: 0.9202 - val_loss: 0.4234 - val_acc: 0.8413\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.2380 - acc: 0.9202 - val_loss: 0.4233 - val_acc: 0.8413\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2380 - acc: 0.9196 - val_loss: 0.4232 - val_acc: 0.8413\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2379 - acc: 0.9196 - val_loss: 0.4233 - val_acc: 0.8413\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2378 - acc: 0.9196 - val_loss: 0.4233 - val_acc: 0.8413\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2378 - acc: 0.9202 - val_loss: 0.4232 - val_acc: 0.8413\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.2377 - acc: 0.9196 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 135us/step - loss: 0.2377 - acc: 0.9196 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.2376 - acc: 0.9190 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2375 - acc: 0.9202 - val_loss: 0.4232 - val_acc: 0.8413\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2375 - acc: 0.9196 - val_loss: 0.4232 - val_acc: 0.8413\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2374 - acc: 0.9202 - val_loss: 0.4232 - val_acc: 0.8413\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2374 - acc: 0.9202 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2373 - acc: 0.9214 - val_loss: 0.4230 - val_acc: 0.8413\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2372 - acc: 0.9208 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2372 - acc: 0.9202 - val_loss: 0.4231 - val_acc: 0.8413\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2371 - acc: 0.9196 - val_loss: 0.4230 - val_acc: 0.8413\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2370 - acc: 0.9208 - val_loss: 0.4229 - val_acc: 0.8413\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2370 - acc: 0.9208 - val_loss: 0.4230 - val_acc: 0.8413\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2369 - acc: 0.9208 - val_loss: 0.4229 - val_acc: 0.8413\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2368 - acc: 0.9202 - val_loss: 0.4228 - val_acc: 0.8413\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2368 - acc: 0.9202 - val_loss: 0.4228 - val_acc: 0.8413\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2367 - acc: 0.9214 - val_loss: 0.4228 - val_acc: 0.8413\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2366 - acc: 0.9214 - val_loss: 0.4228 - val_acc: 0.8413\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2366 - acc: 0.9214 - val_loss: 0.4228 - val_acc: 0.8413\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2365 - acc: 0.9208 - val_loss: 0.4227 - val_acc: 0.8413\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2365 - acc: 0.9214 - val_loss: 0.4226 - val_acc: 0.8413\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2364 - acc: 0.9202 - val_loss: 0.4226 - val_acc: 0.8413\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2363 - acc: 0.9214 - val_loss: 0.4225 - val_acc: 0.8413\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2363 - acc: 0.9208 - val_loss: 0.4226 - val_acc: 0.8413\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2362 - acc: 0.9208 - val_loss: 0.4225 - val_acc: 0.8413\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2361 - acc: 0.9214 - val_loss: 0.4226 - val_acc: 0.8413\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2360 - acc: 0.9214 - val_loss: 0.4225 - val_acc: 0.8413\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2360 - acc: 0.9214 - val_loss: 0.4224 - val_acc: 0.8413\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2359 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2359 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2358 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2357 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2357 - acc: 0.9214 - val_loss: 0.4224 - val_acc: 0.8413\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2356 - acc: 0.9214 - val_loss: 0.4224 - val_acc: 0.8413\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2356 - acc: 0.9214 - val_loss: 0.4222 - val_acc: 0.8413\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2355 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2354 - acc: 0.9214 - val_loss: 0.4223 - val_acc: 0.8413\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2354 - acc: 0.9214 - val_loss: 0.4222 - val_acc: 0.8413\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2353 - acc: 0.9214 - val_loss: 0.4222 - val_acc: 0.8413\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2352 - acc: 0.9214 - val_loss: 0.4221 - val_acc: 0.8413\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2352 - acc: 0.9214 - val_loss: 0.4221 - val_acc: 0.8413\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2352 - acc: 0.9214 - val_loss: 0.4222 - val_acc: 0.8413\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2351 - acc: 0.9214 - val_loss: 0.4221 - val_acc: 0.8413\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2350 - acc: 0.9214 - val_loss: 0.4220 - val_acc: 0.8413\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2349 - acc: 0.9214 - val_loss: 0.4220 - val_acc: 0.8413\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2349 - acc: 0.9214 - val_loss: 0.4220 - val_acc: 0.8413\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2348 - acc: 0.9214 - val_loss: 0.4219 - val_acc: 0.8413\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2347 - acc: 0.9214 - val_loss: 0.4220 - val_acc: 0.8413\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2347 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2346 - acc: 0.9214 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2346 - acc: 0.9214 - val_loss: 0.4217 - val_acc: 0.8413\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2345 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2344 - acc: 0.9214 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2344 - acc: 0.9214 - val_loss: 0.4217 - val_acc: 0.8413\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2344 - acc: 0.9220 - val_loss: 0.4217 - val_acc: 0.8413\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2343 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2342 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2341 - acc: 0.9214 - val_loss: 0.4219 - val_acc: 0.8413\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2341 - acc: 0.9220 - val_loss: 0.4221 - val_acc: 0.8413\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2340 - acc: 0.9220 - val_loss: 0.4220 - val_acc: 0.8413\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2340 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2339 - acc: 0.9220 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2339 - acc: 0.9220 - val_loss: 0.4219 - val_acc: 0.8413\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2338 - acc: 0.9220 - val_loss: 0.4219 - val_acc: 0.8413\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2337 - acc: 0.9214 - val_loss: 0.4218 - val_acc: 0.8413\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2337 - acc: 0.9220 - val_loss: 0.4217 - val_acc: 0.8413\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2336 - acc: 0.9220 - val_loss: 0.4216 - val_acc: 0.8413\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2336 - acc: 0.9220 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2335 - acc: 0.9214 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2335 - acc: 0.9214 - val_loss: 0.4215 - val_acc: 0.8413\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2334 - acc: 0.9220 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2334 - acc: 0.9214 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2333 - acc: 0.9214 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2332 - acc: 0.9214 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2331 - acc: 0.9214 - val_loss: 0.4214 - val_acc: 0.8413\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2331 - acc: 0.9214 - val_loss: 0.4213 - val_acc: 0.8413\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2330 - acc: 0.9214 - val_loss: 0.4213 - val_acc: 0.8413\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2330 - acc: 0.9220 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2329 - acc: 0.9214 - val_loss: 0.4212 - val_acc: 0.8413\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2328 - acc: 0.9214 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2328 - acc: 0.9214 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2327 - acc: 0.9214 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2326 - acc: 0.9214 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2326 - acc: 0.9214 - val_loss: 0.4212 - val_acc: 0.8413\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2325 - acc: 0.9214 - val_loss: 0.4212 - val_acc: 0.8413\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2325 - acc: 0.9214 - val_loss: 0.4213 - val_acc: 0.8413\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2324 - acc: 0.9214 - val_loss: 0.4212 - val_acc: 0.8413\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2323 - acc: 0.9214 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2323 - acc: 0.9214 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2322 - acc: 0.9214 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2322 - acc: 0.9214 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2321 - acc: 0.9220 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2321 - acc: 0.9214 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2320 - acc: 0.9220 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2320 - acc: 0.9220 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2319 - acc: 0.9220 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2318 - acc: 0.9220 - val_loss: 0.4212 - val_acc: 0.8413\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2318 - acc: 0.9220 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2317 - acc: 0.9208 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2317 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2316 - acc: 0.9214 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2316 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2315 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2315 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2314 - acc: 0.9214 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2314 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2313 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2313 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2312 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2311 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2311 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2310 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2310 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2309 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2309 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2308 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2308 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2307 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2307 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2306 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2305 - acc: 0.9226 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2305 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2304 - acc: 0.9220 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2304 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2303 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2303 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2302 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2302 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2301 - acc: 0.9226 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2301 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2300 - acc: 0.9220 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2300 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2299 - acc: 0.9226 - val_loss: 0.4210 - val_acc: 0.8413\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2298 - acc: 0.9226 - val_loss: 0.4211 - val_acc: 0.8413\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2298 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2298 - acc: 0.9226 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2297 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2297 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2296 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2295 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2295 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2294 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.2294 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.2293 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2293 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.2292 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2292 - acc: 0.9226 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2291 - acc: 0.9226 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2291 - acc: 0.9220 - val_loss: 0.4209 - val_acc: 0.8413\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2290 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2290 - acc: 0.9220 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2289 - acc: 0.9220 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2289 - acc: 0.9226 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2288 - acc: 0.9220 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2288 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2287 - acc: 0.9226 - val_loss: 0.4208 - val_acc: 0.8413\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2287 - acc: 0.9226 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2286 - acc: 0.9226 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2286 - acc: 0.9220 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2285 - acc: 0.9226 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2285 - acc: 0.9220 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2284 - acc: 0.9226 - val_loss: 0.4207 - val_acc: 0.8413\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2284 - acc: 0.9226 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2283 - acc: 0.9220 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2283 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2282 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2282 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2282 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2281 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2280 - acc: 0.9226 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2280 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2280 - acc: 0.9220 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2279 - acc: 0.9220 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2278 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2278 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2278 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8413\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2277 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2277 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2276 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8413\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2276 - acc: 0.9226 - val_loss: 0.4206 - val_acc: 0.8413\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2275 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8413\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2275 - acc: 0.9220 - val_loss: 0.4206 - val_acc: 0.8466\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2275 - acc: 0.9214 - val_loss: 0.4206 - val_acc: 0.8466\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2274 - acc: 0.9232 - val_loss: 0.4206 - val_acc: 0.8466\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2274 - acc: 0.9220 - val_loss: 0.4205 - val_acc: 0.8466\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2273 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8466\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2273 - acc: 0.9226 - val_loss: 0.4205 - val_acc: 0.8466\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2272 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8466\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2272 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8466\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2271 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8466\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2271 - acc: 0.9226 - val_loss: 0.4204 - val_acc: 0.8466\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2271 - acc: 0.9226 - val_loss: 0.4203 - val_acc: 0.8466\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2270 - acc: 0.9226 - val_loss: 0.4203 - val_acc: 0.8466\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2270 - acc: 0.9226 - val_loss: 0.4202 - val_acc: 0.8466\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2269 - acc: 0.9220 - val_loss: 0.4203 - val_acc: 0.8466\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2269 - acc: 0.9220 - val_loss: 0.4203 - val_acc: 0.8466\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.2268 - acc: 0.9220 - val_loss: 0.4202 - val_acc: 0.8466\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2268 - acc: 0.9220 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.2268 - acc: 0.9220 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2267 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2267 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.2266 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.2266 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 143us/step - loss: 0.2266 - acc: 0.9226 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 104us/step - loss: 0.2265 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.2265 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2264 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2264 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2264 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2263 - acc: 0.9220 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2263 - acc: 0.9226 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.2262 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2262 - acc: 0.9226 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2262 - acc: 0.9220 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2261 - acc: 0.9220 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2261 - acc: 0.9220 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.2260 - acc: 0.9226 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2260 - acc: 0.9220 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.2260 - acc: 0.9220 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2259 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2259 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2258 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2258 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2257 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2257 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2257 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2256 - acc: 0.9226 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2256 - acc: 0.9232 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2256 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2255 - acc: 0.9232 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2255 - acc: 0.9232 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2255 - acc: 0.9232 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2254 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2254 - acc: 0.9232 - val_loss: 0.4202 - val_acc: 0.8466\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2253 - acc: 0.9232 - val_loss: 0.4202 - val_acc: 0.8466\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2253 - acc: 0.9226 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2252 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2252 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2252 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2251 - acc: 0.9226 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2251 - acc: 0.9232 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2250 - acc: 0.9232 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2250 - acc: 0.9232 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2250 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2249 - acc: 0.9226 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2249 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2248 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.2274 - acc: 0.920 - 0s 52us/step - loss: 0.2248 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2248 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2247 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2247 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2247 - acc: 0.9238 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2246 - acc: 0.9238 - val_loss: 0.4201 - val_acc: 0.8466\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2246 - acc: 0.9232 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2246 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2245 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2245 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2244 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2244 - acc: 0.9238 - val_loss: 0.4200 - val_acc: 0.8466\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2244 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2243 - acc: 0.9238 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2243 - acc: 0.9238 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2242 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2242 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2242 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2241 - acc: 0.9238 - val_loss: 0.4199 - val_acc: 0.8466\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.2241 - acc: 0.9238 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2240 - acc: 0.9243 - val_loss: 0.4198 - val_acc: 0.8466\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.2240 - acc: 0.9243 - val_loss: 0.4197 - val_acc: 0.8466\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2240 - acc: 0.9243 - val_loss: 0.4196 - val_acc: 0.8466\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2239 - acc: 0.9238 - val_loss: 0.4196 - val_acc: 0.8466\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2239 - acc: 0.9238 - val_loss: 0.4196 - val_acc: 0.8466\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2238 - acc: 0.9238 - val_loss: 0.4196 - val_acc: 0.8466\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2238 - acc: 0.9238 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2238 - acc: 0.9243 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2237 - acc: 0.9243 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2237 - acc: 0.9238 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2237 - acc: 0.9243 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2236 - acc: 0.9238 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2236 - acc: 0.9238 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2236 - acc: 0.9243 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2235 - acc: 0.9243 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2235 - acc: 0.9243 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2234 - acc: 0.9243 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.2234 - acc: 0.9243 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2234 - acc: 0.9243 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2233 - acc: 0.9243 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2233 - acc: 0.9243 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2232 - acc: 0.9243 - val_loss: 0.4191 - val_acc: 0.8466\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2232 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2232 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2232 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2231 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2231 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2230 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.2230 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2230 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2229 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2229 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2229 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2228 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2228 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2228 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2227 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2227 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2227 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2226 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2226 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2225 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2225 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2225 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.2224 - acc: 0.9249 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.2224 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2224 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2222 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2222 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2221 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2221 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2221 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2221 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2220 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2220 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.2218 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.2218 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.4186 - val_acc: 0.8466\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2216 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2216 - acc: 0.9249 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2216 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2215 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2215 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2215 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2214 - acc: 0.9249 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2214 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2214 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2213 - acc: 0.9243 - val_loss: 0.4187 - val_acc: 0.8466\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2213 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2213 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2212 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2212 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2212 - acc: 0.9249 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2211 - acc: 0.9249 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2211 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2211 - acc: 0.9249 - val_loss: 0.4188 - val_acc: 0.8466\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2211 - acc: 0.9243 - val_loss: 0.4188 - val_acc: 0.8466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2210 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2210 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2209 - acc: 0.9249 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2209 - acc: 0.9243 - val_loss: 0.4189 - val_acc: 0.8466\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2209 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2209 - acc: 0.9249 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2208 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2208 - acc: 0.9243 - val_loss: 0.4190 - val_acc: 0.8466\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2208 - acc: 0.9243 - val_loss: 0.4191 - val_acc: 0.8466\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2207 - acc: 0.9243 - val_loss: 0.4191 - val_acc: 0.8466\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2207 - acc: 0.9249 - val_loss: 0.4191 - val_acc: 0.8466\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2207 - acc: 0.9249 - val_loss: 0.4191 - val_acc: 0.8466\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2206 - acc: 0.9249 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2206 - acc: 0.9243 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2206 - acc: 0.9243 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2205 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2205 - acc: 0.9243 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2205 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2204 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2204 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2204 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2203 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2203 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2203 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2202 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2202 - acc: 0.9243 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2202 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2201 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2201 - acc: 0.9243 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2201 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2200 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2200 - acc: 0.9243 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2200 - acc: 0.9243 - val_loss: 0.4196 - val_acc: 0.8466\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2200 - acc: 0.9249 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2199 - acc: 0.9249 - val_loss: 0.4195 - val_acc: 0.8466\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2199 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2199 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2198 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2198 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2198 - acc: 0.9249 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2197 - acc: 0.9249 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2197 - acc: 0.9249 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2197 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2197 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2196 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2196 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2196 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8466\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2195 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8466\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2195 - acc: 0.9249 - val_loss: 0.4192 - val_acc: 0.8466\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2195 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8413\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2194 - acc: 0.9249 - val_loss: 0.4193 - val_acc: 0.8413\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2194 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8413\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2194 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8413\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2193 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8413\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2193 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8413\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2193 - acc: 0.9249 - val_loss: 0.4194 - val_acc: 0.8413\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 537us/step - loss: 0.7000 - acc: 0.6253 - val_loss: 0.7149 - val_acc: 0.6085\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.6890 - acc: 0.6277 - val_loss: 0.7062 - val_acc: 0.6032\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.6816 - acc: 0.6318 - val_loss: 0.7001 - val_acc: 0.6138\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.6756 - acc: 0.6318 - val_loss: 0.6954 - val_acc: 0.6032\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.6706 - acc: 0.6359 - val_loss: 0.6912 - val_acc: 0.6032\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.6660 - acc: 0.6395 - val_loss: 0.6878 - val_acc: 0.6032\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.6619 - acc: 0.6413 - val_loss: 0.6846 - val_acc: 0.6190\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.6582 - acc: 0.6418 - val_loss: 0.6816 - val_acc: 0.6296\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6547 - acc: 0.6454 - val_loss: 0.6786 - val_acc: 0.6296\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6515 - acc: 0.6525 - val_loss: 0.6760 - val_acc: 0.6296\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6483 - acc: 0.6602 - val_loss: 0.6736 - val_acc: 0.6296\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6452 - acc: 0.6637 - val_loss: 0.6713 - val_acc: 0.6296\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.6420 - acc: 0.6696 - val_loss: 0.6693 - val_acc: 0.6296\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.6388 - acc: 0.6732 - val_loss: 0.6676 - val_acc: 0.6402\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6357 - acc: 0.6726 - val_loss: 0.6652 - val_acc: 0.6455\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6325 - acc: 0.6779 - val_loss: 0.6632 - val_acc: 0.6561\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.6292 - acc: 0.6797 - val_loss: 0.6609 - val_acc: 0.6561\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6258 - acc: 0.6832 - val_loss: 0.6583 - val_acc: 0.6561\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.6224 - acc: 0.6850 - val_loss: 0.6556 - val_acc: 0.6561\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.6190 - acc: 0.6927 - val_loss: 0.6530 - val_acc: 0.6614\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.6154 - acc: 0.6939 - val_loss: 0.6506 - val_acc: 0.6667\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.6118 - acc: 0.6956 - val_loss: 0.6475 - val_acc: 0.6614\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6080 - acc: 0.6974 - val_loss: 0.6445 - val_acc: 0.6667\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.6043 - acc: 0.6962 - val_loss: 0.6420 - val_acc: 0.6614\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.6006 - acc: 0.7004 - val_loss: 0.6386 - val_acc: 0.6667\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5972 - acc: 0.7063 - val_loss: 0.6366 - val_acc: 0.6667\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5934 - acc: 0.7051 - val_loss: 0.6336 - val_acc: 0.6772\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.5897 - acc: 0.7051 - val_loss: 0.6307 - val_acc: 0.6720\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.5861 - acc: 0.7086 - val_loss: 0.6285 - val_acc: 0.6772\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5821 - acc: 0.7098 - val_loss: 0.6259 - val_acc: 0.6772\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5784 - acc: 0.7157 - val_loss: 0.6236 - val_acc: 0.6772\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5745 - acc: 0.7175 - val_loss: 0.6210 - val_acc: 0.6772\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5710 - acc: 0.7187 - val_loss: 0.6187 - val_acc: 0.6825\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5674 - acc: 0.7204 - val_loss: 0.6153 - val_acc: 0.6772\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.5639 - acc: 0.7258 - val_loss: 0.6136 - val_acc: 0.6825\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5606 - acc: 0.7275 - val_loss: 0.6115 - val_acc: 0.6825\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5570 - acc: 0.7323 - val_loss: 0.6088 - val_acc: 0.6825\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5535 - acc: 0.7335 - val_loss: 0.6062 - val_acc: 0.6772\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5500 - acc: 0.7370 - val_loss: 0.6031 - val_acc: 0.6772\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5467 - acc: 0.7411 - val_loss: 0.6013 - val_acc: 0.6772\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5435 - acc: 0.7411 - val_loss: 0.5988 - val_acc: 0.6878\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5405 - acc: 0.7411 - val_loss: 0.5962 - val_acc: 0.6878\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5373 - acc: 0.7459 - val_loss: 0.5928 - val_acc: 0.6931\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5345 - acc: 0.7488 - val_loss: 0.5903 - val_acc: 0.6984\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5315 - acc: 0.7524 - val_loss: 0.5886 - val_acc: 0.6984\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5286 - acc: 0.7541 - val_loss: 0.5862 - val_acc: 0.6984\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5256 - acc: 0.7565 - val_loss: 0.5836 - val_acc: 0.6984\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5228 - acc: 0.7595 - val_loss: 0.5815 - val_acc: 0.6984\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.5199 - acc: 0.7636 - val_loss: 0.5799 - val_acc: 0.6984\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5173 - acc: 0.7624 - val_loss: 0.5763 - val_acc: 0.6984\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5148 - acc: 0.7689 - val_loss: 0.5742 - val_acc: 0.6984\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5121 - acc: 0.7719 - val_loss: 0.5729 - val_acc: 0.7037\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5093 - acc: 0.7713 - val_loss: 0.5683 - val_acc: 0.7196\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5068 - acc: 0.7760 - val_loss: 0.5665 - val_acc: 0.7143\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5043 - acc: 0.7778 - val_loss: 0.5663 - val_acc: 0.7090\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.5021 - acc: 0.7760 - val_loss: 0.5636 - val_acc: 0.7143\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4991 - acc: 0.7807 - val_loss: 0.5598 - val_acc: 0.7196\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4968 - acc: 0.7855 - val_loss: 0.5599 - val_acc: 0.7196\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4944 - acc: 0.7831 - val_loss: 0.5565 - val_acc: 0.7196\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4918 - acc: 0.7914 - val_loss: 0.5541 - val_acc: 0.7249\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4890 - acc: 0.7920 - val_loss: 0.5518 - val_acc: 0.7249\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4862 - acc: 0.7926 - val_loss: 0.5515 - val_acc: 0.7302\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.4836 - acc: 0.7955 - val_loss: 0.5487 - val_acc: 0.7354\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4805 - acc: 0.8020 - val_loss: 0.5455 - val_acc: 0.7407\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4776 - acc: 0.8026 - val_loss: 0.5440 - val_acc: 0.7354\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4744 - acc: 0.8073 - val_loss: 0.5435 - val_acc: 0.7407\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4712 - acc: 0.8115 - val_loss: 0.5382 - val_acc: 0.7460\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4682 - acc: 0.8126 - val_loss: 0.5360 - val_acc: 0.7566\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.4648 - acc: 0.8209 - val_loss: 0.5341 - val_acc: 0.7566\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4616 - acc: 0.8203 - val_loss: 0.5314 - val_acc: 0.7619\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4582 - acc: 0.8215 - val_loss: 0.5277 - val_acc: 0.7619\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.4549 - acc: 0.8239 - val_loss: 0.5271 - val_acc: 0.7672\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4512 - acc: 0.8268 - val_loss: 0.5235 - val_acc: 0.7619\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4473 - acc: 0.8292 - val_loss: 0.5250 - val_acc: 0.7619\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4440 - acc: 0.8310 - val_loss: 0.5175 - val_acc: 0.7672\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4404 - acc: 0.8351 - val_loss: 0.5156 - val_acc: 0.7672\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4362 - acc: 0.8357 - val_loss: 0.5127 - val_acc: 0.7725\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4327 - acc: 0.8369 - val_loss: 0.5127 - val_acc: 0.7778\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.4290 - acc: 0.8369 - val_loss: 0.5093 - val_acc: 0.7831\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.4255 - acc: 0.8363 - val_loss: 0.5059 - val_acc: 0.7778\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4219 - acc: 0.8404 - val_loss: 0.5017 - val_acc: 0.7831\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4183 - acc: 0.8434 - val_loss: 0.4997 - val_acc: 0.7831\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.4150 - acc: 0.8410 - val_loss: 0.4982 - val_acc: 0.7831\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.4116 - acc: 0.8499 - val_loss: 0.4958 - val_acc: 0.7831\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.4085 - acc: 0.8446 - val_loss: 0.4930 - val_acc: 0.7884\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.4050 - acc: 0.8499 - val_loss: 0.4937 - val_acc: 0.7884\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.4020 - acc: 0.8511 - val_loss: 0.4863 - val_acc: 0.7778\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3990 - acc: 0.8505 - val_loss: 0.4871 - val_acc: 0.7937\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3960 - acc: 0.8546 - val_loss: 0.4845 - val_acc: 0.7884\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3931 - acc: 0.8487 - val_loss: 0.4782 - val_acc: 0.7831\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3902 - acc: 0.8528 - val_loss: 0.4804 - val_acc: 0.7884\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3872 - acc: 0.8505 - val_loss: 0.4744 - val_acc: 0.7884\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3843 - acc: 0.8576 - val_loss: 0.4740 - val_acc: 0.7937\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3818 - acc: 0.8570 - val_loss: 0.4754 - val_acc: 0.7937\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3795 - acc: 0.8570 - val_loss: 0.4668 - val_acc: 0.7884\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3770 - acc: 0.8605 - val_loss: 0.4636 - val_acc: 0.7884\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3747 - acc: 0.8593 - val_loss: 0.4594 - val_acc: 0.7937\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3723 - acc: 0.8611 - val_loss: 0.4570 - val_acc: 0.7884\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3703 - acc: 0.8611 - val_loss: 0.4608 - val_acc: 0.7937\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3681 - acc: 0.8623 - val_loss: 0.4561 - val_acc: 0.7937\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3655 - acc: 0.8617 - val_loss: 0.4500 - val_acc: 0.7989\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3638 - acc: 0.8641 - val_loss: 0.4546 - val_acc: 0.7989\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3619 - acc: 0.8629 - val_loss: 0.4499 - val_acc: 0.7989\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3600 - acc: 0.8641 - val_loss: 0.4502 - val_acc: 0.7989\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3582 - acc: 0.8641 - val_loss: 0.4445 - val_acc: 0.8042\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3561 - acc: 0.8676 - val_loss: 0.4415 - val_acc: 0.7989\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3547 - acc: 0.8623 - val_loss: 0.4441 - val_acc: 0.8095\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3528 - acc: 0.8664 - val_loss: 0.4463 - val_acc: 0.7989\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3512 - acc: 0.8658 - val_loss: 0.4408 - val_acc: 0.8042\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3493 - acc: 0.8658 - val_loss: 0.4377 - val_acc: 0.8042\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3470 - acc: 0.8682 - val_loss: 0.4338 - val_acc: 0.8042\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3463 - acc: 0.8694 - val_loss: 0.4332 - val_acc: 0.8095\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3441 - acc: 0.8712 - val_loss: 0.4320 - val_acc: 0.8201\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3422 - acc: 0.8688 - val_loss: 0.4325 - val_acc: 0.8201\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3404 - acc: 0.8712 - val_loss: 0.4290 - val_acc: 0.8201\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3388 - acc: 0.8688 - val_loss: 0.4277 - val_acc: 0.8201\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3376 - acc: 0.8706 - val_loss: 0.4282 - val_acc: 0.8201\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3353 - acc: 0.8741 - val_loss: 0.4306 - val_acc: 0.8254\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3340 - acc: 0.8712 - val_loss: 0.4215 - val_acc: 0.8201\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3329 - acc: 0.8729 - val_loss: 0.4239 - val_acc: 0.8201\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3317 - acc: 0.8723 - val_loss: 0.4215 - val_acc: 0.8201\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3299 - acc: 0.8753 - val_loss: 0.4195 - val_acc: 0.8148\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3285 - acc: 0.8759 - val_loss: 0.4190 - val_acc: 0.8148\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3275 - acc: 0.8765 - val_loss: 0.4179 - val_acc: 0.8201\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3260 - acc: 0.8771 - val_loss: 0.4194 - val_acc: 0.8254\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3245 - acc: 0.8788 - val_loss: 0.4194 - val_acc: 0.8254\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3239 - acc: 0.8771 - val_loss: 0.4133 - val_acc: 0.8148\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3227 - acc: 0.8783 - val_loss: 0.4168 - val_acc: 0.8201\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3219 - acc: 0.8806 - val_loss: 0.4136 - val_acc: 0.8254\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3208 - acc: 0.8777 - val_loss: 0.4127 - val_acc: 0.8254\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3194 - acc: 0.8806 - val_loss: 0.4110 - val_acc: 0.8254\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3185 - acc: 0.8794 - val_loss: 0.4135 - val_acc: 0.8201\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3170 - acc: 0.8794 - val_loss: 0.4103 - val_acc: 0.8201\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3157 - acc: 0.8824 - val_loss: 0.4148 - val_acc: 0.8201\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3150 - acc: 0.8800 - val_loss: 0.4070 - val_acc: 0.8201\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3138 - acc: 0.8824 - val_loss: 0.4090 - val_acc: 0.8254\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3133 - acc: 0.8812 - val_loss: 0.4104 - val_acc: 0.8307\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3120 - acc: 0.8824 - val_loss: 0.4058 - val_acc: 0.8307\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3112 - acc: 0.8824 - val_loss: 0.4036 - val_acc: 0.8201\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3105 - acc: 0.8812 - val_loss: 0.4024 - val_acc: 0.8254\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3089 - acc: 0.8853 - val_loss: 0.4094 - val_acc: 0.8254\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.3086 - acc: 0.8853 - val_loss: 0.4021 - val_acc: 0.8254\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3080 - acc: 0.8842 - val_loss: 0.4023 - val_acc: 0.8201\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.3067 - acc: 0.8806 - val_loss: 0.4004 - val_acc: 0.8307\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3063 - acc: 0.8871 - val_loss: 0.4018 - val_acc: 0.8307\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3049 - acc: 0.8830 - val_loss: 0.3958 - val_acc: 0.8307\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.3042 - acc: 0.8889 - val_loss: 0.4003 - val_acc: 0.8360\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3031 - acc: 0.8871 - val_loss: 0.3968 - val_acc: 0.8307\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.3023 - acc: 0.8901 - val_loss: 0.3938 - val_acc: 0.8307\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.3017 - acc: 0.8865 - val_loss: 0.3909 - val_acc: 0.8413\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.3006 - acc: 0.8883 - val_loss: 0.3908 - val_acc: 0.8360\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2997 - acc: 0.8907 - val_loss: 0.3856 - val_acc: 0.8413\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2991 - acc: 0.8877 - val_loss: 0.3922 - val_acc: 0.8360\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2987 - acc: 0.8901 - val_loss: 0.3953 - val_acc: 0.8360\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2982 - acc: 0.8907 - val_loss: 0.3856 - val_acc: 0.8360\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2973 - acc: 0.8930 - val_loss: 0.3866 - val_acc: 0.8360\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2967 - acc: 0.8930 - val_loss: 0.3900 - val_acc: 0.8413\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2960 - acc: 0.8936 - val_loss: 0.3915 - val_acc: 0.8360\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2951 - acc: 0.8918 - val_loss: 0.3876 - val_acc: 0.8466\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2942 - acc: 0.8936 - val_loss: 0.3917 - val_acc: 0.8413\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2941 - acc: 0.8930 - val_loss: 0.3922 - val_acc: 0.8307\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2932 - acc: 0.8983 - val_loss: 0.3888 - val_acc: 0.8360\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2925 - acc: 0.8978 - val_loss: 0.3910 - val_acc: 0.8360\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2917 - acc: 0.8954 - val_loss: 0.3887 - val_acc: 0.8360\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2911 - acc: 0.8954 - val_loss: 0.3922 - val_acc: 0.8413\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2908 - acc: 0.8966 - val_loss: 0.3865 - val_acc: 0.8519\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2902 - acc: 0.8983 - val_loss: 0.3907 - val_acc: 0.8466\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2901 - acc: 0.8978 - val_loss: 0.3889 - val_acc: 0.8413\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2894 - acc: 0.8978 - val_loss: 0.3870 - val_acc: 0.8413\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2890 - acc: 0.8966 - val_loss: 0.3902 - val_acc: 0.8413\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2880 - acc: 0.8978 - val_loss: 0.3904 - val_acc: 0.8466\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2880 - acc: 0.9013 - val_loss: 0.3891 - val_acc: 0.8466\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2872 - acc: 0.8995 - val_loss: 0.3861 - val_acc: 0.8519\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2861 - acc: 0.8983 - val_loss: 0.3868 - val_acc: 0.8413\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2863 - acc: 0.9001 - val_loss: 0.3876 - val_acc: 0.8360\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2852 - acc: 0.8978 - val_loss: 0.3871 - val_acc: 0.8466\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2854 - acc: 0.9007 - val_loss: 0.3837 - val_acc: 0.8360\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2847 - acc: 0.9019 - val_loss: 0.3874 - val_acc: 0.8413\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2842 - acc: 0.9007 - val_loss: 0.3826 - val_acc: 0.8466\n",
      "Epoch 180/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2836 - acc: 0.9007 - val_loss: 0.3887 - val_acc: 0.8466\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2837 - acc: 0.9001 - val_loss: 0.3876 - val_acc: 0.8360\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2831 - acc: 0.9007 - val_loss: 0.3839 - val_acc: 0.8360\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2819 - acc: 0.8972 - val_loss: 0.3896 - val_acc: 0.8466\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2823 - acc: 0.8972 - val_loss: 0.3879 - val_acc: 0.8466\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2819 - acc: 0.8989 - val_loss: 0.3872 - val_acc: 0.8413\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2811 - acc: 0.9025 - val_loss: 0.3938 - val_acc: 0.8413\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2807 - acc: 0.9019 - val_loss: 0.3874 - val_acc: 0.8466\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2806 - acc: 0.8995 - val_loss: 0.3820 - val_acc: 0.8466\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2797 - acc: 0.9060 - val_loss: 0.3817 - val_acc: 0.8571\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2791 - acc: 0.9001 - val_loss: 0.3860 - val_acc: 0.8466\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2787 - acc: 0.9043 - val_loss: 0.3855 - val_acc: 0.8307\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2786 - acc: 0.9007 - val_loss: 0.3841 - val_acc: 0.8466\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2779 - acc: 0.9007 - val_loss: 0.3924 - val_acc: 0.8466\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2779 - acc: 0.9048 - val_loss: 0.3790 - val_acc: 0.8519\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2771 - acc: 0.9043 - val_loss: 0.3744 - val_acc: 0.8466\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2766 - acc: 0.9019 - val_loss: 0.3868 - val_acc: 0.8519\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2765 - acc: 0.9031 - val_loss: 0.3810 - val_acc: 0.8519\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2763 - acc: 0.9031 - val_loss: 0.3773 - val_acc: 0.8519\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2753 - acc: 0.9043 - val_loss: 0.3733 - val_acc: 0.8466\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2755 - acc: 0.9031 - val_loss: 0.3802 - val_acc: 0.8519\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2743 - acc: 0.9031 - val_loss: 0.3920 - val_acc: 0.8466\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2749 - acc: 0.9025 - val_loss: 0.3828 - val_acc: 0.8466\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2736 - acc: 0.9043 - val_loss: 0.3769 - val_acc: 0.8466\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2744 - acc: 0.9043 - val_loss: 0.3859 - val_acc: 0.8466\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2739 - acc: 0.9001 - val_loss: 0.3750 - val_acc: 0.8519\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2737 - acc: 0.9037 - val_loss: 0.3806 - val_acc: 0.8466\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2734 - acc: 0.9048 - val_loss: 0.3851 - val_acc: 0.8466\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2729 - acc: 0.9048 - val_loss: 0.3785 - val_acc: 0.8466\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2725 - acc: 0.9043 - val_loss: 0.3793 - val_acc: 0.8519\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2721 - acc: 0.9066 - val_loss: 0.3740 - val_acc: 0.8466\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2715 - acc: 0.9066 - val_loss: 0.3741 - val_acc: 0.8519\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2719 - acc: 0.9031 - val_loss: 0.3761 - val_acc: 0.8519\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2714 - acc: 0.9048 - val_loss: 0.3826 - val_acc: 0.8519\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2717 - acc: 0.9060 - val_loss: 0.3806 - val_acc: 0.8519\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2715 - acc: 0.9048 - val_loss: 0.3774 - val_acc: 0.8519\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2710 - acc: 0.9043 - val_loss: 0.3762 - val_acc: 0.8519\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2699 - acc: 0.9078 - val_loss: 0.3774 - val_acc: 0.8519\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2700 - acc: 0.9072 - val_loss: 0.3804 - val_acc: 0.8466\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2700 - acc: 0.9048 - val_loss: 0.3812 - val_acc: 0.8519\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2697 - acc: 0.9043 - val_loss: 0.3798 - val_acc: 0.8519\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 41us/step - loss: 0.2691 - acc: 0.9060 - val_loss: 0.3841 - val_acc: 0.8466\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2696 - acc: 0.9043 - val_loss: 0.3809 - val_acc: 0.8519\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2689 - acc: 0.9060 - val_loss: 0.3771 - val_acc: 0.8519\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2683 - acc: 0.9066 - val_loss: 0.3754 - val_acc: 0.8519\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2684 - acc: 0.9025 - val_loss: 0.3750 - val_acc: 0.8519\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2677 - acc: 0.9066 - val_loss: 0.3764 - val_acc: 0.8519\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2669 - acc: 0.9072 - val_loss: 0.3738 - val_acc: 0.8466\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2677 - acc: 0.9054 - val_loss: 0.3762 - val_acc: 0.8519\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2665 - acc: 0.9066 - val_loss: 0.3671 - val_acc: 0.8466\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2672 - acc: 0.9096 - val_loss: 0.3778 - val_acc: 0.8519\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2667 - acc: 0.9054 - val_loss: 0.3700 - val_acc: 0.8519\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2664 - acc: 0.9072 - val_loss: 0.3760 - val_acc: 0.8519\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2659 - acc: 0.9072 - val_loss: 0.3772 - val_acc: 0.8519\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2659 - acc: 0.9090 - val_loss: 0.3758 - val_acc: 0.8519\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2662 - acc: 0.9025 - val_loss: 0.3760 - val_acc: 0.8519\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2652 - acc: 0.9060 - val_loss: 0.3829 - val_acc: 0.8466\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2651 - acc: 0.9060 - val_loss: 0.3705 - val_acc: 0.8519\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2646 - acc: 0.9090 - val_loss: 0.3750 - val_acc: 0.8519\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2640 - acc: 0.9090 - val_loss: 0.3788 - val_acc: 0.8519\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2640 - acc: 0.9072 - val_loss: 0.3756 - val_acc: 0.8519\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2640 - acc: 0.9072 - val_loss: 0.3753 - val_acc: 0.8466\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2643 - acc: 0.9066 - val_loss: 0.3756 - val_acc: 0.8466\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2636 - acc: 0.9096 - val_loss: 0.3724 - val_acc: 0.8466\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2629 - acc: 0.9096 - val_loss: 0.3758 - val_acc: 0.8519\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2625 - acc: 0.9084 - val_loss: 0.3735 - val_acc: 0.8519\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2621 - acc: 0.9113 - val_loss: 0.3678 - val_acc: 0.8413\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2622 - acc: 0.9108 - val_loss: 0.3751 - val_acc: 0.8466\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2618 - acc: 0.9078 - val_loss: 0.3811 - val_acc: 0.8466\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2614 - acc: 0.9125 - val_loss: 0.3696 - val_acc: 0.8466\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2619 - acc: 0.9108 - val_loss: 0.3727 - val_acc: 0.8413\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2607 - acc: 0.9113 - val_loss: 0.3717 - val_acc: 0.8413\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2614 - acc: 0.9090 - val_loss: 0.3754 - val_acc: 0.8413\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2612 - acc: 0.9113 - val_loss: 0.3749 - val_acc: 0.8413\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2605 - acc: 0.9131 - val_loss: 0.3808 - val_acc: 0.8413\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2606 - acc: 0.9113 - val_loss: 0.3773 - val_acc: 0.8466\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2604 - acc: 0.9108 - val_loss: 0.3756 - val_acc: 0.8413\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2599 - acc: 0.9125 - val_loss: 0.3765 - val_acc: 0.8519\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2599 - acc: 0.9113 - val_loss: 0.3781 - val_acc: 0.8466\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2592 - acc: 0.9125 - val_loss: 0.3764 - val_acc: 0.8519\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2596 - acc: 0.9102 - val_loss: 0.3711 - val_acc: 0.8466\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2585 - acc: 0.9084 - val_loss: 0.3772 - val_acc: 0.8519\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2589 - acc: 0.9131 - val_loss: 0.3693 - val_acc: 0.8519\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2591 - acc: 0.9119 - val_loss: 0.3759 - val_acc: 0.8519\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2593 - acc: 0.9102 - val_loss: 0.3691 - val_acc: 0.8413\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2581 - acc: 0.9102 - val_loss: 0.3778 - val_acc: 0.8413\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2573 - acc: 0.9125 - val_loss: 0.3732 - val_acc: 0.8519\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2577 - acc: 0.9119 - val_loss: 0.3755 - val_acc: 0.8413\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2577 - acc: 0.9108 - val_loss: 0.3767 - val_acc: 0.8413\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2574 - acc: 0.9096 - val_loss: 0.3704 - val_acc: 0.8466\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2575 - acc: 0.9119 - val_loss: 0.3666 - val_acc: 0.8519\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2572 - acc: 0.9078 - val_loss: 0.3771 - val_acc: 0.8413\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2571 - acc: 0.9119 - val_loss: 0.3697 - val_acc: 0.8466\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2569 - acc: 0.9125 - val_loss: 0.3693 - val_acc: 0.8466\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2567 - acc: 0.9125 - val_loss: 0.3703 - val_acc: 0.8466\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2565 - acc: 0.9108 - val_loss: 0.3718 - val_acc: 0.8519\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2562 - acc: 0.9125 - val_loss: 0.3749 - val_acc: 0.8466\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2560 - acc: 0.9096 - val_loss: 0.3660 - val_acc: 0.8466\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2568 - acc: 0.9119 - val_loss: 0.3727 - val_acc: 0.8413\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2551 - acc: 0.9113 - val_loss: 0.3766 - val_acc: 0.8466\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2558 - acc: 0.9108 - val_loss: 0.3716 - val_acc: 0.8413\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2547 - acc: 0.9084 - val_loss: 0.3675 - val_acc: 0.8466\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2543 - acc: 0.9108 - val_loss: 0.3688 - val_acc: 0.8466\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2554 - acc: 0.9119 - val_loss: 0.3765 - val_acc: 0.8413\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2553 - acc: 0.9113 - val_loss: 0.3682 - val_acc: 0.8519\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2537 - acc: 0.9102 - val_loss: 0.3701 - val_acc: 0.8466\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2553 - acc: 0.9096 - val_loss: 0.3691 - val_acc: 0.8413\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2536 - acc: 0.9090 - val_loss: 0.3675 - val_acc: 0.8519\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2544 - acc: 0.9125 - val_loss: 0.3716 - val_acc: 0.8413\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2543 - acc: 0.9096 - val_loss: 0.3724 - val_acc: 0.8466\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2537 - acc: 0.9090 - val_loss: 0.3712 - val_acc: 0.8466\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2534 - acc: 0.9125 - val_loss: 0.3689 - val_acc: 0.8466\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2539 - acc: 0.9125 - val_loss: 0.3702 - val_acc: 0.8413\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2534 - acc: 0.9102 - val_loss: 0.3728 - val_acc: 0.8519\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2533 - acc: 0.9125 - val_loss: 0.3690 - val_acc: 0.8466\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2534 - acc: 0.9084 - val_loss: 0.3662 - val_acc: 0.8466\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2532 - acc: 0.9113 - val_loss: 0.3683 - val_acc: 0.8466\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2521 - acc: 0.9102 - val_loss: 0.3699 - val_acc: 0.8466\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2539 - acc: 0.9125 - val_loss: 0.3709 - val_acc: 0.8519\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2526 - acc: 0.9119 - val_loss: 0.3748 - val_acc: 0.8413\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2526 - acc: 0.9125 - val_loss: 0.3660 - val_acc: 0.8519\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2522 - acc: 0.9113 - val_loss: 0.3706 - val_acc: 0.8466\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2515 - acc: 0.9149 - val_loss: 0.3717 - val_acc: 0.8519\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2516 - acc: 0.9161 - val_loss: 0.3737 - val_acc: 0.8466\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2516 - acc: 0.9155 - val_loss: 0.3686 - val_acc: 0.8466\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2513 - acc: 0.9108 - val_loss: 0.3658 - val_acc: 0.8466\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2520 - acc: 0.9119 - val_loss: 0.3713 - val_acc: 0.8519\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2517 - acc: 0.9125 - val_loss: 0.3690 - val_acc: 0.8519\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2513 - acc: 0.9108 - val_loss: 0.3659 - val_acc: 0.8519\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2513 - acc: 0.9102 - val_loss: 0.3760 - val_acc: 0.8466\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2509 - acc: 0.9137 - val_loss: 0.3644 - val_acc: 0.8466\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2513 - acc: 0.9143 - val_loss: 0.3722 - val_acc: 0.8466\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2502 - acc: 0.9113 - val_loss: 0.3629 - val_acc: 0.8466\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2509 - acc: 0.9108 - val_loss: 0.3714 - val_acc: 0.8466\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2502 - acc: 0.9131 - val_loss: 0.3632 - val_acc: 0.8466\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2497 - acc: 0.9108 - val_loss: 0.3740 - val_acc: 0.8466\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2503 - acc: 0.9113 - val_loss: 0.3710 - val_acc: 0.8519\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2495 - acc: 0.9113 - val_loss: 0.3680 - val_acc: 0.8466\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2496 - acc: 0.9119 - val_loss: 0.3707 - val_acc: 0.8466\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2496 - acc: 0.9125 - val_loss: 0.3676 - val_acc: 0.8519\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2495 - acc: 0.9137 - val_loss: 0.3660 - val_acc: 0.8466\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2493 - acc: 0.9149 - val_loss: 0.3668 - val_acc: 0.8519\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2491 - acc: 0.9143 - val_loss: 0.3691 - val_acc: 0.8466\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2486 - acc: 0.9119 - val_loss: 0.3775 - val_acc: 0.8413\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2492 - acc: 0.9137 - val_loss: 0.3651 - val_acc: 0.8466\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2490 - acc: 0.9119 - val_loss: 0.3743 - val_acc: 0.8519\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2484 - acc: 0.9137 - val_loss: 0.3678 - val_acc: 0.8466\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2485 - acc: 0.9149 - val_loss: 0.3644 - val_acc: 0.8466\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2487 - acc: 0.9131 - val_loss: 0.3716 - val_acc: 0.8466\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2482 - acc: 0.9096 - val_loss: 0.3666 - val_acc: 0.8519\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2480 - acc: 0.9125 - val_loss: 0.3630 - val_acc: 0.8624\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2485 - acc: 0.9119 - val_loss: 0.3643 - val_acc: 0.8519\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2480 - acc: 0.9178 - val_loss: 0.3636 - val_acc: 0.8519\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2477 - acc: 0.9137 - val_loss: 0.3633 - val_acc: 0.8519\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2471 - acc: 0.9143 - val_loss: 0.3617 - val_acc: 0.8571\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2474 - acc: 0.9143 - val_loss: 0.3631 - val_acc: 0.8519\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2465 - acc: 0.9155 - val_loss: 0.3699 - val_acc: 0.8466\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2466 - acc: 0.9113 - val_loss: 0.3644 - val_acc: 0.8466\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2461 - acc: 0.9167 - val_loss: 0.3637 - val_acc: 0.8571\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2461 - acc: 0.9137 - val_loss: 0.3657 - val_acc: 0.8571\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2461 - acc: 0.9143 - val_loss: 0.3591 - val_acc: 0.8519\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2460 - acc: 0.9102 - val_loss: 0.3632 - val_acc: 0.8571\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2457 - acc: 0.9137 - val_loss: 0.3700 - val_acc: 0.8519\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2458 - acc: 0.9131 - val_loss: 0.3735 - val_acc: 0.8571\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2455 - acc: 0.9178 - val_loss: 0.3673 - val_acc: 0.8519\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2443 - acc: 0.9149 - val_loss: 0.3634 - val_acc: 0.8519\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2438 - acc: 0.9125 - val_loss: 0.3621 - val_acc: 0.8571\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2456 - acc: 0.9125 - val_loss: 0.3623 - val_acc: 0.8519\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2449 - acc: 0.9161 - val_loss: 0.3617 - val_acc: 0.8571\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2440 - acc: 0.9108 - val_loss: 0.3572 - val_acc: 0.8519\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2449 - acc: 0.9113 - val_loss: 0.3663 - val_acc: 0.8519\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2448 - acc: 0.9143 - val_loss: 0.3641 - val_acc: 0.8571\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2442 - acc: 0.9155 - val_loss: 0.3682 - val_acc: 0.8466\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2439 - acc: 0.9131 - val_loss: 0.3670 - val_acc: 0.8519\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2437 - acc: 0.9173 - val_loss: 0.3722 - val_acc: 0.8466\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2440 - acc: 0.9149 - val_loss: 0.3662 - val_acc: 0.8466\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2430 - acc: 0.9167 - val_loss: 0.3609 - val_acc: 0.8466\n",
      "Epoch 357/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2433 - acc: 0.9137 - val_loss: 0.3618 - val_acc: 0.8519\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2427 - acc: 0.9119 - val_loss: 0.3619 - val_acc: 0.8519\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2428 - acc: 0.9137 - val_loss: 0.3657 - val_acc: 0.8519\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2437 - acc: 0.9119 - val_loss: 0.3600 - val_acc: 0.8571\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2418 - acc: 0.9155 - val_loss: 0.3586 - val_acc: 0.8571\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2422 - acc: 0.9178 - val_loss: 0.3627 - val_acc: 0.8571\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2417 - acc: 0.9119 - val_loss: 0.3600 - val_acc: 0.8571\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2422 - acc: 0.9125 - val_loss: 0.3623 - val_acc: 0.8519\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2415 - acc: 0.9173 - val_loss: 0.3613 - val_acc: 0.8413\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2417 - acc: 0.9167 - val_loss: 0.3642 - val_acc: 0.8519\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2417 - acc: 0.9155 - val_loss: 0.3612 - val_acc: 0.8519\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2406 - acc: 0.9173 - val_loss: 0.3628 - val_acc: 0.8519\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2398 - acc: 0.9167 - val_loss: 0.3614 - val_acc: 0.8519\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2404 - acc: 0.9178 - val_loss: 0.3566 - val_acc: 0.8519\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2407 - acc: 0.9167 - val_loss: 0.3601 - val_acc: 0.8466\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2403 - acc: 0.9167 - val_loss: 0.3673 - val_acc: 0.8519\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2395 - acc: 0.9149 - val_loss: 0.3624 - val_acc: 0.8466\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2392 - acc: 0.9178 - val_loss: 0.3600 - val_acc: 0.8466\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2390 - acc: 0.9173 - val_loss: 0.3553 - val_acc: 0.8519\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2399 - acc: 0.9143 - val_loss: 0.3601 - val_acc: 0.8466\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2388 - acc: 0.9190 - val_loss: 0.3553 - val_acc: 0.8571\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2388 - acc: 0.9178 - val_loss: 0.3659 - val_acc: 0.8466\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2392 - acc: 0.9178 - val_loss: 0.3615 - val_acc: 0.8466\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2384 - acc: 0.9190 - val_loss: 0.3682 - val_acc: 0.8466\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2387 - acc: 0.9178 - val_loss: 0.3600 - val_acc: 0.8466\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2380 - acc: 0.9184 - val_loss: 0.3637 - val_acc: 0.8466\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2385 - acc: 0.9155 - val_loss: 0.3645 - val_acc: 0.8466\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2389 - acc: 0.9161 - val_loss: 0.3627 - val_acc: 0.8466\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2374 - acc: 0.9196 - val_loss: 0.3620 - val_acc: 0.8466\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2380 - acc: 0.9196 - val_loss: 0.3604 - val_acc: 0.8519\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2374 - acc: 0.9184 - val_loss: 0.3636 - val_acc: 0.8519\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2365 - acc: 0.9173 - val_loss: 0.3720 - val_acc: 0.8466\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2372 - acc: 0.9196 - val_loss: 0.3590 - val_acc: 0.8519\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2370 - acc: 0.9190 - val_loss: 0.3606 - val_acc: 0.8624\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2366 - acc: 0.9196 - val_loss: 0.3615 - val_acc: 0.8466\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2358 - acc: 0.9178 - val_loss: 0.3593 - val_acc: 0.8519\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2366 - acc: 0.9196 - val_loss: 0.3619 - val_acc: 0.8519\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2361 - acc: 0.9196 - val_loss: 0.3520 - val_acc: 0.8519\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2364 - acc: 0.9208 - val_loss: 0.3630 - val_acc: 0.8466\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2356 - acc: 0.9178 - val_loss: 0.3597 - val_acc: 0.8519\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2358 - acc: 0.9190 - val_loss: 0.3559 - val_acc: 0.8519\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2346 - acc: 0.9226 - val_loss: 0.3607 - val_acc: 0.8571\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2347 - acc: 0.9184 - val_loss: 0.3643 - val_acc: 0.8519\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2349 - acc: 0.9167 - val_loss: 0.3557 - val_acc: 0.8571\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2354 - acc: 0.9196 - val_loss: 0.3608 - val_acc: 0.8571\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2351 - acc: 0.9184 - val_loss: 0.3685 - val_acc: 0.8413\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2346 - acc: 0.9208 - val_loss: 0.3643 - val_acc: 0.8466\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2344 - acc: 0.9202 - val_loss: 0.3670 - val_acc: 0.8466\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2345 - acc: 0.9184 - val_loss: 0.3556 - val_acc: 0.8519\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2343 - acc: 0.9220 - val_loss: 0.3586 - val_acc: 0.8519\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2340 - acc: 0.9208 - val_loss: 0.3691 - val_acc: 0.8466\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2345 - acc: 0.9220 - val_loss: 0.3562 - val_acc: 0.8571\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2342 - acc: 0.9214 - val_loss: 0.3659 - val_acc: 0.8466\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2344 - acc: 0.9196 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2334 - acc: 0.9208 - val_loss: 0.3593 - val_acc: 0.8571\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2333 - acc: 0.9226 - val_loss: 0.3565 - val_acc: 0.8571\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2326 - acc: 0.9232 - val_loss: 0.3645 - val_acc: 0.8413\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2329 - acc: 0.9208 - val_loss: 0.3546 - val_acc: 0.8519\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2325 - acc: 0.9220 - val_loss: 0.3610 - val_acc: 0.8360\n",
      "Epoch 416/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2331 - acc: 0.9226 - val_loss: 0.3721 - val_acc: 0.8466\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2338 - acc: 0.9184 - val_loss: 0.3539 - val_acc: 0.8571\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2324 - acc: 0.9220 - val_loss: 0.3662 - val_acc: 0.8466\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2318 - acc: 0.9214 - val_loss: 0.3716 - val_acc: 0.8413\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2331 - acc: 0.9196 - val_loss: 0.3668 - val_acc: 0.8413\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2321 - acc: 0.9202 - val_loss: 0.3526 - val_acc: 0.8571\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2329 - acc: 0.9208 - val_loss: 0.3582 - val_acc: 0.8571\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2327 - acc: 0.9202 - val_loss: 0.3581 - val_acc: 0.8466\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2320 - acc: 0.9214 - val_loss: 0.3613 - val_acc: 0.8519\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2326 - acc: 0.9202 - val_loss: 0.3699 - val_acc: 0.8413\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2321 - acc: 0.9238 - val_loss: 0.3592 - val_acc: 0.8519\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2310 - acc: 0.9232 - val_loss: 0.3549 - val_acc: 0.8624\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2312 - acc: 0.9202 - val_loss: 0.3626 - val_acc: 0.8466\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2313 - acc: 0.9249 - val_loss: 0.3514 - val_acc: 0.8571\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2303 - acc: 0.9226 - val_loss: 0.3554 - val_acc: 0.8466\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2307 - acc: 0.9220 - val_loss: 0.3554 - val_acc: 0.8466\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2304 - acc: 0.9208 - val_loss: 0.3649 - val_acc: 0.8466\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2305 - acc: 0.9249 - val_loss: 0.3583 - val_acc: 0.8519\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2304 - acc: 0.9226 - val_loss: 0.3602 - val_acc: 0.8519\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2312 - acc: 0.9190 - val_loss: 0.3584 - val_acc: 0.8466\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2303 - acc: 0.9220 - val_loss: 0.3635 - val_acc: 0.8519\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2284 - acc: 0.9238 - val_loss: 0.3535 - val_acc: 0.8466\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2300 - acc: 0.9196 - val_loss: 0.3572 - val_acc: 0.8413\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2290 - acc: 0.9243 - val_loss: 0.3604 - val_acc: 0.8466\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2291 - acc: 0.9208 - val_loss: 0.3688 - val_acc: 0.8413\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2289 - acc: 0.9232 - val_loss: 0.3504 - val_acc: 0.8624\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2290 - acc: 0.9214 - val_loss: 0.3549 - val_acc: 0.8519\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2287 - acc: 0.9249 - val_loss: 0.3599 - val_acc: 0.8413\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2298 - acc: 0.9243 - val_loss: 0.3541 - val_acc: 0.8519\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2287 - acc: 0.9238 - val_loss: 0.3521 - val_acc: 0.8519\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2284 - acc: 0.9238 - val_loss: 0.3641 - val_acc: 0.8466\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2285 - acc: 0.9238 - val_loss: 0.3608 - val_acc: 0.8466\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2280 - acc: 0.9214 - val_loss: 0.3579 - val_acc: 0.8466\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2292 - acc: 0.9220 - val_loss: 0.3662 - val_acc: 0.8466\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2276 - acc: 0.9232 - val_loss: 0.3631 - val_acc: 0.8466\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2272 - acc: 0.9243 - val_loss: 0.3545 - val_acc: 0.8519\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2279 - acc: 0.9226 - val_loss: 0.3622 - val_acc: 0.8466\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2275 - acc: 0.9249 - val_loss: 0.3577 - val_acc: 0.8466\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2270 - acc: 0.9249 - val_loss: 0.3673 - val_acc: 0.8413\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2277 - acc: 0.9238 - val_loss: 0.3618 - val_acc: 0.8413\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2274 - acc: 0.9243 - val_loss: 0.3648 - val_acc: 0.8466\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2265 - acc: 0.9255 - val_loss: 0.3769 - val_acc: 0.8466\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2283 - acc: 0.9226 - val_loss: 0.3598 - val_acc: 0.8413\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2266 - acc: 0.9249 - val_loss: 0.3754 - val_acc: 0.8466\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2268 - acc: 0.9249 - val_loss: 0.3570 - val_acc: 0.8466\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.2254 - acc: 0.9226 - val_loss: 0.3672 - val_acc: 0.8413\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.2264 - acc: 0.9220 - val_loss: 0.3652 - val_acc: 0.8466\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.2264 - acc: 0.9243 - val_loss: 0.3643 - val_acc: 0.8413\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 136us/step - loss: 0.2270 - acc: 0.9232 - val_loss: 0.3620 - val_acc: 0.8519\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.2260 - acc: 0.9279 - val_loss: 0.3583 - val_acc: 0.8519\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2266 - acc: 0.9249 - val_loss: 0.3574 - val_acc: 0.8519\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2260 - acc: 0.9243 - val_loss: 0.3578 - val_acc: 0.8519\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2253 - acc: 0.9261 - val_loss: 0.3595 - val_acc: 0.8413\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2250 - acc: 0.9249 - val_loss: 0.3568 - val_acc: 0.8413\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2253 - acc: 0.9232 - val_loss: 0.3556 - val_acc: 0.8466\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2247 - acc: 0.9243 - val_loss: 0.3557 - val_acc: 0.8466\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2248 - acc: 0.9214 - val_loss: 0.3550 - val_acc: 0.8466\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2252 - acc: 0.9255 - val_loss: 0.3596 - val_acc: 0.8360\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2246 - acc: 0.9220 - val_loss: 0.3769 - val_acc: 0.8413\n",
      "Epoch 475/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2256 - acc: 0.9238 - val_loss: 0.3715 - val_acc: 0.8466\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2250 - acc: 0.9249 - val_loss: 0.3608 - val_acc: 0.8466\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2241 - acc: 0.9273 - val_loss: 0.3590 - val_acc: 0.8413\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2240 - acc: 0.9238 - val_loss: 0.3573 - val_acc: 0.8519\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2243 - acc: 0.9255 - val_loss: 0.3569 - val_acc: 0.8519\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2242 - acc: 0.9238 - val_loss: 0.3619 - val_acc: 0.8466\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.2237 - acc: 0.9243 - val_loss: 0.3596 - val_acc: 0.8466\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.2242 - acc: 0.9261 - val_loss: 0.3595 - val_acc: 0.8466\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.2234 - acc: 0.9243 - val_loss: 0.3629 - val_acc: 0.8466\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.2228 - acc: 0.9238 - val_loss: 0.3628 - val_acc: 0.8466\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 134us/step - loss: 0.2235 - acc: 0.9255 - val_loss: 0.3672 - val_acc: 0.8466\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.2231 - acc: 0.9255 - val_loss: 0.3616 - val_acc: 0.8466\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 152us/step - loss: 0.2237 - acc: 0.9261 - val_loss: 0.3595 - val_acc: 0.8413\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 128us/step - loss: 0.2224 - acc: 0.9255 - val_loss: 0.3527 - val_acc: 0.8519\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 128us/step - loss: 0.2215 - acc: 0.9261 - val_loss: 0.3581 - val_acc: 0.8466\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.2237 - acc: 0.9273 - val_loss: 0.3623 - val_acc: 0.8466\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2235 - acc: 0.9273 - val_loss: 0.3499 - val_acc: 0.8466\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2216 - acc: 0.9267 - val_loss: 0.3549 - val_acc: 0.8466\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.2236 - acc: 0.9238 - val_loss: 0.3578 - val_acc: 0.8519\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 155us/step - loss: 0.2223 - acc: 0.9243 - val_loss: 0.3724 - val_acc: 0.8466\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.2234 - acc: 0.9249 - val_loss: 0.3580 - val_acc: 0.8413\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.2225 - acc: 0.9238 - val_loss: 0.3595 - val_acc: 0.8519\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2221 - acc: 0.9249 - val_loss: 0.3574 - val_acc: 0.8519\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2223 - acc: 0.9261 - val_loss: 0.3592 - val_acc: 0.8519\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2222 - acc: 0.9285 - val_loss: 0.3649 - val_acc: 0.8519\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2233 - acc: 0.9261 - val_loss: 0.3522 - val_acc: 0.8519\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2221 - acc: 0.9279 - val_loss: 0.3679 - val_acc: 0.8519\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2219 - acc: 0.9285 - val_loss: 0.3596 - val_acc: 0.8519\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2219 - acc: 0.9243 - val_loss: 0.3615 - val_acc: 0.8519\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2229 - acc: 0.9261 - val_loss: 0.3601 - val_acc: 0.8466\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2217 - acc: 0.9255 - val_loss: 0.3626 - val_acc: 0.8466\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2223 - acc: 0.9303 - val_loss: 0.3533 - val_acc: 0.8571\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2213 - acc: 0.9279 - val_loss: 0.3623 - val_acc: 0.8519\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2212 - acc: 0.9285 - val_loss: 0.3664 - val_acc: 0.8466\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2213 - acc: 0.9285 - val_loss: 0.3555 - val_acc: 0.8413\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2212 - acc: 0.9291 - val_loss: 0.3596 - val_acc: 0.8466\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2198 - acc: 0.9279 - val_loss: 0.3619 - val_acc: 0.8519\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2207 - acc: 0.9261 - val_loss: 0.3661 - val_acc: 0.8466\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2211 - acc: 0.9303 - val_loss: 0.3687 - val_acc: 0.8466\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2200 - acc: 0.9261 - val_loss: 0.3808 - val_acc: 0.8466\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2204 - acc: 0.9267 - val_loss: 0.3512 - val_acc: 0.8571\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2201 - acc: 0.9267 - val_loss: 0.3630 - val_acc: 0.8466\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2208 - acc: 0.9267 - val_loss: 0.3577 - val_acc: 0.8466\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2196 - acc: 0.9249 - val_loss: 0.3684 - val_acc: 0.8519\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2198 - acc: 0.9273 - val_loss: 0.3643 - val_acc: 0.8519\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2189 - acc: 0.9261 - val_loss: 0.3630 - val_acc: 0.8519\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2195 - acc: 0.9279 - val_loss: 0.3618 - val_acc: 0.8571\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.2204 - acc: 0.9279 - val_loss: 0.3518 - val_acc: 0.8571\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2203 - acc: 0.9285 - val_loss: 0.3574 - val_acc: 0.8519\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2196 - acc: 0.9285 - val_loss: 0.3607 - val_acc: 0.8519\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2206 - acc: 0.9279 - val_loss: 0.3633 - val_acc: 0.8519\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2203 - acc: 0.9267 - val_loss: 0.3598 - val_acc: 0.8571\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2199 - acc: 0.9297 - val_loss: 0.3512 - val_acc: 0.8624\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2194 - acc: 0.9267 - val_loss: 0.3455 - val_acc: 0.8624\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2204 - acc: 0.9279 - val_loss: 0.3598 - val_acc: 0.8571\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2198 - acc: 0.9255 - val_loss: 0.3543 - val_acc: 0.8571\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2193 - acc: 0.9279 - val_loss: 0.3493 - val_acc: 0.8571\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2198 - acc: 0.9255 - val_loss: 0.3450 - val_acc: 0.8624\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2190 - acc: 0.9291 - val_loss: 0.3641 - val_acc: 0.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2187 - acc: 0.9303 - val_loss: 0.3668 - val_acc: 0.8519\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2182 - acc: 0.9291 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2186 - acc: 0.9279 - val_loss: 0.3609 - val_acc: 0.8519\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2177 - acc: 0.9273 - val_loss: 0.3585 - val_acc: 0.8519\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2185 - acc: 0.9297 - val_loss: 0.3581 - val_acc: 0.8519\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2189 - acc: 0.9285 - val_loss: 0.3525 - val_acc: 0.8624\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2187 - acc: 0.9291 - val_loss: 0.3592 - val_acc: 0.8571\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2180 - acc: 0.9279 - val_loss: 0.3508 - val_acc: 0.8624\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2179 - acc: 0.9314 - val_loss: 0.3608 - val_acc: 0.8571\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2176 - acc: 0.9285 - val_loss: 0.3534 - val_acc: 0.8571\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2170 - acc: 0.9291 - val_loss: 0.3458 - val_acc: 0.8624\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2188 - acc: 0.9273 - val_loss: 0.3541 - val_acc: 0.8624\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2167 - acc: 0.9267 - val_loss: 0.3536 - val_acc: 0.8571\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2175 - acc: 0.9267 - val_loss: 0.3593 - val_acc: 0.8519\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2170 - acc: 0.9344 - val_loss: 0.3527 - val_acc: 0.8571\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2172 - acc: 0.9279 - val_loss: 0.3609 - val_acc: 0.8519\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2177 - acc: 0.9320 - val_loss: 0.3506 - val_acc: 0.8624\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 131us/step - loss: 0.2168 - acc: 0.9332 - val_loss: 0.3539 - val_acc: 0.8571\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.2173 - acc: 0.9314 - val_loss: 0.3566 - val_acc: 0.8466\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2187 - acc: 0.9297 - val_loss: 0.3638 - val_acc: 0.8519\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.2160 - acc: 0.9267 - val_loss: 0.3592 - val_acc: 0.8466\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2166 - acc: 0.9303 - val_loss: 0.3575 - val_acc: 0.8519\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.2162 - acc: 0.9291 - val_loss: 0.3520 - val_acc: 0.8624\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.2158 - acc: 0.9291 - val_loss: 0.3657 - val_acc: 0.8519\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.2181 - acc: 0.9297 - val_loss: 0.3566 - val_acc: 0.8519\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2163 - acc: 0.9297 - val_loss: 0.3585 - val_acc: 0.8519\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.2154 - acc: 0.9291 - val_loss: 0.3729 - val_acc: 0.8519\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.2176 - acc: 0.9291 - val_loss: 0.3593 - val_acc: 0.8519\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.2154 - acc: 0.9291 - val_loss: 0.3469 - val_acc: 0.8677\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2163 - acc: 0.9314 - val_loss: 0.3456 - val_acc: 0.8624\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2168 - acc: 0.9297 - val_loss: 0.3526 - val_acc: 0.8571\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.2155 - acc: 0.9326 - val_loss: 0.3663 - val_acc: 0.8519\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.2161 - acc: 0.9309 - val_loss: 0.3506 - val_acc: 0.8624\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 111us/step - loss: 0.2163 - acc: 0.9314 - val_loss: 0.3554 - val_acc: 0.8571\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.2149 - acc: 0.9320 - val_loss: 0.3708 - val_acc: 0.8519\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.2156 - acc: 0.9303 - val_loss: 0.3717 - val_acc: 0.8519\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2145 - acc: 0.9285 - val_loss: 0.3515 - val_acc: 0.8571\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2162 - acc: 0.9309 - val_loss: 0.3528 - val_acc: 0.8571\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2155 - acc: 0.9309 - val_loss: 0.3479 - val_acc: 0.8624\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2149 - acc: 0.9320 - val_loss: 0.3444 - val_acc: 0.8571\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2152 - acc: 0.9320 - val_loss: 0.3568 - val_acc: 0.8571\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2153 - acc: 0.9314 - val_loss: 0.3496 - val_acc: 0.8571\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2142 - acc: 0.9332 - val_loss: 0.3506 - val_acc: 0.8571\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2148 - acc: 0.9303 - val_loss: 0.3535 - val_acc: 0.8571\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2150 - acc: 0.9314 - val_loss: 0.3493 - val_acc: 0.8571\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2150 - acc: 0.9326 - val_loss: 0.3527 - val_acc: 0.8571\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2160 - acc: 0.9303 - val_loss: 0.3524 - val_acc: 0.8571\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2130 - acc: 0.9344 - val_loss: 0.3640 - val_acc: 0.8519\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2143 - acc: 0.9326 - val_loss: 0.3554 - val_acc: 0.8571\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2140 - acc: 0.9326 - val_loss: 0.3663 - val_acc: 0.8571\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2143 - acc: 0.9320 - val_loss: 0.3585 - val_acc: 0.8519\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2137 - acc: 0.9303 - val_loss: 0.3465 - val_acc: 0.8624\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2139 - acc: 0.9314 - val_loss: 0.3521 - val_acc: 0.8519\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2134 - acc: 0.9320 - val_loss: 0.3669 - val_acc: 0.8519\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2142 - acc: 0.9344 - val_loss: 0.3639 - val_acc: 0.8519\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2133 - acc: 0.9338 - val_loss: 0.3603 - val_acc: 0.8519\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2135 - acc: 0.9332 - val_loss: 0.3585 - val_acc: 0.8519\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2137 - acc: 0.9356 - val_loss: 0.3516 - val_acc: 0.8466\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2124 - acc: 0.9326 - val_loss: 0.3416 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2128 - acc: 0.9338 - val_loss: 0.3618 - val_acc: 0.8519\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2131 - acc: 0.9338 - val_loss: 0.3510 - val_acc: 0.8519\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.2132 - acc: 0.9314 - val_loss: 0.3528 - val_acc: 0.8571\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.2121 - acc: 0.9332 - val_loss: 0.3590 - val_acc: 0.8519\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.2128 - acc: 0.9314 - val_loss: 0.3589 - val_acc: 0.8571\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2130 - acc: 0.9314 - val_loss: 0.3499 - val_acc: 0.8571\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2126 - acc: 0.9326 - val_loss: 0.3509 - val_acc: 0.8571\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2128 - acc: 0.9344 - val_loss: 0.3499 - val_acc: 0.8571\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2132 - acc: 0.9338 - val_loss: 0.3601 - val_acc: 0.8571\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2114 - acc: 0.9332 - val_loss: 0.3593 - val_acc: 0.8519\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2119 - acc: 0.9362 - val_loss: 0.3565 - val_acc: 0.8519\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2118 - acc: 0.9356 - val_loss: 0.3452 - val_acc: 0.8571\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2110 - acc: 0.9344 - val_loss: 0.3586 - val_acc: 0.8571\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2112 - acc: 0.9344 - val_loss: 0.3523 - val_acc: 0.8571\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2108 - acc: 0.9344 - val_loss: 0.3437 - val_acc: 0.8571\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2118 - acc: 0.9332 - val_loss: 0.3518 - val_acc: 0.8519\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2110 - acc: 0.9368 - val_loss: 0.3562 - val_acc: 0.8519\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2106 - acc: 0.9332 - val_loss: 0.3395 - val_acc: 0.8571\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2101 - acc: 0.9368 - val_loss: 0.3617 - val_acc: 0.8624\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2112 - acc: 0.9326 - val_loss: 0.3608 - val_acc: 0.8571\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2105 - acc: 0.9368 - val_loss: 0.3462 - val_acc: 0.8571\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2107 - acc: 0.9350 - val_loss: 0.3534 - val_acc: 0.8624\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2100 - acc: 0.9332 - val_loss: 0.3445 - val_acc: 0.8624\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2102 - acc: 0.9350 - val_loss: 0.3486 - val_acc: 0.8571\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2099 - acc: 0.9350 - val_loss: 0.3393 - val_acc: 0.8677\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2099 - acc: 0.9338 - val_loss: 0.3514 - val_acc: 0.8677\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2092 - acc: 0.9362 - val_loss: 0.3541 - val_acc: 0.8519\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2099 - acc: 0.9350 - val_loss: 0.3392 - val_acc: 0.8624\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2096 - acc: 0.9350 - val_loss: 0.3535 - val_acc: 0.8624\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2106 - acc: 0.9356 - val_loss: 0.3438 - val_acc: 0.8571\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2098 - acc: 0.9332 - val_loss: 0.3512 - val_acc: 0.8624\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2100 - acc: 0.9350 - val_loss: 0.3522 - val_acc: 0.8677\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.2101 - acc: 0.9332 - val_loss: 0.3449 - val_acc: 0.8677\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2100 - acc: 0.9356 - val_loss: 0.3512 - val_acc: 0.8624\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2096 - acc: 0.9344 - val_loss: 0.3456 - val_acc: 0.8624\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.2092 - acc: 0.9350 - val_loss: 0.3439 - val_acc: 0.8677\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2094 - acc: 0.9344 - val_loss: 0.3456 - val_acc: 0.8571\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2083 - acc: 0.9350 - val_loss: 0.3362 - val_acc: 0.8677\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2086 - acc: 0.9379 - val_loss: 0.3433 - val_acc: 0.8677\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2085 - acc: 0.9368 - val_loss: 0.3393 - val_acc: 0.8571\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2096 - acc: 0.9356 - val_loss: 0.3552 - val_acc: 0.8624\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2096 - acc: 0.9344 - val_loss: 0.3421 - val_acc: 0.8677\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2097 - acc: 0.9350 - val_loss: 0.3453 - val_acc: 0.8677\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.2091 - acc: 0.9350 - val_loss: 0.3495 - val_acc: 0.8677\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.2090 - acc: 0.9356 - val_loss: 0.3478 - val_acc: 0.8677\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.2082 - acc: 0.9350 - val_loss: 0.3525 - val_acc: 0.8571\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 151us/step - loss: 0.2087 - acc: 0.9362 - val_loss: 0.3412 - val_acc: 0.8624\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 146us/step - loss: 0.2084 - acc: 0.9338 - val_loss: 0.3456 - val_acc: 0.8571\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 123us/step - loss: 0.2080 - acc: 0.9350 - val_loss: 0.3394 - val_acc: 0.8624\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.2081 - acc: 0.9350 - val_loss: 0.3489 - val_acc: 0.8624\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 121us/step - loss: 0.2078 - acc: 0.9368 - val_loss: 0.3416 - val_acc: 0.8571\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 142us/step - loss: 0.2074 - acc: 0.9350 - val_loss: 0.3417 - val_acc: 0.8624\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.2071 - acc: 0.9391 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 145us/step - loss: 0.2072 - acc: 0.9356 - val_loss: 0.3428 - val_acc: 0.8571\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 274us/step - loss: 0.2082 - acc: 0.9344 - val_loss: 0.3466 - val_acc: 0.8571\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 122us/step - loss: 0.2078 - acc: 0.9362 - val_loss: 0.3415 - val_acc: 0.8624\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.2072 - acc: 0.9362 - val_loss: 0.3538 - val_acc: 0.8571\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2072 - acc: 0.9332 - val_loss: 0.3464 - val_acc: 0.8571\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2063 - acc: 0.9350 - val_loss: 0.3423 - val_acc: 0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2075 - acc: 0.9362 - val_loss: 0.3436 - val_acc: 0.8519\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2064 - acc: 0.9362 - val_loss: 0.3510 - val_acc: 0.8519\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2071 - acc: 0.9350 - val_loss: 0.3395 - val_acc: 0.8624\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.2071 - acc: 0.9385 - val_loss: 0.3423 - val_acc: 0.8519\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2066 - acc: 0.9350 - val_loss: 0.3440 - val_acc: 0.8519\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2067 - acc: 0.9368 - val_loss: 0.3455 - val_acc: 0.8571\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2065 - acc: 0.9379 - val_loss: 0.3453 - val_acc: 0.8571\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2066 - acc: 0.9368 - val_loss: 0.3380 - val_acc: 0.8624\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2066 - acc: 0.9350 - val_loss: 0.3389 - val_acc: 0.8624\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2060 - acc: 0.9356 - val_loss: 0.3413 - val_acc: 0.8730\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2067 - acc: 0.9368 - val_loss: 0.3413 - val_acc: 0.8519\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2057 - acc: 0.9362 - val_loss: 0.3479 - val_acc: 0.8571\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2065 - acc: 0.9362 - val_loss: 0.3393 - val_acc: 0.8677\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2049 - acc: 0.9374 - val_loss: 0.3461 - val_acc: 0.8519\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2062 - acc: 0.9356 - val_loss: 0.3308 - val_acc: 0.8730\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2061 - acc: 0.9350 - val_loss: 0.3330 - val_acc: 0.8677\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2054 - acc: 0.9385 - val_loss: 0.3478 - val_acc: 0.8519\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2051 - acc: 0.9362 - val_loss: 0.3456 - val_acc: 0.8624\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2049 - acc: 0.9368 - val_loss: 0.3396 - val_acc: 0.8624\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2060 - acc: 0.9368 - val_loss: 0.3360 - val_acc: 0.8677\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2045 - acc: 0.9391 - val_loss: 0.3382 - val_acc: 0.8677\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2071 - acc: 0.9362 - val_loss: 0.3442 - val_acc: 0.8571\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2046 - acc: 0.9362 - val_loss: 0.3304 - val_acc: 0.8783\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2047 - acc: 0.9379 - val_loss: 0.3384 - val_acc: 0.8571\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2048 - acc: 0.9368 - val_loss: 0.3445 - val_acc: 0.8624\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2041 - acc: 0.9385 - val_loss: 0.3349 - val_acc: 0.8624\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2051 - acc: 0.9374 - val_loss: 0.3377 - val_acc: 0.8624\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2053 - acc: 0.9379 - val_loss: 0.3470 - val_acc: 0.8571\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2048 - acc: 0.9374 - val_loss: 0.3401 - val_acc: 0.8624\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2035 - acc: 0.9391 - val_loss: 0.3492 - val_acc: 0.8571\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2038 - acc: 0.9350 - val_loss: 0.3286 - val_acc: 0.8677\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2040 - acc: 0.9385 - val_loss: 0.3429 - val_acc: 0.8571\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2043 - acc: 0.9379 - val_loss: 0.3490 - val_acc: 0.8519\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2039 - acc: 0.9356 - val_loss: 0.3473 - val_acc: 0.8624\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2040 - acc: 0.9368 - val_loss: 0.3537 - val_acc: 0.8571\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2040 - acc: 0.9374 - val_loss: 0.3352 - val_acc: 0.8571\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2040 - acc: 0.9368 - val_loss: 0.3337 - val_acc: 0.8571\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2035 - acc: 0.9362 - val_loss: 0.3427 - val_acc: 0.8571\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2041 - acc: 0.9391 - val_loss: 0.3343 - val_acc: 0.8624\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2037 - acc: 0.9385 - val_loss: 0.3300 - val_acc: 0.8677\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2041 - acc: 0.9379 - val_loss: 0.3439 - val_acc: 0.8571\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2029 - acc: 0.9379 - val_loss: 0.3352 - val_acc: 0.8571\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.2037 - acc: 0.9356 - val_loss: 0.3305 - val_acc: 0.8677\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2036 - acc: 0.9385 - val_loss: 0.3362 - val_acc: 0.8571\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2022 - acc: 0.9391 - val_loss: 0.3413 - val_acc: 0.8677\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2030 - acc: 0.9356 - val_loss: 0.3362 - val_acc: 0.8677\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2025 - acc: 0.9385 - val_loss: 0.3442 - val_acc: 0.8571\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2039 - acc: 0.9356 - val_loss: 0.3329 - val_acc: 0.8571\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2028 - acc: 0.9397 - val_loss: 0.3472 - val_acc: 0.8624\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2034 - acc: 0.9379 - val_loss: 0.3310 - val_acc: 0.8624\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2030 - acc: 0.9379 - val_loss: 0.3474 - val_acc: 0.8571\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2030 - acc: 0.9374 - val_loss: 0.3511 - val_acc: 0.8571\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2038 - acc: 0.9374 - val_loss: 0.3383 - val_acc: 0.8624\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2028 - acc: 0.9362 - val_loss: 0.3345 - val_acc: 0.8624\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2025 - acc: 0.9379 - val_loss: 0.3320 - val_acc: 0.8624\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2021 - acc: 0.9362 - val_loss: 0.3544 - val_acc: 0.8571\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2030 - acc: 0.9368 - val_loss: 0.3402 - val_acc: 0.8571\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2023 - acc: 0.9415 - val_loss: 0.3373 - val_acc: 0.8571\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2025 - acc: 0.9374 - val_loss: 0.3289 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 156us/step - loss: 0.2025 - acc: 0.9391 - val_loss: 0.3338 - val_acc: 0.8624\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2022 - acc: 0.9356 - val_loss: 0.3374 - val_acc: 0.8624\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2016 - acc: 0.9397 - val_loss: 0.3514 - val_acc: 0.8624\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2035 - acc: 0.9385 - val_loss: 0.3372 - val_acc: 0.8624\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2027 - acc: 0.9385 - val_loss: 0.3337 - val_acc: 0.8624\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2028 - acc: 0.9362 - val_loss: 0.3333 - val_acc: 0.8624\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2024 - acc: 0.9397 - val_loss: 0.3372 - val_acc: 0.8571\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2029 - acc: 0.9362 - val_loss: 0.3378 - val_acc: 0.8571\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2027 - acc: 0.9368 - val_loss: 0.3291 - val_acc: 0.8624\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2031 - acc: 0.9374 - val_loss: 0.3317 - val_acc: 0.8677\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2015 - acc: 0.9379 - val_loss: 0.3294 - val_acc: 0.8677\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2016 - acc: 0.9391 - val_loss: 0.3247 - val_acc: 0.8730\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2022 - acc: 0.9379 - val_loss: 0.3399 - val_acc: 0.8677\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2021 - acc: 0.9403 - val_loss: 0.3376 - val_acc: 0.8624\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2011 - acc: 0.9385 - val_loss: 0.3422 - val_acc: 0.8571\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2015 - acc: 0.9356 - val_loss: 0.3414 - val_acc: 0.8624\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2013 - acc: 0.9397 - val_loss: 0.3410 - val_acc: 0.8571\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2008 - acc: 0.9385 - val_loss: 0.3503 - val_acc: 0.8571\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2012 - acc: 0.9409 - val_loss: 0.3380 - val_acc: 0.8571\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2015 - acc: 0.9397 - val_loss: 0.3329 - val_acc: 0.8624\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2011 - acc: 0.9368 - val_loss: 0.3421 - val_acc: 0.8519\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2014 - acc: 0.9397 - val_loss: 0.3337 - val_acc: 0.8571\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2012 - acc: 0.9368 - val_loss: 0.3349 - val_acc: 0.8624\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2018 - acc: 0.9379 - val_loss: 0.3361 - val_acc: 0.8677\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2007 - acc: 0.9385 - val_loss: 0.3340 - val_acc: 0.8677\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2008 - acc: 0.9391 - val_loss: 0.3405 - val_acc: 0.8571\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2011 - acc: 0.9391 - val_loss: 0.3386 - val_acc: 0.8571\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2015 - acc: 0.9368 - val_loss: 0.3301 - val_acc: 0.8836\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.2007 - acc: 0.9385 - val_loss: 0.3368 - val_acc: 0.8730\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1991 - acc: 0.9409 - val_loss: 0.3292 - val_acc: 0.8677\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.2017 - acc: 0.9391 - val_loss: 0.3343 - val_acc: 0.8730\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1989 - acc: 0.9379 - val_loss: 0.3199 - val_acc: 0.8783\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2002 - acc: 0.9379 - val_loss: 0.3322 - val_acc: 0.8677\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1998 - acc: 0.9368 - val_loss: 0.3439 - val_acc: 0.8624\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2009 - acc: 0.9391 - val_loss: 0.3390 - val_acc: 0.8624\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1991 - acc: 0.9379 - val_loss: 0.3271 - val_acc: 0.8730\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2005 - acc: 0.9403 - val_loss: 0.3392 - val_acc: 0.8571\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2008 - acc: 0.9397 - val_loss: 0.3413 - val_acc: 0.8677\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2002 - acc: 0.9391 - val_loss: 0.3390 - val_acc: 0.8624\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1997 - acc: 0.9397 - val_loss: 0.3379 - val_acc: 0.8624\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1997 - acc: 0.9391 - val_loss: 0.3345 - val_acc: 0.8624\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1988 - acc: 0.9374 - val_loss: 0.3359 - val_acc: 0.8730\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2006 - acc: 0.9379 - val_loss: 0.3387 - val_acc: 0.8624\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1997 - acc: 0.9368 - val_loss: 0.3281 - val_acc: 0.8677\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1986 - acc: 0.9368 - val_loss: 0.3286 - val_acc: 0.8677\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2012 - acc: 0.9391 - val_loss: 0.3386 - val_acc: 0.8730\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2002 - acc: 0.9385 - val_loss: 0.3328 - val_acc: 0.8677\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2004 - acc: 0.9403 - val_loss: 0.3431 - val_acc: 0.8677\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1994 - acc: 0.9385 - val_loss: 0.3374 - val_acc: 0.8677\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1999 - acc: 0.9368 - val_loss: 0.3292 - val_acc: 0.8783\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2000 - acc: 0.9385 - val_loss: 0.3344 - val_acc: 0.8677\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1995 - acc: 0.9397 - val_loss: 0.3288 - val_acc: 0.8730\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1993 - acc: 0.9356 - val_loss: 0.3424 - val_acc: 0.8624\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1999 - acc: 0.9379 - val_loss: 0.3332 - val_acc: 0.8730\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1995 - acc: 0.9379 - val_loss: 0.3343 - val_acc: 0.8730\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1992 - acc: 0.9391 - val_loss: 0.3369 - val_acc: 0.8783\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1991 - acc: 0.9397 - val_loss: 0.3417 - val_acc: 0.8571\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1994 - acc: 0.9374 - val_loss: 0.3351 - val_acc: 0.8730\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1999 - acc: 0.9385 - val_loss: 0.3369 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1990 - acc: 0.9379 - val_loss: 0.3410 - val_acc: 0.8677\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1988 - acc: 0.9409 - val_loss: 0.3398 - val_acc: 0.8677\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1990 - acc: 0.9374 - val_loss: 0.3279 - val_acc: 0.8730\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1979 - acc: 0.9397 - val_loss: 0.3344 - val_acc: 0.8783\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1975 - acc: 0.9403 - val_loss: 0.3538 - val_acc: 0.8571\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2001 - acc: 0.9385 - val_loss: 0.3453 - val_acc: 0.8677\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1978 - acc: 0.9391 - val_loss: 0.3221 - val_acc: 0.8730\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2000 - acc: 0.9397 - val_loss: 0.3404 - val_acc: 0.8730\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1996 - acc: 0.9368 - val_loss: 0.3463 - val_acc: 0.8571\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1982 - acc: 0.9415 - val_loss: 0.3357 - val_acc: 0.8730\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1989 - acc: 0.9385 - val_loss: 0.3352 - val_acc: 0.8677\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1993 - acc: 0.9379 - val_loss: 0.3334 - val_acc: 0.8677\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1986 - acc: 0.9409 - val_loss: 0.3356 - val_acc: 0.8624\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1978 - acc: 0.9379 - val_loss: 0.3616 - val_acc: 0.8624\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1995 - acc: 0.9379 - val_loss: 0.3267 - val_acc: 0.8783\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1983 - acc: 0.9391 - val_loss: 0.3427 - val_acc: 0.8730\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1991 - acc: 0.9368 - val_loss: 0.3364 - val_acc: 0.8783\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1984 - acc: 0.9385 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1987 - acc: 0.9374 - val_loss: 0.3338 - val_acc: 0.8783\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1975 - acc: 0.9391 - val_loss: 0.3389 - val_acc: 0.8677\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1983 - acc: 0.9374 - val_loss: 0.3421 - val_acc: 0.8677\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1971 - acc: 0.9379 - val_loss: 0.3187 - val_acc: 0.8783\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1986 - acc: 0.9385 - val_loss: 0.3491 - val_acc: 0.8677\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1967 - acc: 0.9385 - val_loss: 0.3262 - val_acc: 0.8783\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1989 - acc: 0.9385 - val_loss: 0.3284 - val_acc: 0.8783\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1986 - acc: 0.9403 - val_loss: 0.3230 - val_acc: 0.8783\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1979 - acc: 0.9379 - val_loss: 0.3310 - val_acc: 0.8783\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1984 - acc: 0.9385 - val_loss: 0.3503 - val_acc: 0.8624\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1981 - acc: 0.9403 - val_loss: 0.3420 - val_acc: 0.8730\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1980 - acc: 0.9391 - val_loss: 0.3347 - val_acc: 0.8783\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1986 - acc: 0.9391 - val_loss: 0.3270 - val_acc: 0.8730\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1977 - acc: 0.9397 - val_loss: 0.3359 - val_acc: 0.8730\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1982 - acc: 0.9415 - val_loss: 0.3287 - val_acc: 0.8783\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1972 - acc: 0.9409 - val_loss: 0.3322 - val_acc: 0.8730\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1979 - acc: 0.9397 - val_loss: 0.3394 - val_acc: 0.8783\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1975 - acc: 0.9379 - val_loss: 0.3259 - val_acc: 0.8783\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1972 - acc: 0.9397 - val_loss: 0.3417 - val_acc: 0.8730\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1987 - acc: 0.9397 - val_loss: 0.3330 - val_acc: 0.8783\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1974 - acc: 0.9397 - val_loss: 0.3480 - val_acc: 0.8783\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1958 - acc: 0.9391 - val_loss: 0.3407 - val_acc: 0.8730\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1966 - acc: 0.9379 - val_loss: 0.3447 - val_acc: 0.8677\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1977 - acc: 0.9362 - val_loss: 0.3298 - val_acc: 0.8783\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1972 - acc: 0.9362 - val_loss: 0.3419 - val_acc: 0.8783\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1969 - acc: 0.9409 - val_loss: 0.3407 - val_acc: 0.8783\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1958 - acc: 0.9397 - val_loss: 0.3547 - val_acc: 0.8677\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1975 - acc: 0.9409 - val_loss: 0.3292 - val_acc: 0.8783\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1968 - acc: 0.9391 - val_loss: 0.3436 - val_acc: 0.8730\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1972 - acc: 0.9379 - val_loss: 0.3214 - val_acc: 0.8783\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1971 - acc: 0.9391 - val_loss: 0.3515 - val_acc: 0.8730\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1972 - acc: 0.9397 - val_loss: 0.3226 - val_acc: 0.8677\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1983 - acc: 0.9391 - val_loss: 0.3290 - val_acc: 0.8783\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1966 - acc: 0.9391 - val_loss: 0.3316 - val_acc: 0.8783\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1970 - acc: 0.9403 - val_loss: 0.3441 - val_acc: 0.8730\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1972 - acc: 0.9415 - val_loss: 0.3402 - val_acc: 0.8783\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1969 - acc: 0.9415 - val_loss: 0.3260 - val_acc: 0.8783\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1973 - acc: 0.9379 - val_loss: 0.3375 - val_acc: 0.8783\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1954 - acc: 0.9403 - val_loss: 0.3590 - val_acc: 0.8624\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 43us/step - loss: 0.1973 - acc: 0.9391 - val_loss: 0.3359 - val_acc: 0.8783\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1957 - acc: 0.9409 - val_loss: 0.3184 - val_acc: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 42us/step - loss: 0.1985 - acc: 0.9379 - val_loss: 0.3374 - val_acc: 0.8730\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1967 - acc: 0.9379 - val_loss: 0.3354 - val_acc: 0.8730\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1969 - acc: 0.9379 - val_loss: 0.3515 - val_acc: 0.8677\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1964 - acc: 0.9403 - val_loss: 0.3355 - val_acc: 0.8783\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1962 - acc: 0.9415 - val_loss: 0.3300 - val_acc: 0.8783\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1972 - acc: 0.9385 - val_loss: 0.3375 - val_acc: 0.8783\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1977 - acc: 0.9374 - val_loss: 0.3288 - val_acc: 0.8783\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1961 - acc: 0.9379 - val_loss: 0.3448 - val_acc: 0.8730\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1960 - acc: 0.9391 - val_loss: 0.3229 - val_acc: 0.8783\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1960 - acc: 0.9403 - val_loss: 0.3574 - val_acc: 0.8677\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1961 - acc: 0.9374 - val_loss: 0.3298 - val_acc: 0.8783\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1961 - acc: 0.9403 - val_loss: 0.3356 - val_acc: 0.8783\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1939 - acc: 0.9391 - val_loss: 0.3621 - val_acc: 0.8624\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.1969 - acc: 0.9409 - val_loss: 0.3406 - val_acc: 0.8677\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1981 - acc: 0.9397 - val_loss: 0.3239 - val_acc: 0.8783\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1964 - acc: 0.9421 - val_loss: 0.3203 - val_acc: 0.8783\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.1962 - acc: 0.9362 - val_loss: 0.3467 - val_acc: 0.8730\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 101us/step - loss: 0.1965 - acc: 0.9391 - val_loss: 0.3347 - val_acc: 0.8783\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.1972 - acc: 0.9391 - val_loss: 0.3352 - val_acc: 0.8783\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1974 - acc: 0.9409 - val_loss: 0.3383 - val_acc: 0.8783\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1961 - acc: 0.9421 - val_loss: 0.3359 - val_acc: 0.8783\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1975 - acc: 0.9403 - val_loss: 0.3284 - val_acc: 0.8783\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 87us/step - loss: 0.1961 - acc: 0.9403 - val_loss: 0.3294 - val_acc: 0.8730\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1964 - acc: 0.9421 - val_loss: 0.3236 - val_acc: 0.8783\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1955 - acc: 0.9391 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1967 - acc: 0.9403 - val_loss: 0.3327 - val_acc: 0.8730\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1951 - acc: 0.9403 - val_loss: 0.3311 - val_acc: 0.8783\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1944 - acc: 0.9397 - val_loss: 0.3258 - val_acc: 0.8677\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1956 - acc: 0.9391 - val_loss: 0.3260 - val_acc: 0.8783\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1956 - acc: 0.9385 - val_loss: 0.3411 - val_acc: 0.8783\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1959 - acc: 0.9379 - val_loss: 0.3210 - val_acc: 0.8783\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1956 - acc: 0.9397 - val_loss: 0.3311 - val_acc: 0.8783\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1959 - acc: 0.9409 - val_loss: 0.3405 - val_acc: 0.8730\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1963 - acc: 0.9415 - val_loss: 0.3269 - val_acc: 0.8783\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.1975 - acc: 0.9397 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1957 - acc: 0.9397 - val_loss: 0.3328 - val_acc: 0.8730\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1957 - acc: 0.9421 - val_loss: 0.3463 - val_acc: 0.8677\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.1965 - acc: 0.9403 - val_loss: 0.3312 - val_acc: 0.8730\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 120us/step - loss: 0.1961 - acc: 0.9379 - val_loss: 0.3520 - val_acc: 0.8730\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1945 - acc: 0.9374 - val_loss: 0.3226 - val_acc: 0.8677\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1963 - acc: 0.9379 - val_loss: 0.3307 - val_acc: 0.8783\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1944 - acc: 0.9403 - val_loss: 0.3244 - val_acc: 0.8783\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1956 - acc: 0.9409 - val_loss: 0.3291 - val_acc: 0.8783\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1952 - acc: 0.9421 - val_loss: 0.3339 - val_acc: 0.8730\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1951 - acc: 0.9421 - val_loss: 0.3295 - val_acc: 0.8730\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1947 - acc: 0.9374 - val_loss: 0.3216 - val_acc: 0.8783\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1954 - acc: 0.9391 - val_loss: 0.3275 - val_acc: 0.8783\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1956 - acc: 0.9397 - val_loss: 0.3378 - val_acc: 0.8783\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1939 - acc: 0.9374 - val_loss: 0.3339 - val_acc: 0.8783\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1948 - acc: 0.9368 - val_loss: 0.3441 - val_acc: 0.8783\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1947 - acc: 0.9409 - val_loss: 0.3373 - val_acc: 0.8783\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1957 - acc: 0.9379 - val_loss: 0.3279 - val_acc: 0.8783\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1963 - acc: 0.9415 - val_loss: 0.3306 - val_acc: 0.8783\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1942 - acc: 0.9409 - val_loss: 0.3362 - val_acc: 0.8783\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1949 - acc: 0.9385 - val_loss: 0.3298 - val_acc: 0.8730\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1932 - acc: 0.9409 - val_loss: 0.3485 - val_acc: 0.8677\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1951 - acc: 0.9409 - val_loss: 0.3336 - val_acc: 0.8783\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1943 - acc: 0.9409 - val_loss: 0.3275 - val_acc: 0.8783\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1963 - acc: 0.9391 - val_loss: 0.3377 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1937 - acc: 0.9403 - val_loss: 0.3392 - val_acc: 0.8730\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1942 - acc: 0.9409 - val_loss: 0.3315 - val_acc: 0.8783\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1937 - acc: 0.9403 - val_loss: 0.3337 - val_acc: 0.8783\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1932 - acc: 0.9403 - val_loss: 0.3254 - val_acc: 0.8677\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1958 - acc: 0.9403 - val_loss: 0.3255 - val_acc: 0.8783\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1941 - acc: 0.9427 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1947 - acc: 0.9391 - val_loss: 0.3352 - val_acc: 0.8730\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1959 - acc: 0.9403 - val_loss: 0.3383 - val_acc: 0.8783\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1937 - acc: 0.9403 - val_loss: 0.3289 - val_acc: 0.8783\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1932 - acc: 0.9415 - val_loss: 0.3220 - val_acc: 0.8783\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1927 - acc: 0.9403 - val_loss: 0.3356 - val_acc: 0.8730\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1953 - acc: 0.9379 - val_loss: 0.3301 - val_acc: 0.8730\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1939 - acc: 0.9391 - val_loss: 0.3338 - val_acc: 0.8783\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1937 - acc: 0.9397 - val_loss: 0.3485 - val_acc: 0.8783\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1938 - acc: 0.9409 - val_loss: 0.3309 - val_acc: 0.8783\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1944 - acc: 0.9409 - val_loss: 0.3365 - val_acc: 0.8730\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1937 - acc: 0.9397 - val_loss: 0.3321 - val_acc: 0.8730\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1933 - acc: 0.9391 - val_loss: 0.3300 - val_acc: 0.8730\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1941 - acc: 0.9409 - val_loss: 0.3263 - val_acc: 0.8730\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1939 - acc: 0.9409 - val_loss: 0.3232 - val_acc: 0.8783\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1928 - acc: 0.9415 - val_loss: 0.3539 - val_acc: 0.8677\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1948 - acc: 0.9374 - val_loss: 0.3286 - val_acc: 0.8783\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1948 - acc: 0.9397 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1946 - acc: 0.9421 - val_loss: 0.3318 - val_acc: 0.8783\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1928 - acc: 0.9409 - val_loss: 0.3343 - val_acc: 0.8783\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1930 - acc: 0.9379 - val_loss: 0.3260 - val_acc: 0.8783\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1938 - acc: 0.9397 - val_loss: 0.3228 - val_acc: 0.8730\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1939 - acc: 0.9409 - val_loss: 0.3293 - val_acc: 0.8783\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1939 - acc: 0.9415 - val_loss: 0.3271 - val_acc: 0.8730\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1934 - acc: 0.9409 - val_loss: 0.3374 - val_acc: 0.8730\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1942 - acc: 0.9409 - val_loss: 0.3331 - val_acc: 0.8730\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1921 - acc: 0.9368 - val_loss: 0.3350 - val_acc: 0.8677\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1943 - acc: 0.9379 - val_loss: 0.3369 - val_acc: 0.8730\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1935 - acc: 0.9403 - val_loss: 0.3207 - val_acc: 0.8783\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1929 - acc: 0.9409 - val_loss: 0.3224 - val_acc: 0.8783\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1928 - acc: 0.9427 - val_loss: 0.3287 - val_acc: 0.8783\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1935 - acc: 0.9409 - val_loss: 0.3363 - val_acc: 0.8730\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1932 - acc: 0.9403 - val_loss: 0.3356 - val_acc: 0.8783\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1938 - acc: 0.9439 - val_loss: 0.3392 - val_acc: 0.8730\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1931 - acc: 0.9403 - val_loss: 0.3428 - val_acc: 0.8783\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 86us/step - loss: 0.1908 - acc: 0.9415 - val_loss: 0.3443 - val_acc: 0.8730\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1935 - acc: 0.9385 - val_loss: 0.3220 - val_acc: 0.8836\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1935 - acc: 0.9433 - val_loss: 0.3340 - val_acc: 0.8730\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1920 - acc: 0.9439 - val_loss: 0.3217 - val_acc: 0.8730\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1927 - acc: 0.9403 - val_loss: 0.3278 - val_acc: 0.8783\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1926 - acc: 0.9409 - val_loss: 0.3354 - val_acc: 0.8783\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1930 - acc: 0.9409 - val_loss: 0.3315 - val_acc: 0.8783\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1931 - acc: 0.9421 - val_loss: 0.3332 - val_acc: 0.8783\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1923 - acc: 0.9397 - val_loss: 0.3394 - val_acc: 0.8730\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1942 - acc: 0.9409 - val_loss: 0.3220 - val_acc: 0.8783\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1926 - acc: 0.9397 - val_loss: 0.3312 - val_acc: 0.8783\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1930 - acc: 0.9397 - val_loss: 0.3206 - val_acc: 0.8730\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1905 - acc: 0.9439 - val_loss: 0.3316 - val_acc: 0.8783\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1930 - acc: 0.9409 - val_loss: 0.3288 - val_acc: 0.8730\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1917 - acc: 0.9391 - val_loss: 0.3440 - val_acc: 0.8730\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1934 - acc: 0.9415 - val_loss: 0.3347 - val_acc: 0.8730\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1925 - acc: 0.9403 - val_loss: 0.3239 - val_acc: 0.8730\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1916 - acc: 0.9415 - val_loss: 0.3243 - val_acc: 0.8677\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1913 - acc: 0.9409 - val_loss: 0.3273 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1925 - acc: 0.9415 - val_loss: 0.3366 - val_acc: 0.8783\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1928 - acc: 0.9379 - val_loss: 0.3385 - val_acc: 0.8730\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1943 - acc: 0.9409 - val_loss: 0.3272 - val_acc: 0.8730\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1918 - acc: 0.9409 - val_loss: 0.3240 - val_acc: 0.8730\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1937 - acc: 0.9397 - val_loss: 0.3381 - val_acc: 0.8783\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1924 - acc: 0.9403 - val_loss: 0.3489 - val_acc: 0.8783\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1919 - acc: 0.9415 - val_loss: 0.3542 - val_acc: 0.8677\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1918 - acc: 0.9415 - val_loss: 0.3311 - val_acc: 0.8783\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1915 - acc: 0.9403 - val_loss: 0.3527 - val_acc: 0.8730\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1929 - acc: 0.9403 - val_loss: 0.3316 - val_acc: 0.8677\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.1918 - acc: 0.9397 - val_loss: 0.3241 - val_acc: 0.8783\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 110us/step - loss: 0.1925 - acc: 0.9391 - val_loss: 0.3250 - val_acc: 0.8730\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 113us/step - loss: 0.1932 - acc: 0.9421 - val_loss: 0.3323 - val_acc: 0.8730\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 99us/step - loss: 0.1909 - acc: 0.9409 - val_loss: 0.3279 - val_acc: 0.8783\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 142us/step - loss: 0.1915 - acc: 0.9409 - val_loss: 0.3388 - val_acc: 0.8730\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1918 - acc: 0.9409 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1927 - acc: 0.9403 - val_loss: 0.3328 - val_acc: 0.8783\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 98us/step - loss: 0.1922 - acc: 0.9427 - val_loss: 0.3285 - val_acc: 0.8730\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.1923 - acc: 0.9391 - val_loss: 0.3361 - val_acc: 0.8730\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1921 - acc: 0.9397 - val_loss: 0.3326 - val_acc: 0.8783\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 125us/step - loss: 0.1916 - acc: 0.9391 - val_loss: 0.3294 - val_acc: 0.8730\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1922 - acc: 0.9409 - val_loss: 0.3291 - val_acc: 0.8730\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1930 - acc: 0.9409 - val_loss: 0.3222 - val_acc: 0.8730\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1901 - acc: 0.9415 - val_loss: 0.3259 - val_acc: 0.8677\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1920 - acc: 0.9403 - val_loss: 0.3289 - val_acc: 0.8730\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1919 - acc: 0.9403 - val_loss: 0.3270 - val_acc: 0.8730\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1921 - acc: 0.9409 - val_loss: 0.3204 - val_acc: 0.8783\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1921 - acc: 0.9391 - val_loss: 0.3359 - val_acc: 0.8730\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1904 - acc: 0.9391 - val_loss: 0.3222 - val_acc: 0.8677\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1921 - acc: 0.9379 - val_loss: 0.3299 - val_acc: 0.8730\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1904 - acc: 0.9409 - val_loss: 0.3328 - val_acc: 0.8730\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1920 - acc: 0.9385 - val_loss: 0.3327 - val_acc: 0.8783\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1907 - acc: 0.9409 - val_loss: 0.3242 - val_acc: 0.8783\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1922 - acc: 0.9391 - val_loss: 0.3450 - val_acc: 0.8677\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1921 - acc: 0.9403 - val_loss: 0.3253 - val_acc: 0.8730\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1920 - acc: 0.9385 - val_loss: 0.3377 - val_acc: 0.8783\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1903 - acc: 0.9391 - val_loss: 0.3215 - val_acc: 0.8836\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1917 - acc: 0.9403 - val_loss: 0.3409 - val_acc: 0.8730\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1909 - acc: 0.9391 - val_loss: 0.3437 - val_acc: 0.8783\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1909 - acc: 0.9415 - val_loss: 0.3273 - val_acc: 0.8783\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1917 - acc: 0.9421 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1912 - acc: 0.9409 - val_loss: 0.3285 - val_acc: 0.8783\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 44us/step - loss: 0.1918 - acc: 0.9409 - val_loss: 0.3368 - val_acc: 0.8730\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1923 - acc: 0.9403 - val_loss: 0.3323 - val_acc: 0.8783\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1923 - acc: 0.9391 - val_loss: 0.3403 - val_acc: 0.8730\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1909 - acc: 0.9403 - val_loss: 0.3195 - val_acc: 0.8783\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1922 - acc: 0.9421 - val_loss: 0.3376 - val_acc: 0.8730\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1907 - acc: 0.9421 - val_loss: 0.3418 - val_acc: 0.8730\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1922 - acc: 0.9385 - val_loss: 0.3308 - val_acc: 0.8783\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1915 - acc: 0.9397 - val_loss: 0.3365 - val_acc: 0.8783\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1905 - acc: 0.9403 - val_loss: 0.3340 - val_acc: 0.8783\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1901 - acc: 0.9397 - val_loss: 0.3272 - val_acc: 0.8730\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1915 - acc: 0.9409 - val_loss: 0.3313 - val_acc: 0.8730\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1908 - acc: 0.9403 - val_loss: 0.3332 - val_acc: 0.8730\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 699us/step - loss: 0.6861 - acc: 0.5337 - val_loss: 0.6554 - val_acc: 0.5926\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.6452 - acc: 0.6519 - val_loss: 0.6359 - val_acc: 0.6878\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.6230 - acc: 0.6915 - val_loss: 0.6220 - val_acc: 0.7090\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.6050 - acc: 0.7240 - val_loss: 0.6106 - val_acc: 0.7143\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.5885 - acc: 0.7376 - val_loss: 0.6003 - val_acc: 0.7249\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.5737 - acc: 0.7405 - val_loss: 0.5899 - val_acc: 0.7196\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.5605 - acc: 0.7612 - val_loss: 0.5803 - val_acc: 0.7037\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.5472 - acc: 0.7642 - val_loss: 0.5707 - val_acc: 0.7037\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.5352 - acc: 0.7713 - val_loss: 0.5623 - val_acc: 0.7090\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.5244 - acc: 0.7796 - val_loss: 0.5542 - val_acc: 0.7090\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.5134 - acc: 0.7855 - val_loss: 0.5464 - val_acc: 0.7143\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.5035 - acc: 0.7884 - val_loss: 0.5392 - val_acc: 0.7249\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.4936 - acc: 0.7914 - val_loss: 0.5323 - val_acc: 0.7354\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4836 - acc: 0.7937 - val_loss: 0.5257 - val_acc: 0.7460\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4737 - acc: 0.7967 - val_loss: 0.5192 - val_acc: 0.7513\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.4641 - acc: 0.8002 - val_loss: 0.5122 - val_acc: 0.7619\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.4550 - acc: 0.8044 - val_loss: 0.5060 - val_acc: 0.7619\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.4460 - acc: 0.8097 - val_loss: 0.5001 - val_acc: 0.7672\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.4375 - acc: 0.8168 - val_loss: 0.4958 - val_acc: 0.7725\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.8239 - val_loss: 0.4905 - val_acc: 0.7725\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.4213 - acc: 0.8304 - val_loss: 0.4850 - val_acc: 0.7831\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4130 - acc: 0.8392 - val_loss: 0.4807 - val_acc: 0.7884\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.4055 - acc: 0.8422 - val_loss: 0.4751 - val_acc: 0.7884\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3981 - acc: 0.8452 - val_loss: 0.4710 - val_acc: 0.8095\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3911 - acc: 0.8457 - val_loss: 0.4688 - val_acc: 0.8095\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.3848 - acc: 0.8505 - val_loss: 0.4635 - val_acc: 0.8148\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.3783 - acc: 0.8499 - val_loss: 0.4601 - val_acc: 0.8042\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3725 - acc: 0.8576 - val_loss: 0.4561 - val_acc: 0.8148\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3669 - acc: 0.8599 - val_loss: 0.4544 - val_acc: 0.8095\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3616 - acc: 0.8623 - val_loss: 0.4520 - val_acc: 0.8095\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.3552 - acc: 0.8635 - val_loss: 0.4492 - val_acc: 0.8095\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3501 - acc: 0.8641 - val_loss: 0.4466 - val_acc: 0.8201\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.3450 - acc: 0.8694 - val_loss: 0.4462 - val_acc: 0.8095\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3399 - acc: 0.8729 - val_loss: 0.4444 - val_acc: 0.8148\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3348 - acc: 0.8765 - val_loss: 0.4412 - val_acc: 0.8201\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3307 - acc: 0.8788 - val_loss: 0.4397 - val_acc: 0.8148\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3267 - acc: 0.8794 - val_loss: 0.4376 - val_acc: 0.8095\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3221 - acc: 0.8830 - val_loss: 0.4353 - val_acc: 0.8201\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3184 - acc: 0.8830 - val_loss: 0.4313 - val_acc: 0.8201\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.3140 - acc: 0.8842 - val_loss: 0.4302 - val_acc: 0.8148\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3104 - acc: 0.8859 - val_loss: 0.4273 - val_acc: 0.8148\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3066 - acc: 0.8871 - val_loss: 0.4245 - val_acc: 0.8201\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.3035 - acc: 0.8895 - val_loss: 0.4246 - val_acc: 0.8201\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.3002 - acc: 0.8895 - val_loss: 0.4250 - val_acc: 0.8254\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2967 - acc: 0.8918 - val_loss: 0.4223 - val_acc: 0.8254\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2939 - acc: 0.8924 - val_loss: 0.4207 - val_acc: 0.8254\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2911 - acc: 0.8954 - val_loss: 0.4200 - val_acc: 0.8254\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.2888 - acc: 0.8942 - val_loss: 0.4205 - val_acc: 0.8254\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2860 - acc: 0.8954 - val_loss: 0.4173 - val_acc: 0.8307\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.2840 - acc: 0.8966 - val_loss: 0.4207 - val_acc: 0.8254\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.2817 - acc: 0.8978 - val_loss: 0.4180 - val_acc: 0.8254\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 115us/step - loss: 0.2792 - acc: 0.8995 - val_loss: 0.4173 - val_acc: 0.8254\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2773 - acc: 0.9001 - val_loss: 0.4172 - val_acc: 0.8254\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.2756 - acc: 0.9025 - val_loss: 0.4155 - val_acc: 0.8307\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 118us/step - loss: 0.2734 - acc: 0.9025 - val_loss: 0.4154 - val_acc: 0.8307\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.2723 - acc: 0.9031 - val_loss: 0.4145 - val_acc: 0.8307\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2710 - acc: 0.9031 - val_loss: 0.4148 - val_acc: 0.8307\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2697 - acc: 0.9025 - val_loss: 0.4158 - val_acc: 0.8307\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2675 - acc: 0.9072 - val_loss: 0.4130 - val_acc: 0.8307\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2661 - acc: 0.9060 - val_loss: 0.4160 - val_acc: 0.8307\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2651 - acc: 0.9072 - val_loss: 0.4157 - val_acc: 0.8307\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2636 - acc: 0.9084 - val_loss: 0.4108 - val_acc: 0.8307\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2618 - acc: 0.9084 - val_loss: 0.4107 - val_acc: 0.8307\n",
      "Epoch 64/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2606 - acc: 0.9084 - val_loss: 0.4103 - val_acc: 0.8360\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2590 - acc: 0.9090 - val_loss: 0.4097 - val_acc: 0.8360\n",
      "Epoch 66/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2578 - acc: 0.9108 - val_loss: 0.4094 - val_acc: 0.8360\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2567 - acc: 0.9113 - val_loss: 0.4075 - val_acc: 0.8413\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2561 - acc: 0.9096 - val_loss: 0.4115 - val_acc: 0.8307\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2550 - acc: 0.9084 - val_loss: 0.4085 - val_acc: 0.8360\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2538 - acc: 0.9125 - val_loss: 0.4070 - val_acc: 0.8413\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2530 - acc: 0.9113 - val_loss: 0.4074 - val_acc: 0.8360\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2514 - acc: 0.9119 - val_loss: 0.4043 - val_acc: 0.8413\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2507 - acc: 0.9125 - val_loss: 0.4037 - val_acc: 0.8466\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2497 - acc: 0.9125 - val_loss: 0.4018 - val_acc: 0.8466\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2500 - acc: 0.9131 - val_loss: 0.4007 - val_acc: 0.8466\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2481 - acc: 0.9137 - val_loss: 0.4016 - val_acc: 0.8466\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2479 - acc: 0.9143 - val_loss: 0.3980 - val_acc: 0.8466\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2465 - acc: 0.9137 - val_loss: 0.3996 - val_acc: 0.8466\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2455 - acc: 0.9161 - val_loss: 0.4033 - val_acc: 0.8466\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2452 - acc: 0.9161 - val_loss: 0.4011 - val_acc: 0.8413\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2443 - acc: 0.9184 - val_loss: 0.3974 - val_acc: 0.8466\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2430 - acc: 0.9184 - val_loss: 0.3975 - val_acc: 0.8466\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2424 - acc: 0.9202 - val_loss: 0.3952 - val_acc: 0.8466\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2414 - acc: 0.9167 - val_loss: 0.3974 - val_acc: 0.8466\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2408 - acc: 0.9190 - val_loss: 0.3947 - val_acc: 0.8519\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2401 - acc: 0.9184 - val_loss: 0.3942 - val_acc: 0.8466\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2391 - acc: 0.9214 - val_loss: 0.3951 - val_acc: 0.8413\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2386 - acc: 0.9190 - val_loss: 0.3909 - val_acc: 0.8571\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2382 - acc: 0.9184 - val_loss: 0.3910 - val_acc: 0.8571\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2369 - acc: 0.9190 - val_loss: 0.3939 - val_acc: 0.8519\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2366 - acc: 0.9214 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2359 - acc: 0.9184 - val_loss: 0.3921 - val_acc: 0.8571\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2351 - acc: 0.9214 - val_loss: 0.3908 - val_acc: 0.8571\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2345 - acc: 0.9190 - val_loss: 0.3915 - val_acc: 0.8519\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2336 - acc: 0.9196 - val_loss: 0.3884 - val_acc: 0.8571\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2331 - acc: 0.9208 - val_loss: 0.3887 - val_acc: 0.8571\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2324 - acc: 0.9226 - val_loss: 0.3852 - val_acc: 0.8624\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2320 - acc: 0.9184 - val_loss: 0.3847 - val_acc: 0.8624\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2312 - acc: 0.9238 - val_loss: 0.3861 - val_acc: 0.8624\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2302 - acc: 0.9226 - val_loss: 0.3831 - val_acc: 0.8624\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2296 - acc: 0.9214 - val_loss: 0.3822 - val_acc: 0.8624\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2296 - acc: 0.9208 - val_loss: 0.3824 - val_acc: 0.8624\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2293 - acc: 0.9232 - val_loss: 0.3823 - val_acc: 0.8624\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2282 - acc: 0.9226 - val_loss: 0.3781 - val_acc: 0.8624\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2279 - acc: 0.9243 - val_loss: 0.3814 - val_acc: 0.8624\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2270 - acc: 0.9220 - val_loss: 0.3782 - val_acc: 0.8624\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2266 - acc: 0.9238 - val_loss: 0.3745 - val_acc: 0.8677\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2262 - acc: 0.9273 - val_loss: 0.3770 - val_acc: 0.8624\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2255 - acc: 0.9249 - val_loss: 0.3775 - val_acc: 0.8624\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2245 - acc: 0.9279 - val_loss: 0.3733 - val_acc: 0.8677\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2244 - acc: 0.9255 - val_loss: 0.3734 - val_acc: 0.8677\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2238 - acc: 0.9273 - val_loss: 0.3732 - val_acc: 0.8677\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2228 - acc: 0.9297 - val_loss: 0.3717 - val_acc: 0.8677\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2223 - acc: 0.9261 - val_loss: 0.3719 - val_acc: 0.8677\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2220 - acc: 0.9291 - val_loss: 0.3705 - val_acc: 0.8677\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2217 - acc: 0.9309 - val_loss: 0.3653 - val_acc: 0.8677\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2213 - acc: 0.9273 - val_loss: 0.3688 - val_acc: 0.8677\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.2202 - acc: 0.9303 - val_loss: 0.3705 - val_acc: 0.8677\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2193 - acc: 0.9285 - val_loss: 0.3677 - val_acc: 0.8677\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2193 - acc: 0.9303 - val_loss: 0.3707 - val_acc: 0.8677\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2188 - acc: 0.9279 - val_loss: 0.3684 - val_acc: 0.8677\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2185 - acc: 0.9279 - val_loss: 0.3698 - val_acc: 0.8677\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2185 - acc: 0.9297 - val_loss: 0.3641 - val_acc: 0.8677\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2173 - acc: 0.9285 - val_loss: 0.3676 - val_acc: 0.8677\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2175 - acc: 0.9297 - val_loss: 0.3654 - val_acc: 0.8677\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2164 - acc: 0.9309 - val_loss: 0.3666 - val_acc: 0.8677\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2153 - acc: 0.9279 - val_loss: 0.3644 - val_acc: 0.8677\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2152 - acc: 0.9291 - val_loss: 0.3664 - val_acc: 0.8677\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2148 - acc: 0.9279 - val_loss: 0.3660 - val_acc: 0.8677\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2156 - acc: 0.9285 - val_loss: 0.3674 - val_acc: 0.8677\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2137 - acc: 0.9332 - val_loss: 0.3650 - val_acc: 0.8677\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2138 - acc: 0.9291 - val_loss: 0.3625 - val_acc: 0.8624\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2135 - acc: 0.9303 - val_loss: 0.3602 - val_acc: 0.8624\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2125 - acc: 0.9309 - val_loss: 0.3617 - val_acc: 0.8677\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2120 - acc: 0.9291 - val_loss: 0.3636 - val_acc: 0.8677\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2122 - acc: 0.9285 - val_loss: 0.3617 - val_acc: 0.8571\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2116 - acc: 0.9285 - val_loss: 0.3611 - val_acc: 0.8571\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2110 - acc: 0.9297 - val_loss: 0.3624 - val_acc: 0.8571\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2103 - acc: 0.9303 - val_loss: 0.3623 - val_acc: 0.8571\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2105 - acc: 0.9303 - val_loss: 0.3636 - val_acc: 0.8624\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2099 - acc: 0.9279 - val_loss: 0.3617 - val_acc: 0.8624\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2093 - acc: 0.9303 - val_loss: 0.3612 - val_acc: 0.8571\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2087 - acc: 0.9285 - val_loss: 0.3602 - val_acc: 0.8571\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2092 - acc: 0.9314 - val_loss: 0.3581 - val_acc: 0.8624\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2084 - acc: 0.9285 - val_loss: 0.3586 - val_acc: 0.8571\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2085 - acc: 0.9291 - val_loss: 0.3590 - val_acc: 0.8571\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2079 - acc: 0.9297 - val_loss: 0.3588 - val_acc: 0.8571\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2080 - acc: 0.9279 - val_loss: 0.3606 - val_acc: 0.8571\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2073 - acc: 0.9297 - val_loss: 0.3580 - val_acc: 0.8571\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2067 - acc: 0.9309 - val_loss: 0.3591 - val_acc: 0.8571\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2067 - acc: 0.9279 - val_loss: 0.3557 - val_acc: 0.8624\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2064 - acc: 0.9303 - val_loss: 0.3601 - val_acc: 0.8571\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2061 - acc: 0.9279 - val_loss: 0.3575 - val_acc: 0.8571\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2056 - acc: 0.9273 - val_loss: 0.3566 - val_acc: 0.8571\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2055 - acc: 0.9297 - val_loss: 0.3572 - val_acc: 0.8571\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2057 - acc: 0.9291 - val_loss: 0.3578 - val_acc: 0.8571\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2054 - acc: 0.9297 - val_loss: 0.3566 - val_acc: 0.8571\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2043 - acc: 0.9320 - val_loss: 0.3562 - val_acc: 0.8519\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2044 - acc: 0.9309 - val_loss: 0.3566 - val_acc: 0.8519\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2041 - acc: 0.9309 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2037 - acc: 0.9291 - val_loss: 0.3523 - val_acc: 0.8571\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2032 - acc: 0.9320 - val_loss: 0.3543 - val_acc: 0.8519\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2031 - acc: 0.9309 - val_loss: 0.3528 - val_acc: 0.8519\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2029 - acc: 0.9314 - val_loss: 0.3546 - val_acc: 0.8519\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2024 - acc: 0.9314 - val_loss: 0.3559 - val_acc: 0.8519\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2017 - acc: 0.9314 - val_loss: 0.3548 - val_acc: 0.8519\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2016 - acc: 0.9326 - val_loss: 0.3548 - val_acc: 0.8519\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2012 - acc: 0.9303 - val_loss: 0.3540 - val_acc: 0.8519\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2017 - acc: 0.9303 - val_loss: 0.3533 - val_acc: 0.8519\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2010 - acc: 0.9309 - val_loss: 0.3520 - val_acc: 0.8519\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2007 - acc: 0.9338 - val_loss: 0.3537 - val_acc: 0.8571\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2002 - acc: 0.9314 - val_loss: 0.3553 - val_acc: 0.8519\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2004 - acc: 0.9309 - val_loss: 0.3558 - val_acc: 0.8519\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1999 - acc: 0.9297 - val_loss: 0.3518 - val_acc: 0.8571\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1996 - acc: 0.9338 - val_loss: 0.3564 - val_acc: 0.8519\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1995 - acc: 0.9309 - val_loss: 0.3518 - val_acc: 0.8571\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1993 - acc: 0.9320 - val_loss: 0.3548 - val_acc: 0.8519\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1990 - acc: 0.9332 - val_loss: 0.3527 - val_acc: 0.8519\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1989 - acc: 0.9320 - val_loss: 0.3542 - val_acc: 0.8519\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1985 - acc: 0.9326 - val_loss: 0.3530 - val_acc: 0.8519\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1985 - acc: 0.9332 - val_loss: 0.3580 - val_acc: 0.8519\n",
      "Epoch 182/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1985 - acc: 0.9326 - val_loss: 0.3567 - val_acc: 0.8519\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1984 - acc: 0.9314 - val_loss: 0.3575 - val_acc: 0.8519\n",
      "Epoch 184/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1980 - acc: 0.9326 - val_loss: 0.3535 - val_acc: 0.8519\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1974 - acc: 0.9338 - val_loss: 0.3556 - val_acc: 0.8519\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1972 - acc: 0.9332 - val_loss: 0.3539 - val_acc: 0.8519\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1970 - acc: 0.9326 - val_loss: 0.3537 - val_acc: 0.8571\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1973 - acc: 0.9338 - val_loss: 0.3520 - val_acc: 0.8571\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1965 - acc: 0.9338 - val_loss: 0.3543 - val_acc: 0.8571\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1969 - acc: 0.9344 - val_loss: 0.3539 - val_acc: 0.8519\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1960 - acc: 0.9338 - val_loss: 0.3522 - val_acc: 0.8571\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1964 - acc: 0.9338 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1961 - acc: 0.9309 - val_loss: 0.3562 - val_acc: 0.8519\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1956 - acc: 0.9344 - val_loss: 0.3543 - val_acc: 0.8571\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1958 - acc: 0.9344 - val_loss: 0.3546 - val_acc: 0.8571\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1953 - acc: 0.9344 - val_loss: 0.3547 - val_acc: 0.8571\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1955 - acc: 0.9320 - val_loss: 0.3538 - val_acc: 0.8519\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1945 - acc: 0.9356 - val_loss: 0.3559 - val_acc: 0.8519\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1946 - acc: 0.9350 - val_loss: 0.3579 - val_acc: 0.8519\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1951 - acc: 0.9350 - val_loss: 0.3544 - val_acc: 0.8519\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1942 - acc: 0.9338 - val_loss: 0.3556 - val_acc: 0.8519\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1943 - acc: 0.9350 - val_loss: 0.3534 - val_acc: 0.8519\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1940 - acc: 0.9362 - val_loss: 0.3523 - val_acc: 0.8519\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1936 - acc: 0.9344 - val_loss: 0.3543 - val_acc: 0.8571\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1945 - acc: 0.9362 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1930 - acc: 0.9344 - val_loss: 0.3572 - val_acc: 0.8571\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1930 - acc: 0.9350 - val_loss: 0.3561 - val_acc: 0.8519\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1925 - acc: 0.9368 - val_loss: 0.3528 - val_acc: 0.8677\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1924 - acc: 0.9350 - val_loss: 0.3580 - val_acc: 0.8519\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1924 - acc: 0.9362 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1925 - acc: 0.9362 - val_loss: 0.3560 - val_acc: 0.8519\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1917 - acc: 0.9368 - val_loss: 0.3582 - val_acc: 0.8519\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1917 - acc: 0.9344 - val_loss: 0.3569 - val_acc: 0.8519\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1916 - acc: 0.9362 - val_loss: 0.3570 - val_acc: 0.8519\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1916 - acc: 0.9356 - val_loss: 0.3634 - val_acc: 0.8571\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1917 - acc: 0.9356 - val_loss: 0.3530 - val_acc: 0.8624\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1911 - acc: 0.9379 - val_loss: 0.3543 - val_acc: 0.8624\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1912 - acc: 0.9356 - val_loss: 0.3574 - val_acc: 0.8519\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1907 - acc: 0.9368 - val_loss: 0.3578 - val_acc: 0.8519\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1908 - acc: 0.9362 - val_loss: 0.3534 - val_acc: 0.8624\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1905 - acc: 0.9344 - val_loss: 0.3549 - val_acc: 0.8624\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1902 - acc: 0.9362 - val_loss: 0.3551 - val_acc: 0.8571\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1907 - acc: 0.9374 - val_loss: 0.3522 - val_acc: 0.8624\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1898 - acc: 0.9356 - val_loss: 0.3547 - val_acc: 0.8571\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1902 - acc: 0.9379 - val_loss: 0.3558 - val_acc: 0.8624\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1894 - acc: 0.9356 - val_loss: 0.3534 - val_acc: 0.8624\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1897 - acc: 0.9362 - val_loss: 0.3498 - val_acc: 0.8624\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1894 - acc: 0.9350 - val_loss: 0.3574 - val_acc: 0.8571\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1898 - acc: 0.9356 - val_loss: 0.3525 - val_acc: 0.8677\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1898 - acc: 0.9344 - val_loss: 0.3557 - val_acc: 0.8571\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1890 - acc: 0.9385 - val_loss: 0.3569 - val_acc: 0.8624\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1885 - acc: 0.9379 - val_loss: 0.3532 - val_acc: 0.8624\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1887 - acc: 0.9362 - val_loss: 0.3546 - val_acc: 0.8624\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1883 - acc: 0.9385 - val_loss: 0.3561 - val_acc: 0.8624\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1888 - acc: 0.9356 - val_loss: 0.3572 - val_acc: 0.8571\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1881 - acc: 0.9362 - val_loss: 0.3565 - val_acc: 0.8677\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1882 - acc: 0.9368 - val_loss: 0.3533 - val_acc: 0.8624\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1877 - acc: 0.9374 - val_loss: 0.3555 - val_acc: 0.8624\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1877 - acc: 0.9350 - val_loss: 0.3552 - val_acc: 0.8677\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1878 - acc: 0.9350 - val_loss: 0.3527 - val_acc: 0.8624\n",
      "Epoch 241/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1875 - acc: 0.9350 - val_loss: 0.3543 - val_acc: 0.8624\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1872 - acc: 0.9368 - val_loss: 0.3555 - val_acc: 0.8571\n",
      "Epoch 243/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1874 - acc: 0.9362 - val_loss: 0.3565 - val_acc: 0.8624\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1874 - acc: 0.9362 - val_loss: 0.3547 - val_acc: 0.8677\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1868 - acc: 0.9379 - val_loss: 0.3557 - val_acc: 0.8624\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1867 - acc: 0.9362 - val_loss: 0.3513 - val_acc: 0.8624\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1865 - acc: 0.9374 - val_loss: 0.3512 - val_acc: 0.8624\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1873 - acc: 0.9332 - val_loss: 0.3528 - val_acc: 0.8677\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1867 - acc: 0.9350 - val_loss: 0.3553 - val_acc: 0.8677\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1864 - acc: 0.9362 - val_loss: 0.3555 - val_acc: 0.8624\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 79us/step - loss: 0.1859 - acc: 0.9362 - val_loss: 0.3534 - val_acc: 0.8624\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1861 - acc: 0.9374 - val_loss: 0.3502 - val_acc: 0.8624\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - 0s 107us/step - loss: 0.1860 - acc: 0.9374 - val_loss: 0.3531 - val_acc: 0.8624\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1856 - acc: 0.9362 - val_loss: 0.3493 - val_acc: 0.8624\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1854 - acc: 0.9391 - val_loss: 0.3561 - val_acc: 0.8624\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1852 - acc: 0.9362 - val_loss: 0.3519 - val_acc: 0.8624\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1855 - acc: 0.9379 - val_loss: 0.3523 - val_acc: 0.8677\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1851 - acc: 0.9368 - val_loss: 0.3488 - val_acc: 0.8624\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1848 - acc: 0.9385 - val_loss: 0.3507 - val_acc: 0.8677\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1844 - acc: 0.9362 - val_loss: 0.3497 - val_acc: 0.8624\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1848 - acc: 0.9368 - val_loss: 0.3538 - val_acc: 0.8624\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1849 - acc: 0.9397 - val_loss: 0.3521 - val_acc: 0.8624\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1841 - acc: 0.9356 - val_loss: 0.3545 - val_acc: 0.8677\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1846 - acc: 0.9379 - val_loss: 0.3539 - val_acc: 0.8624\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1840 - acc: 0.9379 - val_loss: 0.3532 - val_acc: 0.8677\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1833 - acc: 0.9368 - val_loss: 0.3524 - val_acc: 0.8624\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1832 - acc: 0.9403 - val_loss: 0.3492 - val_acc: 0.8624\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1833 - acc: 0.9379 - val_loss: 0.3527 - val_acc: 0.8677\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1829 - acc: 0.9374 - val_loss: 0.3530 - val_acc: 0.8624\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1834 - acc: 0.9374 - val_loss: 0.3520 - val_acc: 0.8677\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1832 - acc: 0.9379 - val_loss: 0.3528 - val_acc: 0.8677\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1831 - acc: 0.9397 - val_loss: 0.3487 - val_acc: 0.8624\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1821 - acc: 0.9379 - val_loss: 0.3516 - val_acc: 0.8677\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1829 - acc: 0.9397 - val_loss: 0.3498 - val_acc: 0.8624\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1825 - acc: 0.9368 - val_loss: 0.3524 - val_acc: 0.8677\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1820 - acc: 0.9385 - val_loss: 0.3497 - val_acc: 0.8624\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1821 - acc: 0.9385 - val_loss: 0.3486 - val_acc: 0.8677\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1822 - acc: 0.9356 - val_loss: 0.3512 - val_acc: 0.8677\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1824 - acc: 0.9385 - val_loss: 0.3478 - val_acc: 0.8624\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1816 - acc: 0.9391 - val_loss: 0.3504 - val_acc: 0.8677\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1826 - acc: 0.9385 - val_loss: 0.3529 - val_acc: 0.8677\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 93us/step - loss: 0.1814 - acc: 0.9385 - val_loss: 0.3468 - val_acc: 0.8677\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 95us/step - loss: 0.1811 - acc: 0.9368 - val_loss: 0.3502 - val_acc: 0.8677\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1817 - acc: 0.9374 - val_loss: 0.3461 - val_acc: 0.8677\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 92us/step - loss: 0.1807 - acc: 0.9374 - val_loss: 0.3484 - val_acc: 0.8677\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 94us/step - loss: 0.1812 - acc: 0.9391 - val_loss: 0.3497 - val_acc: 0.8677\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1806 - acc: 0.9385 - val_loss: 0.3485 - val_acc: 0.8677\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1804 - acc: 0.9379 - val_loss: 0.3471 - val_acc: 0.8677\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.1811 - acc: 0.9368 - val_loss: 0.3498 - val_acc: 0.8677\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1809 - acc: 0.9374 - val_loss: 0.3464 - val_acc: 0.8677\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1809 - acc: 0.9379 - val_loss: 0.3445 - val_acc: 0.8677\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1811 - acc: 0.9379 - val_loss: 0.3480 - val_acc: 0.8730\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1800 - acc: 0.9385 - val_loss: 0.3469 - val_acc: 0.8730\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1799 - acc: 0.9368 - val_loss: 0.3479 - val_acc: 0.8730\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1803 - acc: 0.9374 - val_loss: 0.3517 - val_acc: 0.8677\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1805 - acc: 0.9385 - val_loss: 0.3480 - val_acc: 0.8677\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1795 - acc: 0.9391 - val_loss: 0.3465 - val_acc: 0.8730\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1795 - acc: 0.9391 - val_loss: 0.3449 - val_acc: 0.8730\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1794 - acc: 0.9379 - val_loss: 0.3492 - val_acc: 0.8730\n",
      "Epoch 300/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1792 - acc: 0.9397 - val_loss: 0.3464 - val_acc: 0.8677\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1795 - acc: 0.9397 - val_loss: 0.3490 - val_acc: 0.8730\n",
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1793 - acc: 0.9374 - val_loss: 0.3456 - val_acc: 0.8730\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1787 - acc: 0.9362 - val_loss: 0.3472 - val_acc: 0.8730\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1789 - acc: 0.9379 - val_loss: 0.3472 - val_acc: 0.8730\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1784 - acc: 0.9374 - val_loss: 0.3463 - val_acc: 0.8730\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1785 - acc: 0.9385 - val_loss: 0.3470 - val_acc: 0.8730\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1789 - acc: 0.9379 - val_loss: 0.3478 - val_acc: 0.8677\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1783 - acc: 0.9368 - val_loss: 0.3424 - val_acc: 0.8677\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1785 - acc: 0.9362 - val_loss: 0.3461 - val_acc: 0.8730\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1784 - acc: 0.9379 - val_loss: 0.3467 - val_acc: 0.8730\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1781 - acc: 0.9379 - val_loss: 0.3484 - val_acc: 0.8730\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1779 - acc: 0.9374 - val_loss: 0.3455 - val_acc: 0.8677\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1776 - acc: 0.9385 - val_loss: 0.3486 - val_acc: 0.8730\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1776 - acc: 0.9403 - val_loss: 0.3440 - val_acc: 0.8677\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1775 - acc: 0.9391 - val_loss: 0.3471 - val_acc: 0.8730\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1774 - acc: 0.9391 - val_loss: 0.3445 - val_acc: 0.8730\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1774 - acc: 0.9391 - val_loss: 0.3512 - val_acc: 0.8730\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1776 - acc: 0.9385 - val_loss: 0.3453 - val_acc: 0.8730\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1774 - acc: 0.9403 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1774 - acc: 0.9385 - val_loss: 0.3445 - val_acc: 0.8730\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1773 - acc: 0.9391 - val_loss: 0.3477 - val_acc: 0.8730\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1766 - acc: 0.9403 - val_loss: 0.3456 - val_acc: 0.8730\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1773 - acc: 0.9379 - val_loss: 0.3437 - val_acc: 0.8730\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1767 - acc: 0.9403 - val_loss: 0.3473 - val_acc: 0.8730\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1769 - acc: 0.9421 - val_loss: 0.3435 - val_acc: 0.8730\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1770 - acc: 0.9397 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1771 - acc: 0.9397 - val_loss: 0.3443 - val_acc: 0.8730\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1760 - acc: 0.9403 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1758 - acc: 0.9391 - val_loss: 0.3475 - val_acc: 0.8677\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1764 - acc: 0.9403 - val_loss: 0.3467 - val_acc: 0.8730\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1758 - acc: 0.9391 - val_loss: 0.3455 - val_acc: 0.8730\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1758 - acc: 0.9391 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1758 - acc: 0.9385 - val_loss: 0.3487 - val_acc: 0.8730\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1757 - acc: 0.9403 - val_loss: 0.3462 - val_acc: 0.8730\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1756 - acc: 0.9409 - val_loss: 0.3445 - val_acc: 0.8730\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1760 - acc: 0.9385 - val_loss: 0.3451 - val_acc: 0.8730\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1758 - acc: 0.9403 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1753 - acc: 0.9385 - val_loss: 0.3481 - val_acc: 0.8730\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1759 - acc: 0.9397 - val_loss: 0.3456 - val_acc: 0.8730\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1755 - acc: 0.9403 - val_loss: 0.3458 - val_acc: 0.8730\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1754 - acc: 0.9391 - val_loss: 0.3459 - val_acc: 0.8730\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1747 - acc: 0.9421 - val_loss: 0.3444 - val_acc: 0.8730\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1747 - acc: 0.9409 - val_loss: 0.3444 - val_acc: 0.8730\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1743 - acc: 0.9409 - val_loss: 0.3459 - val_acc: 0.8730\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1745 - acc: 0.9391 - val_loss: 0.3473 - val_acc: 0.8730\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1743 - acc: 0.9415 - val_loss: 0.3444 - val_acc: 0.8730\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1751 - acc: 0.9397 - val_loss: 0.3461 - val_acc: 0.8730\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1744 - acc: 0.9409 - val_loss: 0.3442 - val_acc: 0.8730\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1750 - acc: 0.9415 - val_loss: 0.3481 - val_acc: 0.8730\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1742 - acc: 0.9409 - val_loss: 0.3453 - val_acc: 0.8730\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1750 - acc: 0.9421 - val_loss: 0.3433 - val_acc: 0.8677\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 149us/step - loss: 0.1736 - acc: 0.9421 - val_loss: 0.3448 - val_acc: 0.8730\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1739 - acc: 0.9415 - val_loss: 0.3441 - val_acc: 0.8730\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1739 - acc: 0.9391 - val_loss: 0.3468 - val_acc: 0.8730\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1736 - acc: 0.9415 - val_loss: 0.3466 - val_acc: 0.8730\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1741 - acc: 0.9415 - val_loss: 0.3489 - val_acc: 0.8730\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1733 - acc: 0.9403 - val_loss: 0.3461 - val_acc: 0.8730\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1736 - acc: 0.9385 - val_loss: 0.3459 - val_acc: 0.8730\n",
      "Epoch 359/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1733 - acc: 0.9391 - val_loss: 0.3448 - val_acc: 0.8730\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1729 - acc: 0.9409 - val_loss: 0.3418 - val_acc: 0.8730\n",
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1738 - acc: 0.9397 - val_loss: 0.3473 - val_acc: 0.8730\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.1739 - acc: 0.9397 - val_loss: 0.3479 - val_acc: 0.8730\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1730 - acc: 0.9415 - val_loss: 0.3435 - val_acc: 0.8677\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 84us/step - loss: 0.1730 - acc: 0.9403 - val_loss: 0.3444 - val_acc: 0.8730\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1728 - acc: 0.9409 - val_loss: 0.3454 - val_acc: 0.8730\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.1725 - acc: 0.9421 - val_loss: 0.3429 - val_acc: 0.8730\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1727 - acc: 0.9415 - val_loss: 0.3470 - val_acc: 0.8677\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1728 - acc: 0.9415 - val_loss: 0.3430 - val_acc: 0.8730\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1723 - acc: 0.9409 - val_loss: 0.3441 - val_acc: 0.8730\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1720 - acc: 0.9379 - val_loss: 0.3413 - val_acc: 0.8730\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1724 - acc: 0.9427 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1722 - acc: 0.9427 - val_loss: 0.3418 - val_acc: 0.8730\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1717 - acc: 0.9397 - val_loss: 0.3428 - val_acc: 0.8730\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1718 - acc: 0.9421 - val_loss: 0.3433 - val_acc: 0.8730\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1719 - acc: 0.9415 - val_loss: 0.3422 - val_acc: 0.8730\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1716 - acc: 0.9403 - val_loss: 0.3421 - val_acc: 0.8730\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1716 - acc: 0.9409 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1717 - acc: 0.9391 - val_loss: 0.3415 - val_acc: 0.8730\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1717 - acc: 0.9409 - val_loss: 0.3417 - val_acc: 0.8677\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1712 - acc: 0.9433 - val_loss: 0.3478 - val_acc: 0.8730\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1713 - acc: 0.9415 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1718 - acc: 0.9409 - val_loss: 0.3457 - val_acc: 0.8730\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1722 - acc: 0.9385 - val_loss: 0.3449 - val_acc: 0.8730\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1712 - acc: 0.9433 - val_loss: 0.3431 - val_acc: 0.8677\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1711 - acc: 0.9421 - val_loss: 0.3396 - val_acc: 0.8730\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1703 - acc: 0.9409 - val_loss: 0.3449 - val_acc: 0.8730\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1711 - acc: 0.9433 - val_loss: 0.3452 - val_acc: 0.8730\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1709 - acc: 0.9427 - val_loss: 0.3446 - val_acc: 0.8730\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1706 - acc: 0.9415 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1704 - acc: 0.9433 - val_loss: 0.3422 - val_acc: 0.8730\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1714 - acc: 0.9415 - val_loss: 0.3407 - val_acc: 0.8624\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1705 - acc: 0.9415 - val_loss: 0.3420 - val_acc: 0.8730\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1708 - acc: 0.9421 - val_loss: 0.3424 - val_acc: 0.8677\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1702 - acc: 0.9427 - val_loss: 0.3451 - val_acc: 0.8730\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1702 - acc: 0.9421 - val_loss: 0.3421 - val_acc: 0.8677\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1702 - acc: 0.9433 - val_loss: 0.3423 - val_acc: 0.8677\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1707 - acc: 0.9421 - val_loss: 0.3410 - val_acc: 0.8730\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1703 - acc: 0.9421 - val_loss: 0.3432 - val_acc: 0.8730\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1702 - acc: 0.9433 - val_loss: 0.3427 - val_acc: 0.8730\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1708 - acc: 0.9415 - val_loss: 0.3425 - val_acc: 0.8730\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1697 - acc: 0.9421 - val_loss: 0.3424 - val_acc: 0.8730\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1702 - acc: 0.9427 - val_loss: 0.3406 - val_acc: 0.8677\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1698 - acc: 0.9433 - val_loss: 0.3427 - val_acc: 0.8677\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1699 - acc: 0.9409 - val_loss: 0.3418 - val_acc: 0.8730\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1700 - acc: 0.9456 - val_loss: 0.3417 - val_acc: 0.8677\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1693 - acc: 0.9433 - val_loss: 0.3420 - val_acc: 0.8730\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1696 - acc: 0.9427 - val_loss: 0.3433 - val_acc: 0.8677\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1695 - acc: 0.9427 - val_loss: 0.3415 - val_acc: 0.8730\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1691 - acc: 0.9427 - val_loss: 0.3384 - val_acc: 0.8677\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1692 - acc: 0.9415 - val_loss: 0.3438 - val_acc: 0.8730\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1693 - acc: 0.9439 - val_loss: 0.3435 - val_acc: 0.8677\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1695 - acc: 0.9456 - val_loss: 0.3422 - val_acc: 0.8730\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1689 - acc: 0.9409 - val_loss: 0.3420 - val_acc: 0.8730\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1696 - acc: 0.9421 - val_loss: 0.3433 - val_acc: 0.8730\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1692 - acc: 0.9415 - val_loss: 0.3403 - val_acc: 0.8677\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1689 - acc: 0.9444 - val_loss: 0.3453 - val_acc: 0.8677\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1691 - acc: 0.9427 - val_loss: 0.3407 - val_acc: 0.8677\n",
      "Epoch 418/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1684 - acc: 0.9439 - val_loss: 0.3417 - val_acc: 0.8730\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1687 - acc: 0.9433 - val_loss: 0.3439 - val_acc: 0.8677\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 124us/step - loss: 0.1686 - acc: 0.9450 - val_loss: 0.3435 - val_acc: 0.8677\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1683 - acc: 0.9409 - val_loss: 0.3421 - val_acc: 0.8730\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1682 - acc: 0.9433 - val_loss: 0.3436 - val_acc: 0.8730\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1695 - acc: 0.9397 - val_loss: 0.3400 - val_acc: 0.8677\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1689 - acc: 0.9427 - val_loss: 0.3464 - val_acc: 0.8730\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1683 - acc: 0.9439 - val_loss: 0.3415 - val_acc: 0.8730\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1682 - acc: 0.9456 - val_loss: 0.3440 - val_acc: 0.8730\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1682 - acc: 0.9415 - val_loss: 0.3420 - val_acc: 0.8677\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1677 - acc: 0.9439 - val_loss: 0.3414 - val_acc: 0.8677\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1690 - acc: 0.9433 - val_loss: 0.3422 - val_acc: 0.8730\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1681 - acc: 0.9415 - val_loss: 0.3412 - val_acc: 0.8677\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1689 - acc: 0.9433 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1680 - acc: 0.9433 - val_loss: 0.3412 - val_acc: 0.8677\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1681 - acc: 0.9433 - val_loss: 0.3411 - val_acc: 0.8677\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1674 - acc: 0.9427 - val_loss: 0.3410 - val_acc: 0.8677\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1679 - acc: 0.9421 - val_loss: 0.3401 - val_acc: 0.8677\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1681 - acc: 0.9427 - val_loss: 0.3382 - val_acc: 0.8677\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1681 - acc: 0.9439 - val_loss: 0.3391 - val_acc: 0.8624\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1680 - acc: 0.9433 - val_loss: 0.3429 - val_acc: 0.8677\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1673 - acc: 0.9439 - val_loss: 0.3383 - val_acc: 0.8677\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1676 - acc: 0.9433 - val_loss: 0.3371 - val_acc: 0.8677\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1674 - acc: 0.9427 - val_loss: 0.3392 - val_acc: 0.8677\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1672 - acc: 0.9439 - val_loss: 0.3405 - val_acc: 0.8677\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1672 - acc: 0.9427 - val_loss: 0.3435 - val_acc: 0.8730\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1670 - acc: 0.9439 - val_loss: 0.3423 - val_acc: 0.8730\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1672 - acc: 0.9450 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1674 - acc: 0.9439 - val_loss: 0.3401 - val_acc: 0.8677\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1666 - acc: 0.9421 - val_loss: 0.3405 - val_acc: 0.8730\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1673 - acc: 0.9427 - val_loss: 0.3400 - val_acc: 0.8730\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1669 - acc: 0.9444 - val_loss: 0.3418 - val_acc: 0.8677\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1665 - acc: 0.9433 - val_loss: 0.3371 - val_acc: 0.8677\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1669 - acc: 0.9427 - val_loss: 0.3382 - val_acc: 0.8677\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1669 - acc: 0.9439 - val_loss: 0.3406 - val_acc: 0.8624\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1668 - acc: 0.9427 - val_loss: 0.3397 - val_acc: 0.8624\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1672 - acc: 0.9439 - val_loss: 0.3443 - val_acc: 0.8730\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1665 - acc: 0.9409 - val_loss: 0.3392 - val_acc: 0.8624\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1662 - acc: 0.9421 - val_loss: 0.3411 - val_acc: 0.8677\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1670 - acc: 0.9427 - val_loss: 0.3436 - val_acc: 0.8730\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1665 - acc: 0.9397 - val_loss: 0.3376 - val_acc: 0.8624\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1662 - acc: 0.9450 - val_loss: 0.3411 - val_acc: 0.8677\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1664 - acc: 0.9427 - val_loss: 0.3415 - val_acc: 0.8677\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1660 - acc: 0.9433 - val_loss: 0.3410 - val_acc: 0.8730\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1661 - acc: 0.9444 - val_loss: 0.3408 - val_acc: 0.8677\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1661 - acc: 0.9433 - val_loss: 0.3367 - val_acc: 0.8624\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1658 - acc: 0.9433 - val_loss: 0.3415 - val_acc: 0.8730\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1659 - acc: 0.9450 - val_loss: 0.3396 - val_acc: 0.8677\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1662 - acc: 0.9409 - val_loss: 0.3413 - val_acc: 0.8730\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1662 - acc: 0.9409 - val_loss: 0.3405 - val_acc: 0.8677\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1654 - acc: 0.9444 - val_loss: 0.3385 - val_acc: 0.8624\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1659 - acc: 0.9439 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1654 - acc: 0.9450 - val_loss: 0.3399 - val_acc: 0.8624\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1659 - acc: 0.9421 - val_loss: 0.3415 - val_acc: 0.8730\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1658 - acc: 0.9439 - val_loss: 0.3397 - val_acc: 0.8677\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1655 - acc: 0.9427 - val_loss: 0.3399 - val_acc: 0.8730\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1659 - acc: 0.9427 - val_loss: 0.3393 - val_acc: 0.8677\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1652 - acc: 0.9456 - val_loss: 0.3400 - val_acc: 0.8730\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1651 - acc: 0.9433 - val_loss: 0.3421 - val_acc: 0.8730\n",
      "Epoch 477/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1654 - acc: 0.9427 - val_loss: 0.3403 - val_acc: 0.8730\n",
      "Epoch 478/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1661 - acc: 0.9415 - val_loss: 0.3424 - val_acc: 0.8730\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1656 - acc: 0.9444 - val_loss: 0.3369 - val_acc: 0.8677\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1652 - acc: 0.9433 - val_loss: 0.3388 - val_acc: 0.8624\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1653 - acc: 0.9427 - val_loss: 0.3429 - val_acc: 0.8730\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1649 - acc: 0.9439 - val_loss: 0.3378 - val_acc: 0.8624\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1650 - acc: 0.9415 - val_loss: 0.3376 - val_acc: 0.8677\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1646 - acc: 0.9444 - val_loss: 0.3401 - val_acc: 0.8677\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1648 - acc: 0.9439 - val_loss: 0.3409 - val_acc: 0.8730\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1651 - acc: 0.9444 - val_loss: 0.3408 - val_acc: 0.8730\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1649 - acc: 0.9439 - val_loss: 0.3411 - val_acc: 0.8730\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1645 - acc: 0.9427 - val_loss: 0.3393 - val_acc: 0.8730\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1647 - acc: 0.9439 - val_loss: 0.3410 - val_acc: 0.8730\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1648 - acc: 0.9433 - val_loss: 0.3417 - val_acc: 0.8730\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.3384 - val_acc: 0.8730\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1644 - acc: 0.9439 - val_loss: 0.3410 - val_acc: 0.8730\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1645 - acc: 0.9439 - val_loss: 0.3397 - val_acc: 0.8730\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1647 - acc: 0.9433 - val_loss: 0.3396 - val_acc: 0.8730\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1648 - acc: 0.9439 - val_loss: 0.3385 - val_acc: 0.8730\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1651 - acc: 0.9421 - val_loss: 0.3392 - val_acc: 0.8624\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1643 - acc: 0.9439 - val_loss: 0.3402 - val_acc: 0.8677\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1644 - acc: 0.9439 - val_loss: 0.3401 - val_acc: 0.8783\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1638 - acc: 0.9439 - val_loss: 0.3392 - val_acc: 0.8730\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1643 - acc: 0.9433 - val_loss: 0.3399 - val_acc: 0.8677\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1644 - acc: 0.9421 - val_loss: 0.3399 - val_acc: 0.8730\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1639 - acc: 0.9439 - val_loss: 0.3384 - val_acc: 0.8677\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1652 - acc: 0.9433 - val_loss: 0.3389 - val_acc: 0.8730\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1640 - acc: 0.9433 - val_loss: 0.3375 - val_acc: 0.8677\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1642 - acc: 0.9427 - val_loss: 0.3402 - val_acc: 0.8730\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1638 - acc: 0.9450 - val_loss: 0.3389 - val_acc: 0.8730\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1637 - acc: 0.9439 - val_loss: 0.3380 - val_acc: 0.8730\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1640 - acc: 0.9433 - val_loss: 0.3360 - val_acc: 0.8624\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1640 - acc: 0.9421 - val_loss: 0.3346 - val_acc: 0.8730\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1644 - acc: 0.9433 - val_loss: 0.3419 - val_acc: 0.8730\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1637 - acc: 0.9439 - val_loss: 0.3377 - val_acc: 0.8730\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1652 - acc: 0.9433 - val_loss: 0.3414 - val_acc: 0.8730\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1634 - acc: 0.9433 - val_loss: 0.3398 - val_acc: 0.8677\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1632 - acc: 0.9433 - val_loss: 0.3372 - val_acc: 0.8730\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1641 - acc: 0.9415 - val_loss: 0.3403 - val_acc: 0.8730\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1631 - acc: 0.9433 - val_loss: 0.3373 - val_acc: 0.8677\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1633 - acc: 0.9433 - val_loss: 0.3386 - val_acc: 0.8677\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1637 - acc: 0.9439 - val_loss: 0.3367 - val_acc: 0.8730\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1633 - acc: 0.9421 - val_loss: 0.3418 - val_acc: 0.8730\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1636 - acc: 0.9439 - val_loss: 0.3373 - val_acc: 0.8730\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1633 - acc: 0.9433 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1636 - acc: 0.9421 - val_loss: 0.3374 - val_acc: 0.8730\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1634 - acc: 0.9439 - val_loss: 0.3380 - val_acc: 0.8730\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1633 - acc: 0.9433 - val_loss: 0.3401 - val_acc: 0.8730\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1631 - acc: 0.9427 - val_loss: 0.3361 - val_acc: 0.8677\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1637 - acc: 0.9427 - val_loss: 0.3382 - val_acc: 0.8730\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1628 - acc: 0.9444 - val_loss: 0.3386 - val_acc: 0.8730\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1629 - acc: 0.9427 - val_loss: 0.3365 - val_acc: 0.8730\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1627 - acc: 0.9450 - val_loss: 0.3395 - val_acc: 0.8730\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1631 - acc: 0.9421 - val_loss: 0.3386 - val_acc: 0.8783\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1635 - acc: 0.9421 - val_loss: 0.3388 - val_acc: 0.8730\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1630 - acc: 0.9439 - val_loss: 0.3365 - val_acc: 0.8730\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1630 - acc: 0.9433 - val_loss: 0.3360 - val_acc: 0.8783\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1625 - acc: 0.9427 - val_loss: 0.3369 - val_acc: 0.8730\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1631 - acc: 0.9433 - val_loss: 0.3379 - val_acc: 0.8730\n",
      "Epoch 536/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1634 - acc: 0.9433 - val_loss: 0.3331 - val_acc: 0.8730\n",
      "Epoch 537/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1628 - acc: 0.9439 - val_loss: 0.3362 - val_acc: 0.8730\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1620 - acc: 0.9421 - val_loss: 0.3411 - val_acc: 0.8730\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1629 - acc: 0.9433 - val_loss: 0.3401 - val_acc: 0.8783\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1627 - acc: 0.9415 - val_loss: 0.3383 - val_acc: 0.8730\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1631 - acc: 0.9433 - val_loss: 0.3388 - val_acc: 0.8783\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1625 - acc: 0.9433 - val_loss: 0.3391 - val_acc: 0.8730\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1628 - acc: 0.9444 - val_loss: 0.3364 - val_acc: 0.8783\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1626 - acc: 0.9433 - val_loss: 0.3365 - val_acc: 0.8730\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1621 - acc: 0.9427 - val_loss: 0.3376 - val_acc: 0.8730\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1621 - acc: 0.9450 - val_loss: 0.3370 - val_acc: 0.8783\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1621 - acc: 0.9433 - val_loss: 0.3362 - val_acc: 0.8730\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1619 - acc: 0.9456 - val_loss: 0.3366 - val_acc: 0.8730\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1619 - acc: 0.9427 - val_loss: 0.3382 - val_acc: 0.8730\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1620 - acc: 0.9421 - val_loss: 0.3383 - val_acc: 0.8730\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1623 - acc: 0.9450 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1613 - acc: 0.9462 - val_loss: 0.3384 - val_acc: 0.8783\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1616 - acc: 0.9439 - val_loss: 0.3372 - val_acc: 0.8783\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1614 - acc: 0.9450 - val_loss: 0.3355 - val_acc: 0.8783\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1623 - acc: 0.9444 - val_loss: 0.3354 - val_acc: 0.8783\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1613 - acc: 0.9439 - val_loss: 0.3372 - val_acc: 0.8730\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1620 - acc: 0.9450 - val_loss: 0.3362 - val_acc: 0.8730\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1615 - acc: 0.9421 - val_loss: 0.3361 - val_acc: 0.8730\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1618 - acc: 0.9444 - val_loss: 0.3365 - val_acc: 0.8730\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1610 - acc: 0.9421 - val_loss: 0.3384 - val_acc: 0.8783\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1610 - acc: 0.9450 - val_loss: 0.3329 - val_acc: 0.8783\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1616 - acc: 0.9439 - val_loss: 0.3359 - val_acc: 0.8783\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1611 - acc: 0.9450 - val_loss: 0.3347 - val_acc: 0.8783\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1605 - acc: 0.9456 - val_loss: 0.3341 - val_acc: 0.8730\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1611 - acc: 0.9427 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1609 - acc: 0.9462 - val_loss: 0.3336 - val_acc: 0.8783\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1607 - acc: 0.9444 - val_loss: 0.3347 - val_acc: 0.8783\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1610 - acc: 0.9439 - val_loss: 0.3340 - val_acc: 0.8730\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1605 - acc: 0.9439 - val_loss: 0.3312 - val_acc: 0.8783\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1604 - acc: 0.9462 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1609 - acc: 0.9444 - val_loss: 0.3336 - val_acc: 0.8783\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1606 - acc: 0.9450 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1611 - acc: 0.9433 - val_loss: 0.3324 - val_acc: 0.8783\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1607 - acc: 0.9456 - val_loss: 0.3356 - val_acc: 0.8783\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1608 - acc: 0.9439 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1601 - acc: 0.9444 - val_loss: 0.3341 - val_acc: 0.8730\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1607 - acc: 0.9439 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1604 - acc: 0.9439 - val_loss: 0.3352 - val_acc: 0.8730\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1609 - acc: 0.9450 - val_loss: 0.3365 - val_acc: 0.8783\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1601 - acc: 0.9456 - val_loss: 0.3324 - val_acc: 0.8783\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1610 - acc: 0.9456 - val_loss: 0.3335 - val_acc: 0.8730\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1604 - acc: 0.9427 - val_loss: 0.3311 - val_acc: 0.8783\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1607 - acc: 0.9439 - val_loss: 0.3359 - val_acc: 0.8783\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1604 - acc: 0.9450 - val_loss: 0.3342 - val_acc: 0.8730\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1598 - acc: 0.9450 - val_loss: 0.3303 - val_acc: 0.8783\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1602 - acc: 0.9456 - val_loss: 0.3326 - val_acc: 0.8730\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.3338 - val_acc: 0.8783\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1604 - acc: 0.9444 - val_loss: 0.3315 - val_acc: 0.8783\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1600 - acc: 0.9450 - val_loss: 0.3362 - val_acc: 0.8730\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1596 - acc: 0.9444 - val_loss: 0.3317 - val_acc: 0.8783\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1606 - acc: 0.9444 - val_loss: 0.3330 - val_acc: 0.8730\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1600 - acc: 0.9456 - val_loss: 0.3346 - val_acc: 0.8730\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1593 - acc: 0.9468 - val_loss: 0.3322 - val_acc: 0.8783\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1599 - acc: 0.9450 - val_loss: 0.3309 - val_acc: 0.8783\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1596 - acc: 0.9456 - val_loss: 0.3340 - val_acc: 0.8730\n",
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1599 - acc: 0.9433 - val_loss: 0.3335 - val_acc: 0.8730\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1599 - acc: 0.9439 - val_loss: 0.3340 - val_acc: 0.8783\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1597 - acc: 0.9462 - val_loss: 0.3327 - val_acc: 0.8836\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1600 - acc: 0.9456 - val_loss: 0.3318 - val_acc: 0.8730\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1593 - acc: 0.9439 - val_loss: 0.3337 - val_acc: 0.8730\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1593 - acc: 0.9450 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1596 - acc: 0.9462 - val_loss: 0.3325 - val_acc: 0.8783\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1595 - acc: 0.9439 - val_loss: 0.3310 - val_acc: 0.8783\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1598 - acc: 0.9450 - val_loss: 0.3348 - val_acc: 0.8730\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1591 - acc: 0.9456 - val_loss: 0.3312 - val_acc: 0.8836\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1597 - acc: 0.9444 - val_loss: 0.3337 - val_acc: 0.8783\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1592 - acc: 0.9468 - val_loss: 0.3337 - val_acc: 0.8730\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1599 - acc: 0.9450 - val_loss: 0.3331 - val_acc: 0.8836\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1603 - acc: 0.9468 - val_loss: 0.3291 - val_acc: 0.8783\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1591 - acc: 0.9456 - val_loss: 0.3320 - val_acc: 0.8783\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1597 - acc: 0.9456 - val_loss: 0.3333 - val_acc: 0.8783\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1590 - acc: 0.9468 - val_loss: 0.3302 - val_acc: 0.8783\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1590 - acc: 0.9439 - val_loss: 0.3318 - val_acc: 0.8730\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1595 - acc: 0.9456 - val_loss: 0.3296 - val_acc: 0.8836\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9439 - val_loss: 0.3341 - val_acc: 0.8730\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1595 - acc: 0.9450 - val_loss: 0.3302 - val_acc: 0.8783\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1597 - acc: 0.9462 - val_loss: 0.3352 - val_acc: 0.8783\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9456 - val_loss: 0.3326 - val_acc: 0.8783\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1590 - acc: 0.9462 - val_loss: 0.3309 - val_acc: 0.8836\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1592 - acc: 0.9427 - val_loss: 0.3325 - val_acc: 0.8783\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1590 - acc: 0.9456 - val_loss: 0.3322 - val_acc: 0.8783\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1584 - acc: 0.9450 - val_loss: 0.3326 - val_acc: 0.8730\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1585 - acc: 0.9468 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1589 - acc: 0.9462 - val_loss: 0.3339 - val_acc: 0.8836\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1591 - acc: 0.9468 - val_loss: 0.3307 - val_acc: 0.8836\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1593 - acc: 0.9474 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1588 - acc: 0.9439 - val_loss: 0.3323 - val_acc: 0.8730\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1585 - acc: 0.9474 - val_loss: 0.3301 - val_acc: 0.8836\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1596 - acc: 0.9433 - val_loss: 0.3309 - val_acc: 0.8836\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1588 - acc: 0.9486 - val_loss: 0.3337 - val_acc: 0.8783\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1587 - acc: 0.9480 - val_loss: 0.3295 - val_acc: 0.8836\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1582 - acc: 0.9439 - val_loss: 0.3341 - val_acc: 0.8730\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1588 - acc: 0.9456 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1586 - acc: 0.9450 - val_loss: 0.3364 - val_acc: 0.8783\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1588 - acc: 0.9468 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1588 - acc: 0.9462 - val_loss: 0.3326 - val_acc: 0.8836\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9450 - val_loss: 0.3344 - val_acc: 0.8783\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1584 - acc: 0.9462 - val_loss: 0.3305 - val_acc: 0.8836\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1580 - acc: 0.9456 - val_loss: 0.3312 - val_acc: 0.8783\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1584 - acc: 0.9456 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1584 - acc: 0.9450 - val_loss: 0.3367 - val_acc: 0.8730\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1587 - acc: 0.9474 - val_loss: 0.3357 - val_acc: 0.8783\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1586 - acc: 0.9462 - val_loss: 0.3356 - val_acc: 0.8730\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1581 - acc: 0.9462 - val_loss: 0.3314 - val_acc: 0.8836\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1580 - acc: 0.9456 - val_loss: 0.3334 - val_acc: 0.8836\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1582 - acc: 0.9468 - val_loss: 0.3288 - val_acc: 0.8836\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1585 - acc: 0.9439 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1586 - acc: 0.9456 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1576 - acc: 0.9444 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1580 - acc: 0.9462 - val_loss: 0.3315 - val_acc: 0.8783\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1580 - acc: 0.9468 - val_loss: 0.3305 - val_acc: 0.8836\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1588 - acc: 0.9456 - val_loss: 0.3328 - val_acc: 0.8783\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1579 - acc: 0.9474 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 654/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1577 - acc: 0.9474 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1583 - acc: 0.9480 - val_loss: 0.3307 - val_acc: 0.8836\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1582 - acc: 0.9456 - val_loss: 0.3289 - val_acc: 0.8836\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1579 - acc: 0.9456 - val_loss: 0.3317 - val_acc: 0.8783\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1582 - acc: 0.9444 - val_loss: 0.3359 - val_acc: 0.8836\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1576 - acc: 0.9456 - val_loss: 0.3336 - val_acc: 0.8836\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1578 - acc: 0.9486 - val_loss: 0.3297 - val_acc: 0.8836\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1584 - acc: 0.9450 - val_loss: 0.3345 - val_acc: 0.8783\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1574 - acc: 0.9468 - val_loss: 0.3300 - val_acc: 0.8836\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1575 - acc: 0.9433 - val_loss: 0.3346 - val_acc: 0.8836\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1580 - acc: 0.9450 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1581 - acc: 0.9474 - val_loss: 0.3368 - val_acc: 0.8783\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1577 - acc: 0.9474 - val_loss: 0.3300 - val_acc: 0.8836\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1578 - acc: 0.9456 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1572 - acc: 0.9462 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1578 - acc: 0.9480 - val_loss: 0.3286 - val_acc: 0.8889\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1582 - acc: 0.9468 - val_loss: 0.3342 - val_acc: 0.8783\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1579 - acc: 0.9480 - val_loss: 0.3330 - val_acc: 0.8836\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1576 - acc: 0.9474 - val_loss: 0.3302 - val_acc: 0.8889\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1571 - acc: 0.9456 - val_loss: 0.3323 - val_acc: 0.8783\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1575 - acc: 0.9468 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.3328 - val_acc: 0.8783\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1574 - acc: 0.9456 - val_loss: 0.3336 - val_acc: 0.8730\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1573 - acc: 0.9456 - val_loss: 0.3322 - val_acc: 0.8783\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.3351 - val_acc: 0.8836\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1570 - acc: 0.9474 - val_loss: 0.3308 - val_acc: 0.8836\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1575 - acc: 0.9480 - val_loss: 0.3372 - val_acc: 0.8783\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.1574 - acc: 0.9474 - val_loss: 0.3303 - val_acc: 0.8836\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1570 - acc: 0.9456 - val_loss: 0.3325 - val_acc: 0.8783\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1570 - acc: 0.9468 - val_loss: 0.3315 - val_acc: 0.8836\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1567 - acc: 0.9444 - val_loss: 0.3352 - val_acc: 0.8836\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1570 - acc: 0.9468 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1574 - acc: 0.9456 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1568 - acc: 0.9474 - val_loss: 0.3352 - val_acc: 0.8730\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1570 - acc: 0.9468 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1569 - acc: 0.9462 - val_loss: 0.3412 - val_acc: 0.8783\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1567 - acc: 0.9474 - val_loss: 0.3303 - val_acc: 0.8836\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1564 - acc: 0.9450 - val_loss: 0.3359 - val_acc: 0.8730\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1575 - acc: 0.9462 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1568 - acc: 0.9480 - val_loss: 0.3323 - val_acc: 0.8836\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1565 - acc: 0.9444 - val_loss: 0.3358 - val_acc: 0.8783\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1567 - acc: 0.9462 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1565 - acc: 0.9474 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1567 - acc: 0.9474 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1573 - acc: 0.9474 - val_loss: 0.3360 - val_acc: 0.8783\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1564 - acc: 0.9456 - val_loss: 0.3350 - val_acc: 0.8836\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1565 - acc: 0.9468 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1564 - acc: 0.9480 - val_loss: 0.3324 - val_acc: 0.8783\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1563 - acc: 0.9462 - val_loss: 0.3312 - val_acc: 0.8836\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1569 - acc: 0.9468 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1567 - acc: 0.9450 - val_loss: 0.3315 - val_acc: 0.8836\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1561 - acc: 0.9474 - val_loss: 0.3361 - val_acc: 0.8783\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1565 - acc: 0.9480 - val_loss: 0.3295 - val_acc: 0.8836\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1571 - acc: 0.9480 - val_loss: 0.3301 - val_acc: 0.8836\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1557 - acc: 0.9480 - val_loss: 0.3366 - val_acc: 0.8730\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1565 - acc: 0.9474 - val_loss: 0.3337 - val_acc: 0.8836\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1561 - acc: 0.9480 - val_loss: 0.3287 - val_acc: 0.8836\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1567 - acc: 0.9456 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1565 - acc: 0.9474 - val_loss: 0.3312 - val_acc: 0.8836\n",
      "Epoch 713/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1566 - acc: 0.9468 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1558 - acc: 0.9468 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1562 - acc: 0.9462 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1564 - acc: 0.9468 - val_loss: 0.3320 - val_acc: 0.8836\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.3334 - val_acc: 0.8836\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1561 - acc: 0.9456 - val_loss: 0.3334 - val_acc: 0.8783\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9456 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1558 - acc: 0.9468 - val_loss: 0.3323 - val_acc: 0.8783\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1556 - acc: 0.9474 - val_loss: 0.3325 - val_acc: 0.8836\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1559 - acc: 0.9474 - val_loss: 0.3310 - val_acc: 0.8836\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1562 - acc: 0.9474 - val_loss: 0.3326 - val_acc: 0.8783\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1560 - acc: 0.9462 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9468 - val_loss: 0.3305 - val_acc: 0.8783\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1554 - acc: 0.9480 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1555 - acc: 0.9462 - val_loss: 0.3328 - val_acc: 0.8836\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1556 - acc: 0.9480 - val_loss: 0.3340 - val_acc: 0.8836\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1556 - acc: 0.9462 - val_loss: 0.3349 - val_acc: 0.8836\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9450 - val_loss: 0.3346 - val_acc: 0.8836\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9474 - val_loss: 0.3298 - val_acc: 0.8889\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 45us/step - loss: 0.1555 - acc: 0.9444 - val_loss: 0.3366 - val_acc: 0.8836\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9480 - val_loss: 0.3304 - val_acc: 0.8836\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1553 - acc: 0.9480 - val_loss: 0.3301 - val_acc: 0.8783\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1558 - acc: 0.9462 - val_loss: 0.3308 - val_acc: 0.8836\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1558 - acc: 0.9456 - val_loss: 0.3275 - val_acc: 0.8836\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1560 - acc: 0.9456 - val_loss: 0.3315 - val_acc: 0.8836\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.3337 - val_acc: 0.8836\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1554 - acc: 0.9474 - val_loss: 0.3334 - val_acc: 0.8783\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1552 - acc: 0.9480 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1557 - acc: 0.9462 - val_loss: 0.3326 - val_acc: 0.8836\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1562 - acc: 0.9468 - val_loss: 0.3328 - val_acc: 0.8889\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1550 - acc: 0.9456 - val_loss: 0.3342 - val_acc: 0.8783\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1552 - acc: 0.9468 - val_loss: 0.3285 - val_acc: 0.8836\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1553 - acc: 0.9450 - val_loss: 0.3337 - val_acc: 0.8836\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1548 - acc: 0.9468 - val_loss: 0.3299 - val_acc: 0.8836\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1558 - acc: 0.9456 - val_loss: 0.3281 - val_acc: 0.8889\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1559 - acc: 0.9462 - val_loss: 0.3310 - val_acc: 0.8889\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1561 - acc: 0.9486 - val_loss: 0.3327 - val_acc: 0.8783\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1554 - acc: 0.9468 - val_loss: 0.3339 - val_acc: 0.8836\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1555 - acc: 0.9468 - val_loss: 0.3340 - val_acc: 0.8836\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1557 - acc: 0.9480 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1548 - acc: 0.9474 - val_loss: 0.3350 - val_acc: 0.8783\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1551 - acc: 0.9450 - val_loss: 0.3302 - val_acc: 0.8836\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1545 - acc: 0.9468 - val_loss: 0.3355 - val_acc: 0.8783\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1551 - acc: 0.9462 - val_loss: 0.3358 - val_acc: 0.8783\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1552 - acc: 0.9462 - val_loss: 0.3328 - val_acc: 0.8889\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1555 - acc: 0.9474 - val_loss: 0.3296 - val_acc: 0.8836\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1551 - acc: 0.9462 - val_loss: 0.3307 - val_acc: 0.8836\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1550 - acc: 0.9468 - val_loss: 0.3342 - val_acc: 0.8783\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1557 - acc: 0.9444 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1559 - acc: 0.9462 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1554 - acc: 0.9474 - val_loss: 0.3353 - val_acc: 0.8783\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1549 - acc: 0.9480 - val_loss: 0.3331 - val_acc: 0.8836\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1546 - acc: 0.9474 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1547 - acc: 0.9462 - val_loss: 0.3358 - val_acc: 0.8783\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1544 - acc: 0.9462 - val_loss: 0.3330 - val_acc: 0.8889\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1547 - acc: 0.9480 - val_loss: 0.3307 - val_acc: 0.8836\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1542 - acc: 0.9462 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 772/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1547 - acc: 0.9468 - val_loss: 0.3362 - val_acc: 0.8836\n",
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1432 - acc: 0.951 - 0s 64us/step - loss: 0.1549 - acc: 0.9456 - val_loss: 0.3333 - val_acc: 0.8836\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1541 - acc: 0.9486 - val_loss: 0.3311 - val_acc: 0.8836\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1544 - acc: 0.9474 - val_loss: 0.3310 - val_acc: 0.8889\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1548 - acc: 0.9474 - val_loss: 0.3293 - val_acc: 0.8889\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1557 - acc: 0.9474 - val_loss: 0.3320 - val_acc: 0.8889\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1543 - acc: 0.9468 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1548 - acc: 0.9486 - val_loss: 0.3309 - val_acc: 0.8836\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1544 - acc: 0.9474 - val_loss: 0.3324 - val_acc: 0.8783\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1556 - acc: 0.9462 - val_loss: 0.3365 - val_acc: 0.8836\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1537 - acc: 0.9468 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1543 - acc: 0.9474 - val_loss: 0.3314 - val_acc: 0.8836\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1546 - acc: 0.9468 - val_loss: 0.3309 - val_acc: 0.8889\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1543 - acc: 0.9462 - val_loss: 0.3321 - val_acc: 0.8836\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1541 - acc: 0.9474 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1548 - acc: 0.9480 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1544 - acc: 0.9468 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1546 - acc: 0.9456 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1540 - acc: 0.9462 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1543 - acc: 0.9474 - val_loss: 0.3323 - val_acc: 0.8836\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1544 - acc: 0.9456 - val_loss: 0.3329 - val_acc: 0.8836\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1538 - acc: 0.9486 - val_loss: 0.3360 - val_acc: 0.8783\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1551 - acc: 0.9480 - val_loss: 0.3322 - val_acc: 0.8889\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1541 - acc: 0.9480 - val_loss: 0.3345 - val_acc: 0.8836\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1535 - acc: 0.9474 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1538 - acc: 0.9480 - val_loss: 0.3361 - val_acc: 0.8836\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1538 - acc: 0.9468 - val_loss: 0.3313 - val_acc: 0.8889\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1538 - acc: 0.9474 - val_loss: 0.3332 - val_acc: 0.8836\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1537 - acc: 0.9468 - val_loss: 0.3327 - val_acc: 0.8836\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1535 - acc: 0.9474 - val_loss: 0.3347 - val_acc: 0.8889\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1533 - acc: 0.9468 - val_loss: 0.3343 - val_acc: 0.8889\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1541 - acc: 0.9474 - val_loss: 0.3369 - val_acc: 0.8783\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1532 - acc: 0.9474 - val_loss: 0.3322 - val_acc: 0.8889\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1534 - acc: 0.9462 - val_loss: 0.3336 - val_acc: 0.8889\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1532 - acc: 0.9468 - val_loss: 0.3332 - val_acc: 0.8889\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1539 - acc: 0.9474 - val_loss: 0.3334 - val_acc: 0.8889\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1541 - acc: 0.9486 - val_loss: 0.3325 - val_acc: 0.8889\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1535 - acc: 0.9456 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1531 - acc: 0.9474 - val_loss: 0.3328 - val_acc: 0.8889\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1534 - acc: 0.9468 - val_loss: 0.3338 - val_acc: 0.8889\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1532 - acc: 0.9456 - val_loss: 0.3342 - val_acc: 0.8836\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1534 - acc: 0.9480 - val_loss: 0.3327 - val_acc: 0.8889\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1534 - acc: 0.9474 - val_loss: 0.3324 - val_acc: 0.8889\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1530 - acc: 0.9462 - val_loss: 0.3371 - val_acc: 0.8836\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1532 - acc: 0.9486 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1533 - acc: 0.9474 - val_loss: 0.3346 - val_acc: 0.8889\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.3354 - val_acc: 0.8783\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1531 - acc: 0.9474 - val_loss: 0.3342 - val_acc: 0.8836\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1531 - acc: 0.9474 - val_loss: 0.3351 - val_acc: 0.8836\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1531 - acc: 0.9480 - val_loss: 0.3321 - val_acc: 0.8889\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.3360 - val_acc: 0.8783\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1525 - acc: 0.9468 - val_loss: 0.3339 - val_acc: 0.8836\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1529 - acc: 0.9486 - val_loss: 0.3313 - val_acc: 0.8889\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1531 - acc: 0.9462 - val_loss: 0.3297 - val_acc: 0.8889\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1533 - acc: 0.9468 - val_loss: 0.3345 - val_acc: 0.8889\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1528 - acc: 0.9486 - val_loss: 0.3385 - val_acc: 0.8783\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1530 - acc: 0.9480 - val_loss: 0.3334 - val_acc: 0.8889\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.3305 - val_acc: 0.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1528 - acc: 0.9480 - val_loss: 0.3326 - val_acc: 0.8889\n",
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1526 - acc: 0.9474 - val_loss: 0.3380 - val_acc: 0.8889\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1527 - acc: 0.9474 - val_loss: 0.3331 - val_acc: 0.8889\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1524 - acc: 0.9456 - val_loss: 0.3321 - val_acc: 0.8836\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1539 - acc: 0.9474 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1528 - acc: 0.9492 - val_loss: 0.3349 - val_acc: 0.8783\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1526 - acc: 0.9456 - val_loss: 0.3323 - val_acc: 0.8889\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1529 - acc: 0.9474 - val_loss: 0.3338 - val_acc: 0.8836\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1527 - acc: 0.9480 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1523 - acc: 0.9486 - val_loss: 0.3329 - val_acc: 0.8889\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1531 - acc: 0.9486 - val_loss: 0.3320 - val_acc: 0.8836\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1521 - acc: 0.9462 - val_loss: 0.3333 - val_acc: 0.8889\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1530 - acc: 0.9486 - val_loss: 0.3326 - val_acc: 0.8836\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1525 - acc: 0.9468 - val_loss: 0.3349 - val_acc: 0.8889\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1533 - acc: 0.9474 - val_loss: 0.3366 - val_acc: 0.8783\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.3379 - val_acc: 0.8783\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1523 - acc: 0.9474 - val_loss: 0.3330 - val_acc: 0.8836\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1533 - acc: 0.9474 - val_loss: 0.3328 - val_acc: 0.8889\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1536 - acc: 0.9480 - val_loss: 0.3333 - val_acc: 0.8889\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1529 - acc: 0.9492 - val_loss: 0.3309 - val_acc: 0.8889\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1524 - acc: 0.9474 - val_loss: 0.3401 - val_acc: 0.8836\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1528 - acc: 0.9492 - val_loss: 0.3325 - val_acc: 0.8889\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1528 - acc: 0.9468 - val_loss: 0.3324 - val_acc: 0.8836\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1519 - acc: 0.9486 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1519 - acc: 0.9474 - val_loss: 0.3319 - val_acc: 0.8889\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1525 - acc: 0.9486 - val_loss: 0.3341 - val_acc: 0.8836\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1527 - acc: 0.9462 - val_loss: 0.3300 - val_acc: 0.8836\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1521 - acc: 0.9468 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1523 - acc: 0.9474 - val_loss: 0.3322 - val_acc: 0.8836\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1519 - acc: 0.9480 - val_loss: 0.3319 - val_acc: 0.8889\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1520 - acc: 0.9474 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1524 - acc: 0.9468 - val_loss: 0.3318 - val_acc: 0.8889\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1536 - acc: 0.9450 - val_loss: 0.3313 - val_acc: 0.8889\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1521 - acc: 0.9504 - val_loss: 0.3318 - val_acc: 0.8889\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1521 - acc: 0.9474 - val_loss: 0.3291 - val_acc: 0.8942\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1526 - acc: 0.9474 - val_loss: 0.3312 - val_acc: 0.8889\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1516 - acc: 0.9462 - val_loss: 0.3362 - val_acc: 0.8836\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1524 - acc: 0.9468 - val_loss: 0.3347 - val_acc: 0.8836\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1525 - acc: 0.9486 - val_loss: 0.3368 - val_acc: 0.8889\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1526 - acc: 0.9486 - val_loss: 0.3371 - val_acc: 0.8836\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1520 - acc: 0.9456 - val_loss: 0.3345 - val_acc: 0.8836\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1518 - acc: 0.9480 - val_loss: 0.3321 - val_acc: 0.8942\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1523 - acc: 0.9462 - val_loss: 0.3337 - val_acc: 0.8889\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1521 - acc: 0.9492 - val_loss: 0.3367 - val_acc: 0.8836\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1521 - acc: 0.9492 - val_loss: 0.3348 - val_acc: 0.8836\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1517 - acc: 0.9486 - val_loss: 0.3338 - val_acc: 0.8942\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1515 - acc: 0.9474 - val_loss: 0.3342 - val_acc: 0.8889\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1524 - acc: 0.9462 - val_loss: 0.3315 - val_acc: 0.8889\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1537 - acc: 0.9468 - val_loss: 0.3315 - val_acc: 0.8942\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1513 - acc: 0.9462 - val_loss: 0.3330 - val_acc: 0.8889\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1518 - acc: 0.9492 - val_loss: 0.3348 - val_acc: 0.8836\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.3362 - val_acc: 0.8942\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1513 - acc: 0.9480 - val_loss: 0.3311 - val_acc: 0.8889\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1520 - acc: 0.9468 - val_loss: 0.3297 - val_acc: 0.8942\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1519 - acc: 0.9462 - val_loss: 0.3319 - val_acc: 0.8836\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1512 - acc: 0.9486 - val_loss: 0.3320 - val_acc: 0.8942\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1526 - acc: 0.9468 - val_loss: 0.3317 - val_acc: 0.8783\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1519 - acc: 0.9486 - val_loss: 0.3298 - val_acc: 0.8942\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1516 - acc: 0.9480 - val_loss: 0.3314 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1514 - acc: 0.9474 - val_loss: 0.3368 - val_acc: 0.8836\n",
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1516 - acc: 0.9474 - val_loss: 0.3317 - val_acc: 0.8836\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1516 - acc: 0.9468 - val_loss: 0.3316 - val_acc: 0.8836\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1518 - acc: 0.9480 - val_loss: 0.3323 - val_acc: 0.8889\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1514 - acc: 0.9480 - val_loss: 0.3322 - val_acc: 0.8889\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1515 - acc: 0.9480 - val_loss: 0.3338 - val_acc: 0.8889\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1516 - acc: 0.9474 - val_loss: 0.3322 - val_acc: 0.8942\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1520 - acc: 0.9468 - val_loss: 0.3332 - val_acc: 0.8889\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1514 - acc: 0.9486 - val_loss: 0.3313 - val_acc: 0.8836\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1512 - acc: 0.9486 - val_loss: 0.3328 - val_acc: 0.8942\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1512 - acc: 0.9468 - val_loss: 0.3323 - val_acc: 0.8889\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1508 - acc: 0.9480 - val_loss: 0.3331 - val_acc: 0.8942\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1508 - acc: 0.9492 - val_loss: 0.3326 - val_acc: 0.8836\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1518 - acc: 0.9480 - val_loss: 0.3315 - val_acc: 0.8942\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1515 - acc: 0.9480 - val_loss: 0.3369 - val_acc: 0.8783\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1513 - acc: 0.9492 - val_loss: 0.3373 - val_acc: 0.8942\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1515 - acc: 0.9462 - val_loss: 0.3343 - val_acc: 0.8836\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1514 - acc: 0.9492 - val_loss: 0.3306 - val_acc: 0.8942\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1511 - acc: 0.9492 - val_loss: 0.3351 - val_acc: 0.8889\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1513 - acc: 0.9492 - val_loss: 0.3346 - val_acc: 0.8783\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1509 - acc: 0.9480 - val_loss: 0.3370 - val_acc: 0.8889\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1510 - acc: 0.9486 - val_loss: 0.3344 - val_acc: 0.8783\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1513 - acc: 0.9480 - val_loss: 0.3351 - val_acc: 0.8783\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1511 - acc: 0.9474 - val_loss: 0.3349 - val_acc: 0.8942\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1511 - acc: 0.9498 - val_loss: 0.3334 - val_acc: 0.8836\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1508 - acc: 0.9498 - val_loss: 0.3295 - val_acc: 0.8942\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1507 - acc: 0.9480 - val_loss: 0.3340 - val_acc: 0.8942\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1507 - acc: 0.9498 - val_loss: 0.3327 - val_acc: 0.8942\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1509 - acc: 0.9486 - val_loss: 0.3323 - val_acc: 0.8942\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1514 - acc: 0.9492 - val_loss: 0.3334 - val_acc: 0.8836\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1502 - acc: 0.9486 - val_loss: 0.3316 - val_acc: 0.8942\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1507 - acc: 0.9486 - val_loss: 0.3335 - val_acc: 0.8783\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1509 - acc: 0.9486 - val_loss: 0.3318 - val_acc: 0.8942\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1505 - acc: 0.9498 - val_loss: 0.3305 - val_acc: 0.8942\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1512 - acc: 0.9486 - val_loss: 0.3342 - val_acc: 0.8889\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1506 - acc: 0.9498 - val_loss: 0.3325 - val_acc: 0.8889\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1506 - acc: 0.9492 - val_loss: 0.3329 - val_acc: 0.8889\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1511 - acc: 0.9474 - val_loss: 0.3274 - val_acc: 0.8942\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1510 - acc: 0.9480 - val_loss: 0.3294 - val_acc: 0.8942\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1503 - acc: 0.9504 - val_loss: 0.3324 - val_acc: 0.8889\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1503 - acc: 0.9486 - val_loss: 0.3345 - val_acc: 0.8942\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1503 - acc: 0.9480 - val_loss: 0.3336 - val_acc: 0.8942\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.3312 - val_acc: 0.8889\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1509 - acc: 0.9486 - val_loss: 0.3344 - val_acc: 0.8889\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1504 - acc: 0.9509 - val_loss: 0.3333 - val_acc: 0.8836\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1505 - acc: 0.9509 - val_loss: 0.3316 - val_acc: 0.8942\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1511 - acc: 0.9480 - val_loss: 0.3327 - val_acc: 0.8889\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1504 - acc: 0.9492 - val_loss: 0.3313 - val_acc: 0.8942\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1504 - acc: 0.9492 - val_loss: 0.3329 - val_acc: 0.8942\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.3319 - val_acc: 0.8889\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1505 - acc: 0.9486 - val_loss: 0.3292 - val_acc: 0.8942\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1505 - acc: 0.9486 - val_loss: 0.3319 - val_acc: 0.8942\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1502 - acc: 0.9486 - val_loss: 0.3309 - val_acc: 0.8889\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1504 - acc: 0.9498 - val_loss: 0.3301 - val_acc: 0.8942\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1499 - acc: 0.9498 - val_loss: 0.3348 - val_acc: 0.8783\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1512 - acc: 0.9492 - val_loss: 0.3385 - val_acc: 0.8889\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1506 - acc: 0.9515 - val_loss: 0.3350 - val_acc: 0.8942\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1502 - acc: 0.9474 - val_loss: 0.3339 - val_acc: 0.8889\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1505 - acc: 0.9492 - val_loss: 0.3353 - val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1498 - acc: 0.9486 - val_loss: 0.3331 - val_acc: 0.8942\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.3350 - val_acc: 0.8942\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1497 - acc: 0.9498 - val_loss: 0.3358 - val_acc: 0.8836\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1502 - acc: 0.9492 - val_loss: 0.3325 - val_acc: 0.8942\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1504 - acc: 0.9492 - val_loss: 0.3337 - val_acc: 0.8889\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1503 - acc: 0.9474 - val_loss: 0.3342 - val_acc: 0.8783\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1505 - acc: 0.9486 - val_loss: 0.3315 - val_acc: 0.8942\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1501 - acc: 0.9498 - val_loss: 0.3316 - val_acc: 0.8942\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1498 - acc: 0.9504 - val_loss: 0.3314 - val_acc: 0.8889\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1498 - acc: 0.9492 - val_loss: 0.3376 - val_acc: 0.8783\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1497 - acc: 0.9480 - val_loss: 0.3350 - val_acc: 0.8942\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1504 - acc: 0.9474 - val_loss: 0.3372 - val_acc: 0.8889\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1500 - acc: 0.9498 - val_loss: 0.3303 - val_acc: 0.8942\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1505 - acc: 0.9486 - val_loss: 0.3320 - val_acc: 0.8942\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1497 - acc: 0.9509 - val_loss: 0.3322 - val_acc: 0.8942\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1492 - acc: 0.9480 - val_loss: 0.3354 - val_acc: 0.8889\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1494 - acc: 0.9486 - val_loss: 0.3347 - val_acc: 0.8889\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1496 - acc: 0.9492 - val_loss: 0.3315 - val_acc: 0.8942\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1496 - acc: 0.9492 - val_loss: 0.3383 - val_acc: 0.8942\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1500 - acc: 0.9486 - val_loss: 0.3320 - val_acc: 0.8942\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1501 - acc: 0.9486 - val_loss: 0.3386 - val_acc: 0.8942\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1489 - acc: 0.9486 - val_loss: 0.3318 - val_acc: 0.8942\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.3346 - val_acc: 0.8889\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1499 - acc: 0.9486 - val_loss: 0.3300 - val_acc: 0.8942\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1498 - acc: 0.9498 - val_loss: 0.3333 - val_acc: 0.8942\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1497 - acc: 0.9486 - val_loss: 0.3335 - val_acc: 0.8836\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1499 - acc: 0.9509 - val_loss: 0.3341 - val_acc: 0.8942\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1501 - acc: 0.9480 - val_loss: 0.3308 - val_acc: 0.8889\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1493 - acc: 0.9492 - val_loss: 0.3336 - val_acc: 0.8942\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1493 - acc: 0.9498 - val_loss: 0.3297 - val_acc: 0.8942\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1491 - acc: 0.9504 - val_loss: 0.3312 - val_acc: 0.8942\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1504 - acc: 0.9480 - val_loss: 0.3321 - val_acc: 0.8942\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1493 - acc: 0.9498 - val_loss: 0.3331 - val_acc: 0.8942\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1496 - acc: 0.9504 - val_loss: 0.3333 - val_acc: 0.8889\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1491 - acc: 0.9486 - val_loss: 0.3351 - val_acc: 0.8889\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1497 - acc: 0.9498 - val_loss: 0.3337 - val_acc: 0.8942\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1490 - acc: 0.9486 - val_loss: 0.3313 - val_acc: 0.8889\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1491 - acc: 0.9492 - val_loss: 0.3328 - val_acc: 0.8942\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1490 - acc: 0.9509 - val_loss: 0.3325 - val_acc: 0.8942\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1497 - acc: 0.9504 - val_loss: 0.3307 - val_acc: 0.8889\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1493 - acc: 0.9486 - val_loss: 0.3334 - val_acc: 0.8942\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1487 - acc: 0.9498 - val_loss: 0.3305 - val_acc: 0.8889\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1489 - acc: 0.9468 - val_loss: 0.3330 - val_acc: 0.8889\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1489 - acc: 0.9492 - val_loss: 0.3321 - val_acc: 0.8942\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1488 - acc: 0.9504 - val_loss: 0.3340 - val_acc: 0.8836\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1489 - acc: 0.9498 - val_loss: 0.3308 - val_acc: 0.8942\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1489 - acc: 0.9480 - val_loss: 0.3351 - val_acc: 0.8942\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1489 - acc: 0.9509 - val_loss: 0.3347 - val_acc: 0.8942\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 47us/step - loss: 0.1493 - acc: 0.9498 - val_loss: 0.3305 - val_acc: 0.8942\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 46us/step - loss: 0.1492 - acc: 0.9474 - val_loss: 0.3353 - val_acc: 0.8942\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1489 - acc: 0.9486 - val_loss: 0.3320 - val_acc: 0.8942\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1481 - acc: 0.9492 - val_loss: 0.3332 - val_acc: 0.8942\n",
      "Train on 1692 samples, validate on 189 samples\n",
      "Epoch 1/1000\n",
      "1692/1692 [==============================] - 1s 691us/step - loss: 0.6645 - acc: 0.5934 - val_loss: 0.6549 - val_acc: 0.6349\n",
      "Epoch 2/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.6234 - acc: 0.6714 - val_loss: 0.6321 - val_acc: 0.6561\n",
      "Epoch 3/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.5983 - acc: 0.7045 - val_loss: 0.6159 - val_acc: 0.6667\n",
      "Epoch 4/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.5797 - acc: 0.7311 - val_loss: 0.6041 - val_acc: 0.6772\n",
      "Epoch 5/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.5638 - acc: 0.7547 - val_loss: 0.5888 - val_acc: 0.7037\n",
      "Epoch 6/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.5489 - acc: 0.7618 - val_loss: 0.5775 - val_acc: 0.7143\n",
      "Epoch 7/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.5349 - acc: 0.7707 - val_loss: 0.5641 - val_acc: 0.7249\n",
      "Epoch 8/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.5205 - acc: 0.7837 - val_loss: 0.5590 - val_acc: 0.7460\n",
      "Epoch 9/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.5079 - acc: 0.7926 - val_loss: 0.5433 - val_acc: 0.7407\n",
      "Epoch 10/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.4954 - acc: 0.8050 - val_loss: 0.5334 - val_acc: 0.7460\n",
      "Epoch 11/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4832 - acc: 0.8079 - val_loss: 0.5281 - val_acc: 0.7566\n",
      "Epoch 12/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.4727 - acc: 0.8150 - val_loss: 0.5228 - val_acc: 0.7566\n",
      "Epoch 13/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.4621 - acc: 0.8227 - val_loss: 0.5119 - val_acc: 0.7566\n",
      "Epoch 14/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.4524 - acc: 0.8245 - val_loss: 0.5048 - val_acc: 0.7672\n",
      "Epoch 15/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.4443 - acc: 0.8274 - val_loss: 0.4976 - val_acc: 0.7672\n",
      "Epoch 16/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.4358 - acc: 0.8339 - val_loss: 0.4976 - val_acc: 0.7725\n",
      "Epoch 17/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.4282 - acc: 0.8333 - val_loss: 0.4852 - val_acc: 0.7725\n",
      "Epoch 18/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4215 - acc: 0.8410 - val_loss: 0.4828 - val_acc: 0.7672\n",
      "Epoch 19/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.4148 - acc: 0.8404 - val_loss: 0.4822 - val_acc: 0.7778\n",
      "Epoch 20/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.4088 - acc: 0.8422 - val_loss: 0.4762 - val_acc: 0.7831\n",
      "Epoch 21/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.4023 - acc: 0.8434 - val_loss: 0.4695 - val_acc: 0.7884\n",
      "Epoch 22/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3957 - acc: 0.8463 - val_loss: 0.4588 - val_acc: 0.7831\n",
      "Epoch 23/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.3908 - acc: 0.8481 - val_loss: 0.4577 - val_acc: 0.7831\n",
      "Epoch 24/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.3855 - acc: 0.8487 - val_loss: 0.4545 - val_acc: 0.7831\n",
      "Epoch 25/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3798 - acc: 0.8534 - val_loss: 0.4590 - val_acc: 0.7884\n",
      "Epoch 26/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3756 - acc: 0.8570 - val_loss: 0.4475 - val_acc: 0.7884\n",
      "Epoch 27/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3707 - acc: 0.8599 - val_loss: 0.4445 - val_acc: 0.7884\n",
      "Epoch 28/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3661 - acc: 0.8641 - val_loss: 0.4422 - val_acc: 0.7884\n",
      "Epoch 29/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3615 - acc: 0.8617 - val_loss: 0.4459 - val_acc: 0.8042\n",
      "Epoch 30/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.3583 - acc: 0.8652 - val_loss: 0.4352 - val_acc: 0.7884\n",
      "Epoch 31/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3543 - acc: 0.8700 - val_loss: 0.4262 - val_acc: 0.8042\n",
      "Epoch 32/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.3502 - acc: 0.8717 - val_loss: 0.4319 - val_acc: 0.7989\n",
      "Epoch 33/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.3470 - acc: 0.8712 - val_loss: 0.4273 - val_acc: 0.8042\n",
      "Epoch 34/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3439 - acc: 0.8694 - val_loss: 0.4199 - val_acc: 0.7989\n",
      "Epoch 35/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3401 - acc: 0.8735 - val_loss: 0.4150 - val_acc: 0.8042\n",
      "Epoch 36/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3367 - acc: 0.8735 - val_loss: 0.4165 - val_acc: 0.7989\n",
      "Epoch 37/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3338 - acc: 0.8788 - val_loss: 0.4143 - val_acc: 0.8042\n",
      "Epoch 38/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3299 - acc: 0.8783 - val_loss: 0.4147 - val_acc: 0.8042\n",
      "Epoch 39/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3273 - acc: 0.8788 - val_loss: 0.4147 - val_acc: 0.8042\n",
      "Epoch 40/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3241 - acc: 0.8806 - val_loss: 0.4177 - val_acc: 0.8095\n",
      "Epoch 41/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.3209 - acc: 0.8812 - val_loss: 0.4092 - val_acc: 0.8042\n",
      "Epoch 42/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.3180 - acc: 0.8842 - val_loss: 0.4102 - val_acc: 0.8095\n",
      "Epoch 43/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3154 - acc: 0.8830 - val_loss: 0.4096 - val_acc: 0.8095\n",
      "Epoch 44/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3123 - acc: 0.8871 - val_loss: 0.4164 - val_acc: 0.8148\n",
      "Epoch 45/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3108 - acc: 0.8883 - val_loss: 0.4049 - val_acc: 0.8095\n",
      "Epoch 46/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.3078 - acc: 0.8859 - val_loss: 0.4087 - val_acc: 0.8148\n",
      "Epoch 47/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.3064 - acc: 0.8859 - val_loss: 0.4041 - val_acc: 0.8095\n",
      "Epoch 48/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.3039 - acc: 0.8889 - val_loss: 0.4047 - val_acc: 0.8148\n",
      "Epoch 49/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.3019 - acc: 0.8924 - val_loss: 0.4027 - val_acc: 0.8095\n",
      "Epoch 50/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.3001 - acc: 0.8918 - val_loss: 0.3975 - val_acc: 0.8042\n",
      "Epoch 51/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2968 - acc: 0.8942 - val_loss: 0.3916 - val_acc: 0.8095\n",
      "Epoch 52/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2955 - acc: 0.8942 - val_loss: 0.4020 - val_acc: 0.8095\n",
      "Epoch 53/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2934 - acc: 0.8960 - val_loss: 0.3962 - val_acc: 0.8095\n",
      "Epoch 54/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2917 - acc: 0.8960 - val_loss: 0.3951 - val_acc: 0.8095\n",
      "Epoch 55/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2896 - acc: 0.8978 - val_loss: 0.3983 - val_acc: 0.8095\n",
      "Epoch 56/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2879 - acc: 0.8960 - val_loss: 0.3986 - val_acc: 0.8095\n",
      "Epoch 57/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2863 - acc: 0.8983 - val_loss: 0.3942 - val_acc: 0.8042\n",
      "Epoch 58/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2843 - acc: 0.8966 - val_loss: 0.3954 - val_acc: 0.8042\n",
      "Epoch 59/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2834 - acc: 0.9001 - val_loss: 0.3985 - val_acc: 0.8095\n",
      "Epoch 60/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2817 - acc: 0.9007 - val_loss: 0.3924 - val_acc: 0.8042\n",
      "Epoch 61/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2800 - acc: 0.9019 - val_loss: 0.4061 - val_acc: 0.8095\n",
      "Epoch 62/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2796 - acc: 0.8989 - val_loss: 0.3999 - val_acc: 0.8042\n",
      "Epoch 63/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2781 - acc: 0.8995 - val_loss: 0.3933 - val_acc: 0.8042\n",
      "Epoch 64/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2762 - acc: 0.9013 - val_loss: 0.3959 - val_acc: 0.8095\n",
      "Epoch 65/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2759 - acc: 0.9001 - val_loss: 0.3951 - val_acc: 0.8095\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2741 - acc: 0.9007 - val_loss: 0.3953 - val_acc: 0.8042\n",
      "Epoch 67/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2729 - acc: 0.9007 - val_loss: 0.3932 - val_acc: 0.8042\n",
      "Epoch 68/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2713 - acc: 0.9019 - val_loss: 0.3917 - val_acc: 0.8095\n",
      "Epoch 69/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2701 - acc: 0.9013 - val_loss: 0.3955 - val_acc: 0.8095\n",
      "Epoch 70/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2691 - acc: 0.9043 - val_loss: 0.3884 - val_acc: 0.8148\n",
      "Epoch 71/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2680 - acc: 0.9037 - val_loss: 0.3998 - val_acc: 0.8148\n",
      "Epoch 72/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2666 - acc: 0.9019 - val_loss: 0.3850 - val_acc: 0.8201\n",
      "Epoch 73/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2660 - acc: 0.9013 - val_loss: 0.3921 - val_acc: 0.8095\n",
      "Epoch 74/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2652 - acc: 0.9025 - val_loss: 0.3873 - val_acc: 0.8254\n",
      "Epoch 75/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2643 - acc: 0.9037 - val_loss: 0.3889 - val_acc: 0.8201\n",
      "Epoch 76/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2635 - acc: 0.9025 - val_loss: 0.3877 - val_acc: 0.8201\n",
      "Epoch 77/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2626 - acc: 0.9037 - val_loss: 0.3841 - val_acc: 0.8201\n",
      "Epoch 78/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2610 - acc: 0.9031 - val_loss: 0.3806 - val_acc: 0.8254\n",
      "Epoch 79/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2604 - acc: 0.9037 - val_loss: 0.3891 - val_acc: 0.8201\n",
      "Epoch 80/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2592 - acc: 0.9037 - val_loss: 0.3897 - val_acc: 0.8254\n",
      "Epoch 81/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2578 - acc: 0.9060 - val_loss: 0.3984 - val_acc: 0.8254\n",
      "Epoch 82/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2570 - acc: 0.9054 - val_loss: 0.3845 - val_acc: 0.8201\n",
      "Epoch 83/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2568 - acc: 0.9066 - val_loss: 0.3845 - val_acc: 0.8201\n",
      "Epoch 84/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2556 - acc: 0.9084 - val_loss: 0.3873 - val_acc: 0.8307\n",
      "Epoch 85/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2544 - acc: 0.9078 - val_loss: 0.3888 - val_acc: 0.8254\n",
      "Epoch 86/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2534 - acc: 0.9084 - val_loss: 0.3874 - val_acc: 0.8254\n",
      "Epoch 87/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2520 - acc: 0.9096 - val_loss: 0.3968 - val_acc: 0.8307\n",
      "Epoch 88/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2519 - acc: 0.9090 - val_loss: 0.3977 - val_acc: 0.8201\n",
      "Epoch 89/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2509 - acc: 0.9084 - val_loss: 0.3844 - val_acc: 0.8307\n",
      "Epoch 90/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2500 - acc: 0.9119 - val_loss: 0.3826 - val_acc: 0.8307\n",
      "Epoch 91/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2488 - acc: 0.9125 - val_loss: 0.3798 - val_acc: 0.8307\n",
      "Epoch 92/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2485 - acc: 0.9108 - val_loss: 0.3885 - val_acc: 0.8307\n",
      "Epoch 93/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2477 - acc: 0.9119 - val_loss: 0.3805 - val_acc: 0.8307\n",
      "Epoch 94/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2474 - acc: 0.9090 - val_loss: 0.3861 - val_acc: 0.8307\n",
      "Epoch 95/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2465 - acc: 0.9119 - val_loss: 0.3873 - val_acc: 0.8307\n",
      "Epoch 96/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2450 - acc: 0.9119 - val_loss: 0.3935 - val_acc: 0.8254\n",
      "Epoch 97/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2435 - acc: 0.9167 - val_loss: 0.3746 - val_acc: 0.8307\n",
      "Epoch 98/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2444 - acc: 0.9131 - val_loss: 0.3820 - val_acc: 0.8307\n",
      "Epoch 99/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2436 - acc: 0.9143 - val_loss: 0.3820 - val_acc: 0.8307\n",
      "Epoch 100/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2425 - acc: 0.9113 - val_loss: 0.3805 - val_acc: 0.8307\n",
      "Epoch 101/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2419 - acc: 0.9137 - val_loss: 0.3761 - val_acc: 0.8307\n",
      "Epoch 102/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2411 - acc: 0.9149 - val_loss: 0.3775 - val_acc: 0.8307\n",
      "Epoch 103/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.2409 - acc: 0.9155 - val_loss: 0.3811 - val_acc: 0.8307\n",
      "Epoch 104/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2394 - acc: 0.9155 - val_loss: 0.3847 - val_acc: 0.8360\n",
      "Epoch 105/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2395 - acc: 0.9131 - val_loss: 0.3795 - val_acc: 0.8360\n",
      "Epoch 106/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2387 - acc: 0.9137 - val_loss: 0.3886 - val_acc: 0.8360\n",
      "Epoch 107/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.2379 - acc: 0.9161 - val_loss: 0.3784 - val_acc: 0.8254\n",
      "Epoch 108/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2379 - acc: 0.9167 - val_loss: 0.3763 - val_acc: 0.8254\n",
      "Epoch 109/1000\n",
      "1692/1692 [==============================] - 0s 81us/step - loss: 0.2364 - acc: 0.9143 - val_loss: 0.3867 - val_acc: 0.8360\n",
      "Epoch 110/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.2364 - acc: 0.9161 - val_loss: 0.3826 - val_acc: 0.8360\n",
      "Epoch 111/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2361 - acc: 0.9161 - val_loss: 0.3786 - val_acc: 0.8360\n",
      "Epoch 112/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.2363 - acc: 0.9161 - val_loss: 0.3820 - val_acc: 0.8360\n",
      "Epoch 113/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2351 - acc: 0.9178 - val_loss: 0.3786 - val_acc: 0.8360\n",
      "Epoch 114/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2340 - acc: 0.9178 - val_loss: 0.3828 - val_acc: 0.8360\n",
      "Epoch 115/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2334 - acc: 0.9178 - val_loss: 0.3842 - val_acc: 0.8360\n",
      "Epoch 116/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2331 - acc: 0.9184 - val_loss: 0.3804 - val_acc: 0.8360\n",
      "Epoch 117/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2324 - acc: 0.9202 - val_loss: 0.3791 - val_acc: 0.8307\n",
      "Epoch 118/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2323 - acc: 0.9196 - val_loss: 0.3841 - val_acc: 0.8360\n",
      "Epoch 119/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2320 - acc: 0.9190 - val_loss: 0.3840 - val_acc: 0.8360\n",
      "Epoch 120/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2311 - acc: 0.9190 - val_loss: 0.3740 - val_acc: 0.8307\n",
      "Epoch 121/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2308 - acc: 0.9178 - val_loss: 0.3818 - val_acc: 0.8360\n",
      "Epoch 122/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2308 - acc: 0.9184 - val_loss: 0.3788 - val_acc: 0.8360\n",
      "Epoch 123/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2300 - acc: 0.9196 - val_loss: 0.3767 - val_acc: 0.8360\n",
      "Epoch 124/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2291 - acc: 0.9184 - val_loss: 0.3853 - val_acc: 0.8360\n",
      "Epoch 125/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2290 - acc: 0.9208 - val_loss: 0.3811 - val_acc: 0.8360\n",
      "Epoch 126/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2282 - acc: 0.9196 - val_loss: 0.3772 - val_acc: 0.8360\n",
      "Epoch 127/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2278 - acc: 0.9208 - val_loss: 0.3907 - val_acc: 0.8360\n",
      "Epoch 128/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2285 - acc: 0.9190 - val_loss: 0.3722 - val_acc: 0.8413\n",
      "Epoch 129/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2281 - acc: 0.9208 - val_loss: 0.3788 - val_acc: 0.8413\n",
      "Epoch 130/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2272 - acc: 0.9214 - val_loss: 0.3835 - val_acc: 0.8413\n",
      "Epoch 131/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2265 - acc: 0.9226 - val_loss: 0.3854 - val_acc: 0.8413\n",
      "Epoch 132/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2268 - acc: 0.9178 - val_loss: 0.3785 - val_acc: 0.8360\n",
      "Epoch 133/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2263 - acc: 0.9208 - val_loss: 0.3799 - val_acc: 0.8413\n",
      "Epoch 134/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2253 - acc: 0.9232 - val_loss: 0.3755 - val_acc: 0.8360\n",
      "Epoch 135/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2254 - acc: 0.9202 - val_loss: 0.3813 - val_acc: 0.8413\n",
      "Epoch 136/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2248 - acc: 0.9214 - val_loss: 0.3749 - val_acc: 0.8413\n",
      "Epoch 137/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2244 - acc: 0.9249 - val_loss: 0.3874 - val_acc: 0.8413\n",
      "Epoch 138/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2245 - acc: 0.9226 - val_loss: 0.3835 - val_acc: 0.8413\n",
      "Epoch 139/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2244 - acc: 0.9238 - val_loss: 0.3786 - val_acc: 0.8360\n",
      "Epoch 140/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2230 - acc: 0.9220 - val_loss: 0.3778 - val_acc: 0.8466\n",
      "Epoch 141/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2238 - acc: 0.9243 - val_loss: 0.3778 - val_acc: 0.8360\n",
      "Epoch 142/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2230 - acc: 0.9226 - val_loss: 0.3792 - val_acc: 0.8360\n",
      "Epoch 143/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.2224 - acc: 0.9220 - val_loss: 0.3802 - val_acc: 0.8360\n",
      "Epoch 144/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2231 - acc: 0.9226 - val_loss: 0.3781 - val_acc: 0.8360\n",
      "Epoch 145/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2209 - acc: 0.9279 - val_loss: 0.3735 - val_acc: 0.8360\n",
      "Epoch 146/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2226 - acc: 0.9226 - val_loss: 0.3772 - val_acc: 0.8413\n",
      "Epoch 147/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2209 - acc: 0.9255 - val_loss: 0.3842 - val_acc: 0.8519\n",
      "Epoch 148/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2210 - acc: 0.9249 - val_loss: 0.3741 - val_acc: 0.8360\n",
      "Epoch 149/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2205 - acc: 0.9232 - val_loss: 0.3876 - val_acc: 0.8413\n",
      "Epoch 150/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2214 - acc: 0.9255 - val_loss: 0.3828 - val_acc: 0.8413\n",
      "Epoch 151/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2205 - acc: 0.9255 - val_loss: 0.3753 - val_acc: 0.8413\n",
      "Epoch 152/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2200 - acc: 0.9273 - val_loss: 0.3754 - val_acc: 0.8413\n",
      "Epoch 153/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2202 - acc: 0.9243 - val_loss: 0.3897 - val_acc: 0.8466\n",
      "Epoch 154/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2199 - acc: 0.9226 - val_loss: 0.3844 - val_acc: 0.8519\n",
      "Epoch 155/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2195 - acc: 0.9267 - val_loss: 0.3795 - val_acc: 0.8360\n",
      "Epoch 156/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2191 - acc: 0.9255 - val_loss: 0.3836 - val_acc: 0.8466\n",
      "Epoch 157/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2194 - acc: 0.9255 - val_loss: 0.3735 - val_acc: 0.8413\n",
      "Epoch 158/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2183 - acc: 0.9249 - val_loss: 0.3881 - val_acc: 0.8466\n",
      "Epoch 159/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2192 - acc: 0.9255 - val_loss: 0.3823 - val_acc: 0.8466\n",
      "Epoch 160/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2183 - acc: 0.9249 - val_loss: 0.3798 - val_acc: 0.8413\n",
      "Epoch 161/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2179 - acc: 0.9255 - val_loss: 0.3859 - val_acc: 0.8466\n",
      "Epoch 162/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2167 - acc: 0.9279 - val_loss: 0.3730 - val_acc: 0.8413\n",
      "Epoch 163/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2182 - acc: 0.9273 - val_loss: 0.3762 - val_acc: 0.8413\n",
      "Epoch 164/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2171 - acc: 0.9279 - val_loss: 0.3853 - val_acc: 0.8413\n",
      "Epoch 165/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2168 - acc: 0.9255 - val_loss: 0.3914 - val_acc: 0.8413\n",
      "Epoch 166/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2176 - acc: 0.9273 - val_loss: 0.3798 - val_acc: 0.8413\n",
      "Epoch 167/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2169 - acc: 0.9261 - val_loss: 0.3852 - val_acc: 0.8413\n",
      "Epoch 168/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2166 - acc: 0.9267 - val_loss: 0.3786 - val_acc: 0.8413\n",
      "Epoch 169/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2161 - acc: 0.9273 - val_loss: 0.3771 - val_acc: 0.8360\n",
      "Epoch 170/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2155 - acc: 0.9243 - val_loss: 0.3834 - val_acc: 0.8413\n",
      "Epoch 171/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2157 - acc: 0.9267 - val_loss: 0.3913 - val_acc: 0.8466\n",
      "Epoch 172/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2158 - acc: 0.9267 - val_loss: 0.3785 - val_acc: 0.8466\n",
      "Epoch 173/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2154 - acc: 0.9273 - val_loss: 0.3915 - val_acc: 0.8466\n",
      "Epoch 174/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2153 - acc: 0.9273 - val_loss: 0.3758 - val_acc: 0.8466\n",
      "Epoch 175/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2151 - acc: 0.9273 - val_loss: 0.3827 - val_acc: 0.8466\n",
      "Epoch 176/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2144 - acc: 0.9261 - val_loss: 0.3814 - val_acc: 0.8519\n",
      "Epoch 177/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2143 - acc: 0.9273 - val_loss: 0.3801 - val_acc: 0.8466\n",
      "Epoch 178/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2141 - acc: 0.9285 - val_loss: 0.3823 - val_acc: 0.8519\n",
      "Epoch 179/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2129 - acc: 0.9267 - val_loss: 0.3922 - val_acc: 0.8466\n",
      "Epoch 180/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2145 - acc: 0.9279 - val_loss: 0.3894 - val_acc: 0.8519\n",
      "Epoch 181/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2138 - acc: 0.9267 - val_loss: 0.3819 - val_acc: 0.8519\n",
      "Epoch 182/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2141 - acc: 0.9267 - val_loss: 0.3810 - val_acc: 0.8519\n",
      "Epoch 183/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2136 - acc: 0.9267 - val_loss: 0.3851 - val_acc: 0.8413\n",
      "Epoch 184/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2134 - acc: 0.9291 - val_loss: 0.3819 - val_acc: 0.8519\n",
      "Epoch 185/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2132 - acc: 0.9309 - val_loss: 0.3830 - val_acc: 0.8466\n",
      "Epoch 186/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2133 - acc: 0.9297 - val_loss: 0.3823 - val_acc: 0.8466\n",
      "Epoch 187/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2117 - acc: 0.9309 - val_loss: 0.3978 - val_acc: 0.8413\n",
      "Epoch 188/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2127 - acc: 0.9303 - val_loss: 0.3893 - val_acc: 0.8466\n",
      "Epoch 189/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2129 - acc: 0.9285 - val_loss: 0.3896 - val_acc: 0.8466\n",
      "Epoch 190/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2118 - acc: 0.9273 - val_loss: 0.3897 - val_acc: 0.8413\n",
      "Epoch 191/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2123 - acc: 0.9285 - val_loss: 0.3870 - val_acc: 0.8519\n",
      "Epoch 192/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2116 - acc: 0.9291 - val_loss: 0.3896 - val_acc: 0.8466\n",
      "Epoch 193/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2116 - acc: 0.9291 - val_loss: 0.3919 - val_acc: 0.8466\n",
      "Epoch 194/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2116 - acc: 0.9320 - val_loss: 0.3862 - val_acc: 0.8466\n",
      "Epoch 195/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2116 - acc: 0.9285 - val_loss: 0.3847 - val_acc: 0.8466\n",
      "Epoch 196/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2115 - acc: 0.9297 - val_loss: 0.3921 - val_acc: 0.8519\n",
      "Epoch 197/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2112 - acc: 0.9297 - val_loss: 0.3827 - val_acc: 0.8571\n",
      "Epoch 198/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2112 - acc: 0.9303 - val_loss: 0.3808 - val_acc: 0.8466\n",
      "Epoch 199/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2108 - acc: 0.9303 - val_loss: 0.3803 - val_acc: 0.8466\n",
      "Epoch 200/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2103 - acc: 0.9309 - val_loss: 0.3915 - val_acc: 0.8413\n",
      "Epoch 201/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2102 - acc: 0.9297 - val_loss: 0.3859 - val_acc: 0.8466\n",
      "Epoch 202/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2099 - acc: 0.9303 - val_loss: 0.3835 - val_acc: 0.8466\n",
      "Epoch 203/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2103 - acc: 0.9273 - val_loss: 0.3894 - val_acc: 0.8466\n",
      "Epoch 204/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2099 - acc: 0.9303 - val_loss: 0.3791 - val_acc: 0.8466\n",
      "Epoch 205/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.2094 - acc: 0.9291 - val_loss: 0.3798 - val_acc: 0.8413\n",
      "Epoch 206/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2090 - acc: 0.9291 - val_loss: 0.3769 - val_acc: 0.8413\n",
      "Epoch 207/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2093 - acc: 0.9279 - val_loss: 0.3844 - val_acc: 0.8466\n",
      "Epoch 208/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2090 - acc: 0.9314 - val_loss: 0.3914 - val_acc: 0.8413\n",
      "Epoch 209/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2095 - acc: 0.9291 - val_loss: 0.3864 - val_acc: 0.8466\n",
      "Epoch 210/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2093 - acc: 0.9303 - val_loss: 0.3858 - val_acc: 0.8571\n",
      "Epoch 211/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2082 - acc: 0.9303 - val_loss: 0.3963 - val_acc: 0.8413\n",
      "Epoch 212/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2084 - acc: 0.9332 - val_loss: 0.3883 - val_acc: 0.8466\n",
      "Epoch 213/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2082 - acc: 0.9320 - val_loss: 0.3881 - val_acc: 0.8571\n",
      "Epoch 214/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2084 - acc: 0.9297 - val_loss: 0.3841 - val_acc: 0.8519\n",
      "Epoch 215/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2080 - acc: 0.9314 - val_loss: 0.3916 - val_acc: 0.8519\n",
      "Epoch 216/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2080 - acc: 0.9314 - val_loss: 0.3883 - val_acc: 0.8466\n",
      "Epoch 217/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2070 - acc: 0.9297 - val_loss: 0.3828 - val_acc: 0.8571\n",
      "Epoch 218/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2079 - acc: 0.9303 - val_loss: 0.3838 - val_acc: 0.8519\n",
      "Epoch 219/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2076 - acc: 0.9309 - val_loss: 0.3840 - val_acc: 0.8519\n",
      "Epoch 220/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.2069 - acc: 0.9297 - val_loss: 0.3778 - val_acc: 0.8466\n",
      "Epoch 221/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2077 - acc: 0.9314 - val_loss: 0.3871 - val_acc: 0.8519\n",
      "Epoch 222/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2064 - acc: 0.9303 - val_loss: 0.3848 - val_acc: 0.8624\n",
      "Epoch 223/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2068 - acc: 0.9279 - val_loss: 0.3855 - val_acc: 0.8571\n",
      "Epoch 224/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2060 - acc: 0.9338 - val_loss: 0.3851 - val_acc: 0.8571\n",
      "Epoch 225/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2068 - acc: 0.9291 - val_loss: 0.3885 - val_acc: 0.8519\n",
      "Epoch 226/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.2061 - acc: 0.9314 - val_loss: 0.3801 - val_acc: 0.8466\n",
      "Epoch 227/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2062 - acc: 0.9303 - val_loss: 0.3856 - val_acc: 0.8571\n",
      "Epoch 228/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2051 - acc: 0.9303 - val_loss: 0.3996 - val_acc: 0.8413\n",
      "Epoch 229/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2057 - acc: 0.9309 - val_loss: 0.3889 - val_acc: 0.8413\n",
      "Epoch 230/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2059 - acc: 0.9314 - val_loss: 0.3856 - val_acc: 0.8519\n",
      "Epoch 231/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2051 - acc: 0.9314 - val_loss: 0.3907 - val_acc: 0.8466\n",
      "Epoch 232/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.2055 - acc: 0.9326 - val_loss: 0.3850 - val_acc: 0.8519\n",
      "Epoch 233/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2051 - acc: 0.9314 - val_loss: 0.3893 - val_acc: 0.8413\n",
      "Epoch 234/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2047 - acc: 0.9309 - val_loss: 0.3960 - val_acc: 0.8413\n",
      "Epoch 235/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.2054 - acc: 0.9314 - val_loss: 0.3904 - val_acc: 0.8519\n",
      "Epoch 236/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2048 - acc: 0.9332 - val_loss: 0.3807 - val_acc: 0.8519\n",
      "Epoch 237/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.2050 - acc: 0.9326 - val_loss: 0.3931 - val_acc: 0.8466\n",
      "Epoch 238/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2044 - acc: 0.9326 - val_loss: 0.3850 - val_acc: 0.8677\n",
      "Epoch 239/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2049 - acc: 0.9320 - val_loss: 0.3833 - val_acc: 0.8571\n",
      "Epoch 240/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.2036 - acc: 0.9309 - val_loss: 0.3891 - val_acc: 0.8466\n",
      "Epoch 241/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2044 - acc: 0.9338 - val_loss: 0.3901 - val_acc: 0.8413\n",
      "Epoch 242/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2032 - acc: 0.9326 - val_loss: 0.3879 - val_acc: 0.8519\n",
      "Epoch 243/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2034 - acc: 0.9303 - val_loss: 0.3932 - val_acc: 0.8413\n",
      "Epoch 244/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.2029 - acc: 0.9309 - val_loss: 0.3842 - val_acc: 0.8571\n",
      "Epoch 245/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.2026 - acc: 0.9314 - val_loss: 0.3991 - val_acc: 0.8466\n",
      "Epoch 246/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2017 - acc: 0.9338 - val_loss: 0.3771 - val_acc: 0.8466\n",
      "Epoch 247/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.2032 - acc: 0.9332 - val_loss: 0.3782 - val_acc: 0.8466\n",
      "Epoch 248/1000\n",
      "1692/1692 [==============================] - 0s 80us/step - loss: 0.2032 - acc: 0.9297 - val_loss: 0.3866 - val_acc: 0.8624\n",
      "Epoch 249/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2031 - acc: 0.9309 - val_loss: 0.3915 - val_acc: 0.8519\n",
      "Epoch 250/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.2024 - acc: 0.9309 - val_loss: 0.3931 - val_acc: 0.8519\n",
      "Epoch 251/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.2025 - acc: 0.9320 - val_loss: 0.3845 - val_acc: 0.8571\n",
      "Epoch 252/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.2016 - acc: 0.9362 - val_loss: 0.3938 - val_acc: 0.8466\n",
      "Epoch 253/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.2098 - acc: 0.932 - 0s 56us/step - loss: 0.2024 - acc: 0.9338 - val_loss: 0.3967 - val_acc: 0.8519\n",
      "Epoch 254/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.2017 - acc: 0.9309 - val_loss: 0.3991 - val_acc: 0.8519\n",
      "Epoch 255/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2028 - acc: 0.9338 - val_loss: 0.3794 - val_acc: 0.8571\n",
      "Epoch 256/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2012 - acc: 0.9344 - val_loss: 0.3810 - val_acc: 0.8466\n",
      "Epoch 257/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.2026 - acc: 0.9332 - val_loss: 0.3877 - val_acc: 0.8519\n",
      "Epoch 258/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.2017 - acc: 0.9303 - val_loss: 0.3837 - val_acc: 0.8677\n",
      "Epoch 259/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.2014 - acc: 0.9314 - val_loss: 0.3854 - val_acc: 0.8519\n",
      "Epoch 260/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.2010 - acc: 0.9344 - val_loss: 0.3883 - val_acc: 0.8519\n",
      "Epoch 261/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.2016 - acc: 0.9320 - val_loss: 0.3874 - val_acc: 0.8519\n",
      "Epoch 262/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2000 - acc: 0.9338 - val_loss: 0.3940 - val_acc: 0.8571\n",
      "Epoch 263/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2006 - acc: 0.9297 - val_loss: 0.3786 - val_acc: 0.8624\n",
      "Epoch 264/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.2009 - acc: 0.9344 - val_loss: 0.3783 - val_acc: 0.8519\n",
      "Epoch 265/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.2005 - acc: 0.9362 - val_loss: 0.3924 - val_acc: 0.8519\n",
      "Epoch 266/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.2002 - acc: 0.9338 - val_loss: 0.3918 - val_acc: 0.8571\n",
      "Epoch 267/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.2009 - acc: 0.9309 - val_loss: 0.3956 - val_acc: 0.8571\n",
      "Epoch 268/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1999 - acc: 0.9344 - val_loss: 0.3857 - val_acc: 0.8571\n",
      "Epoch 269/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1998 - acc: 0.9320 - val_loss: 0.3938 - val_acc: 0.8624\n",
      "Epoch 270/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1988 - acc: 0.9350 - val_loss: 0.3788 - val_acc: 0.8571\n",
      "Epoch 271/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1989 - acc: 0.9344 - val_loss: 0.3829 - val_acc: 0.8571\n",
      "Epoch 272/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1988 - acc: 0.9338 - val_loss: 0.3858 - val_acc: 0.8571\n",
      "Epoch 273/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1993 - acc: 0.9332 - val_loss: 0.3808 - val_acc: 0.8571\n",
      "Epoch 274/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.2001 - acc: 0.9309 - val_loss: 0.3860 - val_acc: 0.8624\n",
      "Epoch 275/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1984 - acc: 0.9326 - val_loss: 0.3866 - val_acc: 0.8571\n",
      "Epoch 276/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1988 - acc: 0.9344 - val_loss: 0.3841 - val_acc: 0.8624\n",
      "Epoch 277/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1983 - acc: 0.9362 - val_loss: 0.3777 - val_acc: 0.8571\n",
      "Epoch 278/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1979 - acc: 0.9338 - val_loss: 0.3858 - val_acc: 0.8571\n",
      "Epoch 279/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1974 - acc: 0.9320 - val_loss: 0.3817 - val_acc: 0.8624\n",
      "Epoch 280/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1974 - acc: 0.9314 - val_loss: 0.3891 - val_acc: 0.8571\n",
      "Epoch 281/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1989 - acc: 0.9326 - val_loss: 0.3814 - val_acc: 0.8571\n",
      "Epoch 282/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1984 - acc: 0.9362 - val_loss: 0.3854 - val_acc: 0.8677\n",
      "Epoch 283/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1980 - acc: 0.9326 - val_loss: 0.3854 - val_acc: 0.8677\n",
      "Epoch 284/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1972 - acc: 0.9374 - val_loss: 0.3857 - val_acc: 0.8677\n",
      "Epoch 285/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1975 - acc: 0.9338 - val_loss: 0.3786 - val_acc: 0.8571\n",
      "Epoch 286/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1977 - acc: 0.9362 - val_loss: 0.3823 - val_acc: 0.8677\n",
      "Epoch 287/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1969 - acc: 0.9356 - val_loss: 0.3845 - val_acc: 0.8677\n",
      "Epoch 288/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1968 - acc: 0.9356 - val_loss: 0.3877 - val_acc: 0.8624\n",
      "Epoch 289/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1940 - acc: 0.9344 - val_loss: 0.4027 - val_acc: 0.8677\n",
      "Epoch 290/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1959 - acc: 0.9362 - val_loss: 0.3853 - val_acc: 0.8677\n",
      "Epoch 291/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1957 - acc: 0.9356 - val_loss: 0.3917 - val_acc: 0.8677\n",
      "Epoch 292/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1964 - acc: 0.9356 - val_loss: 0.3858 - val_acc: 0.8624\n",
      "Epoch 293/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1962 - acc: 0.9344 - val_loss: 0.3775 - val_acc: 0.8519\n",
      "Epoch 294/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1960 - acc: 0.9374 - val_loss: 0.3877 - val_acc: 0.8624\n",
      "Epoch 295/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1953 - acc: 0.9362 - val_loss: 0.3694 - val_acc: 0.8571\n",
      "Epoch 296/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1956 - acc: 0.9350 - val_loss: 0.3829 - val_acc: 0.8677\n",
      "Epoch 297/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1951 - acc: 0.9374 - val_loss: 0.3910 - val_acc: 0.8677\n",
      "Epoch 298/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1951 - acc: 0.9368 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 299/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1954 - acc: 0.9356 - val_loss: 0.3888 - val_acc: 0.8624\n",
      "Epoch 300/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1954 - acc: 0.9356 - val_loss: 0.3879 - val_acc: 0.8730\n",
      "Epoch 301/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1946 - acc: 0.9350 - val_loss: 0.3844 - val_acc: 0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1947 - acc: 0.9374 - val_loss: 0.3819 - val_acc: 0.8677\n",
      "Epoch 303/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1947 - acc: 0.9374 - val_loss: 0.3873 - val_acc: 0.8677\n",
      "Epoch 304/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1935 - acc: 0.9362 - val_loss: 0.3735 - val_acc: 0.8519\n",
      "Epoch 305/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1943 - acc: 0.9397 - val_loss: 0.3895 - val_acc: 0.8624\n",
      "Epoch 306/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1926 - acc: 0.9362 - val_loss: 0.3798 - val_acc: 0.8624\n",
      "Epoch 307/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1935 - acc: 0.9368 - val_loss: 0.3827 - val_acc: 0.8677\n",
      "Epoch 308/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1935 - acc: 0.9379 - val_loss: 0.3895 - val_acc: 0.8571\n",
      "Epoch 309/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1924 - acc: 0.9368 - val_loss: 0.3723 - val_acc: 0.8519\n",
      "Epoch 310/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1942 - acc: 0.9362 - val_loss: 0.3800 - val_acc: 0.8624\n",
      "Epoch 311/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1936 - acc: 0.9356 - val_loss: 0.3892 - val_acc: 0.8677\n",
      "Epoch 312/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1934 - acc: 0.9368 - val_loss: 0.3817 - val_acc: 0.8624\n",
      "Epoch 313/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1928 - acc: 0.9356 - val_loss: 0.3926 - val_acc: 0.8571\n",
      "Epoch 314/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1927 - acc: 0.9326 - val_loss: 0.3851 - val_acc: 0.8624\n",
      "Epoch 315/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1929 - acc: 0.9326 - val_loss: 0.3822 - val_acc: 0.8677\n",
      "Epoch 316/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1923 - acc: 0.9362 - val_loss: 0.3784 - val_acc: 0.8624\n",
      "Epoch 317/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1920 - acc: 0.9344 - val_loss: 0.3846 - val_acc: 0.8624\n",
      "Epoch 318/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1925 - acc: 0.9368 - val_loss: 0.3843 - val_acc: 0.8624\n",
      "Epoch 319/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1914 - acc: 0.9385 - val_loss: 0.3919 - val_acc: 0.8624\n",
      "Epoch 320/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1922 - acc: 0.9368 - val_loss: 0.3857 - val_acc: 0.8677\n",
      "Epoch 321/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1915 - acc: 0.9391 - val_loss: 0.3801 - val_acc: 0.8624\n",
      "Epoch 322/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1910 - acc: 0.9379 - val_loss: 0.3741 - val_acc: 0.8571\n",
      "Epoch 323/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1911 - acc: 0.9385 - val_loss: 0.3914 - val_acc: 0.8624\n",
      "Epoch 324/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1908 - acc: 0.9391 - val_loss: 0.3825 - val_acc: 0.8677\n",
      "Epoch 325/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1919 - acc: 0.9368 - val_loss: 0.3868 - val_acc: 0.8677\n",
      "Epoch 326/1000\n",
      "1692/1692 [==============================] - 0s 126us/step - loss: 0.1905 - acc: 0.9379 - val_loss: 0.3883 - val_acc: 0.8624\n",
      "Epoch 327/1000\n",
      "1692/1692 [==============================] - 0s 119us/step - loss: 0.1907 - acc: 0.9391 - val_loss: 0.3781 - val_acc: 0.8624\n",
      "Epoch 328/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1906 - acc: 0.9350 - val_loss: 0.3883 - val_acc: 0.8624\n",
      "Epoch 329/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1901 - acc: 0.9397 - val_loss: 0.3933 - val_acc: 0.8571\n",
      "Epoch 330/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1898 - acc: 0.9403 - val_loss: 0.3991 - val_acc: 0.8624\n",
      "Epoch 331/1000\n",
      "1692/1692 [==============================] - 0s 114us/step - loss: 0.1909 - acc: 0.9391 - val_loss: 0.3952 - val_acc: 0.8624\n",
      "Epoch 332/1000\n",
      "1692/1692 [==============================] - 0s 132us/step - loss: 0.1895 - acc: 0.9385 - val_loss: 0.3885 - val_acc: 0.8624\n",
      "Epoch 333/1000\n",
      "1692/1692 [==============================] - 0s 136us/step - loss: 0.1894 - acc: 0.9374 - val_loss: 0.3993 - val_acc: 0.8624\n",
      "Epoch 334/1000\n",
      "1692/1692 [==============================] - 0s 144us/step - loss: 0.1897 - acc: 0.9427 - val_loss: 0.3860 - val_acc: 0.8624\n",
      "Epoch 335/1000\n",
      "1692/1692 [==============================] - 0s 131us/step - loss: 0.1895 - acc: 0.9356 - val_loss: 0.3949 - val_acc: 0.8677\n",
      "Epoch 336/1000\n",
      "1692/1692 [==============================] - 0s 132us/step - loss: 0.1893 - acc: 0.9397 - val_loss: 0.3828 - val_acc: 0.8519\n",
      "Epoch 337/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1891 - acc: 0.9385 - val_loss: 0.3831 - val_acc: 0.8677\n",
      "Epoch 338/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1893 - acc: 0.9362 - val_loss: 0.3907 - val_acc: 0.8677\n",
      "Epoch 339/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1890 - acc: 0.9397 - val_loss: 0.3782 - val_acc: 0.8624\n",
      "Epoch 340/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1881 - acc: 0.9391 - val_loss: 0.3914 - val_acc: 0.8677\n",
      "Epoch 341/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1883 - acc: 0.9385 - val_loss: 0.3780 - val_acc: 0.8571\n",
      "Epoch 342/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1884 - acc: 0.9374 - val_loss: 0.3937 - val_acc: 0.8624\n",
      "Epoch 343/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1881 - acc: 0.9403 - val_loss: 0.4031 - val_acc: 0.8624\n",
      "Epoch 344/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1886 - acc: 0.9356 - val_loss: 0.3907 - val_acc: 0.8677\n",
      "Epoch 345/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1885 - acc: 0.9379 - val_loss: 0.3945 - val_acc: 0.8624\n",
      "Epoch 346/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1882 - acc: 0.9362 - val_loss: 0.3886 - val_acc: 0.8624\n",
      "Epoch 347/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1882 - acc: 0.9374 - val_loss: 0.3827 - val_acc: 0.8571\n",
      "Epoch 348/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1871 - acc: 0.9391 - val_loss: 0.3965 - val_acc: 0.8624\n",
      "Epoch 349/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1866 - acc: 0.9374 - val_loss: 0.3900 - val_acc: 0.8571\n",
      "Epoch 350/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1872 - acc: 0.9391 - val_loss: 0.3864 - val_acc: 0.8677\n",
      "Epoch 351/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1872 - acc: 0.9397 - val_loss: 0.3884 - val_acc: 0.8624\n",
      "Epoch 352/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1868 - acc: 0.9397 - val_loss: 0.3988 - val_acc: 0.8677\n",
      "Epoch 353/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1871 - acc: 0.9391 - val_loss: 0.3928 - val_acc: 0.8571\n",
      "Epoch 354/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1866 - acc: 0.9397 - val_loss: 0.3813 - val_acc: 0.8571\n",
      "Epoch 355/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1857 - acc: 0.9409 - val_loss: 0.3768 - val_acc: 0.8519\n",
      "Epoch 356/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1865 - acc: 0.9421 - val_loss: 0.3853 - val_acc: 0.8624\n",
      "Epoch 357/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1865 - acc: 0.9421 - val_loss: 0.3978 - val_acc: 0.8571\n",
      "Epoch 358/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1862 - acc: 0.9421 - val_loss: 0.3899 - val_acc: 0.8624\n",
      "Epoch 359/1000\n",
      "1692/1692 [==============================] - 0s 88us/step - loss: 0.1860 - acc: 0.9427 - val_loss: 0.3889 - val_acc: 0.8571\n",
      "Epoch 360/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1864 - acc: 0.9409 - val_loss: 0.3805 - val_acc: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1860 - acc: 0.9379 - val_loss: 0.4000 - val_acc: 0.8624\n",
      "Epoch 362/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1852 - acc: 0.9397 - val_loss: 0.3885 - val_acc: 0.8571\n",
      "Epoch 363/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1857 - acc: 0.9409 - val_loss: 0.4026 - val_acc: 0.8624\n",
      "Epoch 364/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1852 - acc: 0.9397 - val_loss: 0.3941 - val_acc: 0.8624\n",
      "Epoch 365/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1849 - acc: 0.9403 - val_loss: 0.3943 - val_acc: 0.8624\n",
      "Epoch 366/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1851 - acc: 0.9409 - val_loss: 0.3912 - val_acc: 0.8624\n",
      "Epoch 367/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1843 - acc: 0.9427 - val_loss: 0.3911 - val_acc: 0.8624\n",
      "Epoch 368/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1851 - acc: 0.9415 - val_loss: 0.4000 - val_acc: 0.8624\n",
      "Epoch 369/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1853 - acc: 0.9427 - val_loss: 0.4015 - val_acc: 0.8624\n",
      "Epoch 370/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1850 - acc: 0.9415 - val_loss: 0.3989 - val_acc: 0.8624\n",
      "Epoch 371/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1843 - acc: 0.9403 - val_loss: 0.3925 - val_acc: 0.8571\n",
      "Epoch 372/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1834 - acc: 0.9385 - val_loss: 0.3902 - val_acc: 0.8571\n",
      "Epoch 373/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1834 - acc: 0.9409 - val_loss: 0.4068 - val_acc: 0.8624\n",
      "Epoch 374/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1841 - acc: 0.9397 - val_loss: 0.3916 - val_acc: 0.8624\n",
      "Epoch 375/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1839 - acc: 0.9415 - val_loss: 0.3911 - val_acc: 0.8571\n",
      "Epoch 376/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1840 - acc: 0.9433 - val_loss: 0.3973 - val_acc: 0.8624\n",
      "Epoch 377/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1840 - acc: 0.9403 - val_loss: 0.3904 - val_acc: 0.8571\n",
      "Epoch 378/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1839 - acc: 0.9409 - val_loss: 0.3918 - val_acc: 0.8571\n",
      "Epoch 379/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1840 - acc: 0.9415 - val_loss: 0.3860 - val_acc: 0.8624\n",
      "Epoch 380/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1831 - acc: 0.9415 - val_loss: 0.4028 - val_acc: 0.8571\n",
      "Epoch 381/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1842 - acc: 0.9409 - val_loss: 0.4046 - val_acc: 0.8624\n",
      "Epoch 382/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1826 - acc: 0.9391 - val_loss: 0.3809 - val_acc: 0.8571\n",
      "Epoch 383/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1831 - acc: 0.9403 - val_loss: 0.3954 - val_acc: 0.8571\n",
      "Epoch 384/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1828 - acc: 0.9415 - val_loss: 0.3878 - val_acc: 0.8624\n",
      "Epoch 385/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1833 - acc: 0.9433 - val_loss: 0.3908 - val_acc: 0.8519\n",
      "Epoch 386/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1829 - acc: 0.9427 - val_loss: 0.3813 - val_acc: 0.8624\n",
      "Epoch 387/1000\n",
      "1692/1692 [==============================] - 0s 89us/step - loss: 0.1828 - acc: 0.9421 - val_loss: 0.3922 - val_acc: 0.8571\n",
      "Epoch 388/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1823 - acc: 0.9415 - val_loss: 0.3848 - val_acc: 0.8677\n",
      "Epoch 389/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1826 - acc: 0.9433 - val_loss: 0.4029 - val_acc: 0.8677\n",
      "Epoch 390/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1799 - acc: 0.9421 - val_loss: 0.3782 - val_acc: 0.8624\n",
      "Epoch 391/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1826 - acc: 0.9433 - val_loss: 0.3885 - val_acc: 0.8624\n",
      "Epoch 392/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1818 - acc: 0.9415 - val_loss: 0.4089 - val_acc: 0.8571\n",
      "Epoch 393/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1825 - acc: 0.9409 - val_loss: 0.4127 - val_acc: 0.8624\n",
      "Epoch 394/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1827 - acc: 0.9421 - val_loss: 0.4045 - val_acc: 0.8677\n",
      "Epoch 395/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1817 - acc: 0.9415 - val_loss: 0.3868 - val_acc: 0.8519\n",
      "Epoch 396/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1819 - acc: 0.9421 - val_loss: 0.4039 - val_acc: 0.8677\n",
      "Epoch 397/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1823 - acc: 0.9409 - val_loss: 0.4004 - val_acc: 0.8624\n",
      "Epoch 398/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1803 - acc: 0.9427 - val_loss: 0.3885 - val_acc: 0.8519\n",
      "Epoch 399/1000\n",
      "1692/1692 [==============================] - 0s 100us/step - loss: 0.1812 - acc: 0.9427 - val_loss: 0.3902 - val_acc: 0.8571\n",
      "Epoch 400/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1813 - acc: 0.9415 - val_loss: 0.4075 - val_acc: 0.8677\n",
      "Epoch 401/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1822 - acc: 0.9421 - val_loss: 0.3913 - val_acc: 0.8624\n",
      "Epoch 402/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1812 - acc: 0.9415 - val_loss: 0.3849 - val_acc: 0.8677\n",
      "Epoch 403/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1807 - acc: 0.9439 - val_loss: 0.3958 - val_acc: 0.8571\n",
      "Epoch 404/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1806 - acc: 0.9439 - val_loss: 0.4098 - val_acc: 0.8677\n",
      "Epoch 405/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1812 - acc: 0.942 - 0s 59us/step - loss: 0.1815 - acc: 0.9391 - val_loss: 0.3987 - val_acc: 0.8519\n",
      "Epoch 406/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1808 - acc: 0.9421 - val_loss: 0.3898 - val_acc: 0.8677\n",
      "Epoch 407/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1800 - acc: 0.9433 - val_loss: 0.3879 - val_acc: 0.8571\n",
      "Epoch 408/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1804 - acc: 0.9415 - val_loss: 0.3883 - val_acc: 0.8677\n",
      "Epoch 409/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1799 - acc: 0.9421 - val_loss: 0.4067 - val_acc: 0.8571\n",
      "Epoch 410/1000\n",
      "1692/1692 [==============================] - 0s 83us/step - loss: 0.1813 - acc: 0.9397 - val_loss: 0.3970 - val_acc: 0.8571\n",
      "Epoch 411/1000\n",
      "1692/1692 [==============================] - 0s 105us/step - loss: 0.1806 - acc: 0.9439 - val_loss: 0.4002 - val_acc: 0.8571\n",
      "Epoch 412/1000\n",
      "1692/1692 [==============================] - 0s 108us/step - loss: 0.1805 - acc: 0.9385 - val_loss: 0.3989 - val_acc: 0.8624\n",
      "Epoch 413/1000\n",
      "1692/1692 [==============================] - 0s 112us/step - loss: 0.1806 - acc: 0.9421 - val_loss: 0.4027 - val_acc: 0.8519\n",
      "Epoch 414/1000\n",
      "1692/1692 [==============================] - 0s 90us/step - loss: 0.1799 - acc: 0.9415 - val_loss: 0.4016 - val_acc: 0.8624\n",
      "Epoch 415/1000\n",
      "1692/1692 [==============================] - 0s 109us/step - loss: 0.1797 - acc: 0.9427 - val_loss: 0.3935 - val_acc: 0.8624\n",
      "Epoch 416/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1789 - acc: 0.9439 - val_loss: 0.3846 - val_acc: 0.8571\n",
      "Epoch 417/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1799 - acc: 0.9409 - val_loss: 0.3913 - val_acc: 0.8624\n",
      "Epoch 418/1000\n",
      "1692/1692 [==============================] - 0s 91us/step - loss: 0.1793 - acc: 0.9421 - val_loss: 0.3963 - val_acc: 0.8519\n",
      "Epoch 419/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1796 - acc: 0.9391 - val_loss: 0.3961 - val_acc: 0.8571\n",
      "Epoch 420/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1791 - acc: 0.9415 - val_loss: 0.3947 - val_acc: 0.8571\n",
      "Epoch 421/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1786 - acc: 0.9427 - val_loss: 0.4020 - val_acc: 0.8519\n",
      "Epoch 422/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1795 - acc: 0.9415 - val_loss: 0.3980 - val_acc: 0.8677\n",
      "Epoch 423/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1792 - acc: 0.9427 - val_loss: 0.3907 - val_acc: 0.8624\n",
      "Epoch 424/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1792 - acc: 0.9439 - val_loss: 0.3951 - val_acc: 0.8571\n",
      "Epoch 425/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1780 - acc: 0.9433 - val_loss: 0.4079 - val_acc: 0.8624\n",
      "Epoch 426/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1797 - acc: 0.9439 - val_loss: 0.3882 - val_acc: 0.8519\n",
      "Epoch 427/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1777 - acc: 0.9427 - val_loss: 0.3827 - val_acc: 0.8571\n",
      "Epoch 428/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1785 - acc: 0.9421 - val_loss: 0.3898 - val_acc: 0.8571\n",
      "Epoch 429/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1787 - acc: 0.9415 - val_loss: 0.3906 - val_acc: 0.8571\n",
      "Epoch 430/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1787 - acc: 0.9421 - val_loss: 0.3869 - val_acc: 0.8571\n",
      "Epoch 431/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1792 - acc: 0.9433 - val_loss: 0.3969 - val_acc: 0.8624\n",
      "Epoch 432/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1781 - acc: 0.9409 - val_loss: 0.4076 - val_acc: 0.8624\n",
      "Epoch 433/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1772 - acc: 0.9421 - val_loss: 0.3850 - val_acc: 0.8571\n",
      "Epoch 434/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1782 - acc: 0.9397 - val_loss: 0.3940 - val_acc: 0.8677\n",
      "Epoch 435/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1775 - acc: 0.9415 - val_loss: 0.3937 - val_acc: 0.8571\n",
      "Epoch 436/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1779 - acc: 0.9444 - val_loss: 0.3820 - val_acc: 0.8624\n",
      "Epoch 437/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1783 - acc: 0.9415 - val_loss: 0.3986 - val_acc: 0.8677\n",
      "Epoch 438/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1773 - acc: 0.9427 - val_loss: 0.4104 - val_acc: 0.8571\n",
      "Epoch 439/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1782 - acc: 0.9415 - val_loss: 0.3942 - val_acc: 0.8571\n",
      "Epoch 440/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1775 - acc: 0.9433 - val_loss: 0.3888 - val_acc: 0.8571\n",
      "Epoch 441/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1767 - acc: 0.9439 - val_loss: 0.4020 - val_acc: 0.8519\n",
      "Epoch 442/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1774 - acc: 0.9427 - val_loss: 0.3920 - val_acc: 0.8624\n",
      "Epoch 443/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1766 - acc: 0.9409 - val_loss: 0.4021 - val_acc: 0.8571\n",
      "Epoch 444/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1783 - acc: 0.9439 - val_loss: 0.4047 - val_acc: 0.8571\n",
      "Epoch 445/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1771 - acc: 0.9439 - val_loss: 0.3938 - val_acc: 0.8571\n",
      "Epoch 446/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1770 - acc: 0.9421 - val_loss: 0.4065 - val_acc: 0.8677\n",
      "Epoch 447/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1768 - acc: 0.9433 - val_loss: 0.4036 - val_acc: 0.8677\n",
      "Epoch 448/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1768 - acc: 0.9439 - val_loss: 0.4044 - val_acc: 0.8571\n",
      "Epoch 449/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1774 - acc: 0.9415 - val_loss: 0.3894 - val_acc: 0.8571\n",
      "Epoch 450/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1772 - acc: 0.9427 - val_loss: 0.4018 - val_acc: 0.8571\n",
      "Epoch 451/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1764 - acc: 0.9403 - val_loss: 0.3908 - val_acc: 0.8571\n",
      "Epoch 452/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1769 - acc: 0.9421 - val_loss: 0.3878 - val_acc: 0.8624\n",
      "Epoch 453/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1759 - acc: 0.9462 - val_loss: 0.3947 - val_acc: 0.8571\n",
      "Epoch 454/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1764 - acc: 0.9403 - val_loss: 0.4049 - val_acc: 0.8519\n",
      "Epoch 455/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1770 - acc: 0.9403 - val_loss: 0.3965 - val_acc: 0.8624\n",
      "Epoch 456/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1758 - acc: 0.9421 - val_loss: 0.3976 - val_acc: 0.8571\n",
      "Epoch 457/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1768 - acc: 0.9421 - val_loss: 0.4014 - val_acc: 0.8624\n",
      "Epoch 458/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1762 - acc: 0.9415 - val_loss: 0.3879 - val_acc: 0.8571\n",
      "Epoch 459/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1764 - acc: 0.9409 - val_loss: 0.3923 - val_acc: 0.8571\n",
      "Epoch 460/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1755 - acc: 0.9433 - val_loss: 0.4058 - val_acc: 0.8624\n",
      "Epoch 461/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1765 - acc: 0.9421 - val_loss: 0.3982 - val_acc: 0.8571\n",
      "Epoch 462/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1754 - acc: 0.9427 - val_loss: 0.4024 - val_acc: 0.8519\n",
      "Epoch 463/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1757 - acc: 0.9444 - val_loss: 0.3956 - val_acc: 0.8519\n",
      "Epoch 464/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1761 - acc: 0.9415 - val_loss: 0.3985 - val_acc: 0.8571\n",
      "Epoch 465/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1753 - acc: 0.9409 - val_loss: 0.4000 - val_acc: 0.8519\n",
      "Epoch 466/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1759 - acc: 0.9450 - val_loss: 0.4019 - val_acc: 0.8519\n",
      "Epoch 467/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1747 - acc: 0.9421 - val_loss: 0.3955 - val_acc: 0.8466\n",
      "Epoch 468/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1756 - acc: 0.9409 - val_loss: 0.4052 - val_acc: 0.8624\n",
      "Epoch 469/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1750 - acc: 0.9421 - val_loss: 0.4067 - val_acc: 0.8624\n",
      "Epoch 470/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1753 - acc: 0.9403 - val_loss: 0.3899 - val_acc: 0.8571\n",
      "Epoch 471/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1757 - acc: 0.9409 - val_loss: 0.4023 - val_acc: 0.8624\n",
      "Epoch 472/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1753 - acc: 0.9421 - val_loss: 0.3906 - val_acc: 0.8571\n",
      "Epoch 473/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1754 - acc: 0.9444 - val_loss: 0.3907 - val_acc: 0.8571\n",
      "Epoch 474/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1751 - acc: 0.9403 - val_loss: 0.4053 - val_acc: 0.8571\n",
      "Epoch 475/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1743 - acc: 0.9421 - val_loss: 0.4153 - val_acc: 0.8677\n",
      "Epoch 476/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1754 - acc: 0.9415 - val_loss: 0.3897 - val_acc: 0.8624\n",
      "Epoch 477/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1749 - acc: 0.9409 - val_loss: 0.4083 - val_acc: 0.8624\n",
      "Epoch 478/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1746 - acc: 0.9450 - val_loss: 0.3984 - val_acc: 0.8519\n",
      "Epoch 479/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1742 - acc: 0.9403 - val_loss: 0.3947 - val_acc: 0.8624\n",
      "Epoch 480/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1753 - acc: 0.9433 - val_loss: 0.3891 - val_acc: 0.8571\n",
      "Epoch 481/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1735 - acc: 0.9439 - val_loss: 0.4086 - val_acc: 0.8624\n",
      "Epoch 482/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1739 - acc: 0.9439 - val_loss: 0.4030 - val_acc: 0.8624\n",
      "Epoch 483/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1751 - acc: 0.9409 - val_loss: 0.3908 - val_acc: 0.8571\n",
      "Epoch 484/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1731 - acc: 0.9433 - val_loss: 0.3856 - val_acc: 0.8677\n",
      "Epoch 485/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1746 - acc: 0.9427 - val_loss: 0.3950 - val_acc: 0.8571\n",
      "Epoch 486/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1742 - acc: 0.9444 - val_loss: 0.3871 - val_acc: 0.8571\n",
      "Epoch 487/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1740 - acc: 0.9403 - val_loss: 0.4052 - val_acc: 0.8624\n",
      "Epoch 488/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1741 - acc: 0.9444 - val_loss: 0.3994 - val_acc: 0.8571\n",
      "Epoch 489/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1738 - acc: 0.9409 - val_loss: 0.3877 - val_acc: 0.8624\n",
      "Epoch 490/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1737 - acc: 0.9433 - val_loss: 0.3975 - val_acc: 0.8466\n",
      "Epoch 491/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1729 - acc: 0.9415 - val_loss: 0.3875 - val_acc: 0.8624\n",
      "Epoch 492/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1743 - acc: 0.9415 - val_loss: 0.3966 - val_acc: 0.8624\n",
      "Epoch 493/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1738 - acc: 0.9409 - val_loss: 0.3945 - val_acc: 0.8571\n",
      "Epoch 494/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1735 - acc: 0.9427 - val_loss: 0.3914 - val_acc: 0.8571\n",
      "Epoch 495/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1728 - acc: 0.9433 - val_loss: 0.3992 - val_acc: 0.8519\n",
      "Epoch 496/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1735 - acc: 0.9421 - val_loss: 0.3990 - val_acc: 0.8624\n",
      "Epoch 497/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1735 - acc: 0.9427 - val_loss: 0.3950 - val_acc: 0.8571\n",
      "Epoch 498/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1722 - acc: 0.9403 - val_loss: 0.4124 - val_acc: 0.8677\n",
      "Epoch 499/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1728 - acc: 0.9450 - val_loss: 0.3931 - val_acc: 0.8571\n",
      "Epoch 500/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1734 - acc: 0.9433 - val_loss: 0.3976 - val_acc: 0.8571\n",
      "Epoch 501/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1731 - acc: 0.9421 - val_loss: 0.4074 - val_acc: 0.8677\n",
      "Epoch 502/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1729 - acc: 0.9427 - val_loss: 0.3981 - val_acc: 0.8571\n",
      "Epoch 503/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1733 - acc: 0.9391 - val_loss: 0.3951 - val_acc: 0.8624\n",
      "Epoch 504/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1733 - acc: 0.9403 - val_loss: 0.4065 - val_acc: 0.8519\n",
      "Epoch 505/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1720 - acc: 0.9415 - val_loss: 0.3882 - val_acc: 0.8624\n",
      "Epoch 506/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1720 - acc: 0.9439 - val_loss: 0.3971 - val_acc: 0.8624\n",
      "Epoch 507/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1730 - acc: 0.9415 - val_loss: 0.4001 - val_acc: 0.8677\n",
      "Epoch 508/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1725 - acc: 0.9439 - val_loss: 0.3999 - val_acc: 0.8571\n",
      "Epoch 509/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1716 - acc: 0.9409 - val_loss: 0.3984 - val_acc: 0.8624\n",
      "Epoch 510/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1727 - acc: 0.9439 - val_loss: 0.3938 - val_acc: 0.8624\n",
      "Epoch 511/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1725 - acc: 0.9433 - val_loss: 0.4044 - val_acc: 0.8677\n",
      "Epoch 512/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1715 - acc: 0.9403 - val_loss: 0.4149 - val_acc: 0.8677\n",
      "Epoch 513/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1720 - acc: 0.9409 - val_loss: 0.3945 - val_acc: 0.8519\n",
      "Epoch 514/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1720 - acc: 0.9427 - val_loss: 0.4023 - val_acc: 0.8519\n",
      "Epoch 515/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1715 - acc: 0.9391 - val_loss: 0.3973 - val_acc: 0.8624\n",
      "Epoch 516/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1719 - acc: 0.9462 - val_loss: 0.4013 - val_acc: 0.8624\n",
      "Epoch 517/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1724 - acc: 0.9415 - val_loss: 0.4052 - val_acc: 0.8677\n",
      "Epoch 518/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1729 - acc: 0.9439 - val_loss: 0.3959 - val_acc: 0.8624\n",
      "Epoch 519/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1717 - acc: 0.9415 - val_loss: 0.3936 - val_acc: 0.8571\n",
      "Epoch 520/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1721 - acc: 0.9403 - val_loss: 0.4041 - val_acc: 0.8571\n",
      "Epoch 521/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1711 - acc: 0.9433 - val_loss: 0.3982 - val_acc: 0.8571\n",
      "Epoch 522/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1722 - acc: 0.9415 - val_loss: 0.3986 - val_acc: 0.8677\n",
      "Epoch 523/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1713 - acc: 0.9421 - val_loss: 0.3876 - val_acc: 0.8677\n",
      "Epoch 524/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1710 - acc: 0.9433 - val_loss: 0.3919 - val_acc: 0.8624\n",
      "Epoch 525/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1725 - acc: 0.9433 - val_loss: 0.3912 - val_acc: 0.8624\n",
      "Epoch 526/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1706 - acc: 0.9421 - val_loss: 0.4132 - val_acc: 0.8624\n",
      "Epoch 527/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1701 - acc: 0.9433 - val_loss: 0.4093 - val_acc: 0.8571\n",
      "Epoch 528/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1713 - acc: 0.9409 - val_loss: 0.3999 - val_acc: 0.8624\n",
      "Epoch 529/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1709 - acc: 0.9415 - val_loss: 0.4002 - val_acc: 0.8624\n",
      "Epoch 530/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1712 - acc: 0.9433 - val_loss: 0.3912 - val_acc: 0.8677\n",
      "Epoch 531/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1708 - acc: 0.9415 - val_loss: 0.4044 - val_acc: 0.8730\n",
      "Epoch 532/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1710 - acc: 0.9385 - val_loss: 0.3915 - val_acc: 0.8624\n",
      "Epoch 533/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1717 - acc: 0.9397 - val_loss: 0.3906 - val_acc: 0.8571\n",
      "Epoch 534/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1718 - acc: 0.9421 - val_loss: 0.3959 - val_acc: 0.8624\n",
      "Epoch 535/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1714 - acc: 0.9415 - val_loss: 0.4063 - val_acc: 0.8624\n",
      "Epoch 536/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1709 - acc: 0.9421 - val_loss: 0.3862 - val_acc: 0.8677\n",
      "Epoch 537/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1710 - acc: 0.9415 - val_loss: 0.3895 - val_acc: 0.8624\n",
      "Epoch 538/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1696 - acc: 0.9415 - val_loss: 0.4074 - val_acc: 0.8624\n",
      "Epoch 539/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1713 - acc: 0.9427 - val_loss: 0.3969 - val_acc: 0.8624\n",
      "Epoch 540/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1704 - acc: 0.9397 - val_loss: 0.3953 - val_acc: 0.8624\n",
      "Epoch 541/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1711 - acc: 0.9421 - val_loss: 0.4009 - val_acc: 0.8677\n",
      "Epoch 542/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1711 - acc: 0.9409 - val_loss: 0.3967 - val_acc: 0.8571\n",
      "Epoch 543/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1702 - acc: 0.9421 - val_loss: 0.4136 - val_acc: 0.8677\n",
      "Epoch 544/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1704 - acc: 0.9421 - val_loss: 0.3978 - val_acc: 0.8624\n",
      "Epoch 545/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1705 - acc: 0.9421 - val_loss: 0.3965 - val_acc: 0.8624\n",
      "Epoch 546/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1706 - acc: 0.9439 - val_loss: 0.4022 - val_acc: 0.8677\n",
      "Epoch 547/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1699 - acc: 0.9427 - val_loss: 0.3885 - val_acc: 0.8624\n",
      "Epoch 548/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1698 - acc: 0.9421 - val_loss: 0.3897 - val_acc: 0.8677\n",
      "Epoch 549/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1691 - acc: 0.9427 - val_loss: 0.3861 - val_acc: 0.8624\n",
      "Epoch 550/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1703 - acc: 0.9409 - val_loss: 0.4023 - val_acc: 0.8677\n",
      "Epoch 551/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1702 - acc: 0.9409 - val_loss: 0.4000 - val_acc: 0.8677\n",
      "Epoch 552/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1699 - acc: 0.9415 - val_loss: 0.3988 - val_acc: 0.8571\n",
      "Epoch 553/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1697 - acc: 0.9403 - val_loss: 0.3911 - val_acc: 0.8624\n",
      "Epoch 554/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1706 - acc: 0.9439 - val_loss: 0.4049 - val_acc: 0.8677\n",
      "Epoch 555/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1694 - acc: 0.9427 - val_loss: 0.3912 - val_acc: 0.8624\n",
      "Epoch 556/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1700 - acc: 0.9427 - val_loss: 0.3897 - val_acc: 0.8730\n",
      "Epoch 557/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1694 - acc: 0.9427 - val_loss: 0.3924 - val_acc: 0.8677\n",
      "Epoch 558/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1695 - acc: 0.9433 - val_loss: 0.4091 - val_acc: 0.8677\n",
      "Epoch 559/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1699 - acc: 0.9409 - val_loss: 0.4020 - val_acc: 0.8624\n",
      "Epoch 560/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1697 - acc: 0.9421 - val_loss: 0.4109 - val_acc: 0.8677\n",
      "Epoch 561/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1703 - acc: 0.9415 - val_loss: 0.3982 - val_acc: 0.8677\n",
      "Epoch 562/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1696 - acc: 0.9403 - val_loss: 0.4047 - val_acc: 0.8677\n",
      "Epoch 563/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1694 - acc: 0.9433 - val_loss: 0.3971 - val_acc: 0.8571\n",
      "Epoch 564/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1689 - acc: 0.9409 - val_loss: 0.3946 - val_acc: 0.8624\n",
      "Epoch 565/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1685 - acc: 0.9403 - val_loss: 0.4043 - val_acc: 0.8677\n",
      "Epoch 566/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.3944 - val_acc: 0.8624\n",
      "Epoch 567/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1692 - acc: 0.9433 - val_loss: 0.3961 - val_acc: 0.8730\n",
      "Epoch 568/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1690 - acc: 0.9409 - val_loss: 0.3924 - val_acc: 0.8677\n",
      "Epoch 569/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1683 - acc: 0.9409 - val_loss: 0.3964 - val_acc: 0.8677\n",
      "Epoch 570/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1691 - acc: 0.9444 - val_loss: 0.3998 - val_acc: 0.8624\n",
      "Epoch 571/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1687 - acc: 0.9421 - val_loss: 0.3977 - val_acc: 0.8677\n",
      "Epoch 572/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1680 - acc: 0.9427 - val_loss: 0.3953 - val_acc: 0.8677\n",
      "Epoch 573/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1677 - acc: 0.9427 - val_loss: 0.3901 - val_acc: 0.8677\n",
      "Epoch 574/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1685 - acc: 0.9415 - val_loss: 0.4023 - val_acc: 0.8677\n",
      "Epoch 575/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1768 - acc: 0.933 - 0s 52us/step - loss: 0.1690 - acc: 0.9409 - val_loss: 0.3916 - val_acc: 0.8624\n",
      "Epoch 576/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1688 - acc: 0.9415 - val_loss: 0.4004 - val_acc: 0.8677\n",
      "Epoch 577/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1683 - acc: 0.9415 - val_loss: 0.3991 - val_acc: 0.8677\n",
      "Epoch 578/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1683 - acc: 0.9421 - val_loss: 0.3934 - val_acc: 0.8730\n",
      "Epoch 579/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1699 - acc: 0.9427 - val_loss: 0.3944 - val_acc: 0.8624\n",
      "Epoch 580/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1681 - acc: 0.9403 - val_loss: 0.4008 - val_acc: 0.8624\n",
      "Epoch 581/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1689 - acc: 0.9415 - val_loss: 0.4007 - val_acc: 0.8677\n",
      "Epoch 582/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1679 - acc: 0.9415 - val_loss: 0.3930 - val_acc: 0.8624\n",
      "Epoch 583/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1677 - acc: 0.9427 - val_loss: 0.3988 - val_acc: 0.8624\n",
      "Epoch 584/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1683 - acc: 0.9439 - val_loss: 0.4084 - val_acc: 0.8677\n",
      "Epoch 585/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1684 - acc: 0.9379 - val_loss: 0.4002 - val_acc: 0.8677\n",
      "Epoch 586/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1680 - acc: 0.9421 - val_loss: 0.4023 - val_acc: 0.8677\n",
      "Epoch 587/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1676 - acc: 0.9409 - val_loss: 0.3928 - val_acc: 0.8677\n",
      "Epoch 588/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1679 - acc: 0.9397 - val_loss: 0.3889 - val_acc: 0.8730\n",
      "Epoch 589/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1676 - acc: 0.9409 - val_loss: 0.3858 - val_acc: 0.8677\n",
      "Epoch 590/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1682 - acc: 0.9409 - val_loss: 0.3972 - val_acc: 0.8677\n",
      "Epoch 591/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1682 - acc: 0.9439 - val_loss: 0.3954 - val_acc: 0.8677\n",
      "Epoch 592/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1674 - acc: 0.9433 - val_loss: 0.4045 - val_acc: 0.8677\n",
      "Epoch 593/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1669 - acc: 0.9433 - val_loss: 0.3888 - val_acc: 0.8677\n",
      "Epoch 594/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1682 - acc: 0.9415 - val_loss: 0.4021 - val_acc: 0.8571\n",
      "Epoch 595/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1676 - acc: 0.9415 - val_loss: 0.4101 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1678 - acc: 0.9444 - val_loss: 0.3986 - val_acc: 0.8730\n",
      "Epoch 597/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1672 - acc: 0.9421 - val_loss: 0.3999 - val_acc: 0.8677\n",
      "Epoch 598/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1677 - acc: 0.9439 - val_loss: 0.3994 - val_acc: 0.8730\n",
      "Epoch 599/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1674 - acc: 0.9439 - val_loss: 0.3981 - val_acc: 0.8677\n",
      "Epoch 600/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1670 - acc: 0.9421 - val_loss: 0.4015 - val_acc: 0.8624\n",
      "Epoch 601/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1659 - acc: 0.9415 - val_loss: 0.4064 - val_acc: 0.8783\n",
      "Epoch 602/1000\n",
      "1692/1692 [==============================] - 0s 106us/step - loss: 0.1665 - acc: 0.9391 - val_loss: 0.3922 - val_acc: 0.8730\n",
      "Epoch 603/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.1669 - acc: 0.9391 - val_loss: 0.3874 - val_acc: 0.8730\n",
      "Epoch 604/1000\n",
      "1692/1692 [==============================] - 0s 97us/step - loss: 0.1674 - acc: 0.9439 - val_loss: 0.4040 - val_acc: 0.8677\n",
      "Epoch 605/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1674 - acc: 0.9427 - val_loss: 0.3982 - val_acc: 0.8677\n",
      "Epoch 606/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1666 - acc: 0.9427 - val_loss: 0.3906 - val_acc: 0.8624\n",
      "Epoch 607/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1664 - acc: 0.9433 - val_loss: 0.4003 - val_acc: 0.8677\n",
      "Epoch 608/1000\n",
      "1692/1692 [==============================] - 0s 96us/step - loss: 0.1670 - acc: 0.9427 - val_loss: 0.4072 - val_acc: 0.8677\n",
      "Epoch 609/1000\n",
      "1692/1692 [==============================] - 0s 78us/step - loss: 0.1659 - acc: 0.9397 - val_loss: 0.4014 - val_acc: 0.8677\n",
      "Epoch 610/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1664 - acc: 0.9403 - val_loss: 0.3913 - val_acc: 0.8677\n",
      "Epoch 611/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1669 - acc: 0.9421 - val_loss: 0.3951 - val_acc: 0.8677\n",
      "Epoch 612/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1674 - acc: 0.9444 - val_loss: 0.3914 - val_acc: 0.8677\n",
      "Epoch 613/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1674 - acc: 0.9385 - val_loss: 0.4032 - val_acc: 0.8730\n",
      "Epoch 614/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1667 - acc: 0.9433 - val_loss: 0.4004 - val_acc: 0.8624\n",
      "Epoch 615/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1676 - acc: 0.9427 - val_loss: 0.3989 - val_acc: 0.8677\n",
      "Epoch 616/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1667 - acc: 0.9391 - val_loss: 0.4029 - val_acc: 0.8783\n",
      "Epoch 617/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1659 - acc: 0.9427 - val_loss: 0.3975 - val_acc: 0.8730\n",
      "Epoch 618/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1662 - acc: 0.9409 - val_loss: 0.4003 - val_acc: 0.8783\n",
      "Epoch 619/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1653 - acc: 0.9415 - val_loss: 0.4061 - val_acc: 0.8730\n",
      "Epoch 620/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1667 - acc: 0.9391 - val_loss: 0.3937 - val_acc: 0.8624\n",
      "Epoch 621/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1663 - acc: 0.9415 - val_loss: 0.3919 - val_acc: 0.8624\n",
      "Epoch 622/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1663 - acc: 0.9427 - val_loss: 0.4013 - val_acc: 0.8783\n",
      "Epoch 623/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1660 - acc: 0.9427 - val_loss: 0.4151 - val_acc: 0.8730\n",
      "Epoch 624/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1673 - acc: 0.9427 - val_loss: 0.4011 - val_acc: 0.8677\n",
      "Epoch 625/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1658 - acc: 0.9427 - val_loss: 0.4069 - val_acc: 0.8730\n",
      "Epoch 626/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1665 - acc: 0.9415 - val_loss: 0.4031 - val_acc: 0.8730\n",
      "Epoch 627/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1663 - acc: 0.9391 - val_loss: 0.4007 - val_acc: 0.8677\n",
      "Epoch 628/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1663 - acc: 0.9421 - val_loss: 0.4004 - val_acc: 0.8730\n",
      "Epoch 629/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1662 - acc: 0.9421 - val_loss: 0.3964 - val_acc: 0.8624\n",
      "Epoch 630/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1656 - acc: 0.9415 - val_loss: 0.4037 - val_acc: 0.8730\n",
      "Epoch 631/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1656 - acc: 0.9403 - val_loss: 0.4093 - val_acc: 0.8730\n",
      "Epoch 632/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1668 - acc: 0.9409 - val_loss: 0.4025 - val_acc: 0.8677\n",
      "Epoch 633/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1656 - acc: 0.9421 - val_loss: 0.4018 - val_acc: 0.8730\n",
      "Epoch 634/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1648 - acc: 0.9415 - val_loss: 0.4003 - val_acc: 0.8677\n",
      "Epoch 635/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1647 - acc: 0.9391 - val_loss: 0.3854 - val_acc: 0.8677\n",
      "Epoch 636/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1662 - acc: 0.9409 - val_loss: 0.4029 - val_acc: 0.8730\n",
      "Epoch 637/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1662 - acc: 0.9397 - val_loss: 0.3960 - val_acc: 0.8677\n",
      "Epoch 638/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1653 - acc: 0.9427 - val_loss: 0.4052 - val_acc: 0.8730\n",
      "Epoch 639/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1647 - acc: 0.9450 - val_loss: 0.3992 - val_acc: 0.8677\n",
      "Epoch 640/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1657 - acc: 0.9415 - val_loss: 0.4123 - val_acc: 0.8730\n",
      "Epoch 641/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1657 - acc: 0.9421 - val_loss: 0.3976 - val_acc: 0.8730\n",
      "Epoch 642/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1657 - acc: 0.9409 - val_loss: 0.3931 - val_acc: 0.8677\n",
      "Epoch 643/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1650 - acc: 0.9415 - val_loss: 0.3970 - val_acc: 0.8677\n",
      "Epoch 644/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1657 - acc: 0.9397 - val_loss: 0.3952 - val_acc: 0.8677\n",
      "Epoch 645/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1644 - acc: 0.9439 - val_loss: 0.4077 - val_acc: 0.8730\n",
      "Epoch 646/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1648 - acc: 0.9439 - val_loss: 0.4098 - val_acc: 0.8730\n",
      "Epoch 647/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1643 - acc: 0.9409 - val_loss: 0.4017 - val_acc: 0.8730\n",
      "Epoch 648/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1656 - acc: 0.9415 - val_loss: 0.4039 - val_acc: 0.8730\n",
      "Epoch 649/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1635 - acc: 0.9439 - val_loss: 0.4171 - val_acc: 0.8783\n",
      "Epoch 650/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1656 - acc: 0.9427 - val_loss: 0.4011 - val_acc: 0.8677\n",
      "Epoch 651/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1650 - acc: 0.9409 - val_loss: 0.3965 - val_acc: 0.8624\n",
      "Epoch 652/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1651 - acc: 0.9427 - val_loss: 0.4037 - val_acc: 0.8730\n",
      "Epoch 653/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1650 - acc: 0.9427 - val_loss: 0.3982 - val_acc: 0.8624\n",
      "Epoch 654/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1653 - acc: 0.9415 - val_loss: 0.3997 - val_acc: 0.8730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 655/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1646 - acc: 0.9403 - val_loss: 0.4096 - val_acc: 0.8730\n",
      "Epoch 656/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1649 - acc: 0.9397 - val_loss: 0.4173 - val_acc: 0.8730\n",
      "Epoch 657/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1652 - acc: 0.9421 - val_loss: 0.4154 - val_acc: 0.8730\n",
      "Epoch 658/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1646 - acc: 0.9409 - val_loss: 0.4094 - val_acc: 0.8783\n",
      "Epoch 659/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1649 - acc: 0.9415 - val_loss: 0.3998 - val_acc: 0.8730\n",
      "Epoch 660/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1651 - acc: 0.9415 - val_loss: 0.3991 - val_acc: 0.8677\n",
      "Epoch 661/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1652 - acc: 0.9385 - val_loss: 0.4047 - val_acc: 0.8730\n",
      "Epoch 662/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1650 - acc: 0.9409 - val_loss: 0.4084 - val_acc: 0.8677\n",
      "Epoch 663/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1648 - acc: 0.9391 - val_loss: 0.4008 - val_acc: 0.8730\n",
      "Epoch 664/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1645 - acc: 0.9450 - val_loss: 0.3985 - val_acc: 0.8677\n",
      "Epoch 665/1000\n",
      "1692/1692 [==============================] - 0s 82us/step - loss: 0.1641 - acc: 0.9391 - val_loss: 0.4035 - val_acc: 0.8730\n",
      "Epoch 666/1000\n",
      "1692/1692 [==============================] - 0s 85us/step - loss: 0.1644 - acc: 0.9403 - val_loss: 0.4016 - val_acc: 0.8730\n",
      "Epoch 667/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1642 - acc: 0.9421 - val_loss: 0.3922 - val_acc: 0.8677\n",
      "Epoch 668/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1646 - acc: 0.9415 - val_loss: 0.3934 - val_acc: 0.8624\n",
      "Epoch 669/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1637 - acc: 0.9433 - val_loss: 0.3966 - val_acc: 0.8624\n",
      "Epoch 670/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1637 - acc: 0.9397 - val_loss: 0.4168 - val_acc: 0.8730\n",
      "Epoch 671/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1643 - acc: 0.9385 - val_loss: 0.4005 - val_acc: 0.8677\n",
      "Epoch 672/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1641 - acc: 0.9379 - val_loss: 0.3981 - val_acc: 0.8677\n",
      "Epoch 673/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1642 - acc: 0.9409 - val_loss: 0.4004 - val_acc: 0.8677\n",
      "Epoch 674/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1633 - acc: 0.9421 - val_loss: 0.4089 - val_acc: 0.8783\n",
      "Epoch 675/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1634 - acc: 0.9415 - val_loss: 0.3981 - val_acc: 0.8730\n",
      "Epoch 676/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1643 - acc: 0.9409 - val_loss: 0.4001 - val_acc: 0.8624\n",
      "Epoch 677/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1642 - acc: 0.9409 - val_loss: 0.4048 - val_acc: 0.8677\n",
      "Epoch 678/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1631 - acc: 0.9409 - val_loss: 0.4024 - val_acc: 0.8677\n",
      "Epoch 679/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1623 - acc: 0.9427 - val_loss: 0.4081 - val_acc: 0.8730\n",
      "Epoch 680/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1640 - acc: 0.9391 - val_loss: 0.4024 - val_acc: 0.8677\n",
      "Epoch 681/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1630 - acc: 0.9427 - val_loss: 0.4006 - val_acc: 0.8624\n",
      "Epoch 682/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1646 - acc: 0.9409 - val_loss: 0.4105 - val_acc: 0.8730\n",
      "Epoch 683/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1632 - acc: 0.9415 - val_loss: 0.4130 - val_acc: 0.8783\n",
      "Epoch 684/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1637 - acc: 0.9415 - val_loss: 0.4011 - val_acc: 0.8730\n",
      "Epoch 685/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1633 - acc: 0.9433 - val_loss: 0.4026 - val_acc: 0.8677\n",
      "Epoch 686/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1640 - acc: 0.9403 - val_loss: 0.4053 - val_acc: 0.8730\n",
      "Epoch 687/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1624 - acc: 0.9444 - val_loss: 0.4086 - val_acc: 0.8730\n",
      "Epoch 688/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1643 - acc: 0.9415 - val_loss: 0.4011 - val_acc: 0.8677\n",
      "Epoch 689/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1634 - acc: 0.9403 - val_loss: 0.4037 - val_acc: 0.8677\n",
      "Epoch 690/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1631 - acc: 0.9439 - val_loss: 0.4098 - val_acc: 0.8783\n",
      "Epoch 691/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1633 - acc: 0.9427 - val_loss: 0.4069 - val_acc: 0.8730\n",
      "Epoch 692/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1639 - acc: 0.9409 - val_loss: 0.4006 - val_acc: 0.8730\n",
      "Epoch 693/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1633 - acc: 0.9421 - val_loss: 0.4111 - val_acc: 0.8677\n",
      "Epoch 694/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1634 - acc: 0.9421 - val_loss: 0.4082 - val_acc: 0.8677\n",
      "Epoch 695/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1636 - acc: 0.9415 - val_loss: 0.4058 - val_acc: 0.8730\n",
      "Epoch 696/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1631 - acc: 0.9421 - val_loss: 0.4072 - val_acc: 0.8677\n",
      "Epoch 697/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1632 - acc: 0.9427 - val_loss: 0.4163 - val_acc: 0.8730\n",
      "Epoch 698/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1638 - acc: 0.9421 - val_loss: 0.4111 - val_acc: 0.8677\n",
      "Epoch 699/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1633 - acc: 0.9415 - val_loss: 0.4110 - val_acc: 0.8677\n",
      "Epoch 700/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1623 - acc: 0.9374 - val_loss: 0.4293 - val_acc: 0.8677\n",
      "Epoch 701/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1635 - acc: 0.9433 - val_loss: 0.4065 - val_acc: 0.8677\n",
      "Epoch 702/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1631 - acc: 0.9409 - val_loss: 0.3992 - val_acc: 0.8730\n",
      "Epoch 703/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1626 - acc: 0.9427 - val_loss: 0.4124 - val_acc: 0.8730\n",
      "Epoch 704/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1631 - acc: 0.9427 - val_loss: 0.4008 - val_acc: 0.8677\n",
      "Epoch 705/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1628 - acc: 0.9415 - val_loss: 0.4097 - val_acc: 0.8677\n",
      "Epoch 706/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1633 - acc: 0.9421 - val_loss: 0.4028 - val_acc: 0.8677\n",
      "Epoch 707/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1626 - acc: 0.9427 - val_loss: 0.4001 - val_acc: 0.8677\n",
      "Epoch 708/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1634 - acc: 0.9403 - val_loss: 0.4114 - val_acc: 0.8677\n",
      "Epoch 709/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1634 - acc: 0.9397 - val_loss: 0.4056 - val_acc: 0.8730\n",
      "Epoch 710/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1632 - acc: 0.9391 - val_loss: 0.4039 - val_acc: 0.8677\n",
      "Epoch 711/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1631 - acc: 0.9450 - val_loss: 0.4078 - val_acc: 0.8730\n",
      "Epoch 712/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1635 - acc: 0.9415 - val_loss: 0.4063 - val_acc: 0.8730\n",
      "Epoch 713/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1636 - acc: 0.9421 - val_loss: 0.4081 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 714/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1627 - acc: 0.9391 - val_loss: 0.4031 - val_acc: 0.8624\n",
      "Epoch 715/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1627 - acc: 0.9409 - val_loss: 0.4070 - val_acc: 0.8677\n",
      "Epoch 716/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1622 - acc: 0.9421 - val_loss: 0.4135 - val_acc: 0.8677\n",
      "Epoch 717/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9415 - val_loss: 0.4060 - val_acc: 0.8677\n",
      "Epoch 718/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1630 - acc: 0.9427 - val_loss: 0.4050 - val_acc: 0.8624\n",
      "Epoch 719/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1623 - acc: 0.9439 - val_loss: 0.3970 - val_acc: 0.8677\n",
      "Epoch 720/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1618 - acc: 0.9450 - val_loss: 0.4106 - val_acc: 0.8677\n",
      "Epoch 721/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1637 - acc: 0.9409 - val_loss: 0.4102 - val_acc: 0.8677\n",
      "Epoch 722/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1625 - acc: 0.9403 - val_loss: 0.4193 - val_acc: 0.8730\n",
      "Epoch 723/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9421 - val_loss: 0.4091 - val_acc: 0.8730\n",
      "Epoch 724/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1616 - acc: 0.9415 - val_loss: 0.3971 - val_acc: 0.8677\n",
      "Epoch 725/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1629 - acc: 0.9427 - val_loss: 0.4111 - val_acc: 0.8730\n",
      "Epoch 726/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1625 - acc: 0.9403 - val_loss: 0.4124 - val_acc: 0.8730\n",
      "Epoch 727/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1628 - acc: 0.9439 - val_loss: 0.4151 - val_acc: 0.8730\n",
      "Epoch 728/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1625 - acc: 0.9415 - val_loss: 0.3987 - val_acc: 0.8677\n",
      "Epoch 729/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1621 - acc: 0.9385 - val_loss: 0.4183 - val_acc: 0.8730\n",
      "Epoch 730/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1621 - acc: 0.9403 - val_loss: 0.4062 - val_acc: 0.8730\n",
      "Epoch 731/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1639 - acc: 0.9433 - val_loss: 0.4122 - val_acc: 0.8730\n",
      "Epoch 732/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1626 - acc: 0.9397 - val_loss: 0.4077 - val_acc: 0.8730\n",
      "Epoch 733/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1620 - acc: 0.9450 - val_loss: 0.4060 - val_acc: 0.8730\n",
      "Epoch 734/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1625 - acc: 0.9415 - val_loss: 0.4000 - val_acc: 0.8677\n",
      "Epoch 735/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1622 - acc: 0.9421 - val_loss: 0.4054 - val_acc: 0.8677\n",
      "Epoch 736/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1621 - acc: 0.9427 - val_loss: 0.4053 - val_acc: 0.8677\n",
      "Epoch 737/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1617 - acc: 0.9433 - val_loss: 0.4000 - val_acc: 0.8730\n",
      "Epoch 738/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1634 - acc: 0.9403 - val_loss: 0.4073 - val_acc: 0.8677\n",
      "Epoch 739/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1625 - acc: 0.9433 - val_loss: 0.4036 - val_acc: 0.8677\n",
      "Epoch 740/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1614 - acc: 0.9427 - val_loss: 0.4068 - val_acc: 0.8730\n",
      "Epoch 741/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1621 - acc: 0.9409 - val_loss: 0.4079 - val_acc: 0.8730\n",
      "Epoch 742/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1617 - acc: 0.9433 - val_loss: 0.4035 - val_acc: 0.8730\n",
      "Epoch 743/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1619 - acc: 0.9415 - val_loss: 0.4125 - val_acc: 0.8730\n",
      "Epoch 744/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1617 - acc: 0.9409 - val_loss: 0.4062 - val_acc: 0.8730\n",
      "Epoch 745/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1615 - acc: 0.9427 - val_loss: 0.4049 - val_acc: 0.8730\n",
      "Epoch 746/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1623 - acc: 0.9421 - val_loss: 0.4096 - val_acc: 0.8677\n",
      "Epoch 747/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1622 - acc: 0.9433 - val_loss: 0.4128 - val_acc: 0.8730\n",
      "Epoch 748/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1634 - acc: 0.9433 - val_loss: 0.4168 - val_acc: 0.8730\n",
      "Epoch 749/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1620 - acc: 0.9409 - val_loss: 0.4205 - val_acc: 0.8730\n",
      "Epoch 750/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1623 - acc: 0.9421 - val_loss: 0.4038 - val_acc: 0.8677\n",
      "Epoch 751/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1616 - acc: 0.9421 - val_loss: 0.4048 - val_acc: 0.8730\n",
      "Epoch 752/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1630 - acc: 0.9403 - val_loss: 0.4074 - val_acc: 0.8730\n",
      "Epoch 753/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1616 - acc: 0.9427 - val_loss: 0.4020 - val_acc: 0.8677\n",
      "Epoch 754/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1619 - acc: 0.9450 - val_loss: 0.4060 - val_acc: 0.8730\n",
      "Epoch 755/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1607 - acc: 0.9409 - val_loss: 0.3996 - val_acc: 0.8730\n",
      "Epoch 756/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1619 - acc: 0.9415 - val_loss: 0.4133 - val_acc: 0.8783\n",
      "Epoch 757/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1627 - acc: 0.9421 - val_loss: 0.4057 - val_acc: 0.8730\n",
      "Epoch 758/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1622 - acc: 0.9415 - val_loss: 0.4135 - val_acc: 0.8677\n",
      "Epoch 759/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1620 - acc: 0.9439 - val_loss: 0.4170 - val_acc: 0.8624\n",
      "Epoch 760/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1619 - acc: 0.9409 - val_loss: 0.4076 - val_acc: 0.8730\n",
      "Epoch 761/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1610 - acc: 0.9415 - val_loss: 0.4067 - val_acc: 0.8730\n",
      "Epoch 762/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1617 - acc: 0.9415 - val_loss: 0.4097 - val_acc: 0.8730\n",
      "Epoch 763/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1615 - acc: 0.9421 - val_loss: 0.4061 - val_acc: 0.8730\n",
      "Epoch 764/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1609 - acc: 0.9433 - val_loss: 0.4022 - val_acc: 0.8730\n",
      "Epoch 765/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1615 - acc: 0.9403 - val_loss: 0.4120 - val_acc: 0.8677\n",
      "Epoch 766/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1614 - acc: 0.9439 - val_loss: 0.4107 - val_acc: 0.8677\n",
      "Epoch 767/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1611 - acc: 0.9421 - val_loss: 0.4016 - val_acc: 0.8677\n",
      "Epoch 768/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1609 - acc: 0.9433 - val_loss: 0.4243 - val_acc: 0.8677\n",
      "Epoch 769/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1622 - acc: 0.9427 - val_loss: 0.4058 - val_acc: 0.8677\n",
      "Epoch 770/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1616 - acc: 0.9397 - val_loss: 0.4095 - val_acc: 0.8677\n",
      "Epoch 771/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1619 - acc: 0.9421 - val_loss: 0.4054 - val_acc: 0.8677\n",
      "Epoch 772/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1615 - acc: 0.9421 - val_loss: 0.4051 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 773/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1618 - acc: 0.9421 - val_loss: 0.4184 - val_acc: 0.8730\n",
      "Epoch 774/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1617 - acc: 0.9421 - val_loss: 0.4082 - val_acc: 0.8783\n",
      "Epoch 775/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1607 - acc: 0.9439 - val_loss: 0.4121 - val_acc: 0.8730\n",
      "Epoch 776/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1606 - acc: 0.9427 - val_loss: 0.4012 - val_acc: 0.8730\n",
      "Epoch 777/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1620 - acc: 0.9433 - val_loss: 0.4034 - val_acc: 0.8783\n",
      "Epoch 778/1000\n",
      "1692/1692 [==============================] - 0s 48us/step - loss: 0.1617 - acc: 0.9450 - val_loss: 0.4040 - val_acc: 0.8677\n",
      "Epoch 779/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1607 - acc: 0.9444 - val_loss: 0.4110 - val_acc: 0.8730\n",
      "Epoch 780/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1608 - acc: 0.9427 - val_loss: 0.4213 - val_acc: 0.8730\n",
      "Epoch 781/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1618 - acc: 0.9421 - val_loss: 0.4159 - val_acc: 0.8783\n",
      "Epoch 782/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1618 - acc: 0.9409 - val_loss: 0.4114 - val_acc: 0.8730\n",
      "Epoch 783/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1610 - acc: 0.9415 - val_loss: 0.4188 - val_acc: 0.8730\n",
      "Epoch 784/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1612 - acc: 0.9427 - val_loss: 0.4079 - val_acc: 0.8730\n",
      "Epoch 785/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1618 - acc: 0.9421 - val_loss: 0.4076 - val_acc: 0.8783\n",
      "Epoch 786/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1606 - acc: 0.9391 - val_loss: 0.4117 - val_acc: 0.8677\n",
      "Epoch 787/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1613 - acc: 0.9415 - val_loss: 0.4080 - val_acc: 0.8677\n",
      "Epoch 788/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1608 - acc: 0.9444 - val_loss: 0.4111 - val_acc: 0.8730\n",
      "Epoch 789/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1605 - acc: 0.9409 - val_loss: 0.4103 - val_acc: 0.8730\n",
      "Epoch 790/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1612 - acc: 0.9421 - val_loss: 0.4057 - val_acc: 0.8730\n",
      "Epoch 791/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1612 - acc: 0.9415 - val_loss: 0.4049 - val_acc: 0.8677\n",
      "Epoch 792/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1613 - acc: 0.9427 - val_loss: 0.4039 - val_acc: 0.8783\n",
      "Epoch 793/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1611 - acc: 0.9403 - val_loss: 0.4087 - val_acc: 0.8730\n",
      "Epoch 794/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1608 - acc: 0.9427 - val_loss: 0.4099 - val_acc: 0.8677\n",
      "Epoch 795/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1605 - acc: 0.9415 - val_loss: 0.4042 - val_acc: 0.8730\n",
      "Epoch 796/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1598 - acc: 0.9433 - val_loss: 0.4106 - val_acc: 0.8783\n",
      "Epoch 797/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1611 - acc: 0.9409 - val_loss: 0.4086 - val_acc: 0.8730\n",
      "Epoch 798/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1613 - acc: 0.9439 - val_loss: 0.4154 - val_acc: 0.8783\n",
      "Epoch 799/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1598 - acc: 0.9439 - val_loss: 0.4104 - val_acc: 0.8677\n",
      "Epoch 800/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1612 - acc: 0.9403 - val_loss: 0.4097 - val_acc: 0.8677\n",
      "Epoch 801/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1600 - acc: 0.9433 - val_loss: 0.4210 - val_acc: 0.8783\n",
      "Epoch 802/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1602 - acc: 0.9415 - val_loss: 0.4142 - val_acc: 0.8730\n",
      "Epoch 803/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1609 - acc: 0.9415 - val_loss: 0.4061 - val_acc: 0.8730\n",
      "Epoch 804/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1609 - acc: 0.9439 - val_loss: 0.4198 - val_acc: 0.8783\n",
      "Epoch 805/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1611 - acc: 0.9439 - val_loss: 0.4113 - val_acc: 0.8677\n",
      "Epoch 806/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1604 - acc: 0.9415 - val_loss: 0.4094 - val_acc: 0.8730\n",
      "Epoch 807/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1610 - acc: 0.9427 - val_loss: 0.4133 - val_acc: 0.8783\n",
      "Epoch 808/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1612 - acc: 0.9427 - val_loss: 0.4062 - val_acc: 0.8730\n",
      "Epoch 809/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1604 - acc: 0.9427 - val_loss: 0.4185 - val_acc: 0.8730\n",
      "Epoch 810/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1599 - acc: 0.9421 - val_loss: 0.4218 - val_acc: 0.8677\n",
      "Epoch 811/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1604 - acc: 0.9421 - val_loss: 0.4114 - val_acc: 0.8730\n",
      "Epoch 812/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1600 - acc: 0.9391 - val_loss: 0.4164 - val_acc: 0.8730\n",
      "Epoch 813/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1596 - acc: 0.9427 - val_loss: 0.4215 - val_acc: 0.8783\n",
      "Epoch 814/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1600 - acc: 0.9421 - val_loss: 0.4030 - val_acc: 0.8730\n",
      "Epoch 815/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1599 - acc: 0.9421 - val_loss: 0.4229 - val_acc: 0.8783\n",
      "Epoch 816/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1605 - acc: 0.9397 - val_loss: 0.4091 - val_acc: 0.8730\n",
      "Epoch 817/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1611 - acc: 0.9433 - val_loss: 0.4075 - val_acc: 0.8677\n",
      "Epoch 818/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1599 - acc: 0.9433 - val_loss: 0.4103 - val_acc: 0.8783\n",
      "Epoch 819/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1608 - acc: 0.9421 - val_loss: 0.4083 - val_acc: 0.8677\n",
      "Epoch 820/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1605 - acc: 0.9421 - val_loss: 0.4075 - val_acc: 0.8677\n",
      "Epoch 821/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1601 - acc: 0.9433 - val_loss: 0.4150 - val_acc: 0.8624\n",
      "Epoch 822/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1600 - acc: 0.9433 - val_loss: 0.4203 - val_acc: 0.8730\n",
      "Epoch 823/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1601 - acc: 0.9427 - val_loss: 0.4080 - val_acc: 0.8783\n",
      "Epoch 824/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1605 - acc: 0.9421 - val_loss: 0.4199 - val_acc: 0.8730\n",
      "Epoch 825/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1593 - acc: 0.9433 - val_loss: 0.4077 - val_acc: 0.8783\n",
      "Epoch 826/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1609 - acc: 0.9409 - val_loss: 0.4150 - val_acc: 0.8730\n",
      "Epoch 827/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1600 - acc: 0.9421 - val_loss: 0.4100 - val_acc: 0.8730\n",
      "Epoch 828/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1594 - acc: 0.9427 - val_loss: 0.4116 - val_acc: 0.8677\n",
      "Epoch 829/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1607 - acc: 0.9439 - val_loss: 0.4163 - val_acc: 0.8677\n",
      "Epoch 830/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1596 - acc: 0.9427 - val_loss: 0.4113 - val_acc: 0.8730\n",
      "Epoch 831/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1594 - acc: 0.9433 - val_loss: 0.4220 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 832/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1608 - acc: 0.9415 - val_loss: 0.4174 - val_acc: 0.8783\n",
      "Epoch 833/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1604 - acc: 0.9409 - val_loss: 0.4145 - val_acc: 0.8730\n",
      "Epoch 834/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1593 - acc: 0.9427 - val_loss: 0.4219 - val_acc: 0.8783\n",
      "Epoch 835/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1605 - acc: 0.9433 - val_loss: 0.4091 - val_acc: 0.8730\n",
      "Epoch 836/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1591 - acc: 0.9427 - val_loss: 0.4061 - val_acc: 0.8677\n",
      "Epoch 837/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1605 - acc: 0.9444 - val_loss: 0.4133 - val_acc: 0.8730\n",
      "Epoch 838/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1597 - acc: 0.9409 - val_loss: 0.4201 - val_acc: 0.8730\n",
      "Epoch 839/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1602 - acc: 0.9403 - val_loss: 0.4129 - val_acc: 0.8783\n",
      "Epoch 840/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1604 - acc: 0.9433 - val_loss: 0.4138 - val_acc: 0.8783\n",
      "Epoch 841/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1595 - acc: 0.9427 - val_loss: 0.4071 - val_acc: 0.8677\n",
      "Epoch 842/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1605 - acc: 0.9397 - val_loss: 0.4135 - val_acc: 0.8783\n",
      "Epoch 843/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1593 - acc: 0.9427 - val_loss: 0.4154 - val_acc: 0.8730\n",
      "Epoch 844/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1606 - acc: 0.9433 - val_loss: 0.4087 - val_acc: 0.8677\n",
      "Epoch 845/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1597 - acc: 0.9427 - val_loss: 0.4171 - val_acc: 0.8677\n",
      "Epoch 846/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1591 - acc: 0.9403 - val_loss: 0.4158 - val_acc: 0.8677\n",
      "Epoch 847/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1599 - acc: 0.9409 - val_loss: 0.4164 - val_acc: 0.8677\n",
      "Epoch 848/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1591 - acc: 0.9456 - val_loss: 0.4129 - val_acc: 0.8783\n",
      "Epoch 849/1000\n",
      "1692/1692 [==============================] - 0s 49us/step - loss: 0.1598 - acc: 0.9409 - val_loss: 0.4139 - val_acc: 0.8624\n",
      "Epoch 850/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1603 - acc: 0.9427 - val_loss: 0.4135 - val_acc: 0.8730\n",
      "Epoch 851/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1592 - acc: 0.9439 - val_loss: 0.4121 - val_acc: 0.8730\n",
      "Epoch 852/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1596 - acc: 0.9409 - val_loss: 0.4144 - val_acc: 0.8730\n",
      "Epoch 853/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1595 - acc: 0.9403 - val_loss: 0.4231 - val_acc: 0.8730\n",
      "Epoch 854/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1596 - acc: 0.9421 - val_loss: 0.4088 - val_acc: 0.8783\n",
      "Epoch 855/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1590 - acc: 0.9421 - val_loss: 0.4229 - val_acc: 0.8783\n",
      "Epoch 856/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1598 - acc: 0.9439 - val_loss: 0.4140 - val_acc: 0.8730\n",
      "Epoch 857/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1591 - acc: 0.9433 - val_loss: 0.4203 - val_acc: 0.8783\n",
      "Epoch 858/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1594 - acc: 0.9433 - val_loss: 0.4160 - val_acc: 0.8730\n",
      "Epoch 859/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1594 - acc: 0.9409 - val_loss: 0.4218 - val_acc: 0.8783\n",
      "Epoch 860/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1599 - acc: 0.9456 - val_loss: 0.4236 - val_acc: 0.8783\n",
      "Epoch 861/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1592 - acc: 0.9433 - val_loss: 0.4258 - val_acc: 0.8783\n",
      "Epoch 862/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1602 - acc: 0.9427 - val_loss: 0.4193 - val_acc: 0.8677\n",
      "Epoch 863/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1590 - acc: 0.9462 - val_loss: 0.4132 - val_acc: 0.8730\n",
      "Epoch 864/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1595 - acc: 0.9433 - val_loss: 0.4222 - val_acc: 0.8783\n",
      "Epoch 865/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1600 - acc: 0.9421 - val_loss: 0.4165 - val_acc: 0.8730\n",
      "Epoch 866/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1598 - acc: 0.9444 - val_loss: 0.4124 - val_acc: 0.8783\n",
      "Epoch 867/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1598 - acc: 0.9415 - val_loss: 0.4139 - val_acc: 0.8783\n",
      "Epoch 868/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1584 - acc: 0.9439 - val_loss: 0.4227 - val_acc: 0.8836\n",
      "Epoch 869/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1600 - acc: 0.9433 - val_loss: 0.4137 - val_acc: 0.8783\n",
      "Epoch 870/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1587 - acc: 0.9415 - val_loss: 0.4113 - val_acc: 0.8783\n",
      "Epoch 871/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1597 - acc: 0.9421 - val_loss: 0.4129 - val_acc: 0.8783\n",
      "Epoch 872/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1586 - acc: 0.9415 - val_loss: 0.4114 - val_acc: 0.8783\n",
      "Epoch 873/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1591 - acc: 0.9450 - val_loss: 0.4180 - val_acc: 0.8783\n",
      "Epoch 874/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1589 - acc: 0.9444 - val_loss: 0.4183 - val_acc: 0.8624\n",
      "Epoch 875/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1582 - acc: 0.9450 - val_loss: 0.4226 - val_acc: 0.8783\n",
      "Epoch 876/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1595 - acc: 0.9433 - val_loss: 0.4200 - val_acc: 0.8783\n",
      "Epoch 877/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1585 - acc: 0.9421 - val_loss: 0.4099 - val_acc: 0.8783\n",
      "Epoch 878/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1589 - acc: 0.9433 - val_loss: 0.4073 - val_acc: 0.8730\n",
      "Epoch 879/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1588 - acc: 0.9450 - val_loss: 0.4216 - val_acc: 0.8783\n",
      "Epoch 880/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1593 - acc: 0.9427 - val_loss: 0.4176 - val_acc: 0.8783\n",
      "Epoch 881/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1597 - acc: 0.9415 - val_loss: 0.4183 - val_acc: 0.8783\n",
      "Epoch 882/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1585 - acc: 0.9421 - val_loss: 0.4093 - val_acc: 0.8836\n",
      "Epoch 883/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1587 - acc: 0.9439 - val_loss: 0.4190 - val_acc: 0.8783\n",
      "Epoch 884/1000\n",
      "1692/1692 [==============================] - 0s 73us/step - loss: 0.1582 - acc: 0.9433 - val_loss: 0.4072 - val_acc: 0.8783\n",
      "Epoch 885/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1581 - acc: 0.9439 - val_loss: 0.4348 - val_acc: 0.8783\n",
      "Epoch 886/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1591 - acc: 0.9433 - val_loss: 0.4183 - val_acc: 0.8783\n",
      "Epoch 887/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1579 - acc: 0.9444 - val_loss: 0.4290 - val_acc: 0.8730\n",
      "Epoch 888/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1588 - acc: 0.9444 - val_loss: 0.4154 - val_acc: 0.8783\n",
      "Epoch 889/1000\n",
      "1692/1692 [==============================] - 0s 74us/step - loss: 0.1593 - acc: 0.9415 - val_loss: 0.4200 - val_acc: 0.8730\n",
      "Epoch 890/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1580 - acc: 0.9427 - val_loss: 0.4258 - val_acc: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 891/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1589 - acc: 0.9415 - val_loss: 0.4186 - val_acc: 0.8783\n",
      "Epoch 892/1000\n",
      "1692/1692 [==============================] - 0s 76us/step - loss: 0.1583 - acc: 0.9439 - val_loss: 0.4172 - val_acc: 0.8730\n",
      "Epoch 893/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1577 - acc: 0.9456 - val_loss: 0.4152 - val_acc: 0.8730\n",
      "Epoch 894/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1579 - acc: 0.9433 - val_loss: 0.4206 - val_acc: 0.8730\n",
      "Epoch 895/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1581 - acc: 0.9421 - val_loss: 0.4254 - val_acc: 0.8730\n",
      "Epoch 896/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1592 - acc: 0.9439 - val_loss: 0.4179 - val_acc: 0.8783\n",
      "Epoch 897/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1584 - acc: 0.9444 - val_loss: 0.4191 - val_acc: 0.8730\n",
      "Epoch 898/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1580 - acc: 0.9427 - val_loss: 0.4196 - val_acc: 0.8783\n",
      "Epoch 899/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1587 - acc: 0.9456 - val_loss: 0.4147 - val_acc: 0.8730\n",
      "Epoch 900/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1579 - acc: 0.9397 - val_loss: 0.4119 - val_acc: 0.8783\n",
      "Epoch 901/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1581 - acc: 0.9427 - val_loss: 0.4268 - val_acc: 0.8730\n",
      "Epoch 902/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1584 - acc: 0.9415 - val_loss: 0.4201 - val_acc: 0.8730\n",
      "Epoch 903/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1582 - acc: 0.9421 - val_loss: 0.4248 - val_acc: 0.8730\n",
      "Epoch 904/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1590 - acc: 0.9409 - val_loss: 0.4258 - val_acc: 0.8730\n",
      "Epoch 905/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1586 - acc: 0.9421 - val_loss: 0.4097 - val_acc: 0.8730\n",
      "Epoch 906/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1575 - acc: 0.9433 - val_loss: 0.4212 - val_acc: 0.8783\n",
      "Epoch 907/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1586 - acc: 0.9433 - val_loss: 0.4210 - val_acc: 0.8783\n",
      "Epoch 908/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1587 - acc: 0.9409 - val_loss: 0.4219 - val_acc: 0.8783\n",
      "Epoch 909/1000\n",
      "1692/1692 [==============================] - 0s 77us/step - loss: 0.1581 - acc: 0.9421 - val_loss: 0.4141 - val_acc: 0.8730\n",
      "Epoch 910/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1581 - acc: 0.9421 - val_loss: 0.4117 - val_acc: 0.8677\n",
      "Epoch 911/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1581 - acc: 0.9409 - val_loss: 0.4135 - val_acc: 0.8783\n",
      "Epoch 912/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1573 - acc: 0.9439 - val_loss: 0.4230 - val_acc: 0.8783\n",
      "Epoch 913/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1581 - acc: 0.9403 - val_loss: 0.4092 - val_acc: 0.8783\n",
      "Epoch 914/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1583 - acc: 0.9433 - val_loss: 0.4151 - val_acc: 0.8836\n",
      "Epoch 915/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1581 - acc: 0.9433 - val_loss: 0.4176 - val_acc: 0.8783\n",
      "Epoch 916/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1577 - acc: 0.9415 - val_loss: 0.4250 - val_acc: 0.8677\n",
      "Epoch 917/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1587 - acc: 0.9433 - val_loss: 0.4210 - val_acc: 0.8783\n",
      "Epoch 918/1000\n",
      "1692/1692 [==============================] - 0s 68us/step - loss: 0.1581 - acc: 0.9433 - val_loss: 0.4162 - val_acc: 0.8783\n",
      "Epoch 919/1000\n",
      "1692/1692 [==============================] - 0s 72us/step - loss: 0.1579 - acc: 0.9439 - val_loss: 0.4158 - val_acc: 0.8783\n",
      "Epoch 920/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1579 - acc: 0.9421 - val_loss: 0.4283 - val_acc: 0.8783\n",
      "Epoch 921/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1580 - acc: 0.9450 - val_loss: 0.4207 - val_acc: 0.8730\n",
      "Epoch 922/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1579 - acc: 0.9421 - val_loss: 0.4153 - val_acc: 0.8836\n",
      "Epoch 923/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1576 - acc: 0.9421 - val_loss: 0.4187 - val_acc: 0.8783\n",
      "Epoch 924/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1576 - acc: 0.9433 - val_loss: 0.4251 - val_acc: 0.8783\n",
      "Epoch 925/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1575 - acc: 0.9456 - val_loss: 0.4146 - val_acc: 0.8783\n",
      "Epoch 926/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1573 - acc: 0.9421 - val_loss: 0.4067 - val_acc: 0.8730\n",
      "Epoch 927/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1579 - acc: 0.9439 - val_loss: 0.4141 - val_acc: 0.8836\n",
      "Epoch 928/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1578 - acc: 0.9421 - val_loss: 0.4232 - val_acc: 0.8730\n",
      "Epoch 929/1000\n",
      "1692/1692 [==============================] - 0s 70us/step - loss: 0.1582 - acc: 0.9433 - val_loss: 0.4215 - val_acc: 0.8730\n",
      "Epoch 930/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1585 - acc: 0.9450 - val_loss: 0.4195 - val_acc: 0.8836\n",
      "Epoch 931/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1576 - acc: 0.9439 - val_loss: 0.4194 - val_acc: 0.8783\n",
      "Epoch 932/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1571 - acc: 0.9415 - val_loss: 0.4045 - val_acc: 0.8836\n",
      "Epoch 933/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1576 - acc: 0.9415 - val_loss: 0.4235 - val_acc: 0.8730\n",
      "Epoch 934/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1574 - acc: 0.9444 - val_loss: 0.4106 - val_acc: 0.8836\n",
      "Epoch 935/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1575 - acc: 0.9456 - val_loss: 0.4199 - val_acc: 0.8783\n",
      "Epoch 936/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1562 - acc: 0.9468 - val_loss: 0.4268 - val_acc: 0.8783\n",
      "Epoch 937/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1582 - acc: 0.9433 - val_loss: 0.4104 - val_acc: 0.8836\n",
      "Epoch 938/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1576 - acc: 0.9433 - val_loss: 0.4196 - val_acc: 0.8730\n",
      "Epoch 939/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1576 - acc: 0.9409 - val_loss: 0.4154 - val_acc: 0.8730\n",
      "Epoch 940/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1588 - acc: 0.9433 - val_loss: 0.4089 - val_acc: 0.8836\n",
      "Epoch 941/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1569 - acc: 0.9427 - val_loss: 0.4166 - val_acc: 0.8783\n",
      "Epoch 942/1000\n",
      "1692/1692 [==============================] - 0s 75us/step - loss: 0.1577 - acc: 0.9427 - val_loss: 0.4173 - val_acc: 0.8836\n",
      "Epoch 943/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1582 - acc: 0.9433 - val_loss: 0.4075 - val_acc: 0.8730\n",
      "Epoch 944/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1576 - acc: 0.9415 - val_loss: 0.4081 - val_acc: 0.8783\n",
      "Epoch 945/1000\n",
      "1692/1692 [==============================] - 0s 65us/step - loss: 0.1582 - acc: 0.9421 - val_loss: 0.4135 - val_acc: 0.8836\n",
      "Epoch 946/1000\n",
      "1692/1692 [==============================] - ETA: 0s - loss: 0.1628 - acc: 0.945 - 0s 52us/step - loss: 0.1570 - acc: 0.9409 - val_loss: 0.4219 - val_acc: 0.8783\n",
      "Epoch 947/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1572 - acc: 0.9450 - val_loss: 0.4143 - val_acc: 0.8783\n",
      "Epoch 948/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1575 - acc: 0.9439 - val_loss: 0.4220 - val_acc: 0.8783\n",
      "Epoch 949/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1564 - acc: 0.9415 - val_loss: 0.4258 - val_acc: 0.8783\n",
      "Epoch 950/1000\n",
      "1692/1692 [==============================] - 0s 66us/step - loss: 0.1576 - acc: 0.9415 - val_loss: 0.4216 - val_acc: 0.8783\n",
      "Epoch 951/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1572 - acc: 0.9433 - val_loss: 0.4191 - val_acc: 0.8836\n",
      "Epoch 952/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1576 - acc: 0.9397 - val_loss: 0.4126 - val_acc: 0.8836\n",
      "Epoch 953/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1576 - acc: 0.9439 - val_loss: 0.4151 - val_acc: 0.8783\n",
      "Epoch 954/1000\n",
      "1692/1692 [==============================] - 0s 60us/step - loss: 0.1570 - acc: 0.9427 - val_loss: 0.4292 - val_acc: 0.8730\n",
      "Epoch 955/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1573 - acc: 0.9439 - val_loss: 0.4275 - val_acc: 0.8730\n",
      "Epoch 956/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1567 - acc: 0.9462 - val_loss: 0.4158 - val_acc: 0.8836\n",
      "Epoch 957/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1577 - acc: 0.9444 - val_loss: 0.4173 - val_acc: 0.8783\n",
      "Epoch 958/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1573 - acc: 0.9409 - val_loss: 0.4159 - val_acc: 0.8836\n",
      "Epoch 959/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.4240 - val_acc: 0.8783\n",
      "Epoch 960/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.4165 - val_acc: 0.8783\n",
      "Epoch 961/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1567 - acc: 0.9433 - val_loss: 0.4130 - val_acc: 0.8783\n",
      "Epoch 962/1000\n",
      "1692/1692 [==============================] - 0s 71us/step - loss: 0.1566 - acc: 0.9439 - val_loss: 0.4157 - val_acc: 0.8836\n",
      "Epoch 963/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1573 - acc: 0.9444 - val_loss: 0.4150 - val_acc: 0.8783\n",
      "Epoch 964/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1565 - acc: 0.9427 - val_loss: 0.4258 - val_acc: 0.8783\n",
      "Epoch 965/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1578 - acc: 0.9427 - val_loss: 0.4216 - val_acc: 0.8836\n",
      "Epoch 966/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1567 - acc: 0.9427 - val_loss: 0.4162 - val_acc: 0.8783\n",
      "Epoch 967/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1563 - acc: 0.9468 - val_loss: 0.4213 - val_acc: 0.8783\n",
      "Epoch 968/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1572 - acc: 0.9415 - val_loss: 0.4192 - val_acc: 0.8783\n",
      "Epoch 969/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1561 - acc: 0.9450 - val_loss: 0.4171 - val_acc: 0.8783\n",
      "Epoch 970/1000\n",
      "1692/1692 [==============================] - 0s 51us/step - loss: 0.1579 - acc: 0.9433 - val_loss: 0.4169 - val_acc: 0.8836\n",
      "Epoch 971/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1574 - acc: 0.9409 - val_loss: 0.4240 - val_acc: 0.8836\n",
      "Epoch 972/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1574 - acc: 0.9415 - val_loss: 0.4212 - val_acc: 0.8677\n",
      "Epoch 973/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1571 - acc: 0.9433 - val_loss: 0.4288 - val_acc: 0.8783\n",
      "Epoch 974/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1573 - acc: 0.9421 - val_loss: 0.4205 - val_acc: 0.8730\n",
      "Epoch 975/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1574 - acc: 0.9427 - val_loss: 0.4235 - val_acc: 0.8730\n",
      "Epoch 976/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1565 - acc: 0.9415 - val_loss: 0.4141 - val_acc: 0.8783\n",
      "Epoch 977/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1564 - acc: 0.9444 - val_loss: 0.4261 - val_acc: 0.8730\n",
      "Epoch 978/1000\n",
      "1692/1692 [==============================] - 0s 52us/step - loss: 0.1557 - acc: 0.9439 - val_loss: 0.4271 - val_acc: 0.8783\n",
      "Epoch 979/1000\n",
      "1692/1692 [==============================] - 0s 50us/step - loss: 0.1564 - acc: 0.9403 - val_loss: 0.4168 - val_acc: 0.8783\n",
      "Epoch 980/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1568 - acc: 0.9456 - val_loss: 0.4160 - val_acc: 0.8836\n",
      "Epoch 981/1000\n",
      "1692/1692 [==============================] - 0s 61us/step - loss: 0.1566 - acc: 0.9433 - val_loss: 0.4189 - val_acc: 0.8836\n",
      "Epoch 982/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1551 - acc: 0.9456 - val_loss: 0.4188 - val_acc: 0.8783\n",
      "Epoch 983/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1568 - acc: 0.9433 - val_loss: 0.4171 - val_acc: 0.8783\n",
      "Epoch 984/1000\n",
      "1692/1692 [==============================] - 0s 63us/step - loss: 0.1562 - acc: 0.9427 - val_loss: 0.4166 - val_acc: 0.8783\n",
      "Epoch 985/1000\n",
      "1692/1692 [==============================] - 0s 67us/step - loss: 0.1573 - acc: 0.9444 - val_loss: 0.4212 - val_acc: 0.8836\n",
      "Epoch 986/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1569 - acc: 0.9433 - val_loss: 0.4192 - val_acc: 0.8783\n",
      "Epoch 987/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1559 - acc: 0.9427 - val_loss: 0.4222 - val_acc: 0.8836\n",
      "Epoch 988/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1562 - acc: 0.9474 - val_loss: 0.4173 - val_acc: 0.8836\n",
      "Epoch 989/1000\n",
      "1692/1692 [==============================] - 0s 54us/step - loss: 0.1567 - acc: 0.9433 - val_loss: 0.4220 - val_acc: 0.8836\n",
      "Epoch 990/1000\n",
      "1692/1692 [==============================] - 0s 62us/step - loss: 0.1556 - acc: 0.9415 - val_loss: 0.4149 - val_acc: 0.8783\n",
      "Epoch 991/1000\n",
      "1692/1692 [==============================] - 0s 53us/step - loss: 0.1570 - acc: 0.9427 - val_loss: 0.4203 - val_acc: 0.8783\n",
      "Epoch 992/1000\n",
      "1692/1692 [==============================] - 0s 64us/step - loss: 0.1565 - acc: 0.9450 - val_loss: 0.4263 - val_acc: 0.8889\n",
      "Epoch 993/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1570 - acc: 0.9433 - val_loss: 0.4195 - val_acc: 0.8836\n",
      "Epoch 994/1000\n",
      "1692/1692 [==============================] - 0s 59us/step - loss: 0.1560 - acc: 0.9433 - val_loss: 0.4136 - val_acc: 0.8836\n",
      "Epoch 995/1000\n",
      "1692/1692 [==============================] - 0s 69us/step - loss: 0.1562 - acc: 0.9427 - val_loss: 0.4186 - val_acc: 0.8783\n",
      "Epoch 996/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1563 - acc: 0.9433 - val_loss: 0.4273 - val_acc: 0.8730\n",
      "Epoch 997/1000\n",
      "1692/1692 [==============================] - 0s 58us/step - loss: 0.1560 - acc: 0.9456 - val_loss: 0.4184 - val_acc: 0.8836\n",
      "Epoch 998/1000\n",
      "1692/1692 [==============================] - 0s 56us/step - loss: 0.1565 - acc: 0.9462 - val_loss: 0.4239 - val_acc: 0.8889\n",
      "Epoch 999/1000\n",
      "1692/1692 [==============================] - 0s 57us/step - loss: 0.1563 - acc: 0.9444 - val_loss: 0.4210 - val_acc: 0.8836\n",
      "Epoch 1000/1000\n",
      "1692/1692 [==============================] - 0s 55us/step - loss: 0.1553 - acc: 0.9450 - val_loss: 0.4268 - val_acc: 0.8783\n"
     ]
    }
   ],
   "source": [
    "# Model1 Definition\n",
    "\n",
    "optimizer_var = ['rmsprop', 'adagrad', 'SGD', 'adamax', 'adadelta']\n",
    "hum_sub_opt_accuracy = []\n",
    "for i in range(len(optimizer_var)):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(12, input_dim=9, activation='relu'))\n",
    "    model1.add(Dense(8, activation = 'relu'))\n",
    "    model1.add(Dense(1, activation='sigmoid'))\n",
    "    model1.compile(optimizer=optimizer_var[i], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model1, iterating on the data in batches of 32 samples\n",
    "    summary = model1.fit(x_train_human_sub, y_train_human_sub, validation_split = 0.1, nb_epoch=1000, batch_size=32)\n",
    "    scores1 = model1.evaluate(x=x_test_human_sub, y=y_test_human_sub, verbose=0)\n",
    "    scores_acc1 = scores[1]\n",
    "    hum_sub_opt_accuracy.append(scores_acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xu8VXWd//HXOziIhoJchlQQLDWvIHi8pOYNK3VMQ1JzzAtOMf2KrPllZSNNZnezi2Rj6WiIKSROMnYzHZIcMQsUVBBTJJQjggiCEGFcPvPH93tweziXvWBv9jnwfj4e+8Fa33X77C9nr8/+fr9rra2IwMzMrIi31DoAMzPreJw8zMysMCcPMzMrzMnDzMwKc/IwM7PCnDzMzKwwJw/rUCTtLWm1pE5buP1qSW+vdFxmOxonD6sqSZdIelLSGkmLJd0gqUeB7RdIOqVxPiJeiIhuEbFhS+LJ287fkm3LIamrpBWSTm5m2fck3bWF+71KUkg6cuujNNt6Th5WNZI+A3wL+CzQHTgaGADcL6lLLWOrFEmdS+cjYi3wM+CiJut1As4Hbt2CYwi4EFgOXLzFwW4BJT5P2OYiwi+/Kv4CdgNWA+c2Ke8GvAxcmuevAu4inXBXAY8Bg/Oy24CNwN/yvj4HDAQC6JzXmQp8FXg4r/MLoBdwO/AaMB0YWHL8APYF9szrN77WpI/DpvUuBeYCrwK/BQY02ccngGeBvzTz3o/J72WXkrLT8/tujPvzwIt5vT8Dw1qpy+NzHXwYWAZ0abL8oznWVcBTwNBc3h/4ObA0b3d9SZ3/tGT75ur0a8C0fNx9gZElx5gP/EuTGM4CZuU6fw44FTgHeLTJep8BJtf679OvrX/VPAC/ts9XPnmsbzwhNVl2KzAhT18FrAM+CNQBlwN/Aery8gXAKSXbNneimwe8g9S6eQp4BjgF6AyMB35Ssn0A+zYT0+0lMX0g7/PAvI8xwMNN9nE/0BPYuYX3/wzw4ZL5CcD38/Q7gYXAniXv6R2t1OXNwJ25fpYBZ5csO4eUhI4AlE/0A4BOwOPA94C3Al2B40rqvK3k8QJwcH7/dcA/5joWcAIp2TYmqSOBlcB7SL0ZewEHADuRWksHlhxrJjCi1n+ffm39y81Rq5bewCsRsb6ZZS/l5Y0ejYi7ImId8F3Sie7oAsf6SUQ8FxErgd8Az0XE/+RjTwKGtLaxpM+TTnaX5qJ/Ab4REXPzPr4OHCZpQMlm34iI5RHxtxZ2O57cdSVpN9I388Yuqw2kE+tBkuoiYkFEPNdCbLuQEsQduX7u4s1dVx8BromI6ZHMi4jnSSf0PYHPRsRfI2JtRDzUWj00MS4i5kTE+ohYFxG/ynUcEfF74D7g3XndfwZuiYj7I2JjRLwYEU9HxOukFuWH83s5mJSoflkgDmunnDysWl4BejcdE8j2yMsbLWyciIiNQAPpxFeuJSXTf2tmvltLG0o6DfgU8IGSRDAAuC4PfK8gfXsW6Rv1ZjG3YDxwkqS9SK2qeRExEyAi5gGfJrUAXpY0UVJL73c4qQX36zx/O3CapD55vj+pm6ip/sDzLSTvcrzp/Uk6TdIjkpbnOjmdN74AtBQDpIT5TyXjNnfmpGIdnJOHVcsfgNeBs0sLJb0VOA2YUlLcv2T5W4B+wKJcVLXHPkt6J+nkdm5ElJ4sF5L69HuUvHaOiIdL1mk1roh4Afhf4ALSSXN8k+V3RMRxpEQVpAsLmnMxKfm9IGkxqSVVRxp8b4z1Hc1stxDYu4Xk/Vdgl5L5tzX3FhonJO0E/BdwLdA3InqQkpnaiIGIeAT4O6mV8k+kcSzbDjh5WFXkLqQvAz+QdKqkOkkDSSe/Bt58Ejlc0tn5RPdpUtJ5JC9bAlT8vozclfTfwJhmunN+BHwhd7Mgqbukc7bgMLcCo4FjSS2GxmO/U9LJ+aS8ltQ62uzS49xqGQacARyWX4NJiaax6+o/gcslHZ6vjNo3d6/9idQ9+E1Jb82XEB+bt5kFHJ/vmekOfKGN99GF1M22FFifW2vvLVl+MzBS0jBJb5G0l6QDSpaPB64H1hfsOrN2zMnDqiYirgH+jfSN9TXgj6RvqcOadF38N3Ae6cqmC0kDwuvysm8AY3IX0uUVDG8oaeD6u/nGwdWSVue47yadoCdKeg2YTWotFXUXsDswJSJeKinfCfgmqetuMfAPpHpq6kJgVkTcFxGLG1/AWGCQpEMiYhLpyqg7SFdCTQZ6RroP5v2kAfQXSAn7vPz+7ieNRTwBPEobYxARsQq4jDRo/yqpBXFPyfI/ka7G+h5p4Pz3pBZVo9uAQ3CrY7uiCP8YlNWOpKtIVz99uNaxWHVI2pl0mfLQiHi21vFYZbjlYWbV9v+A6U4c25fmBtPMzCpC0gLSwPoHahyKVZi7rczMrDB3W5mZWWHbTbdV7969Y+DAgbUOw8ysQ3n00UdfiYg+ba/5ZttN8hg4cCAzZsyodRhmZh2KpOe3ZDt3W5mZWWFOHmZmVpiTh5mZFbbdjHmY2fZr3bp1NDQ0sHbt2lqH0mF17dqVfv36UVdXV5H9OXmYWbvX0NDArrvuysCBA0lPd7ciIoJly5bR0NDAPvvsU5F9utvKzNq9tWvX0qtXLyeOLSSJXr16VbTl5uRhZh2CE8fWqXT9OXmYmVlhTh5mZmW6++67kcTTTz9d61BqzgPmZrZdmfa2aaxbsm6z8rq+dRy7+NhmtijfhAkTOO6445g4cSJXXXXVVu2rJRs2bKBTp05V2XclueVhZtuV5hJHa+XlWr16NdOmTePmm29m4sSJm8qvueYaDj30UAYPHswVV1wBwLx58zjllFMYPHgwQ4cO5bnnnmPq1KmcccYZm7YbPXo048aNA9Ljla6++mqOO+44Jk2axE033cQRRxzB4MGDGTFiBGvWrAFgyZIlDB8+nMGDBzN48GAefvhhvvjFL3Lddddt2u+VV17J2LFjt+q9lsMtDzPrUJ799LOsnrV6i7adeeLMZsu7HdaN/b6/X6vbTp48mVNPPZX999+fnj178thjj7FkyRImT57MH//4R3bZZReWL18OwAUXXMAVV1zB8OHDWbt2LRs3bmThwoWt7r9r16489FD6ifdly5bx0Y9+FIAxY8Zw880388lPfpLLLruME044gbvvvpsNGzawevVq9txzT84++2w+9alPsXHjRiZOnMif/vSnolVTmJOHmVkZJkyYwKc//WkAPvShDzFhwgQ2btzIyJEj2WWXXQDo2bMnq1at4sUXX2T48OFASgrlOO+88zZNz549mzFjxrBixQpWr17N+973PgB+97vfMX78eAA6depE9+7d6d69O7169WLmzJksWbKEIUOG0KtXr4q975Y4eZhZh9JWC2Gqpra4bMjUIVt0zGXLlvG73/2O2bNnI4kNGzYgiREjRmx2CWxLP7DXuXNnNm7cuGm+6T0Xb33rWzdNX3LJJUyePJnBgwczbtw4pk6d2mp8H/nIRxg3bhyLFy/m0ksvLfjutozHPMzM2nDXXXdx0UUX8fzzz7NgwQIWLlzIPvvsQ8+ePbnllls2jUksX76c3XbbjX79+jF58mQAXn/9ddasWcOAAQN46qmneP3111m5ciVTpkxp8XirVq1ijz32YN26ddx+++2byocNG8YNN9wApIH11157DYDhw4dz7733Mn369E2tlGqrWvKQdIuklyXNbmG5JI2VNE/SE5KGNlm+m6QXJV1frRjNbPtT17f5Zze1VF6OCRMmbOqGajRixAgWLVrEmWeeSX19PYcddhjXXnstALfddhtjx45l0KBBHHPMMSxevJj+/ftz7rnnMmjQIC644AKGDGm5FfSVr3yFo446ive85z0ccMABm8qvu+46HnjgAQ499FAOP/xw5syZA0CXLl046aSTOPfcc7fZlVpV+w1zSccDq4HxEXFIM8tPBz4JnA4cBVwXEUeVLL8O6AMsj4jRbR2vvr4+/GNQZtunuXPncuCBB9Y6jHZr48aNDB06lEmTJrHffi136zVXj5IejYj6osesWssjIh4ElreyylmkxBIR8QjQQ9IeAJIOB/oC91UrPjOz7cFTTz3Fvvvuy7Bhw1pNHJVWywHzvYDSa9cagL0kLQG+A1wIDGttB5JGAaMA9t577yqFaWbWfh100EHMnz9/mx+3lgPmzT2lK4CPA7+OiNYvigYi4saIqI+I+j59Cv9+u5l1INXqYt9RVLr+atnyaAD6l8z3AxYB7wLeLenjQDegi6TVEXFFDWI0s3aga9euLFu2zI9l30KNv+dR7j0n5ahl8rgHGC1pImnAfGVEvARc0LiCpEuAeicOsx1bv379aGhoYOnSpbUOpcNq/CXBSqla8pA0ATgR6C2pAfgSUAcQET8Cfk260moesAYYWa1YzKxjq6urq9gv4FllVC15RMT5bSwP4BNtrDMOGFe5qMzMrBJ8h7mZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYVVLHpJukfSypNktLJeksZLmSXpC0tBcfpikP0iak8vPq1aMZma2ZarZ8hgHnNrK8tOA/fJrFHBDLl8DXBQRB+ftvy+pRxXjNDOzgjpXa8cR8aCkga2schYwPiICeERSD0l7RMQzJftYJOlloA+wolqxmplZMbUc89gLWFgy35DLNpF0JNAFeK65HUgaJWmGpBlLly6tWqBmZvZmtUweaqYsNi2U9gBuA0ZGxMbmdhARN0ZEfUTU9+nTp0phmplZU7VMHg1A/5L5fsAiAEm7Ab8CxkTEIzWIzczMWlHL5HEPcFG+6upoYGVEvCSpC3A3aTxkUg3jMzOzFlRtwFzSBOBEoLekBuBLQB1ARPwI+DVwOjCPdIXVyLzpucDxQC9Jl+SySyJiVrViNTOzYqp5tdX5bSwP4BPNlP8U+Gm14jIzs63nO8zNzKwwJw8zMyvMycPMzApz8jAzs8KcPMzMrDAnDzMzK8zJw8zMCnPyMDOzwpw8zMysMCcPMzMrzMnDzMwKc/IwM7PCnDzMzKwwJw8zMyvMycPMzApz8jAzs8KcPMzMrDAnDzMzK8zJw8zMCnPyMDOzwpw8zMysMCcPMzMrzMnDzMwKazN5SBotafdtEYyZmXUM5bQ83gZMl3SnpFMlqdpBmZlZ+9Zm8oiIMcB+wM3AJcCzkr4u6R1Vjs3MzNqpssY8IiKAxfm1HtgduEvSNVWMzczM2qnOba0g6TLgYuAV4D+Bz0bEOklvAZ4FPlfdEM3MrL1pM3kAvYGzI+L50sKI2CjpjOqEZWZm7Vk53Va/BpY3zkjaVdJRABExt1qBmZlZ+1VO8rgBWF0y/9dc1ipJt0h6WdLsFpZL0lhJ8yQ9IWloybKLJT2bXxeXEaOZmW1D5SQP5QFzIHVXUV531zjg1FaWn0a6ims/YBQ5IUnqCXwJOAo4EviS7zMxM2tfykke8yVdJqkuvz4FzG9ro4h4kJLurmacBYyP5BGgh6Q9gPcB90fE8oh4Fbif1pOQmZltY+Ukj48BxwAvAg2kFsGoChx7L2BhyXxDLmup3MzM2ok2u58i4mXgQ1U4dnN3qkcr5ZvvQBpFTmR777135SIzM7NWlXOfR1fgn4GDga6N5RFx6VYeuwHoXzLfD1iUy09sUj61uR1ExI3AjQD19fXNJhgzM6u8crqtbiM93+p9wO9JJ/NVFTj2PcBF+aqro4GVEfES8FvgvZJ2zwPl781lZmbWTpRz1dS+EXGOpLMi4lZJd1DGyVzSBFILorekBtIVVHUAEfEj0v0jpwPzgDXAyLxsuaSvANPzrq6OiNYG3s3MbBsrJ3msy/+ukHQI6flWA9vaKCLOb2N5AJ9oYdktwC1lxGZmZjVQTvK4MXcfjSF1NXUDvljVqMzMrF1rNXnkhx++lu+3eBB4+zaJahuZ9rZprFuybrPyur51HLv42BpE1L65vopxfRXj+iqm1vXV6oB5vpt8dNWjqJHmKr618h2d66sY11cxrq9ial1f5XRb3S/pcuBnpOdaAWlgu2pRtQMzT5xZ6xA6FNdXMa6vYlxf7U85yaPxfo7Swe1gO+vCMjOz8pVzh/k+2yKQ9mbI1CG1DqHdmaqpLS5zfW3O9VWM66uY1uprWyjnDvOLmiuPiPGVD8fMzDqCcu4wP6Lk9W7gKuDMKsa0zdT1rStUvqNzfRXj+irG9VVMretLJT/VUd4GUnfgtohoVwmkvr4+ZsyYUeswzMw6FEmPRkR90e3KaXk0tYb0A05mZraDKmfM4xe88Uj0twAHAXdWMygzM2vfyrlU99qS6fXA8xHRUKV4zMysAygnebwAvBQRawEk7SxpYEQsqGpkZmbWbpUz5jEJ2FgyvyGXmZnZDqqc5NE5Iv7eOJOnu1QvJDMza+/KSR5LJW26LFfSWcAr1QvJzMzau3LGPD4G3C7p+jzfADR717mZme0Yynm21XPA0ZK6kW4qrMTvl5uZWQfWZreVpK9L6hERqyNilaTdJX11WwRnZmbtUzljHqdFxIrGmfyrgqdXLyQzM2vvykkenSTt1DgjaWdgp1bWNzOz7Vw5A+Y/BaZI+kmeHwncWr2QzMysvStnwPwaSU8ApwAC7gUGVDswMzNrv8p9qu5i0l3mI4BhwNyqRWRmZu1eiy0PSfsDHwLOB5YBPyNdqnvSNorNzMzaqda6rZ4G/hd4f0TMA5D0r9skKjMza9da67YaQequekDSTZKGkcY8zMxsB9di8oiIuyPiPOAAYCrwr0BfSTdIeu82is/MzNqhNgfMI+KvEXF7RJwB9ANmAVdUPTIzM2u3Cv2GeUQsj4gfR8TJ1QrIzMzav0LJw8zMDKqcPCSdKunPkuZJ2qyrS9IASVMkPSFpqqR+JcuukTRH0lxJYyV5sN7MrJ2oWvKQ1An4IXAacBBwvqSDmqx2LTA+IgYBVwPfyNseAxwLDAIOAY4ATqhWrGZmVkw1Wx5HAvMiYn7+6dqJwFlN1jkImJKnHyhZHkBX0s/d7gTUAUuqGKuZmRVQzeSxF7CwZL4hl5V6nHQ/CcBwYFdJvSLiD6Rk8lJ+/TYiNnskiqRRkmZImrF06dKKvwEzM2teNZNHc2MU0WT+cuAESTNJ3VIvAusl7QscSLo0eC/gZEnHb7aziBsjoj4i6vv06VPZ6M3MrEXlPJJ9SzUA/Uvm+wGLSleIiEXA2QD5Z25HRMRKSaOARyJidV72G+Bo4MEqxmtmZmWqZstjOrCfpH0kdSE9ZPGe0hUk9ZbUGMMXgFvy9AukFklnSXWkVomf5Gtm1k5ULXlExHpgNPBb0on/zoiYI+lqSWfm1U4E/izpGaAv8LVcfhfwHPAkaVzk8Yj4RbViNTOzYhTRdBiiY6qvr48ZM2bUOgwzsw5F0qMRUV90O99hbmZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlZYVZOHpFMl/VnSPElXNLN8gKQpkp6QNFVSv5Jle0u6T9JcSU9JGljNWM3MrHxVSx6SOgE/BE4DDgLOl3RQk9WuBcZHxCDgauAbJcvGA9+OiAOBI4GXqxWrmZkVU82Wx5HAvIiYHxF/ByYCZzVZ5yBgSp5+oHF5TjKdI+J+gIhYHRFrqhirmZkVUM3ksRewsGS+IZeVehwYkaeHA7tK6gXsD6yQ9HNJMyV9O7dk3kTSKEkzJM1YunRpFd6CmZk1p5rJQ82URZP5y4ETJM0ETgBeBNYDnYF35+VHAG8HLtlsZxE3RkR9RNT36dOngqGbmVlrqpk8GoD+JfP9gEWlK0TEoog4OyKGAFfmspV525m5y2s9MBkYWsVYzcysgGomj+nAfpL2kdQF+BBwT+kKknpLaozhC8AtJdvuLqmxOXEy8FQVYzUzswKqljxyi2E08FtgLnBnRMyRdLWkM/NqJwJ/lvQM0Bf4Wt52A6nLaoqkJ0ldYDdVK1YzMytGEU2HITqm+vr6mDFjRq3DMDPrUCQ9GhH1RbfzHeZmZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmCKi1jFUhKSlwPNbsYvewCsVCmdH4PoqxvVVjOurmK2prwER0afoRttN8thakmZERH2t4+goXF/FuL6KcX0VU4v6creVmZkV5uRhZmaFOXm84cZaB9DBuL6KcX0V4/oqZpvXl8c8zMysMLc8zMysMCcPMzMrzMnDNpF0iaTrt/ExT5T0y215zEqTdKWkOZKekDRL0lGSOkv6uqRnc9ksSVeWbLMhl82R9Lik/y9pu/481uLvq6PakrqStEBS73LWkdRD0se3JsbOW7NxeyFJpPGbjVU8RqeI2FCt/W9PtsX/R3sh6V3AGcDQiHg9f3i7AF8F3gYcGhFrJe0KfKZk079FxGF5H/8A3AF0B760Td+A7ah6AB8H/mNLd9Bhv+lIGihprqT/AB4DNkj6lqRHJf2PpCMlTZU0X9KZeZuDJf0pf+N7QtJ+eT9PS7o1l90laZe8/gJJ/y7pIeAcSYdJeiSvd7ek3fN6UyV9X9LDkmZLOrJmFdMKSZNz/cyRNCqXjZT0jKTfA8eWrPt+SX+UNDPXZ99c3kfS/ZIek/RjSc/nbzJN/z/6S7pB0ox8vC+X7PvUXOcPAWdv21qouD2AVyLidYCIeAVYAXwU+GRErM3lqyLiquZ2EBEvA6OA0TnxdkgV+vu6Kn8W78ufv7MlXSPpSUn3SqrL6/27pOn583ajks657MS8zjckfW3b10TbKlRXvXI9zZT0Y0Al23y45Fz3Y0mdmoTwTeAdefm3JXWTNCV/rp+UdFabbyIiOuQLGAhsBI7O8wGclqfvBu4D6oDBwKxc/gPggjzdBdg57yeAY3P5LcDleXoB8LmSYz4BnJCnrwa+n6enAjfl6eOB2bWunxbqrGf+d2dgNrAX8ALQJ9fHNOD6vM7uvHE13keA7+Tp64Ev5OlTc931bvr/0eR4nXIdDQK6AguB/Uh/7HcCv6x13WxFnXYDZgHPkL7FnZDf58w2tlvdTNmrQN9av6ca/31dBTxU8tld0+Rz/YHSY+Xp24D35+mDgbnAe4CZQJda10sV62os8O95+h9LPosHAr8A6vKy/wAuytMLSj6vs0vi6Qzslqd7A/Maj9nSq6N3Wz0fEY/k6b8D9+bpJ4HXI2KdpCdJFQXwB+BKSf2An0fEs/mL3sKImJbX+SlwGXBtnv8ZgKTuQI+I+H0uvxWYVBLLBICIeFDSbpJ6RMSKCr7XSrhM0vA83R+4EJgaEUsBJP0M2D8v7wf8TNIepD/mv+Ty44DhABFxr6RXS/Zf+v8BcG7+VtWZ9A39IFJr9y8R8Ww+5k9J37o7pIhYLelw4N3ASaS/l6+XriNpJPApoBdwTEQsbGF3HbbVkVXi7wvgNyWf3U68+XM9ME+fJOlzwC5AT2AO8IuImCPpNtLJ810R8fcqvM9KqERdHU9uuUfEr0o+i8OAw4Hp+fy2M/ByG/EI+Lqk40lfAvcC+gKLW9qgw3ZbZX8tmV4XOW2S3nxjN8JG8thORNwBnAn8DfitpJPz+k1vdimd/yvlaW0fNZeb8qeQPlCDSd/KnqblOH9A+uZzKPAvpBYDtH6C21RXkvYBLgeGRcQg4Fcl+2hXdbO1ImJDREyNiC8Bo4H3A3srjXMQET+JNL6xknQy3IyktwMbaPtD3i5V8O8L3vzZbfq57iypK+nb9Afz9jc12f5QUtdh3wq8tYqrcF01t42AWyPisPx6Z7TQZVriAlKr5/D8t7qkyXE209GTRyH5Azo/IsYC95C6FyB90N+Vp88nNZvfJCJWAq9KencuuhD4fckq5+VjHAeszOu3J92BVyNijaQDgKNJ30hOzH2ndcA5TdZ/MU9fXFL+EHAugKT3kprUzdmNlExW5j7a03L508A+kt6R58/furdVW5LeKWm/kqLDgD8DNwPX5xMduc+5Swv76AP8iHSC6KiJtVJ/X+VoPKm9Iqkb8MHGBZLOJrXwjgfGSupR/K1UXaXq6kHSSR9Jp/HGZ3EK8EGlCzGQ1FPSgCYxrAJ2bXKMl3OL7ySg6fqb6ejdVkWdB3xY0jpSc+xq0kluLnBxHnR6Frihhe0vBn6kNKA+HxhZsuxVSQ/n/V1apfi3xr3AxyQ9QTq5PQK8ROpj/kOefow3vhlfBUyS9GJed59c/mVggqTzSMnzJdIfYrfSg0XE45JmkroT5pP6cIl05dEo4FeSXiElo0Oq8H63lW7AD/JJaj2pr3gUqZXxFWC2pFWk1u6twKK83c6SZpH69teT+u2/u41jr6RK/X21KSJWSLqJ1I21AJgOoHSl2zdJrd2FSpe6Xkfx5FRtlf4sPkb6LL4AEBFPSRoD3Kd0+fc64BOU/GRFRCyTNE3SbOA3wLeAX0iaQRrDe7qtN7HDP55E0kDSgO0Wn8AkTSUNss+oUFjtlqSdgA0RsT631m7IzVwz24HsaC0P23p7A3fmbzR/J12SamY7mB2+5WFmZsXtUAPmZmZWGU4eZmZWmJOHmZkV5uRhlknqJ+m/lZ6E+5yk6yQ1e29GXv9NTyaVtKekuwoXifHAAAACOUlEQVQe82pJp2xN3Ga14AFzMzY9CfiPpEuPf5Jv6rsRWB4Rn21hm4Fs5WXelSCpc0Ssr2UMtuPxpbpmycnA2oj4CaRHjkj6V+Avkv4CvA/YiXSD1h0R8WVKnkwK3A/8kJxMJF0CfIB0o9chwHdId5hfSHr8xukRsVzSOOCXpJvd/jPH0gk4JCKU78T/IenREWuAj0bE03m75cAQ0g1lpY97N6s6Jw+z5GDg0dKCiHhN0gukz8mRpCSwhvTAuV8BV5BO8o2/yzGwyT4PIZ3cu5LuPP98RAyR9D3gIuD7JceaQXq0CZK+zRsPA7wR+Fh+iOdRpGc6NT6TbX/glPDvzFgNOHmYJaLlh8wFcH9ELAOQ9HPS04Unt7HPByJiFbBK0krSk14hPVZjUHMbSDoXGAq8Nz+36RjSoykaV9mpZPVJThxWK04eZskcYERpgaTdSI/L3sCWPTX59ZLpjSXzm5703OR4B5OeV3R87jZ7C7Cilce/lPvEZ7OK89VWZskUYBdJF8Gmp+B+BxhH6qp6T3466c6ksYxpbP5k0i2m9HsxE0k/2rMUUrcZaczlnLyOJA2uxPHMtpaThxmQH4U+nPRzw8+SfhlwLfBveZWHSE++nQX8V0TMyN1Y05R+CvXbWxnCB0iPwb5J6adBZ+XyC4B/lvQ4qXXU9s+Dmm0DvlTXrA35yqn6iBhd61jM2gu3PMzMrDC3PMzMrDC3PMzMrDAnDzMzK8zJw8zMCnPyMDOzwpw8zMyssP8DCL/f7z/wAe8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Optimizer Vs Accuracy')\n",
    "plt.plot(optimizer_var,hum_sub_opt_accuracy,'ms-', label='Accuracy')\n",
    "#plt.axis([min(optimizer_var), max(optimizer_var), min(hum_con_opt)-0.01, max(hum_con_opt)])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Optimizer')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSC Dataset with Feature Concatentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11154, 1024)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing datasets for GSC Features with Feature Concatentation\n",
    "x_train_GSC1 = train_GSC.iloc[:,5].values.tolist()\n",
    "x_train_GSC_con = np.array(x_train_GSC1)\n",
    "y_train_GSC1 = train_GSC.iloc[:,2].values.tolist()\n",
    "y_train_GSC_con = np.array(y_train_GSC1)\n",
    "x_test_GSC1 = test_GSC.iloc[:,5].values.tolist()\n",
    "x_test_GSC_con = np.array(x_test_GSC1)\n",
    "y_test_GSC1 = test_GSC.iloc[:,2].values.tolist()\n",
    "y_test_GSC_con = np.array(y_test_GSC1)\n",
    "x_test_GSC_con.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/40\n",
      "90339/90339 [==============================] - 33s 362us/step - loss: 0.1215 - acc: 0.9544 - val_loss: 0.0595 - val_acc: 0.9854\n",
      "Epoch 2/40\n",
      "90339/90339 [==============================] - 28s 308us/step - loss: 0.0591 - acc: 0.9847 - val_loss: 0.0566 - val_acc: 0.9870\n",
      "Epoch 3/40\n",
      "90339/90339 [==============================] - 25s 281us/step - loss: 0.0498 - acc: 0.9889 - val_loss: 0.0415 - val_acc: 0.9917\n",
      "Epoch 4/40\n",
      "90339/90339 [==============================] - 25s 280us/step - loss: 0.0382 - acc: 0.9934 - val_loss: 0.0417 - val_acc: 0.9929\n",
      "Epoch 5/40\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.0384 - acc: 0.9938 - val_loss: 0.0529 - val_acc: 0.9915\n",
      "Epoch 6/40\n",
      "90339/90339 [==============================] - 25s 273us/step - loss: 0.0386 - acc: 0.9939 - val_loss: 0.0352 - val_acc: 0.9931\n",
      "Epoch 7/40\n",
      "90339/90339 [==============================] - 30s 334us/step - loss: 0.0380 - acc: 0.9938 - val_loss: 0.0475 - val_acc: 0.9918\n",
      "Epoch 8/40\n",
      "90339/90339 [==============================] - 26s 283us/step - loss: 0.0449 - acc: 0.9919 - val_loss: 0.0397 - val_acc: 0.9920\n",
      "Epoch 9/40\n",
      "90339/90339 [==============================] - 28s 311us/step - loss: 0.0468 - acc: 0.9916 - val_loss: 0.1337 - val_acc: 0.9781\n",
      "Epoch 10/40\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0563 - acc: 0.9894 - val_loss: 0.0667 - val_acc: 0.9864\n",
      "Epoch 11/40\n",
      "90339/90339 [==============================] - 26s 290us/step - loss: 0.0637 - acc: 0.9867 - val_loss: 0.0727 - val_acc: 0.9872\n",
      "Epoch 12/40\n",
      "90339/90339 [==============================] - 26s 285us/step - loss: 0.0603 - acc: 0.9880 - val_loss: 0.1277 - val_acc: 0.9810\n",
      "Epoch 13/40\n",
      "90339/90339 [==============================] - 27s 303us/step - loss: 0.0709 - acc: 0.9864 - val_loss: 0.0617 - val_acc: 0.9875\n",
      "Epoch 14/40\n",
      "90339/90339 [==============================] - 31s 340us/step - loss: 0.0614 - acc: 0.9869 - val_loss: 0.0641 - val_acc: 0.9869\n",
      "Epoch 15/40\n",
      "90339/90339 [==============================] - 26s 293us/step - loss: 0.0575 - acc: 0.9874 - val_loss: 0.0759 - val_acc: 0.9844\n",
      "Epoch 16/40\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0578 - acc: 0.9878 - val_loss: 0.0691 - val_acc: 0.9874\n",
      "Epoch 17/40\n",
      "90339/90339 [==============================] - 24s 271us/step - loss: 0.0547 - acc: 0.9878 - val_loss: 0.0681 - val_acc: 0.9891\n",
      "Epoch 18/40\n",
      "90339/90339 [==============================] - 33s 365us/step - loss: 0.0546 - acc: 0.9880 - val_loss: 0.0594 - val_acc: 0.9883\n",
      "Epoch 19/40\n",
      "90339/90339 [==============================] - 31s 338us/step - loss: 0.0516 - acc: 0.9897 - val_loss: 0.0770 - val_acc: 0.9895\n",
      "Epoch 20/40\n",
      "90339/90339 [==============================] - 31s 341us/step - loss: 0.0529 - acc: 0.9902 - val_loss: 0.0717 - val_acc: 0.9875\n",
      "Epoch 21/40\n",
      "90339/90339 [==============================] - 28s 314us/step - loss: 0.0518 - acc: 0.9909 - val_loss: 0.0609 - val_acc: 0.9901\n",
      "Epoch 22/40\n",
      "90339/90339 [==============================] - 25s 280us/step - loss: 0.0521 - acc: 0.9912 - val_loss: 0.0782 - val_acc: 0.9805\n",
      "Epoch 23/40\n",
      "90339/90339 [==============================] - 24s 263us/step - loss: 0.0526 - acc: 0.9913 - val_loss: 0.0563 - val_acc: 0.9919\n",
      "Epoch 24/40\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0545 - acc: 0.9912 - val_loss: 0.0749 - val_acc: 0.9898\n",
      "Epoch 25/40\n",
      "90339/90339 [==============================] - 27s 298us/step - loss: 0.0527 - acc: 0.9914 - val_loss: 0.0691 - val_acc: 0.9919\n",
      "Epoch 26/40\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0530 - acc: 0.9914 - val_loss: 0.0623 - val_acc: 0.9910\n",
      "Epoch 27/40\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0509 - acc: 0.9917 - val_loss: 0.0675 - val_acc: 0.9910\n",
      "Epoch 28/40\n",
      "90339/90339 [==============================] - 24s 270us/step - loss: 0.0521 - acc: 0.9912 - val_loss: 0.0734 - val_acc: 0.9905\n",
      "Epoch 29/40\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0493 - acc: 0.9911 - val_loss: 0.0554 - val_acc: 0.9907\n",
      "Epoch 30/40\n",
      "90339/90339 [==============================] - 27s 296us/step - loss: 0.0469 - acc: 0.9911 - val_loss: 0.0671 - val_acc: 0.9905\n",
      "Epoch 31/40\n",
      "90339/90339 [==============================] - 24s 271us/step - loss: 0.0464 - acc: 0.9914 - val_loss: 0.0487 - val_acc: 0.9917\n",
      "Epoch 32/40\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.0453 - acc: 0.9916 - val_loss: 0.0640 - val_acc: 0.9914\n",
      "Epoch 33/40\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0466 - acc: 0.9916 - val_loss: 0.0510 - val_acc: 0.9914\n",
      "Epoch 34/40\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0447 - acc: 0.9916 - val_loss: 0.0504 - val_acc: 0.9916\n",
      "Epoch 35/40\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0449 - acc: 0.9918 - val_loss: 0.0577 - val_acc: 0.9916\n",
      "Epoch 36/40\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0450 - acc: 0.9917 - val_loss: 0.0643 - val_acc: 0.9907\n",
      "Epoch 37/40\n",
      "90339/90339 [==============================] - 24s 271us/step - loss: 0.0452 - acc: 0.9917 - val_loss: 0.0487 - val_acc: 0.9920\n",
      "Epoch 38/40\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0442 - acc: 0.9918 - val_loss: 0.0479 - val_acc: 0.9906\n",
      "Epoch 39/40\n",
      "90339/90339 [==============================] - 28s 305us/step - loss: 0.0445 - acc: 0.9915 - val_loss: 0.0458 - val_acc: 0.9920\n",
      "Epoch 40/40\n",
      "90339/90339 [==============================] - 26s 283us/step - loss: 0.0453 - acc: 0.9909 - val_loss: 0.0468 - val_acc: 0.9918\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/40\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.1215 - acc: 0.9565 - val_loss: 0.0566 - val_acc: 0.9850\n",
      "Epoch 2/40\n",
      "90339/90339 [==============================] - 26s 284us/step - loss: 0.0468 - acc: 0.9878 - val_loss: 0.0535 - val_acc: 0.9878\n",
      "Epoch 3/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0363 - acc: 0.9924 - val_loss: 0.0386 - val_acc: 0.9926\n",
      "Epoch 4/40\n",
      "90339/90339 [==============================] - 25s 273us/step - loss: 0.0362 - acc: 0.9928 - val_loss: 0.0420 - val_acc: 0.9926\n",
      "Epoch 5/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0369 - acc: 0.9927 - val_loss: 0.0349 - val_acc: 0.9929\n",
      "Epoch 6/40\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0377 - acc: 0.9925 - val_loss: 0.0564 - val_acc: 0.9928\n",
      "Epoch 7/40\n",
      "90339/90339 [==============================] - 26s 288us/step - loss: 0.0378 - acc: 0.9927 - val_loss: 0.0451 - val_acc: 0.9925\n",
      "Epoch 8/40\n",
      "90339/90339 [==============================] - 26s 293us/step - loss: 0.0357 - acc: 0.9932 - val_loss: 0.0495 - val_acc: 0.9905\n",
      "Epoch 9/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0361 - acc: 0.9931 - val_loss: 0.0369 - val_acc: 0.9930\n",
      "Epoch 10/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0348 - acc: 0.9936 - val_loss: 0.0412 - val_acc: 0.9932\n",
      "Epoch 11/40\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0355 - acc: 0.9939 - val_loss: 0.0361 - val_acc: 0.9932\n",
      "Epoch 12/40\n",
      "90339/90339 [==============================] - 25s 280us/step - loss: 0.0355 - acc: 0.9939 - val_loss: 0.0408 - val_acc: 0.9930ETA:  - ETA: 8s - loss: 0.035 - ETA: 7s - loss: 0.0356 - acc: 0. - ETA: 7s\n",
      "Epoch 13/40\n",
      "90339/90339 [==============================] - 26s 289us/step - loss: 0.0352 - acc: 0.9941 - val_loss: 0.0509 - val_acc: 0.9932\n",
      "Epoch 14/40\n",
      "90339/90339 [==============================] - 25s 273us/step - loss: 0.0343 - acc: 0.9941 - val_loss: 0.0345 - val_acc: 0.9931\n",
      "Epoch 15/40\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0347 - acc: 0.9941 - val_loss: 0.0502 - val_acc: 0.9908\n",
      "Epoch 16/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0348 - acc: 0.9940 - val_loss: 0.0447 - val_acc: 0.9927\n",
      "Epoch 17/40\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0337 - acc: 0.9941 - val_loss: 0.0388 - val_acc: 0.9925\n",
      "Epoch 18/40\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0326 - acc: 0.9941 - val_loss: 0.0390 - val_acc: 0.9932\n",
      "Epoch 19/40\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.0372 - val_acc: 0.9931\n",
      "Epoch 20/40\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0329 - acc: 0.9941 - val_loss: 0.0372 - val_acc: 0.9929\n",
      "Epoch 21/40\n",
      "90339/90339 [==============================] - 24s 261us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.0389 - val_acc: 0.9926\n",
      "Epoch 22/40\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0321 - acc: 0.9942 - val_loss: 0.0367 - val_acc: 0.9930\n",
      "Epoch 23/40\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.0318 - acc: 0.9943 - val_loss: 0.0389 - val_acc: 0.9928\n",
      "Epoch 24/40\n",
      "90339/90339 [==============================] - 24s 270us/step - loss: 0.0322 - acc: 0.9942 - val_loss: 0.0426 - val_acc: 0.9927\n",
      "Epoch 25/40\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0326 - acc: 0.9942 - val_loss: 0.0456 - val_acc: 0.9917\n",
      "Epoch 26/40\n",
      "90339/90339 [==============================] - 26s 289us/step - loss: 0.0324 - acc: 0.9942 - val_loss: 0.0383 - val_acc: 0.9929\n",
      "Epoch 27/40\n",
      "90339/90339 [==============================] - 40s 443us/step - loss: 0.0329 - acc: 0.9940 - val_loss: 0.0372 - val_acc: 0.9924\n",
      "Epoch 28/40\n",
      "90339/90339 [==============================] - 26s 283us/step - loss: 0.0326 - acc: 0.9940 - val_loss: 0.0391 - val_acc: 0.9926\n",
      "Epoch 29/40\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0319 - acc: 0.9943 - val_loss: 0.0373 - val_acc: 0.9933\n",
      "Epoch 30/40\n",
      "90339/90339 [==============================] - 26s 289us/step - loss: 0.0321 - acc: 0.9943 - val_loss: 0.0404 - val_acc: 0.9927\n",
      "Epoch 31/40\n",
      "90339/90339 [==============================] - 30s 336us/step - loss: 0.0319 - acc: 0.9943 - val_loss: 0.0423 - val_acc: 0.9923\n",
      "Epoch 32/40\n",
      "90339/90339 [==============================] - 27s 303us/step - loss: 0.0322 - acc: 0.9943 - val_loss: 0.0408 - val_acc: 0.9927\n",
      "Epoch 33/40\n",
      "90339/90339 [==============================] - 32s 351us/step - loss: 0.0321 - acc: 0.9942 - val_loss: 0.0374 - val_acc: 0.9932\n",
      "Epoch 34/40\n",
      "90339/90339 [==============================] - 29s 320us/step - loss: 0.0322 - acc: 0.9942 - val_loss: 0.0387 - val_acc: 0.9929\n",
      "Epoch 35/40\n",
      "90339/90339 [==============================] - 29s 323us/step - loss: 0.0312 - acc: 0.9943 - val_loss: 0.0433 - val_acc: 0.9926\n",
      "Epoch 36/40\n",
      "90339/90339 [==============================] - 30s 327us/step - loss: 0.0325 - acc: 0.9943 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "Epoch 37/40\n",
      "90339/90339 [==============================] - 31s 342us/step - loss: 0.0318 - acc: 0.9943 - val_loss: 0.0422 - val_acc: 0.9924\n",
      "Epoch 38/40\n",
      "90339/90339 [==============================] - 29s 326us/step - loss: 0.0322 - acc: 0.9943 - val_loss: 0.0413 - val_acc: 0.9926\n",
      "Epoch 39/40\n",
      "90339/90339 [==============================] - 29s 319us/step - loss: 0.0320 - acc: 0.9943 - val_loss: 0.0382 - val_acc: 0.9928\n",
      "Epoch 40/40\n",
      "90339/90339 [==============================] - 30s 329us/step - loss: 0.0316 - acc: 0.9943 - val_loss: 0.0500 - val_acc: 0.9899\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/40\n",
      "90339/90339 [==============================] - 31s 349us/step - loss: 0.1012 - acc: 0.9634 - val_loss: 0.0555 - val_acc: 0.9865\n",
      "Epoch 2/40\n",
      "90339/90339 [==============================] - 31s 344us/step - loss: 0.0472 - acc: 0.9883 - val_loss: 0.0525 - val_acc: 0.9870\n",
      "Epoch 3/40\n",
      "90339/90339 [==============================] - 30s 334us/step - loss: 0.0416 - acc: 0.9904 - val_loss: 0.0454 - val_acc: 0.9901\n",
      "Epoch 4/40\n",
      "90339/90339 [==============================] - 29s 326us/step - loss: 0.0381 - acc: 0.9920 - val_loss: 0.0392 - val_acc: 0.9925\n",
      "Epoch 5/40\n",
      "90339/90339 [==============================] - 30s 334us/step - loss: 0.0359 - acc: 0.9928 - val_loss: 0.0421 - val_acc: 0.9928\n",
      "Epoch 6/40\n",
      "90339/90339 [==============================] - 31s 345us/step - loss: 0.0368 - acc: 0.9929 - val_loss: 0.0353 - val_acc: 0.9930\n",
      "Epoch 7/40\n",
      "90339/90339 [==============================] - 29s 323us/step - loss: 0.0352 - acc: 0.9935 - val_loss: 0.0394 - val_acc: 0.9931\n",
      "Epoch 8/40\n",
      "90339/90339 [==============================] - 29s 318us/step - loss: 0.0339 - acc: 0.9938 - val_loss: 0.0434 - val_acc: 0.9931\n",
      "Epoch 9/40\n",
      "90339/90339 [==============================] - 28s 309us/step - loss: 0.0338 - acc: 0.9941 - val_loss: 0.0385 - val_acc: 0.9932\n",
      "Epoch 10/40\n",
      "90339/90339 [==============================] - 28s 307us/step - loss: 0.0327 - acc: 0.9942 - val_loss: 0.0381 - val_acc: 0.9931\n",
      "Epoch 11/40\n",
      "90339/90339 [==============================] - 29s 322us/step - loss: 0.0328 - acc: 0.9942 - val_loss: 0.0376 - val_acc: 0.9931\n",
      "Epoch 12/40\n",
      "90339/90339 [==============================] - 27s 299us/step - loss: 0.0326 - acc: 0.9942 - val_loss: 0.0376 - val_acc: 0.9931\n",
      "Epoch 13/40\n",
      "90339/90339 [==============================] - 28s 308us/step - loss: 0.0321 - acc: 0.9942 - val_loss: 0.0395 - val_acc: 0.9931\n",
      "Epoch 14/40\n",
      "90339/90339 [==============================] - 27s 302us/step - loss: 0.0324 - acc: 0.9942 - val_loss: 0.0398 - val_acc: 0.9931\n",
      "Epoch 15/40\n",
      "90339/90339 [==============================] - 28s 307us/step - loss: 0.0326 - acc: 0.9942 - val_loss: 0.0390 - val_acc: 0.9932\n",
      "Epoch 16/40\n",
      "90339/90339 [==============================] - 26s 293us/step - loss: 0.0328 - acc: 0.9942 - val_loss: 0.0405 - val_acc: 0.9927\n",
      "Epoch 17/40\n",
      "90339/90339 [==============================] - 28s 310us/step - loss: 0.0323 - acc: 0.9942 - val_loss: 0.0375 - val_acc: 0.9932\n",
      "Epoch 18/40\n",
      "90339/90339 [==============================] - 28s 306us/step - loss: 0.0320 - acc: 0.9943 - val_loss: 0.0365 - val_acc: 0.9932\n",
      "Epoch 19/40\n",
      "90339/90339 [==============================] - 29s 317us/step - loss: 0.0317 - acc: 0.9944 - val_loss: 0.0375 - val_acc: 0.9931\n",
      "Epoch 20/40\n",
      "90339/90339 [==============================] - 32s 352us/step - loss: 0.0314 - acc: 0.9943 - val_loss: 0.0416 - val_acc: 0.9931\n",
      "Epoch 21/40\n",
      "90339/90339 [==============================] - 29s 319us/step - loss: 0.0320 - acc: 0.9944 - val_loss: 0.0389 - val_acc: 0.9929\n",
      "Epoch 22/40\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0320 - acc: 0.9944 - val_loss: 0.0342 - val_acc: 0.9933\n",
      "Epoch 23/40\n",
      "90339/90339 [==============================] - 29s 326us/step - loss: 0.0319 - acc: 0.9943 - val_loss: 0.0379 - val_acc: 0.9932\n",
      "Epoch 24/40\n",
      "90339/90339 [==============================] - 33s 366us/step - loss: 0.0318 - acc: 0.9944 - val_loss: 0.0395 - val_acc: 0.9925\n",
      "Epoch 25/40\n",
      "90339/90339 [==============================] - 28s 307us/step - loss: 0.0319 - acc: 0.9943 - val_loss: 0.0436 - val_acc: 0.9919\n",
      "Epoch 26/40\n",
      "90339/90339 [==============================] - 29s 319us/step - loss: 0.0326 - acc: 0.9944 - val_loss: 0.0432 - val_acc: 0.9924\n",
      "Epoch 27/40\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0318 - acc: 0.9944 - val_loss: 0.0401 - val_acc: 0.9928\n",
      "Epoch 28/40\n",
      "90339/90339 [==============================] - 31s 348us/step - loss: 0.0319 - acc: 0.9944 - val_loss: 0.0387 - val_acc: 0.9929\n",
      "Epoch 29/40\n",
      "90339/90339 [==============================] - 29s 317us/step - loss: 0.0314 - acc: 0.9945 - val_loss: 0.0390 - val_acc: 0.9929\n",
      "Epoch 30/40\n",
      "90339/90339 [==============================] - 36s 394us/step - loss: 0.0313 - acc: 0.9945 - val_loss: 0.0393 - val_acc: 0.9924\n",
      "Epoch 31/40\n",
      "90339/90339 [==============================] - 30s 327us/step - loss: 0.0317 - acc: 0.9945 - val_loss: 0.0416 - val_acc: 0.9927\n",
      "Epoch 32/40\n",
      "90339/90339 [==============================] - 29s 324us/step - loss: 0.0313 - acc: 0.9945 - val_loss: 0.0393 - val_acc: 0.9928\n",
      "Epoch 33/40\n",
      "90339/90339 [==============================] - 37s 414us/step - loss: 0.0317 - acc: 0.9945 - val_loss: 0.0406 - val_acc: 0.9925\n",
      "Epoch 34/40\n",
      "90339/90339 [==============================] - 31s 348us/step - loss: 0.0317 - acc: 0.9945 - val_loss: 0.0452 - val_acc: 0.9917\n",
      "Epoch 35/40\n",
      "90339/90339 [==============================] - 33s 362us/step - loss: 0.0314 - acc: 0.9945 - val_loss: 0.0417 - val_acc: 0.9918\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90339/90339 [==============================] - 33s 363us/step - loss: 0.0311 - acc: 0.9944 - val_loss: 0.0428 - val_acc: 0.9917\n",
      "Epoch 37/40\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0319 - acc: 0.9944 - val_loss: 0.0448 - val_acc: 0.9918\n",
      "Epoch 38/40\n",
      "90339/90339 [==============================] - 24s 266us/step - loss: 0.0310 - acc: 0.9945 - val_loss: 0.0522 - val_acc: 0.9914\n",
      "Epoch 39/40\n",
      "90339/90339 [==============================] - 26s 284us/step - loss: 0.0321 - acc: 0.9944 - val_loss: 0.0492 - val_acc: 0.9909\n",
      "Epoch 40/40\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.0321 - acc: 0.9944 - val_loss: 0.0456 - val_acc: 0.9909\n"
     ]
    }
   ],
   "source": [
    "# Model2 Definition\n",
    "activation_function = ['relu', 'sigmoid', 'tanh']\n",
    "GSC_con_act_accuracy = []\n",
    "for i in range(len(activation_function)):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(12, input_dim=1024, activation='relu'))\n",
    "    model2.add(Dense(8, activation=activation_function[i]))\n",
    "    model2.add(Dense(1, activation='sigmoid'))\n",
    "    model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model2, iterating on the data in batches of 32 samples\n",
    "    summary2 = model2.fit(x_train_GSC_con, y_train_GSC_con, validation_split = 0.1, nb_epoch=40, batch_size=10)\n",
    "    scores2 = model2.evaluate(x=x_test_GSC_con, y=y_test_GSC_con, verbose=0)\n",
    "    scores_acc2 = scores2[1]\n",
    "    GSC_con_act_accuracy.append(scores_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Activation Function Vs Accuracy')\n",
    "plt.plot(activation_function,GSC_con_act_accuracy,'yo-', label='Accuracy')\n",
    "plt.axis([min(activation_function), max(activation_function), min(GSC_con_act_accuracy)-0.1, max(GSC_con_act_accuracy)+0.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('activation_function')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model Accuracy\n",
    "scores2 = model2.evaluate(x=x_test_GSC_con, y=y_test_GSC_con, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for GSC Dataset with Feature Concatentation \n",
      "\n",
      "Test Accuracy =  99%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for GSC Dataset with Feature Concatentation \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 43s 471us/step - loss: 0.1020 - acc: 0.9637 - val_loss: 0.0579 - val_acc: 0.9877\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 24s 266us/step - loss: 0.0526 - acc: 0.9869 - val_loss: 0.0538 - val_acc: 0.9881\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 28s 315us/step - loss: 0.0446 - acc: 0.9905 - val_loss: 0.0438 - val_acc: 0.9913\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 29s 316us/step - loss: 0.0394 - acc: 0.9931 - val_loss: 0.0535 - val_acc: 0.9913\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 27s 294us/step - loss: 0.0404 - acc: 0.9933 - val_loss: 0.0517 - val_acc: 0.9914\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 28s 307us/step - loss: 0.0429 - acc: 0.9933 - val_loss: 0.0520 - val_acc: 0.9929\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 29s 318us/step - loss: 0.0448 - acc: 0.9927 - val_loss: 0.0534 - val_acc: 0.9915\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 24s 271us/step - loss: 0.0466 - acc: 0.9912 - val_loss: 0.0456 - val_acc: 0.9898\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 32s 355us/step - loss: 0.0497 - acc: 0.9903 - val_loss: 0.0759 - val_acc: 0.9877\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 30s 330us/step - loss: 0.0546 - acc: 0.9885 - val_loss: 0.0549 - val_acc: 0.9892\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 34s 374us/step - loss: 0.0543 - acc: 0.9890 - val_loss: 0.1344 - val_acc: 0.9553\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 28s 313us/step - loss: 0.0552 - acc: 0.9876 - val_loss: 0.0482 - val_acc: 0.9898\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 29s 323us/step - loss: 0.0525 - acc: 0.9893 - val_loss: 0.0547 - val_acc: 0.9882\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 25s 282us/step - loss: 0.0509 - acc: 0.9898 - val_loss: 0.0505 - val_acc: 0.9907\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 27s 296us/step - loss: 0.0493 - acc: 0.9903 - val_loss: 0.0547 - val_acc: 0.9897\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 29s 317us/step - loss: 0.0507 - acc: 0.9898 - val_loss: 0.0662 - val_acc: 0.9879\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0612 - acc: 0.9863 - val_loss: 0.0612 - val_acc: 0.9876\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 25s 278us/step - loss: 0.0644 - acc: 0.9860 - val_loss: 0.0590 - val_acc: 0.9876\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 25s 278us/step - loss: 0.0616 - acc: 0.9864 - val_loss: 0.0703 - val_acc: 0.9870\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 25s 282us/step - loss: 0.0565 - acc: 0.9871 - val_loss: 0.0602 - val_acc: 0.9889\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 25s 280us/step - loss: 0.0511 - acc: 0.9891 - val_loss: 0.0624 - val_acc: 0.9874\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 25s 282us/step - loss: 0.0527 - acc: 0.9890 - val_loss: 0.0652 - val_acc: 0.9880\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0504 - acc: 0.9888 - val_loss: 0.0674 - val_acc: 0.9868\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 25s 278us/step - loss: 0.0488 - acc: 0.9899 - val_loss: 0.0565 - val_acc: 0.9896\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0477 - acc: 0.9906 - val_loss: 0.0507 - val_acc: 0.9888\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0490 - acc: 0.9909 - val_loss: 0.0519 - val_acc: 0.9909\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0479 - acc: 0.9906 - val_loss: 0.0767 - val_acc: 0.9886\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0480 - acc: 0.9917 - val_loss: 0.0642 - val_acc: 0.9911\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 26s 288us/step - loss: 0.0484 - acc: 0.9921 - val_loss: 0.0549 - val_acc: 0.9908\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 25s 276us/step - loss: 0.0491 - acc: 0.9920 - val_loss: 0.0668 - val_acc: 0.9910\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 22s 246us/step - loss: 0.1501 - acc: 0.9413 - val_loss: 0.1235 - val_acc: 0.9546\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 21s 233us/step - loss: 0.0431 - acc: 0.9890 - val_loss: 0.0529 - val_acc: 0.9839\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 21s 233us/step - loss: 0.0334 - acc: 0.9917 - val_loss: 0.0342 - val_acc: 0.9923\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 21s 235us/step - loss: 0.0299 - acc: 0.9925 - val_loss: 0.0361 - val_acc: 0.9920\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 22s 243us/step - loss: 0.0267 - acc: 0.9933 - val_loss: 0.0368 - val_acc: 0.9896\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0250 - acc: 0.9933 - val_loss: 0.0327 - val_acc: 0.9922\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 31s 345us/step - loss: 0.0239 - acc: 0.9933 - val_loss: 0.0362 - val_acc: 0.9914\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0223 - acc: 0.9937 - val_loss: 0.0378 - val_acc: 0.9887\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.0308 - val_acc: 0.9927\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0361 - val_acc: 0.9902\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 27s 304us/step - loss: 0.0189 - acc: 0.9942 - val_loss: 0.0363 - val_acc: 0.9915\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 24s 270us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0314 - val_acc: 0.9921\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 23s 251us/step - loss: 0.0166 - acc: 0.9950 - val_loss: 0.0303 - val_acc: 0.9923\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.0158 - acc: 0.9948 - val_loss: 0.0377 - val_acc: 0.9916\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 20s 220us/step - loss: 0.0154 - acc: 0.9951 - val_loss: 0.0363 - val_acc: 0.9917\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0381 - val_acc: 0.9920\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 19s 216us/step - loss: 0.0135 - acc: 0.9954 - val_loss: 0.0560 - val_acc: 0.9875\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0316 - val_acc: 0.9914\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 29s 316us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0339 - val_acc: 0.9913\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 22s 240us/step - loss: 0.0119 - acc: 0.9957 - val_loss: 0.0356 - val_acc: 0.9915\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 20s 224us/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.0520 - val_acc: 0.9900\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 20s 221us/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.0340 - val_acc: 0.9916\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0373 - val_acc: 0.9917\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0362 - val_acc: 0.9913\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 20s 218us/step - loss: 0.0091 - acc: 0.9969 - val_loss: 0.0367 - val_acc: 0.9911\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 30s 328us/step - loss: 0.0085 - acc: 0.9969 - val_loss: 0.0327 - val_acc: 0.9912\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 27s 299us/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0451 - val_acc: 0.9912\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 24s 266us/step - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0405 - val_acc: 0.9913\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 24s 266us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0384 - val_acc: 0.9915\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 22s 241us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0384 - val_acc: 0.9917\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 28s 306us/step - loss: 0.1294 - acc: 0.9528 - val_loss: 0.0451 - val_acc: 0.9900\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 32s 355us/step - loss: 0.0384 - acc: 0.9910 - val_loss: 0.0374 - val_acc: 0.9919\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0319 - acc: 0.9924 - val_loss: 0.0385 - val_acc: 0.9908\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 22s 247us/step - loss: 0.0281 - acc: 0.9930 - val_loss: 0.0390 - val_acc: 0.9906\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 23s 249us/step - loss: 0.0251 - acc: 0.9934 - val_loss: 0.0403 - val_acc: 0.9891\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 22s 249us/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0421 - val_acc: 0.9914\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 22s 246us/step - loss: 0.0208 - acc: 0.9942 - val_loss: 0.0400 - val_acc: 0.9917\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 22s 249us/step - loss: 0.0193 - acc: 0.9942 - val_loss: 0.0362 - val_acc: 0.9920\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 22s 247us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0380 - val_acc: 0.9921\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 22s 247us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0404 - val_acc: 0.9918\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0377 - val_acc: 0.9877\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9913\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 22s 247us/step - loss: 0.0135 - acc: 0.9956 - val_loss: 0.0436 - val_acc: 0.9913\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0449 - val_acc: 0.9912\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0386 - val_acc: 0.9916\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0110 - acc: 0.9963 - val_loss: 0.0570 - val_acc: 0.9900\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 26s 282us/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.0516 - val_acc: 0.9913\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0096 - acc: 0.9969 - val_loss: 0.0401 - val_acc: 0.9915\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 24s 270us/step - loss: 0.0095 - acc: 0.9969 - val_loss: 0.0424 - val_acc: 0.9904\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0419 - val_acc: 0.9912\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0486 - val_acc: 0.9852\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 24s 268us/step - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0450 - val_acc: 0.9914\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 25s 276us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0458 - val_acc: 0.9912\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0064 - acc: 0.9976 - val_loss: 0.0456 - val_acc: 0.9902\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0506 - val_acc: 0.9897\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0490 - val_acc: 0.9878\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 25s 271us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0636 - val_acc: 0.9909\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 24s 268us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0499 - val_acc: 0.9897\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 24s 271us/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0599 - val_acc: 0.9904\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 25s 281us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0546 - val_acc: 0.9911\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 31s 340us/step - loss: 0.1114 - acc: 0.9603 - val_loss: 0.0609 - val_acc: 0.9846\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 28s 315us/step - loss: 0.0446 - acc: 0.9886 - val_loss: 0.0427 - val_acc: 0.9911\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 28s 315us/step - loss: 0.0401 - acc: 0.9905 - val_loss: 0.0450 - val_acc: 0.9904\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 30s 334us/step - loss: 0.0374 - acc: 0.9913 - val_loss: 0.0443 - val_acc: 0.9920\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 30s 327us/step - loss: 0.0349 - acc: 0.9923 - val_loss: 0.0512 - val_acc: 0.9909\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 30s 329us/step - loss: 0.0329 - acc: 0.9928 - val_loss: 0.0457 - val_acc: 0.9918\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 30s 327us/step - loss: 0.0320 - acc: 0.9932 - val_loss: 0.0440 - val_acc: 0.9913\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 33s 361us/step - loss: 0.0320 - acc: 0.9934 - val_loss: 0.0448 - val_acc: 0.9904\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 31s 347us/step - loss: 0.0319 - acc: 0.9934 - val_loss: 0.0443 - val_acc: 0.9907\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 30s 331us/step - loss: 0.0312 - acc: 0.9938 - val_loss: 0.0474 - val_acc: 0.9912\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 30s 336us/step - loss: 0.0313 - acc: 0.9936 - val_loss: 0.0421 - val_acc: 0.9917\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 33s 370us/step - loss: 0.0313 - acc: 0.9937 - val_loss: 0.0443 - val_acc: 0.9927\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0330 - acc: 0.9938 - val_loss: 0.0435 - val_acc: 0.9926\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 30s 331us/step - loss: 0.0329 - acc: 0.9938 - val_loss: 0.0539 - val_acc: 0.9921\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 31s 338us/step - loss: 0.0331 - acc: 0.9938 - val_loss: 0.0453 - val_acc: 0.9929\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 30s 331us/step - loss: 0.0323 - acc: 0.9937 - val_loss: 0.0488 - val_acc: 0.9917\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 31s 339us/step - loss: 0.0324 - acc: 0.9939 - val_loss: 0.0478 - val_acc: 0.9894\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 30s 329us/step - loss: 0.0316 - acc: 0.9939 - val_loss: 0.0464 - val_acc: 0.9921\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 30s 336us/step - loss: 0.0319 - acc: 0.9939 - val_loss: 0.0455 - val_acc: 0.9922\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 31s 342us/step - loss: 0.0317 - acc: 0.9938 - val_loss: 0.0600 - val_acc: 0.9913\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 31s 344us/step - loss: 0.0320 - acc: 0.9938 - val_loss: 0.0506 - val_acc: 0.9914\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 30s 331us/step - loss: 0.0320 - acc: 0.9938 - val_loss: 0.0510 - val_acc: 0.9920\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 30s 331us/step - loss: 0.0318 - acc: 0.9939 - val_loss: 0.0542 - val_acc: 0.9921\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 30s 330us/step - loss: 0.0321 - acc: 0.9937 - val_loss: 0.0519 - val_acc: 0.9914\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 30s 330us/step - loss: 0.0319 - acc: 0.9939 - val_loss: 0.0553 - val_acc: 0.9916\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90339/90339 [==============================] - 30s 329us/step - loss: 0.0318 - acc: 0.9939 - val_loss: 0.0514 - val_acc: 0.9908\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 29s 325us/step - loss: 0.0320 - acc: 0.9939 - val_loss: 0.0533 - val_acc: 0.9915\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 37s 405us/step - loss: 0.0321 - acc: 0.9939 - val_loss: 0.0461 - val_acc: 0.9915\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0318 - acc: 0.9938 - val_loss: 0.0495 - val_acc: 0.9921\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 30s 334us/step - loss: 0.0313 - acc: 0.9938 - val_loss: 0.0425 - val_acc: 0.9919\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "optimizer_var = ['rmsprop', 'SGD', 'adamax', 'adadelta']\n",
    "GSC_con_opt_accuracy = []\n",
    "for i in range(len(optimizer_var)):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(12, input_dim=1024, activation='relu'))\n",
    "    model2.add(Dense(8, activation='relu'))\n",
    "    model2.add(Dense(1, activation='sigmoid'))\n",
    "    model2.compile(optimizer=optimizer_var[i], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model, iterating on the data in batches of 32 samples\n",
    "    summary2 = model2.fit(x_train_GSC_con, y_train_GSC_con, validation_split = 0.1, nb_epoch=30, batch_size=10)\n",
    "    scores2 = model2.evaluate(x=x_test_GSC_con, y=y_test_GSC_con, verbose=0)\n",
    "    scores_acc2 = scores2[1]\n",
    "    GSC_con_opt_accuracy.append(scores_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Optimizer Vs Accuracy')\n",
    "plt.plot(optimizer_var,GSC_con_opt_accuracy,'ro-', label='Accuracy')\n",
    "#plt.axis([min(optimizer_var), max(optimizer_var), min(hum_con_opt)-0.01, max(hum_con_opt)])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Optimizer')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1b42ec6080>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b43002588>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b432eb748>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b4334f908>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating plots for accuracy and loss using matplotlib package\n",
    "%matplotlib inline\n",
    "df3 = pd.DataFrame(summary2.history)\n",
    "df3.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for GSC Dataset with Feature Concatenation \n",
      "\n",
      "Test Accuracy =  99%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for GSC Dataset with Feature Concatenation \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSC Dataset with Feature Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11154, 512)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing datasets for GSC Features with Feature Subtraction\n",
    "x_train_GSC2 = train_GSC.iloc[:,6].values.tolist()\n",
    "x_train_GSC_sub = np.array(x_train_GSC2)\n",
    "y_train_GSC2 = train_GSC.iloc[:,2].values.tolist()\n",
    "y_train_GSC_sub = np.array(y_train_GSC2)\n",
    "x_test_GSC2 = test_GSC.iloc[:,6].values.tolist()\n",
    "x_test_GSC_sub = np.array(x_test_GSC2)\n",
    "y_test_GSC2 = test_GSC.iloc[:,2].values.tolist()\n",
    "y_test_GSC_sub = np.array(y_test_GSC2)\n",
    "x_test_GSC_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 23s 254us/step - loss: 0.1963 - acc: 0.9146 - val_loss: 0.1115 - val_acc: 0.9606\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 21s 231us/step - loss: 0.0825 - acc: 0.9719 - val_loss: 0.0787 - val_acc: 0.9732\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 21s 230us/step - loss: 0.0642 - acc: 0.9796 - val_loss: 0.0792 - val_acc: 0.973841 -\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 21s 230us/step - loss: 0.0570 - acc: 0.9829 - val_loss: 0.0793 - val_acc: 0.97650s - loss: 0.0570 - acc: \n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 22s 241us/step - loss: 0.0532 - acc: 0.9844 - val_loss: 0.0789 - val_acc: 0.9758ETA: 0s - loss: 0.053\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0488 - acc: 0.9862 - val_loss: 0.0814 - val_acc: 0.9768\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0486 - acc: 0.9863 - val_loss: 0.0889 - val_acc: 0.9747\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 21s 234us/step - loss: 0.0493 - acc: 0.9861 - val_loss: 0.0771 - val_acc: 0.9767\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 21s 232us/step - loss: 0.0509 - acc: 0.9853 - val_loss: 0.0904 - val_acc: 0.9739\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 22s 245us/step - loss: 0.0518 - acc: 0.9855 - val_loss: 0.0871 - val_acc: 0.9730\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 25s 281us/step - loss: 0.0516 - acc: 0.9854 - val_loss: 0.0965 - val_acc: 0.9713\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0529 - acc: 0.9849 - val_loss: 0.0985 - val_acc: 0.9709\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 22s 238us/step - loss: 0.0533 - acc: 0.9852 - val_loss: 0.0990 - val_acc: 0.9727\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 23s 249us/step - loss: 0.0539 - acc: 0.9849 - val_loss: 0.1056 - val_acc: 0.9706\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 23s 255us/step - loss: 0.0536 - acc: 0.9849 - val_loss: 0.1112 - val_acc: 0.9702\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0531 - acc: 0.9853 - val_loss: 0.1029 - val_acc: 0.9691\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 22s 246us/step - loss: 0.0542 - acc: 0.9849 - val_loss: 0.1102 - val_acc: 0.9691\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 19s 211us/step - loss: 0.0541 - acc: 0.9848 - val_loss: 0.1075 - val_acc: 0.9700\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 19s 211us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.1083 - val_acc: 0.9692\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0555 - acc: 0.9848 - val_loss: 0.1127 - val_acc: 0.9689\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.1085 - val_acc: 0.9680\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0562 - acc: 0.9849 - val_loss: 0.1177 - val_acc: 0.9686\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0564 - acc: 0.9852 - val_loss: 0.1176 - val_acc: 0.9678\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 21s 230us/step - loss: 0.0576 - acc: 0.9844 - val_loss: 0.1092 - val_acc: 0.9684\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0579 - acc: 0.9846 - val_loss: 0.1261 - val_acc: 0.9665\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0588 - acc: 0.9845 - val_loss: 0.1154 - val_acc: 0.9675\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 19s 212us/step - loss: 0.0590 - acc: 0.9846 - val_loss: 0.1232 - val_acc: 0.9678\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0599 - acc: 0.9845 - val_loss: 0.1260 - val_acc: 0.9664\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 19s 213us/step - loss: 0.0613 - acc: 0.9838 - val_loss: 0.1759 - val_acc: 0.9638\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0610 - acc: 0.9837 - val_loss: 0.1262 - val_acc: 0.9677\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 21s 232us/step - loss: 0.1711 - acc: 0.9322 - val_loss: 0.0887 - val_acc: 0.9674\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 19s 214us/step - loss: 0.0632 - acc: 0.9795 - val_loss: 0.0808 - val_acc: 0.9727\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 20s 216us/step - loss: 0.0517 - acc: 0.9847 - val_loss: 0.0751 - val_acc: 0.9781\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 20s 217us/step - loss: 0.0479 - acc: 0.9862 - val_loss: 0.0769 - val_acc: 0.9781 loss: 0 - ETA: 1s\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 20s 218us/step - loss: 0.0461 - acc: 0.9873 - val_loss: 0.0775 - val_acc: 0.9775\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 20s 217us/step - loss: 0.0443 - acc: 0.9881 - val_loss: 0.0765 - val_acc: 0.9778043 - ETA: 1s\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 22s 245us/step - loss: 0.0431 - acc: 0.9886 - val_loss: 0.0763 - val_acc: 0.9789\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 28s 314us/step - loss: 0.0413 - acc: 0.9890 - val_loss: 0.0753 - val_acc: 0.9796\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 29s 320us/step - loss: 0.0404 - acc: 0.9894 - val_loss: 0.0758 - val_acc: 0.9789\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.0398 - acc: 0.9898 - val_loss: 0.0815 - val_acc: 0.9784\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0395 - acc: 0.9899 - val_loss: 0.0786 - val_acc: 0.9790\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0386 - acc: 0.9903 - val_loss: 0.0808 - val_acc: 0.9790\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 28s 305us/step - loss: 0.0393 - acc: 0.9903 - val_loss: 0.0789 - val_acc: 0.9803\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 36s 402us/step - loss: 0.0389 - acc: 0.9903 - val_loss: 0.0819 - val_acc: 0.9798\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0388 - acc: 0.9906 - val_loss: 0.0808 - val_acc: 0.9802\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 27s 294us/step - loss: 0.0392 - acc: 0.9904 - val_loss: 0.0802 - val_acc: 0.9794\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 27s 300us/step - loss: 0.0389 - acc: 0.9905 - val_loss: 0.0785 - val_acc: 0.9803\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0387 - acc: 0.9906 - val_loss: 0.0812 - val_acc: 0.9800\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 23s 254us/step - loss: 0.0385 - acc: 0.9907 - val_loss: 0.0883 - val_acc: 0.9793\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 27s 299us/step - loss: 0.0386 - acc: 0.9909 - val_loss: 0.0845 - val_acc: 0.9807\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 24s 267us/step - loss: 0.0382 - acc: 0.9910 - val_loss: 0.0830 - val_acc: 0.9805\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 23s 254us/step - loss: 0.0379 - acc: 0.9910 - val_loss: 0.0853 - val_acc: 0.9806\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0378 - acc: 0.9911 - val_loss: 0.0843 - val_acc: 0.9805\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0382 - acc: 0.9913 - val_loss: 0.0868 - val_acc: 0.9805\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 23s 252us/step - loss: 0.0375 - acc: 0.9913 - val_loss: 0.0879 - val_acc: 0.9805\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 23s 252us/step - loss: 0.0383 - acc: 0.9913 - val_loss: 0.0867 - val_acc: 0.9809\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0378 - acc: 0.9914 - val_loss: 0.0869 - val_acc: 0.9805\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 21s 237us/step - loss: 0.0380 - acc: 0.9915 - val_loss: 0.0880 - val_acc: 0.9808\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 22s 242us/step - loss: 0.0377 - acc: 0.9914 - val_loss: 0.0929 - val_acc: 0.9792\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 21s 238us/step - loss: 0.0386 - acc: 0.9913 - val_loss: 0.0868 - val_acc: 0.9807\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 23s 252us/step - loss: 0.1456 - acc: 0.9426 - val_loss: 0.0826 - val_acc: 0.9712\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 22s 239us/step - loss: 0.0579 - acc: 0.9819 - val_loss: 0.0637 - val_acc: 0.9817\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 22s 240us/step - loss: 0.0473 - acc: 0.9863 - val_loss: 0.0602 - val_acc: 0.9839\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0415 - acc: 0.9883 - val_loss: 0.0599 - val_acc: 0.9836\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 22s 242us/step - loss: 0.0388 - acc: 0.9893 - val_loss: 0.0610 - val_acc: 0.9831\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 22s 242us/step - loss: 0.0371 - acc: 0.9901 - val_loss: 0.0625 - val_acc: 0.9835\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 22s 245us/step - loss: 0.0362 - acc: 0.9909 - val_loss: 0.0661 - val_acc: 0.98250s - loss: 0.0355 -\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0359 - acc: 0.9908 - val_loss: 0.0633 - val_acc: 0.9824\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 22s 243us/step - loss: 0.0356 - acc: 0.9911 - val_loss: 0.0686 - val_acc: 0.9820\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0353 - acc: 0.9912 - val_loss: 0.0689 - val_acc: 0.9821\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0354 - acc: 0.9913 - val_loss: 0.0691 - val_acc: 0.9812\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0351 - acc: 0.9916 - val_loss: 0.0681 - val_acc: 0.9836\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.0353 - acc: 0.9916 - val_loss: 0.0651 - val_acc: 0.9838\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0348 - acc: 0.9915 - val_loss: 0.0645 - val_acc: 0.9835\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 24s 264us/step - loss: 0.0345 - acc: 0.9918 - val_loss: 0.0701 - val_acc: 0.9829\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 23s 257us/step - loss: 0.0348 - acc: 0.9917 - val_loss: 0.0701 - val_acc: 0.9829\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 24s 265us/step - loss: 0.0344 - acc: 0.9921 - val_loss: 0.0703 - val_acc: 0.9824\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0341 - acc: 0.9921 - val_loss: 0.0739 - val_acc: 0.9828\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 26s 291us/step - loss: 0.0340 - acc: 0.9921 - val_loss: 0.0669 - val_acc: 0.9827\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.0332 - acc: 0.9924 - val_loss: 0.0761 - val_acc: 0.9827\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 25s 278us/step - loss: 0.0337 - acc: 0.9925 - val_loss: 0.0790 - val_acc: 0.9828\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 28s 305us/step - loss: 0.0344 - acc: 0.9923 - val_loss: 0.0728 - val_acc: 0.9820\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 24s 265us/step - loss: 0.0339 - acc: 0.9924 - val_loss: 0.0697 - val_acc: 0.9835\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 23s 251us/step - loss: 0.0331 - acc: 0.9926 - val_loss: 0.0738 - val_acc: 0.9824\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0336 - acc: 0.9924 - val_loss: 0.0704 - val_acc: 0.9834\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 32s 350us/step - loss: 0.0332 - acc: 0.9927 - val_loss: 0.0682 - val_acc: 0.9832\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 30s 336us/step - loss: 0.0341 - acc: 0.9925 - val_loss: 0.0734 - val_acc: 0.9827\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 30s 333us/step - loss: 0.0332 - acc: 0.9925 - val_loss: 0.0758 - val_acc: 0.9836\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 23s 254us/step - loss: 0.0336 - acc: 0.9927 - val_loss: 0.0759 - val_acc: 0.9822\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 23s 257us/step - loss: 0.0338 - acc: 0.9926 - val_loss: 0.0782 - val_acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "# Model3 Definition\n",
    "activation_function = ['relu', 'sigmoid', 'tanh']\n",
    "GSC_sub_act_accuracy = []\n",
    "for i in range(len(activation_function)):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(12, input_dim=512, activation='relu'))\n",
    "    model3.add(Dense(8, activation=activation_function[i]))\n",
    "    model3.add(Dense(1, activation='sigmoid'))\n",
    "    model3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model3, iterating on the data in batches of 32 samples\n",
    "    summary3 = model3.fit(x_train_GSC_sub, y_train_GSC_sub, validation_split = 0.1, nb_epoch=30, batch_size=10)\n",
    "    scores3 = model3.evaluate(x=x_test_GSC_sub, y=y_test_GSC_sub, verbose=0)\n",
    "    scores_acc3 = scores3[1]\n",
    "    GSC_sub_act_accuracy.append(scores_acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Activation Function Vs Accuracy')\n",
    "plt.plot(activation_function,GSC_sub_act_accuracy,'ms-', label='Accuracy')\n",
    "plt.axis([min(activation_function), max(activation_function), min(GSC_sub_act_accuracy)-0.1, max(GSC_sub_act_accuracy)+0.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('activation_function')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for GSC Dataset with Feature Subtraction \n",
      "\n",
      "Test Accuracy =  99%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for GSC Dataset with Feature Subtraction \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores3[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 25s 275us/step - loss: 0.1991 - acc: 0.9146 - val_loss: 0.1073 - val_acc: 0.9611\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 24s 261us/step - loss: 0.0865 - acc: 0.9691 - val_loss: 0.0905 - val_acc: 0.9692\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 27s 303us/step - loss: 0.0698 - acc: 0.9778 - val_loss: 0.0812 - val_acc: 0.9729\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 26s 290us/step - loss: 0.0608 - acc: 0.9818 - val_loss: 0.0858 - val_acc: 0.9746\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0565 - acc: 0.9838 - val_loss: 0.0750 - val_acc: 0.9775\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 25s 276us/step - loss: 0.0542 - acc: 0.9849 - val_loss: 0.0772 - val_acc: 0.9762\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 28s 309us/step - loss: 0.0551 - acc: 0.9846 - val_loss: 0.0888 - val_acc: 0.9749\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 28s 314us/step - loss: 0.0555 - acc: 0.9844 - val_loss: 0.0835 - val_acc: 0.9755\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 29s 316us/step - loss: 0.0576 - acc: 0.9840 - val_loss: 0.0895 - val_acc: 0.9730\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0585 - acc: 0.9838 - val_loss: 0.1043 - val_acc: 0.9728\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 29s 326us/step - loss: 0.0594 - acc: 0.9837 - val_loss: 0.1026 - val_acc: 0.9711\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 35s 392us/step - loss: 0.0607 - acc: 0.9833 - val_loss: 0.0993 - val_acc: 0.9717\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 29s 320us/step - loss: 0.0621 - acc: 0.9829 - val_loss: 0.1045 - val_acc: 0.9680\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 30s 329us/step - loss: 0.0638 - acc: 0.9820 - val_loss: 0.1120 - val_acc: 0.9687\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 28s 306us/step - loss: 0.0641 - acc: 0.9826 - val_loss: 0.1105 - val_acc: 0.9673\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 22s 240us/step - loss: 0.0669 - acc: 0.9817 - val_loss: 0.1144 - val_acc: 0.9657\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 26s 292us/step - loss: 0.0684 - acc: 0.9809 - val_loss: 0.1200 - val_acc: 0.9654\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 23s 257us/step - loss: 0.0684 - acc: 0.9813 - val_loss: 0.1281 - val_acc: 0.9662\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 30s 337us/step - loss: 0.0679 - acc: 0.9816 - val_loss: 0.1287 - val_acc: 0.9642\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 21s 235us/step - loss: 0.0687 - acc: 0.9814 - val_loss: 0.1199 - val_acc: 0.9659\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 30s 337us/step - loss: 0.0706 - acc: 0.9816 - val_loss: 0.1195 - val_acc: 0.9652\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 24s 260us/step - loss: 0.0718 - acc: 0.9814 - val_loss: 0.1305 - val_acc: 0.9640\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0728 - acc: 0.9811 - val_loss: 0.1344 - val_acc: 0.9642\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 23s 253us/step - loss: 0.0736 - acc: 0.9813 - val_loss: 0.1319 - val_acc: 0.9659\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 23s 259us/step - loss: 0.0733 - acc: 0.9810 - val_loss: 0.1336 - val_acc: 0.9635\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 25s 278us/step - loss: 0.0747 - acc: 0.9808 - val_loss: 0.1398 - val_acc: 0.9650\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 23s 257us/step - loss: 0.0759 - acc: 0.9808 - val_loss: 0.2306 - val_acc: 0.9523\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 23s 258us/step - loss: 0.0770 - acc: 0.9808 - val_loss: 0.1598 - val_acc: 0.9616\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 25s 281us/step - loss: 0.0786 - acc: 0.9807 - val_loss: 0.1500 - val_acc: 0.9638\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 32s 358us/step - loss: 0.0789 - acc: 0.9801 - val_loss: 0.1530 - val_acc: 0.9640\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 27s 298us/step - loss: 0.2896 - acc: 0.8672 - val_loss: 0.1362 - val_acc: 0.9479\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 28s 307us/step - loss: 0.0984 - acc: 0.9642 - val_loss: 0.0933 - val_acc: 0.9643\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 27s 297us/step - loss: 0.0717 - acc: 0.9750 - val_loss: 0.0862 - val_acc: 0.9672\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.0590 - acc: 0.9802 - val_loss: 0.0762 - val_acc: 0.9715\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 23s 252us/step - loss: 0.0513 - acc: 0.9828 - val_loss: 0.0795 - val_acc: 0.9710\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 21s 232us/step - loss: 0.0469 - acc: 0.9849 - val_loss: 0.0732 - val_acc: 0.9744\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 19s 210us/step - loss: 0.0429 - acc: 0.9868 - val_loss: 0.0726 - val_acc: 0.9754\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 21s 230us/step - loss: 0.0396 - acc: 0.9881 - val_loss: 0.0759 - val_acc: 0.9741\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 22s 238us/step - loss: 0.0376 - acc: 0.9888 - val_loss: 0.0724 - val_acc: 0.9737\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 21s 232us/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.0716 - val_acc: 0.9770\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 24s 269us/step - loss: 0.0338 - acc: 0.9899 - val_loss: 0.0685 - val_acc: 0.9778\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 23s 255us/step - loss: 0.0335 - acc: 0.9901 - val_loss: 0.0739 - val_acc: 0.9749\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 20s 223us/step - loss: 0.0315 - acc: 0.9910 - val_loss: 0.0723 - val_acc: 0.9757\n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 20s 223us/step - loss: 0.0306 - acc: 0.9910 - val_loss: 0.0678 - val_acc: 0.9772\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 20s 222us/step - loss: 0.0292 - acc: 0.9916 - val_loss: 0.0741 - val_acc: 0.9764\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 20s 224us/step - loss: 0.0279 - acc: 0.9921 - val_loss: 0.0708 - val_acc: 0.9771\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 20s 225us/step - loss: 0.0272 - acc: 0.9924 - val_loss: 0.0770 - val_acc: 0.9771\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 20s 222us/step - loss: 0.0266 - acc: 0.9926 - val_loss: 0.0766 - val_acc: 0.9756\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 20s 223us/step - loss: 0.0268 - acc: 0.9924 - val_loss: 0.0734 - val_acc: 0.9765\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 20s 222us/step - loss: 0.0256 - acc: 0.9929 - val_loss: 0.0735 - val_acc: 0.9777\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 20s 221us/step - loss: 0.0249 - acc: 0.9928 - val_loss: 0.0805 - val_acc: 0.9755\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 20s 223us/step - loss: 0.0253 - acc: 0.9930 - val_loss: 0.0780 - val_acc: 0.9748\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 20s 226us/step - loss: 0.0242 - acc: 0.9934 - val_loss: 0.0750 - val_acc: 0.9776\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 20s 220us/step - loss: 0.0235 - acc: 0.9935 - val_loss: 0.0787 - val_acc: 0.9762\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 20s 221us/step - loss: 0.0235 - acc: 0.9934 - val_loss: 0.0793 - val_acc: 0.9748\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 23s 255us/step - loss: 0.0217 - acc: 0.9942 - val_loss: 0.0812 - val_acc: 0.9754\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 20s 225us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0793 - val_acc: 0.9761\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 25s 279us/step - loss: 0.0221 - acc: 0.9939 - val_loss: 0.0817 - val_acc: 0.9758\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 22s 241us/step - loss: 0.0217 - acc: 0.9941 - val_loss: 0.0795 - val_acc: 0.9764\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0215 - acc: 0.9941 - val_loss: 0.0835 - val_acc: 0.9740\n",
      "Train on 90339 samples, validate on 10038 samples\n",
      "Epoch 1/30\n",
      "90339/90339 [==============================] - 113s 1ms/step - loss: 0.2087 - acc: 0.9130 - val_loss: 0.1079 - val_acc: 0.9589\n",
      "Epoch 2/30\n",
      "90339/90339 [==============================] - 30s 332us/step - loss: 0.0798 - acc: 0.9722 - val_loss: 0.0752 - val_acc: 0.9750\n",
      "Epoch 3/30\n",
      "90339/90339 [==============================] - 35s 386us/step - loss: 0.0592 - acc: 0.9806 - val_loss: 0.0700 - val_acc: 0.9758\n",
      "Epoch 4/30\n",
      "90339/90339 [==============================] - 32s 354us/step - loss: 0.0491 - acc: 0.9846 - val_loss: 0.0634 - val_acc: 0.9799\n",
      "Epoch 5/30\n",
      "90339/90339 [==============================] - 23s 257us/step - loss: 0.0424 - acc: 0.9872 - val_loss: 0.0611 - val_acc: 0.9804\n",
      "Epoch 6/30\n",
      "90339/90339 [==============================] - 23s 250us/step - loss: 0.0378 - acc: 0.9887 - val_loss: 0.0584 - val_acc: 0.9813\n",
      "Epoch 7/30\n",
      "90339/90339 [==============================] - 23s 250us/step - loss: 0.0342 - acc: 0.9900 - val_loss: 0.0617 - val_acc: 0.9798\n",
      "Epoch 8/30\n",
      "90339/90339 [==============================] - 22s 248us/step - loss: 0.0313 - acc: 0.9909 - val_loss: 0.0608 - val_acc: 0.9792\n",
      "Epoch 9/30\n",
      "90339/90339 [==============================] - 26s 286us/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.0588 - val_acc: 0.9802\n",
      "Epoch 10/30\n",
      "90339/90339 [==============================] - 25s 276us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 0.0612 - val_acc: 0.9806\n",
      "Epoch 11/30\n",
      "90339/90339 [==============================] - 24s 262us/step - loss: 0.0252 - acc: 0.9930 - val_loss: 0.0619 - val_acc: 0.9804\n",
      "Epoch 12/30\n",
      "90339/90339 [==============================] - 24s 262us/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0639 - val_acc: 0.9800\n",
      "Epoch 13/30\n",
      "90339/90339 [==============================] - 26s 290us/step - loss: 0.0222 - acc: 0.9939 - val_loss: 0.0641 - val_acc: 0.9809c: \n",
      "Epoch 14/30\n",
      "90339/90339 [==============================] - 22s 246us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0651 - val_acc: 0.9796\n",
      "Epoch 15/30\n",
      "90339/90339 [==============================] - 22s 243us/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0648 - val_acc: 0.9812\n",
      "Epoch 16/30\n",
      "90339/90339 [==============================] - 25s 277us/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0670 - val_acc: 0.9793\n",
      "Epoch 17/30\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0675 - val_acc: 0.9809\n",
      "Epoch 18/30\n",
      "90339/90339 [==============================] - 22s 244us/step - loss: 0.0173 - acc: 0.9955 - val_loss: 0.0681 - val_acc: 0.9795\n",
      "Epoch 19/30\n",
      "90339/90339 [==============================] - 28s 312us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0686 - val_acc: 0.9800\n",
      "Epoch 20/30\n",
      "90339/90339 [==============================] - 26s 287us/step - loss: 0.0158 - acc: 0.9960 - val_loss: 0.0751 - val_acc: 0.9797\n",
      "Epoch 21/30\n",
      "90339/90339 [==============================] - 29s 326us/step - loss: 0.0154 - acc: 0.9960 - val_loss: 0.0731 - val_acc: 0.9801\n",
      "Epoch 22/30\n",
      "90339/90339 [==============================] - 24s 263us/step - loss: 0.0150 - acc: 0.9963 - val_loss: 0.0749 - val_acc: 0.9787\n",
      "Epoch 23/30\n",
      "90339/90339 [==============================] - 25s 274us/step - loss: 0.0143 - acc: 0.9963 - val_loss: 0.0762 - val_acc: 0.9785\n",
      "Epoch 24/30\n",
      "90339/90339 [==============================] - 24s 263us/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0770 - val_acc: 0.9794\n",
      "Epoch 25/30\n",
      "90339/90339 [==============================] - 24s 268us/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0772 - val_acc: 0.9794\n",
      "Epoch 26/30\n",
      "90339/90339 [==============================] - 23s 256us/step - loss: 0.0130 - acc: 0.9968 - val_loss: 0.0792 - val_acc: 0.9787 - loss: 0.0130  - ETA: 0s - loss: 0.0131 - acc: 0.\n",
      "Epoch 27/30\n",
      "90339/90339 [==============================] - 25s 272us/step - loss: 0.0126 - acc: 0.9968 - val_loss: 0.0804 - val_acc: 0.9787\n",
      "Epoch 28/30\n",
      "90339/90339 [==============================] - 25s 273us/step - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0839 - val_acc: 0.9785\n",
      "Epoch 29/30\n",
      "90339/90339 [==============================] - 27s 294us/step - loss: 0.0120 - acc: 0.9969 - val_loss: 0.0842 - val_acc: 0.9800\n",
      "Epoch 30/30\n",
      "90339/90339 [==============================] - 27s 303us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.0877 - val_acc: 0.9785\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "optimizer_var = ['rmsprop', 'SGD', 'adamax']\n",
    "GSC_sub_opt_accuracy = []\n",
    "for i in range(len(optimizer_var)):\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(12, input_dim=512, activation='relu'))\n",
    "    model3.add(Dense(8, activation='relu'))\n",
    "    model3.add(Dense(1, activation='sigmoid'))\n",
    "    model3.compile(optimizer=optimizer_var[i], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model, iterating on the data in batches of 32 samples\n",
    "    summary3 = model3.fit(x_train_GSC_sub, y_train_GSC_sub, validation_split = 0.1, nb_epoch=30, batch_size=10)\n",
    "    scores3 = model3.evaluate(x=x_test_GSC_sub, y=y_test_GSC_sub, verbose=0)\n",
    "    scores_acc3 = scores3[1]\n",
    "    GSC_sub_opt_accuracy.append(scores_acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Optimizer Vs Accuracy')\n",
    "plt.plot(optimizer_var,GSC_sub_opt_accuracy,'bd-', label='Accuracy')\n",
    "#plt.axis([min(optimizer_var), max(optimizer_var), min(hum_con_opt)-0.01, max(hum_con_opt)])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Optimizer')\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1d86ef9dd8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b3bf2a390>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b4200d048>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1b41f5be10>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating plots for accuracy and loss using matplotlib package\n",
    "%matplotlib inline\n",
    "df3 = pd.DataFrame(summary3.history)\n",
    "df3.plot(subplots=True, grid=True, figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Accuracy for GSC Dataset with Feature Subtraction \n",
      "\n",
      "Test Accuracy =  100%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation of Accuracy for GSC Dataset with Feature Subtraction \\n\")\n",
    "print(\"Test Accuracy =  %.0f%%\" %(scores3[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------LOGISTIC REGRESSION -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN-OBSERVED DATA\n",
    "human_raw = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "human_differentpairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "human_samepairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "# GSC DATA\n",
    "GSC_raw = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/GSC-Features.csv\")\n",
    "GSC_differentpairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\")\n",
    "GSC_samepairs = pd.read_csv(\"/Users/yashahuja/Desktop/ML_Project_2/GSC-Dataset/GSC-Features-Data/same_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sigmoid function (returns target values between 0 and 1)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the human data sets in order to make training, testing and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0359a</td>\n",
       "      <td>0359b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 2, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0577b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 0, 3, 2, 2, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0577a</td>\n",
       "      <td>0577c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 3, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1120b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1120c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1120b</td>\n",
       "      <td>1120c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1121b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1121c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1121b</td>\n",
       "      <td>1121c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1229c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 0, 0, 2, 2, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1302b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1302c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1302b</td>\n",
       "      <td>1302c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1305a</td>\n",
       "      <td>1305b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1305a</td>\n",
       "      <td>1305c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1305b</td>\n",
       "      <td>1305c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1386a</td>\n",
       "      <td>1386b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1386a</td>\n",
       "      <td>1386c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1386b</td>\n",
       "      <td>1386c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1391c</td>\n",
       "      <td>1391b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 2, 1, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1391c</td>\n",
       "      <td>1391a</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 3, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1489a</td>\n",
       "      <td>1489b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1489a</td>\n",
       "      <td>1489c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 2, 1, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1489b</td>\n",
       "      <td>1489c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 2, 1, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1517a</td>\n",
       "      <td>1517b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1517a</td>\n",
       "      <td>1517c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 0, 1, 0, 2, 2, 1, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1517b</td>\n",
       "      <td>1517c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[3, 0, 1, 0, 2, 2, 1, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0331b</td>\n",
       "      <td>0331a</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 1, 0, 0, 2, 2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0367a</td>\n",
       "      <td>0367b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0367a</td>\n",
       "      <td>0367c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>1363a</td>\n",
       "      <td>1363c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 1, 2, 3, 2, 3, 2]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 2, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>1500a</td>\n",
       "      <td>1500b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 4, 2, 3, 1, 3, 2]</td>\n",
       "      <td>[0, 2, 1, 0, 0, 2, 2, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>1271a</td>\n",
       "      <td>1271b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 2, 0, 1, 1, 4, 1]</td>\n",
       "      <td>[3, 4, 2, 2, 0, 2, 1, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1271a</td>\n",
       "      <td>1271c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 2, 0, 1, 1, 4, 1]</td>\n",
       "      <td>[0, 1, 2, 4, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0469a</td>\n",
       "      <td>0469b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 3, 1, 1, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[3, 2, 1, 2, 0, 2, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0469a</td>\n",
       "      <td>0469c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 3, 1, 1, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 0, 2, 3, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0469b</td>\n",
       "      <td>0469c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 2, 1, 2, 0, 2, 1, 1, 1]</td>\n",
       "      <td>[2, 0, 1, 1, 0, 2, 3, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0873a</td>\n",
       "      <td>0873b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 0, 2, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[3, 0, 2, 3, 0, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0431b</td>\n",
       "      <td>0431a</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 0, 2, 1, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[3, 2, 1, 3, 1, 2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0460b</td>\n",
       "      <td>0460c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 4, 0, 0, 2, 3, 3, 3, 2]</td>\n",
       "      <td>[2, 4, 2, 1, 2, 2, 2, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>0347a</td>\n",
       "      <td>0347b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 3, 0, 3, 1, 4, 1]</td>\n",
       "      <td>[1, 1, 0, 0, 0, 2, 2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>1140a</td>\n",
       "      <td>1140b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 0, 2, 2, 1, 1, 3, 2]</td>\n",
       "      <td>[0, 0, 1, 3, 0, 1, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0375c</td>\n",
       "      <td>0375b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 4, 2, 2, 2, 1, 1]</td>\n",
       "      <td>[1, 3, 0, 1, 0, 2, 0, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>1406a</td>\n",
       "      <td>1406b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 2, 4, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 0, 0, 1, 0, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>1406a</td>\n",
       "      <td>1406c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 2, 4, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 0, 1, 4, 0, 2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>1406b</td>\n",
       "      <td>1406c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 1, 0, 2, 2, 2, 2]</td>\n",
       "      <td>[1, 0, 1, 4, 0, 2, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>0911b</td>\n",
       "      <td>0911c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 3, 1, 1, 2, 2, 2, 1, 1]</td>\n",
       "      <td>[2, 4, 2, 3, 2, 3, 3, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>0401b</td>\n",
       "      <td>0401c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 2, 3, 2, 3, 3, 2, 2]</td>\n",
       "      <td>[1, 3, 1, 3, 0, 3, 0, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>0376c</td>\n",
       "      <td>0376b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 0, 0, 3, 2, 2, 3, 2]</td>\n",
       "      <td>[1, 2, 0, 2, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0317a</td>\n",
       "      <td>0317b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 2, 0, 1, 1, 2, 1]</td>\n",
       "      <td>[0, 4, 0, 0, 0, 2, 2, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0317a</td>\n",
       "      <td>0317c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 2, 0, 1, 1, 2, 1]</td>\n",
       "      <td>[0, 0, 0, 2, 2, 3, 1, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>0317b</td>\n",
       "      <td>0317c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 4, 0, 0, 0, 2, 2, 3, 2]</td>\n",
       "      <td>[0, 0, 0, 2, 2, 3, 1, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0387b</td>\n",
       "      <td>0387a</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 0, 1, 0, 1, 0, 2, 1]</td>\n",
       "      <td>[0, 1, 0, 2, 0, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>1198a</td>\n",
       "      <td>1198b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 0, 2, 4, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[1, 0, 1, 3, 2, 0, 2, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0549b</td>\n",
       "      <td>0549c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 2, 2, 3, 3, 4, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 3, 3, 3, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>0594b</td>\n",
       "      <td>0594c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 3, 2, 0, 0, 3, 2]</td>\n",
       "      <td>[0, 4, 2, 4, 2, 2, 0, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>1271b</td>\n",
       "      <td>1271c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 2, 2, 0, 2, 1, 3, 2]</td>\n",
       "      <td>[0, 1, 2, 4, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>0556b</td>\n",
       "      <td>0556c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 2, 0, 1, 0, 2, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 3, 1, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>1414b</td>\n",
       "      <td>1414c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 4, 0, 0, 0, 1, 1, 1, 1]</td>\n",
       "      <td>[1, 0, 1, 1, 3, 1, 1, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0323a</td>\n",
       "      <td>0323b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 2, 4, 2, 2, 3, 2, 2]</td>\n",
       "      <td>[3, 0, 0, 1, 0, 3, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_id_A img_id_B  target                 feature_id_A  \\\n",
       "0      0359a    0359b       1  [2, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "1      0577a    0577b       1  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "2      0577a    0577c       1  [2, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "3      1120a    1120b       1  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "4      1120a    1120c       1  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "5      1120b    1120c       1  [1, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "6      1121a    1121b       1  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "7      1121a    1121c       1  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "8      1121b    1121c       1  [2, 1, 1, 0, 2, 2, 0, 3, 2]   \n",
       "9      1229b    1229c       1  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "10     1302a    1302b       1  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "11     1302a    1302c       1  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "12     1302b    1302c       1  [3, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "13     1305a    1305b       1  [1, 1, 1, 0, 2, 2, 0, 0, 2]   \n",
       "14     1305a    1305c       1  [1, 1, 1, 0, 2, 2, 0, 0, 2]   \n",
       "15     1305b    1305c       1  [2, 1, 1, 0, 2, 2, 1, 2, 2]   \n",
       "16     1386a    1386b       1  [1, 1, 1, 0, 2, 2, 0, 3, 2]   \n",
       "17     1386a    1386c       1  [1, 1, 1, 0, 2, 2, 0, 3, 2]   \n",
       "18     1386b    1386c       1  [3, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "19     1391c    1391b       1  [2, 1, 1, 0, 2, 2, 1, 1, 2]   \n",
       "20     1391c    1391a       1  [2, 1, 1, 0, 2, 2, 1, 1, 2]   \n",
       "21     1489a    1489b       1  [1, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "22     1489a    1489c       1  [1, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "23     1489b    1489c       1  [2, 1, 1, 3, 2, 2, 1, 2, 2]   \n",
       "24     1517a    1517b       1  [2, 1, 1, 1, 2, 2, 0, 2, 2]   \n",
       "25     1517a    1517c       1  [2, 1, 1, 1, 2, 2, 0, 2, 2]   \n",
       "26     1517b    1517c       1  [1, 1, 1, 3, 2, 2, 0, 3, 2]   \n",
       "27     0331b    0331a       1  [3, 1, 1, 0, 2, 2, 0, 0, 2]   \n",
       "28     0367a    0367b       1  [2, 1, 1, 0, 2, 2, 0, 4, 2]   \n",
       "29     0367a    0367c       1  [2, 1, 1, 0, 2, 2, 0, 4, 2]   \n",
       "..       ...      ...     ...                          ...   \n",
       "761    1363a    1363c       1  [0, 0, 1, 1, 2, 3, 2, 3, 2]   \n",
       "762    1500a    1500b       1  [3, 4, 1, 4, 2, 3, 1, 3, 2]   \n",
       "763    1271a    1271b       1  [1, 1, 1, 2, 0, 1, 1, 4, 1]   \n",
       "764    1271a    1271c       1  [1, 1, 1, 2, 0, 1, 1, 4, 1]   \n",
       "765    0469a    0469b       1  [2, 3, 1, 1, 2, 2, 3, 4, 2]   \n",
       "766    0469a    0469c       1  [2, 3, 1, 1, 2, 2, 3, 4, 2]   \n",
       "767    0469b    0469c       1  [3, 2, 1, 2, 0, 2, 1, 1, 1]   \n",
       "768    0873a    0873b       1  [2, 2, 0, 2, 2, 2, 3, 0, 2]   \n",
       "769    0431b    0431a       1  [3, 0, 2, 1, 2, 3, 0, 2, 2]   \n",
       "770    0460b    0460c       1  [1, 4, 0, 0, 2, 3, 3, 3, 2]   \n",
       "771    0347a    0347b       1  [2, 4, 1, 3, 0, 3, 1, 4, 1]   \n",
       "772    1140a    1140b       1  [3, 4, 0, 2, 2, 1, 1, 3, 2]   \n",
       "773    0375c    0375b       1  [2, 4, 1, 4, 2, 2, 2, 1, 1]   \n",
       "774    1406a    1406b       1  [1, 2, 2, 4, 2, 2, 0, 2, 2]   \n",
       "775    1406a    1406c       1  [1, 2, 2, 4, 2, 2, 0, 2, 2]   \n",
       "776    1406b    1406c       1  [1, 0, 0, 1, 0, 2, 2, 2, 2]   \n",
       "777    0911b    0911c       1  [1, 3, 1, 1, 2, 2, 2, 1, 1]   \n",
       "778    0401b    0401c       1  [2, 0, 2, 3, 2, 3, 3, 2, 2]   \n",
       "779    0376c    0376b       1  [2, 1, 0, 0, 3, 2, 2, 3, 2]   \n",
       "780    0317a    0317b       1  [1, 1, 0, 2, 0, 1, 1, 2, 1]   \n",
       "781    0317a    0317c       1  [1, 1, 0, 2, 0, 1, 1, 2, 1]   \n",
       "782    0317b    0317c       1  [0, 4, 0, 0, 0, 2, 2, 3, 2]   \n",
       "783    0387b    0387a       1  [2, 2, 0, 1, 0, 1, 0, 2, 1]   \n",
       "784    1198a    1198b       1  [3, 0, 2, 4, 2, 2, 0, 4, 2]   \n",
       "785    0549b    0549c       1  [0, 1, 0, 2, 2, 3, 3, 4, 2]   \n",
       "786    0594b    0594c       1  [0, 0, 0, 3, 2, 0, 0, 3, 2]   \n",
       "787    1271b    1271c       1  [3, 4, 2, 2, 0, 2, 1, 3, 2]   \n",
       "788    0556b    0556c       1  [0, 2, 0, 1, 0, 2, 1, 1, 1]   \n",
       "789    1414b    1414c       1  [0, 4, 0, 0, 0, 1, 1, 1, 1]   \n",
       "790    0323a    0323b       1  [3, 4, 2, 4, 2, 2, 3, 2, 2]   \n",
       "\n",
       "                    feature_id_B  \n",
       "0    [3, 2, 1, 0, 2, 2, 3, 0, 2]  \n",
       "1    [2, 1, 0, 3, 2, 2, 1, 2, 2]  \n",
       "2    [1, 1, 1, 1, 2, 3, 0, 0, 2]  \n",
       "3    [1, 1, 1, 0, 2, 2, 0, 2, 2]  \n",
       "4    [2, 1, 1, 0, 2, 2, 0, 0, 2]  \n",
       "5    [2, 1, 1, 0, 2, 2, 0, 0, 2]  \n",
       "6    [2, 1, 1, 0, 2, 2, 0, 3, 2]  \n",
       "7    [1, 1, 1, 0, 2, 2, 0, 1, 2]  \n",
       "8    [1, 1, 1, 0, 2, 2, 0, 1, 2]  \n",
       "9    [2, 1, 0, 0, 2, 2, 1, 2, 2]  \n",
       "10   [3, 1, 1, 0, 2, 2, 0, 2, 2]  \n",
       "11   [2, 1, 1, 3, 2, 2, 0, 3, 2]  \n",
       "12   [2, 1, 1, 3, 2, 2, 0, 3, 2]  \n",
       "13   [2, 1, 1, 0, 2, 2, 1, 2, 2]  \n",
       "14   [1, 1, 1, 3, 2, 2, 0, 1, 2]  \n",
       "15   [1, 1, 1, 3, 2, 2, 0, 1, 2]  \n",
       "16   [3, 1, 1, 0, 2, 2, 0, 1, 2]  \n",
       "17   [3, 1, 1, 3, 2, 2, 0, 2, 2]  \n",
       "18   [3, 1, 1, 3, 2, 2, 0, 2, 2]  \n",
       "19   [3, 4, 1, 0, 2, 2, 1, 4, 2]  \n",
       "20   [3, 4, 1, 3, 2, 3, 0, 3, 2]  \n",
       "21   [2, 1, 1, 3, 2, 2, 1, 2, 2]  \n",
       "22   [2, 4, 1, 0, 2, 2, 1, 1, 2]  \n",
       "23   [2, 4, 1, 0, 2, 2, 1, 1, 2]  \n",
       "24   [1, 1, 1, 3, 2, 2, 0, 3, 2]  \n",
       "25   [3, 0, 1, 0, 2, 2, 1, 4, 2]  \n",
       "26   [3, 0, 1, 0, 2, 2, 1, 4, 2]  \n",
       "27   [3, 1, 0, 0, 2, 2, 3, 2, 2]  \n",
       "28   [1, 1, 1, 0, 2, 2, 1, 2, 2]  \n",
       "29   [3, 1, 1, 3, 2, 2, 0, 1, 2]  \n",
       "..                           ...  \n",
       "761  [0, 1, 0, 1, 0, 2, 0, 1, 1]  \n",
       "762  [0, 2, 1, 0, 0, 2, 2, 2, 1]  \n",
       "763  [3, 4, 2, 2, 0, 2, 1, 3, 2]  \n",
       "764  [0, 1, 2, 4, 2, 2, 2, 2, 2]  \n",
       "765  [3, 2, 1, 2, 0, 2, 1, 1, 1]  \n",
       "766  [2, 0, 1, 1, 0, 2, 3, 0, 1]  \n",
       "767  [2, 0, 1, 1, 0, 2, 3, 0, 1]  \n",
       "768  [3, 0, 2, 3, 0, 2, 0, 3, 2]  \n",
       "769  [3, 2, 1, 3, 1, 2, 3, 2, 2]  \n",
       "770  [2, 4, 2, 1, 2, 2, 2, 1, 2]  \n",
       "771  [1, 1, 0, 0, 0, 2, 2, 0, 1]  \n",
       "772  [0, 0, 1, 3, 0, 1, 1, 2, 1]  \n",
       "773  [1, 3, 0, 1, 0, 2, 0, 2, 1]  \n",
       "774  [1, 0, 0, 1, 0, 2, 2, 2, 2]  \n",
       "775  [1, 0, 1, 4, 0, 2, 3, 2, 2]  \n",
       "776  [1, 0, 1, 4, 0, 2, 3, 2, 2]  \n",
       "777  [2, 4, 2, 3, 2, 3, 3, 2, 2]  \n",
       "778  [1, 3, 1, 3, 0, 3, 0, 4, 2]  \n",
       "779  [1, 2, 0, 2, 0, 1, 1, 1, 1]  \n",
       "780  [0, 4, 0, 0, 0, 2, 2, 3, 2]  \n",
       "781  [0, 0, 0, 2, 2, 3, 1, 3, 2]  \n",
       "782  [0, 0, 0, 2, 2, 3, 1, 3, 2]  \n",
       "783  [0, 1, 0, 2, 0, 1, 2, 2, 2]  \n",
       "784  [1, 0, 1, 3, 2, 0, 2, 2, 1]  \n",
       "785  [3, 1, 1, 3, 3, 3, 3, 4, 2]  \n",
       "786  [0, 4, 2, 4, 2, 2, 0, 3, 2]  \n",
       "787  [0, 1, 2, 4, 2, 2, 2, 2, 2]  \n",
       "788  [0, 1, 0, 1, 0, 3, 1, 0, 1]  \n",
       "789  [1, 0, 1, 1, 3, 1, 1, 2, 1]  \n",
       "790  [3, 0, 0, 1, 0, 3, 1, 1, 1]  \n",
       "\n",
       "[791 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking features of images for the human observed dataset(diff pairs)\n",
    "human_differentpairs['feature_id_A'] = 'NaN'\n",
    "human_differentpairs['feature_id_B'] = 'NaN'\n",
    "for i in human_differentpairs.index[1400:2900]:\n",
    "    a=human_raw[human_raw[\"img_id\"] == human_differentpairs.at[i,\"img_id_A\"]]\n",
    "    b=human_raw[human_raw[\"img_id\"] == human_differentpairs.at[i,\"img_id_B\"]]\n",
    "    human_differentpairs.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    human_differentpairs.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])\n",
    "human_df1=human_differentpairs[1400:2900]\n",
    "# Taking features of images for the human observed dataset(same pairs)\n",
    "human_samepairs['feature_id_A'] = 'NaN'\n",
    "human_samepairs['feature_id_B'] = 'NaN'\n",
    "for i in human_samepairs.index:\n",
    "    a=human_raw[human_raw[\"img_id\"] == human_samepairs.at[i,\"img_id_A\"]]\n",
    "    b=human_raw[human_raw[\"img_id\"] == human_samepairs.at[i,\"img_id_B\"]]\n",
    "    human_samepairs.at[i,'feature_id_A']=np.array(a.iloc[0][2:11])\n",
    "    human_samepairs.at[i,'feature_id_B']=np.array(b.iloc[0][2:11])\n",
    "human_samepairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AconB</th>\n",
       "      <th>feature_id_AsubB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0571a</td>\n",
       "      <td>0571c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 4, 1, 3, 2, 3, 0, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 1, 0, 1, 0, 1, 1]</td>\n",
       "      <td>[2, 4, 1, 3, 2, 3, 0, 1, 2, 1, 0, 1, 1, 0, 1, ...</td>\n",
       "      <td>[1, 4, 0, 2, 2, 2, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1134a</td>\n",
       "      <td>1134c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 3, 2, 2, 1, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 3, 2, 2, 1, 0, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, -1, 0, 3, 0, 0, -2, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0594a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 2, 1, 0, 2, 2, 1, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 3, 0, 0, -1, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1319a</td>\n",
       "      <td>1319c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 0, 1, 0, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[1, 0, 1, 0, 2, 0, 1, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 2, 2, 3, 4, 2, 1, 0, 1, 0, 2, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 2, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1366b</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 2, 1, 3, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>1302a</td>\n",
       "      <td>0592b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 2, 1, 1, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 2, 1, 1, 2, 2, ...</td>\n",
       "      <td>[1, -1, 0, 2, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1343a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 2, 1, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 0, 1, 0, 2, 1, ...</td>\n",
       "      <td>[0, 1, 0, 3, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1231a</td>\n",
       "      <td>1231b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 1, 0, 2]</td>\n",
       "      <td>[0, 0, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[1, 2, 0, 0, 0, 0, 1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1563a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[0, 1, 1, 3, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 0, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1240a</td>\n",
       "      <td>1240c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 1, 3, 2, 2, 3, 1, 2]</td>\n",
       "      <td>[2, 1, 0, 0, 2, 3, 1, 1, 2]</td>\n",
       "      <td>[1, 0, 1, 3, 2, 2, 3, 1, 2, 2, 1, 0, 0, 2, 3, ...</td>\n",
       "      <td>[-1, -1, 1, 3, 0, -1, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1352c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 2, 1, 0, 2, 2, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 2, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, -1, 0, 3, 0, 0, -1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1178b</td>\n",
       "      <td>1178c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 0, 1, 3, 2, 2, 3, 1, 2]</td>\n",
       "      <td>[2, 4, 1, 3, 0, 2, 1, 0, 2]</td>\n",
       "      <td>[3, 0, 1, 3, 2, 2, 3, 1, 2, 2, 4, 1, 3, 0, 2, ...</td>\n",
       "      <td>[1, -4, 0, 0, 2, 0, 2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1309a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, 3, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1336a</td>\n",
       "      <td>1336b</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 2, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 0, 4, 2]</td>\n",
       "      <td>[0, 1, 1, 2, 2, 2, 0, 0, 2, 3, 1, 1, 3, 2, 3, ...</td>\n",
       "      <td>[-3, 0, 0, -1, 0, -1, 0, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1134c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 0, -3, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1468a</td>\n",
       "      <td>1468b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 1, 1, 2, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 0, 0, 2, 3, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>[-2, 0, 0, 0, 0, 0, -1, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1491a</td>\n",
       "      <td>1491c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 1, 0, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 1, 3, 2, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, -1, 0, 0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1289b</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, -3, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1164c</td>\n",
       "      <td>1164b</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 2, 1, 2, 1, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 3, 0, 2, 2, 2, 1, 1, 2, 1, 2, ...</td>\n",
       "      <td>[1, 0, 0, 1, 1, 1, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1448c</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 3, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-2, -3, 0, 0, 0, 0, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1180b</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 1, 1]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 0, -1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1531c</td>\n",
       "      <td>1531a</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 3, 1, 2, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 3, 1, 2, 2, 2, 0, 1, 0, 2, 3, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>1302a</td>\n",
       "      <td>0375a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 0, 1, 3, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 0, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, 0, 0, 0, 0, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1152b</td>\n",
       "      <td>1152a</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[3, 4, 0, 2, 2, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 3, 0, 3, 2, 3, 4, 0, 2, 2, 1, ...</td>\n",
       "      <td>[-1, -3, 1, -1, 0, 2, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>1363b</td>\n",
       "      <td>1363a</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 0, 2, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 1, 1, 2, 3, 2, 3, 2]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 2, 3, ...</td>\n",
       "      <td>[0, 1, 0, 0, -2, -1, -2, -1, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0424a</td>\n",
       "      <td>0424c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 3, 3, 2]</td>\n",
       "      <td>[1, 2, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, 3, 0, 0, -3, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1197a</td>\n",
       "      <td>1197b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 4, 1, 3, 0, 2, 1, 0, 2]</td>\n",
       "      <td>[0, 0, 1, 4, 2, 3, 0, 4, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 0, 2, 1, 0, 2, 0, 0, 1, 4, 2, 3, ...</td>\n",
       "      <td>[1, 4, 0, -1, -2, -1, 1, -4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1289b</td>\n",
       "      <td>1289c</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 1, 4, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, -1, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1180c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 4, 1, 0, 2, 3, ...</td>\n",
       "      <td>[0, -3, 0, 3, 0, -1, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0489c</td>\n",
       "      <td>0489a</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 1, 2, 1]</td>\n",
       "      <td>[1, 4, 2, 0, 2, 3, 1, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 4, 2, 0, 2, 3, ...</td>\n",
       "      <td>[0, -3, -1, 1, 0, -1, 0, 0, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1121a</td>\n",
       "      <td>0349a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 1, 2, 1, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[-1, 0, 0, 2, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1120a</td>\n",
       "      <td>1551a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 2, 3, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, 3, 0, 0, -3, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>1229b</td>\n",
       "      <td>0565c</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 2, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1336c</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 3, 0, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 3, ...</td>\n",
       "      <td>[1, 0, 0, 3, 0, -1, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1468a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, ...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1329c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 0, 2, 2, 3, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 0, 0, 3, 0, 0, -3, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0363b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 1, 0, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, ...</td>\n",
       "      <td>[1, 0, 0, 2, 0, 1, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>1229b</td>\n",
       "      <td>0552c</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 2, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 2, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, -1, 0, 3, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1252b</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 2, 1, 3, 2, 2, 1, 1, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>1302a</td>\n",
       "      <td>0314b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[0, 0, 1, 0, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 0, 0, 1, 0, 2, 2, ...</td>\n",
       "      <td>[2, 1, 0, 3, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0350b</td>\n",
       "      <td>0350c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 1, 1, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 0, 1, 1, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 1, 1, 4, 2, 2, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>[0, 1, 0, 3, 1, 0, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1236a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 0, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 0, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, 1, 0, 0, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>1302a</td>\n",
       "      <td>0458b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 2, 1, 3, 2, 2, 1, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 2, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, 0, -1, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1255a</td>\n",
       "      <td>1255b</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 0, 1]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 1, 4, 2]</td>\n",
       "      <td>[1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -1, 0, 0, 0, -4, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1446c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1483a</td>\n",
       "      <td>1483c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 3, 2, 3, 0, 3, 2]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 3, 0, 3, 2, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 2, 2, 2, 0, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1236b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 4, 1, 0, 2, 2, 1, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 4, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-1, -3, 0, 3, 0, 0, -1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1461b</td>\n",
       "      <td>1461c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 1, 0, 2, 1, 0, 0, 2]</td>\n",
       "      <td>[2, 4, 1, 3, 2, 1, 0, 3, 2]</td>\n",
       "      <td>[2, 2, 1, 0, 2, 1, 0, 0, 2, 2, 4, 1, 3, 2, 1, ...</td>\n",
       "      <td>[0, -2, 0, -3, 0, 0, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0469a</td>\n",
       "      <td>0469c</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 3, 1, 1, 2, 2, 3, 4, 2]</td>\n",
       "      <td>[2, 0, 1, 1, 0, 2, 3, 0, 1]</td>\n",
       "      <td>[2, 3, 1, 1, 2, 2, 3, 4, 2, 2, 0, 1, 1, 0, 2, ...</td>\n",
       "      <td>[0, 3, 0, 0, 2, 0, 0, 4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0477a</td>\n",
       "      <td>0477b</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 0, 0, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 1, 2, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, -2, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>1121a</td>\n",
       "      <td>1453c</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 0, 2, 2, 0, 2, 1]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 0, 0, -1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>1229b</td>\n",
       "      <td>0331b</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...</td>\n",
       "      <td>[-2, 0, 0, 3, 0, 0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>1229b</td>\n",
       "      <td>1451a</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 4, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, -3, 0, 0, 0, 0, 0, -2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>1302a</td>\n",
       "      <td>1375b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[1, 4, 1, 3, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 4, 1, 3, 2, 2, ...</td>\n",
       "      <td>[1, -3, 0, 0, 0, 0, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1302a</td>\n",
       "      <td>0422b</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 2, 0, 3, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 4, 1, 0, 2, 2, ...</td>\n",
       "      <td>[0, -3, 0, 3, 0, 0, 0, -3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1386b</td>\n",
       "      <td>1386c</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[3, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[3, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 3, 2, 2, ...</td>\n",
       "      <td>[0, 0, 0, -3, 0, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>0507b</td>\n",
       "      <td>0507c</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 3, 2, 1, 1, 4, 2]</td>\n",
       "      <td>[3, 4, 1, 1, 2, 1, 0, 1, 2]</td>\n",
       "      <td>[0, 1, 1, 3, 2, 1, 1, 4, 2, 3, 4, 1, 1, 2, 1, ...</td>\n",
       "      <td>[-3, -3, 0, 2, 0, 0, 1, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>1229b</td>\n",
       "      <td>0532c</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 2, 1, 3, 2, 3, 0, 2, 2]</td>\n",
       "      <td>[1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 3, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, -1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1120a</td>\n",
       "      <td>0319a</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2]</td>\n",
       "      <td>[2, 4, 1, 0, 2, 3, 1, 2, 2]</td>\n",
       "      <td>[2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 4, 1, 0, 2, 3, ...</td>\n",
       "      <td>[0, -3, 0, 3, 0, -1, -1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0496c</td>\n",
       "      <td>0496d</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2]</td>\n",
       "      <td>[2, 1, 1, 1, 2, 2, 0, 1, 1]</td>\n",
       "      <td>[3, 4, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, ...</td>\n",
       "      <td>[1, 3, 0, 2, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2291 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     img_id_A img_id_B  target                 feature_id_A  \\\n",
       "436     0571a    0571c       1  [2, 4, 1, 3, 2, 3, 0, 1, 2]   \n",
       "293     1134a    1134c       1  [2, 0, 1, 3, 2, 2, 1, 0, 2]   \n",
       "1469    1120a    0594a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "592     1319a    1319c       1  [2, 0, 1, 0, 2, 2, 3, 4, 2]   \n",
       "2222    1229b    1366b       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2784    1302a    0592b       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "2827    1302a    1343a       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "352     1231a    1231b       1  [1, 2, 1, 0, 2, 2, 1, 0, 2]   \n",
       "2883    1302a    1563a       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "549     1240a    1240c       1  [1, 0, 1, 3, 2, 2, 3, 1, 2]   \n",
       "1684    1121a    1352c       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "597     1178b    1178c       1  [3, 0, 1, 3, 2, 2, 3, 1, 2]   \n",
       "1542    1121a    1309a       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "406     1336a    1336b       1  [0, 1, 1, 2, 2, 2, 0, 0, 2]   \n",
       "2787    1302a    1134c       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "174     1468a    1468b       1  [1, 1, 1, 2, 2, 2, 0, 0, 2]   \n",
       "83      1491a    1491c       1  [3, 1, 1, 0, 2, 2, 1, 3, 2]   \n",
       "2107    1229b    1289b       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "208     1164c    1164b       1  [3, 1, 1, 3, 2, 3, 0, 2, 2]   \n",
       "2312    1229b    1448c       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2201    1229b    1180b       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "317     1531c    1531a       1  [3, 1, 1, 0, 2, 3, 1, 2, 2]   \n",
       "2839    1302a    0375a       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "328     1152b    1152a       1  [2, 1, 1, 1, 2, 3, 0, 3, 2]   \n",
       "692     1363b    1363a       1  [0, 1, 1, 1, 0, 2, 0, 2, 1]   \n",
       "230     0424a    0424c       1  [1, 2, 1, 3, 2, 2, 0, 1, 2]   \n",
       "641     1197a    1197b       1  [1, 4, 1, 3, 0, 2, 1, 0, 2]   \n",
       "138     1289b    1289c       1  [1, 4, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2887    1302a    1180c       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "396     0489c    0489a       1  [1, 1, 1, 1, 2, 2, 1, 2, 1]   \n",
       "...       ...      ...     ...                          ...   \n",
       "1771    1121a    0349a       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "1447    1120a    1551a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2022    1229b    0565c       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2355    1229b    1336c       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2639    1302a    1468a       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "1928    1121a    1329c       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "1496    1120a    0363b       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2114    1229b    0552c       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2257    1229b    1252b       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2768    1302a    0314b       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "355     0350b    0350c       1  [2, 1, 1, 3, 2, 1, 1, 4, 2]   \n",
       "1733    1121a    1236a       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "2896    1302a    0458b       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "280     1255a    1255b       1  [1, 1, 1, 0, 2, 2, 1, 0, 1]   \n",
       "2525    1302a    1446c       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "177     1483a    1483c       1  [2, 1, 1, 3, 2, 3, 0, 3, 2]   \n",
       "1804    1121a    1236b       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "427     1461b    1461c       1  [2, 2, 1, 0, 2, 1, 0, 0, 2]   \n",
       "766     0469a    0469c       1  [2, 3, 1, 1, 2, 2, 3, 4, 2]   \n",
       "243     0477a    0477b       1  [2, 1, 1, 1, 2, 1, 0, 0, 2]   \n",
       "1545    1121a    1453c       0  [2, 1, 1, 3, 2, 2, 0, 1, 2]   \n",
       "2014    1229b    0331b       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2277    1229b    1451a       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "2674    1302a    1375b       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "2588    1302a    0422b       0  [2, 1, 1, 3, 2, 2, 0, 0, 2]   \n",
       "18      1386b    1386c       1  [3, 1, 1, 0, 2, 2, 0, 1, 2]   \n",
       "576     0507b    0507c       1  [0, 1, 1, 3, 2, 1, 1, 4, 2]   \n",
       "2409    1229b    0532c       0  [1, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "1443    1120a    0319a       0  [2, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "231     0496c    0496d       1  [3, 4, 1, 3, 2, 2, 0, 1, 2]   \n",
       "\n",
       "                     feature_id_B  \\\n",
       "436   [1, 0, 1, 1, 0, 1, 0, 1, 1]   \n",
       "293   [2, 1, 1, 0, 2, 2, 3, 4, 2]   \n",
       "1469  [3, 2, 1, 0, 2, 2, 1, 4, 2]   \n",
       "592   [1, 0, 1, 0, 2, 0, 1, 4, 2]   \n",
       "2222  [2, 2, 1, 3, 2, 2, 1, 2, 2]   \n",
       "2784  [1, 2, 1, 1, 2, 2, 0, 2, 2]   \n",
       "2827  [2, 0, 1, 0, 2, 1, 0, 0, 2]   \n",
       "352   [0, 0, 1, 0, 2, 2, 0, 1, 2]   \n",
       "2883  [0, 1, 1, 3, 2, 3, 0, 0, 2]   \n",
       "549   [2, 1, 0, 0, 2, 3, 1, 1, 2]   \n",
       "1684  [2, 2, 1, 0, 2, 2, 1, 2, 2]   \n",
       "597   [2, 4, 1, 3, 0, 2, 1, 0, 2]   \n",
       "1542  [0, 1, 1, 0, 2, 2, 0, 2, 2]   \n",
       "406   [3, 1, 1, 3, 2, 3, 0, 4, 2]   \n",
       "2787  [2, 1, 1, 0, 2, 2, 3, 4, 2]   \n",
       "174   [3, 1, 1, 2, 2, 2, 1, 3, 2]   \n",
       "83    [1, 1, 1, 1, 2, 2, 1, 0, 2]   \n",
       "2107  [1, 4, 1, 3, 2, 2, 0, 2, 2]   \n",
       "208   [2, 1, 1, 2, 1, 2, 1, 1, 2]   \n",
       "2312  [3, 4, 1, 3, 2, 2, 1, 1, 2]   \n",
       "2201  [1, 1, 1, 0, 2, 2, 1, 1, 1]   \n",
       "317   [2, 0, 1, 0, 2, 3, 0, 3, 2]   \n",
       "2839  [3, 0, 1, 3, 2, 2, 0, 4, 2]   \n",
       "328   [3, 4, 0, 2, 2, 1, 0, 2, 2]   \n",
       "692   [0, 0, 1, 1, 2, 3, 2, 3, 2]   \n",
       "230   [2, 1, 1, 0, 2, 2, 3, 3, 2]   \n",
       "641   [0, 0, 1, 4, 2, 3, 0, 4, 2]   \n",
       "138   [1, 1, 1, 3, 2, 2, 1, 4, 2]   \n",
       "2887  [2, 4, 1, 0, 2, 3, 0, 3, 2]   \n",
       "396   [1, 4, 2, 0, 2, 3, 1, 2, 2]   \n",
       "...                           ...   \n",
       "1771  [3, 1, 1, 1, 2, 1, 0, 1, 2]   \n",
       "1447  [0, 1, 1, 0, 2, 2, 3, 3, 2]   \n",
       "2022  [2, 1, 1, 1, 2, 2, 0, 1, 2]   \n",
       "2355  [0, 1, 1, 0, 2, 3, 0, 0, 2]   \n",
       "2639  [1, 1, 1, 2, 2, 2, 0, 0, 2]   \n",
       "1928  [0, 1, 1, 0, 2, 2, 3, 0, 2]   \n",
       "1496  [1, 1, 1, 1, 2, 1, 0, 1, 1]   \n",
       "2114  [1, 2, 1, 0, 2, 2, 0, 2, 2]   \n",
       "2257  [2, 2, 1, 3, 2, 2, 1, 1, 2]   \n",
       "2768  [0, 0, 1, 0, 2, 2, 0, 2, 2]   \n",
       "355   [2, 0, 1, 0, 1, 1, 0, 2, 2]   \n",
       "1733  [3, 0, 1, 3, 2, 2, 0, 3, 2]   \n",
       "2896  [3, 2, 1, 3, 2, 2, 1, 3, 2]   \n",
       "280   [1, 1, 1, 1, 2, 2, 1, 4, 2]   \n",
       "2525  [3, 1, 1, 3, 2, 2, 0, 3, 2]   \n",
       "177   [1, 1, 1, 1, 0, 1, 0, 1, 1]   \n",
       "1804  [3, 4, 1, 0, 2, 2, 1, 0, 2]   \n",
       "427   [2, 4, 1, 3, 2, 1, 0, 3, 2]   \n",
       "766   [2, 0, 1, 1, 0, 2, 3, 0, 1]   \n",
       "243   [2, 1, 1, 1, 2, 3, 0, 2, 2]   \n",
       "1545  [2, 1, 1, 0, 2, 2, 0, 2, 1]   \n",
       "2014  [3, 1, 1, 0, 2, 2, 0, 0, 2]   \n",
       "2277  [1, 4, 1, 3, 2, 2, 0, 4, 2]   \n",
       "2674  [1, 4, 1, 3, 2, 2, 0, 3, 2]   \n",
       "2588  [2, 4, 1, 0, 2, 2, 0, 3, 2]   \n",
       "18    [3, 1, 1, 3, 2, 2, 0, 2, 2]   \n",
       "576   [3, 4, 1, 1, 2, 1, 0, 1, 2]   \n",
       "2409  [2, 2, 1, 3, 2, 3, 0, 2, 2]   \n",
       "1443  [2, 4, 1, 0, 2, 3, 1, 2, 2]   \n",
       "231   [2, 1, 1, 1, 2, 2, 0, 1, 1]   \n",
       "\n",
       "                                       feature_id_AconB  \\\n",
       "436   [2, 4, 1, 3, 2, 3, 0, 1, 2, 1, 0, 1, 1, 0, 1, ...   \n",
       "293   [2, 0, 1, 3, 2, 2, 1, 0, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "1469  [2, 1, 1, 3, 2, 2, 0, 2, 2, 3, 2, 1, 0, 2, 2, ...   \n",
       "592   [2, 0, 1, 0, 2, 2, 3, 4, 2, 1, 0, 1, 0, 2, 0, ...   \n",
       "2222  [1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 2, ...   \n",
       "2784  [2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 2, 1, 1, 2, 2, ...   \n",
       "2827  [2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 0, 1, 0, 2, 1, ...   \n",
       "352   [1, 2, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 0, 2, 2, ...   \n",
       "2883  [2, 1, 1, 3, 2, 2, 0, 0, 2, 0, 1, 1, 3, 2, 3, ...   \n",
       "549   [1, 0, 1, 3, 2, 2, 3, 1, 2, 2, 1, 0, 0, 2, 3, ...   \n",
       "1684  [2, 1, 1, 3, 2, 2, 0, 1, 2, 2, 2, 1, 0, 2, 2, ...   \n",
       "597   [3, 0, 1, 3, 2, 2, 3, 1, 2, 2, 4, 1, 3, 0, 2, ...   \n",
       "1542  [2, 1, 1, 3, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...   \n",
       "406   [0, 1, 1, 2, 2, 2, 0, 0, 2, 3, 1, 1, 3, 2, 3, ...   \n",
       "2787  [2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "174   [1, 1, 1, 2, 2, 2, 0, 0, 2, 3, 1, 1, 2, 2, 2, ...   \n",
       "83    [3, 1, 1, 0, 2, 2, 1, 3, 2, 1, 1, 1, 1, 2, 2, ...   \n",
       "2107  [1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "208   [3, 1, 1, 3, 2, 3, 0, 2, 2, 2, 1, 1, 2, 1, 2, ...   \n",
       "2312  [1, 1, 1, 3, 2, 2, 0, 2, 2, 3, 4, 1, 3, 2, 2, ...   \n",
       "2201  [1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 2, ...   \n",
       "317   [3, 1, 1, 0, 2, 3, 1, 2, 2, 2, 0, 1, 0, 2, 3, ...   \n",
       "2839  [2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 0, 1, 3, 2, 2, ...   \n",
       "328   [2, 1, 1, 1, 2, 3, 0, 3, 2, 3, 4, 0, 2, 2, 1, ...   \n",
       "692   [0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 2, 3, ...   \n",
       "230   [1, 2, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "641   [1, 4, 1, 3, 0, 2, 1, 0, 2, 0, 0, 1, 4, 2, 3, ...   \n",
       "138   [1, 4, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 3, 2, 2, ...   \n",
       "2887  [2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 4, 1, 0, 2, 3, ...   \n",
       "396   [1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 4, 2, 0, 2, 3, ...   \n",
       "...                                                 ...   \n",
       "1771  [2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 1, 1, 1, 2, 1, ...   \n",
       "1447  [2, 1, 1, 3, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 2, ...   \n",
       "2022  [1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, ...   \n",
       "2355  [1, 1, 1, 3, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 3, ...   \n",
       "2639  [2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, ...   \n",
       "1928  [2, 1, 1, 3, 2, 2, 0, 1, 2, 0, 1, 1, 0, 2, 2, ...   \n",
       "1496  [2, 1, 1, 3, 2, 2, 0, 2, 2, 1, 1, 1, 1, 2, 1, ...   \n",
       "2114  [1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 2, 1, 0, 2, 2, ...   \n",
       "2257  [1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 2, ...   \n",
       "2768  [2, 1, 1, 3, 2, 2, 0, 0, 2, 0, 0, 1, 0, 2, 2, ...   \n",
       "355   [2, 1, 1, 3, 2, 1, 1, 4, 2, 2, 0, 1, 0, 1, 1, ...   \n",
       "1733  [2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 0, 1, 3, 2, 2, ...   \n",
       "2896  [2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 2, 1, 3, 2, 2, ...   \n",
       "280   [1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 2, ...   \n",
       "2525  [2, 1, 1, 3, 2, 2, 0, 0, 2, 3, 1, 1, 3, 2, 2, ...   \n",
       "177   [2, 1, 1, 3, 2, 3, 0, 3, 2, 1, 1, 1, 1, 0, 1, ...   \n",
       "1804  [2, 1, 1, 3, 2, 2, 0, 1, 2, 3, 4, 1, 0, 2, 2, ...   \n",
       "427   [2, 2, 1, 0, 2, 1, 0, 0, 2, 2, 4, 1, 3, 2, 1, ...   \n",
       "766   [2, 3, 1, 1, 2, 2, 3, 4, 2, 2, 0, 1, 1, 0, 2, ...   \n",
       "243   [2, 1, 1, 1, 2, 1, 0, 0, 2, 2, 1, 1, 1, 2, 3, ...   \n",
       "1545  [2, 1, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 2, ...   \n",
       "2014  [1, 1, 1, 3, 2, 2, 0, 2, 2, 3, 1, 1, 0, 2, 2, ...   \n",
       "2277  [1, 1, 1, 3, 2, 2, 0, 2, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "2674  [2, 1, 1, 3, 2, 2, 0, 0, 2, 1, 4, 1, 3, 2, 2, ...   \n",
       "2588  [2, 1, 1, 3, 2, 2, 0, 0, 2, 2, 4, 1, 0, 2, 2, ...   \n",
       "18    [3, 1, 1, 0, 2, 2, 0, 1, 2, 3, 1, 1, 3, 2, 2, ...   \n",
       "576   [0, 1, 1, 3, 2, 1, 1, 4, 2, 3, 4, 1, 1, 2, 1, ...   \n",
       "2409  [1, 1, 1, 3, 2, 2, 0, 2, 2, 2, 2, 1, 3, 2, 3, ...   \n",
       "1443  [2, 1, 1, 3, 2, 2, 0, 2, 2, 2, 4, 1, 0, 2, 3, ...   \n",
       "231   [3, 4, 1, 3, 2, 2, 0, 1, 2, 2, 1, 1, 1, 2, 2, ...   \n",
       "\n",
       "                      feature_id_AsubB  \n",
       "436        [1, 4, 0, 2, 2, 2, 0, 0, 1]  \n",
       "293     [0, -1, 0, 3, 0, 0, -2, -4, 0]  \n",
       "1469   [-1, -1, 0, 3, 0, 0, -1, -2, 0]  \n",
       "592        [1, 0, 0, 0, 0, 2, 2, 0, 0]  \n",
       "2222    [-1, -1, 0, 0, 0, 0, -1, 0, 0]  \n",
       "2784     [1, -1, 0, 2, 0, 0, 0, -2, 0]  \n",
       "2827       [0, 1, 0, 3, 0, 1, 0, 0, 0]  \n",
       "352       [1, 2, 0, 0, 0, 0, 1, -1, 0]  \n",
       "2883      [2, 0, 0, 0, 0, -1, 0, 0, 0]  \n",
       "549     [-1, -1, 1, 3, 0, -1, 2, 0, 0]  \n",
       "1684    [0, -1, 0, 3, 0, 0, -1, -1, 0]  \n",
       "597       [1, -4, 0, 0, 2, 0, 2, 1, 0]  \n",
       "1542      [2, 0, 0, 3, 0, 0, 0, -1, 0]  \n",
       "406    [-3, 0, 0, -1, 0, -1, 0, -4, 0]  \n",
       "2787     [0, 0, 0, 3, 0, 0, -3, -4, 0]  \n",
       "174     [-2, 0, 0, 0, 0, 0, -1, -3, 0]  \n",
       "83        [2, 0, 0, -1, 0, 0, 0, 3, 0]  \n",
       "2107      [0, -3, 0, 0, 0, 0, 0, 0, 0]  \n",
       "208       [1, 0, 0, 1, 1, 1, -1, 1, 0]  \n",
       "2312    [-2, -3, 0, 0, 0, 0, -1, 1, 0]  \n",
       "2201      [0, 0, 0, 3, 0, 0, -1, 1, 1]  \n",
       "317       [1, 1, 0, 0, 0, 0, 1, -1, 0]  \n",
       "2839     [-1, 1, 0, 0, 0, 0, 0, -4, 0]  \n",
       "328     [-1, -3, 1, -1, 0, 2, 0, 1, 0]  \n",
       "692   [0, 1, 0, 0, -2, -1, -2, -1, -1]  \n",
       "230     [-1, 1, 0, 3, 0, 0, -3, -2, 0]  \n",
       "641    [1, 4, 0, -1, -2, -1, 1, -4, 0]  \n",
       "138      [0, 3, 0, 0, 0, 0, -1, -2, 0]  \n",
       "2887    [0, -3, 0, 3, 0, -1, 0, -3, 0]  \n",
       "396    [0, -3, -1, 1, 0, -1, 0, 0, -1]  \n",
       "...                                ...  \n",
       "1771      [-1, 0, 0, 2, 0, 1, 0, 0, 0]  \n",
       "1447     [2, 0, 0, 3, 0, 0, -3, -1, 0]  \n",
       "2022      [-1, 0, 0, 2, 0, 0, 0, 1, 0]  \n",
       "2355      [1, 0, 0, 3, 0, -1, 0, 2, 0]  \n",
       "2639       [1, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "1928      [2, 0, 0, 3, 0, 0, -3, 1, 0]  \n",
       "1496       [1, 0, 0, 2, 0, 1, 0, 1, 1]  \n",
       "2114      [0, -1, 0, 3, 0, 0, 0, 0, 0]  \n",
       "2257    [-1, -1, 0, 0, 0, 0, -1, 1, 0]  \n",
       "2768      [2, 1, 0, 3, 0, 0, 0, -2, 0]  \n",
       "355        [0, 1, 0, 3, 1, 0, 1, 2, 0]  \n",
       "1733     [-1, 1, 0, 0, 0, 0, 0, -2, 0]  \n",
       "2896   [-1, -1, 0, 0, 0, 0, -1, -3, 0]  \n",
       "280     [0, 0, 0, -1, 0, 0, 0, -4, -1]  \n",
       "2525     [-1, 0, 0, 0, 0, 0, 0, -3, 0]  \n",
       "177        [1, 0, 0, 2, 2, 2, 0, 2, 1]  \n",
       "1804    [-1, -3, 0, 3, 0, 0, -1, 1, 0]  \n",
       "427     [0, -2, 0, -3, 0, 0, 0, -3, 0]  \n",
       "766        [0, 3, 0, 0, 2, 0, 0, 4, 1]  \n",
       "243      [0, 0, 0, 0, 0, -2, 0, -2, 0]  \n",
       "1545      [0, 0, 0, 3, 0, 0, 0, -1, 1]  \n",
       "2014      [-2, 0, 0, 3, 0, 0, 0, 2, 0]  \n",
       "2277     [0, -3, 0, 0, 0, 0, 0, -2, 0]  \n",
       "2674     [1, -3, 0, 0, 0, 0, 0, -3, 0]  \n",
       "2588     [0, -3, 0, 3, 0, 0, 0, -3, 0]  \n",
       "18       [0, 0, 0, -3, 0, 0, 0, -1, 0]  \n",
       "576      [-3, -3, 0, 2, 0, 0, 1, 3, 0]  \n",
       "2409    [-1, -1, 0, 0, 0, -1, 0, 0, 0]  \n",
       "1443    [0, -3, 0, 3, 0, -1, -1, 0, 0]  \n",
       "231        [1, 3, 0, 2, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[2291 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for human observed dataset(diff pairs)\n",
    "a = np.array(human_df1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "human_df1['feature_id_AconB'] = np.hstack((a[:, 0], a[:, 1])).tolist()\n",
    "human_df1['feature_id_AconB']\n",
    "# Subtracting the features of images for human observed dataset(diff pairs)\n",
    "human_df1['feature_id_AsubB'] = np.subtract(a[:, 0], a[:, 1]).tolist()\n",
    "human_df1['feature_id_AsubB']\n",
    "\n",
    "# Concatenating the features of images for human observed dataset(same pairs)\n",
    "b = np.array(human_samepairs[['feature_id_A','feature_id_B']].values.tolist())\n",
    "human_samepairs['feature_id_AconB'] = np.hstack((b[:, 0], b[:, 1])).tolist()\n",
    "human_samepairs['feature_id_AconB']\n",
    "# Subtracting the features of images for human observed dataset(same pairs)\n",
    "human_samepairs['feature_id_AsubB'] = np.subtract(b[:, 0], b[:, 1]).tolist()\n",
    "human_samepairs['feature_id_AsubB']\n",
    "\n",
    "# Combining the same and different pairs for the human observerd dataset \n",
    "human_df = human_df1.append(human_samepairs)\n",
    "\n",
    "# Shuffling indexes to avoid biasing\n",
    "human_df = shuffle(human_df)\n",
    "human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the GSC data sets in order to make training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking features of images for the GSC observed dataset(diff pairs)\n",
    "GSC_differentpairs['feature_id_A'] = 'NaN'\n",
    "GSC_differentpairs['feature_id_B'] = 'NaN'\n",
    "for i in GSC_differentpairs.index[100000:180000]:\n",
    "    a=GSC_raw[GSC_raw[\"img_id\"] == GSC_differentpairs.at[i,\"img_id_A\"]]\n",
    "    b=GSC_raw[GSC_raw[\"img_id\"] == GSC_differentpairs.at[i,\"img_id_B\"]]\n",
    "    GSC_differentpairs.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    GSC_differentpairs.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])\n",
    "GSC_differentpairs1=GSC_differentpairs[100000:180000]\n",
    "\n",
    "# Taking features of images for the GSC observed dataset(same pairs)\n",
    "GSC_samepairs['feature_id_A'] = 'NaN'\n",
    "GSC_samepairs['feature_id_B'] = 'NaN'\n",
    "for i in GSC_samepairs.index[10000:55000]:\n",
    "    a=GSC_raw[GSC_raw[\"img_id\"] == GSC_samepairs.at[i,\"img_id_A\"]]\n",
    "    b=GSC_raw[GSC_raw[\"img_id\"] == GSC_samepairs.at[i,\"img_id_B\"]]\n",
    "    GSC_samepairs.at[i,'feature_id_A']=np.array(a.iloc[0][1:513])\n",
    "    GSC_samepairs.at[i,'feature_id_B']=np.array(b.iloc[0][1:513])\n",
    "GSC_samepairs=GSC_samepairs[10000:55000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id_A</th>\n",
       "      <th>img_id_B</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_id_A</th>\n",
       "      <th>feature_id_B</th>\n",
       "      <th>feature_id_AconB</th>\n",
       "      <th>feature_id_AsubB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144712</th>\n",
       "      <td>0298a_num1.png</td>\n",
       "      <td>0324a_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157376</th>\n",
       "      <td>0323a_num1.png</td>\n",
       "      <td>0366c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50943</th>\n",
       "      <td>1097a_num6.png</td>\n",
       "      <td>1097b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114373</th>\n",
       "      <td>0236a_num1.png</td>\n",
       "      <td>0283b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136951</th>\n",
       "      <td>0281a_num1.png</td>\n",
       "      <td>0337a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37377</th>\n",
       "      <td>0819c_num2.png</td>\n",
       "      <td>0819c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122893</th>\n",
       "      <td>0253a_num1.png</td>\n",
       "      <td>0302c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106257</th>\n",
       "      <td>0220a_num1.png</td>\n",
       "      <td>0253b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27046</th>\n",
       "      <td>0599a_num2.png</td>\n",
       "      <td>0599a_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112393</th>\n",
       "      <td>0232a_num1.png</td>\n",
       "      <td>0280c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136074</th>\n",
       "      <td>0280a_num3.png</td>\n",
       "      <td>0291c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20467</th>\n",
       "      <td>0463a_num1.png</td>\n",
       "      <td>0463b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>0959b_num4.png</td>\n",
       "      <td>0959c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30199</th>\n",
       "      <td>0671a_num1.png</td>\n",
       "      <td>0671bb_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125134</th>\n",
       "      <td>0258a_num1.png</td>\n",
       "      <td>0276a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169691</th>\n",
       "      <td>0348a_num1.png</td>\n",
       "      <td>0372a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129386</th>\n",
       "      <td>0266a_num2.png</td>\n",
       "      <td>0314b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39225</th>\n",
       "      <td>0853a_num4.png</td>\n",
       "      <td>0853b_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51556</th>\n",
       "      <td>1110a_num2.png</td>\n",
       "      <td>1110b_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161981</th>\n",
       "      <td>0332a_num1.png</td>\n",
       "      <td>0386a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107131</th>\n",
       "      <td>0222a_num2.png</td>\n",
       "      <td>0238a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128354</th>\n",
       "      <td>0264c_num1.png</td>\n",
       "      <td>0309b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154678</th>\n",
       "      <td>0318b_num1.png</td>\n",
       "      <td>0338a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108255</th>\n",
       "      <td>0224a_num1.png</td>\n",
       "      <td>0255c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14068</th>\n",
       "      <td>0312c_num4.png</td>\n",
       "      <td>0312c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28889</th>\n",
       "      <td>0641b_num3.png</td>\n",
       "      <td>0641c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159545</th>\n",
       "      <td>0328a_num1.png</td>\n",
       "      <td>0334c_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176839</th>\n",
       "      <td>0364b_num1.png</td>\n",
       "      <td>0405a_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142015</th>\n",
       "      <td>0293a_num1.png</td>\n",
       "      <td>0297b_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107657</th>\n",
       "      <td>0223a_num1.png</td>\n",
       "      <td>0242b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162178</th>\n",
       "      <td>0333a_num1.png</td>\n",
       "      <td>0353a_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177764</th>\n",
       "      <td>0366a_num1.png</td>\n",
       "      <td>0397a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158828</th>\n",
       "      <td>0326b_num1.png</td>\n",
       "      <td>0363b_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127097</th>\n",
       "      <td>0262a_num1.png</td>\n",
       "      <td>0276a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>0230a_num3.png</td>\n",
       "      <td>0230b_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27012</th>\n",
       "      <td>0598b_num3.png</td>\n",
       "      <td>0598c_num3.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113311</th>\n",
       "      <td>0234a_num1.png</td>\n",
       "      <td>0272c_num7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22943</th>\n",
       "      <td>0511b_num3.png</td>\n",
       "      <td>0511b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112348</th>\n",
       "      <td>0232a_num1.png</td>\n",
       "      <td>0275b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166687</th>\n",
       "      <td>0342a_num1.png</td>\n",
       "      <td>0366c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38354</th>\n",
       "      <td>0836b_num3.png</td>\n",
       "      <td>0836c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44325</th>\n",
       "      <td>0956b_num1.png</td>\n",
       "      <td>0956c_num5.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114650</th>\n",
       "      <td>0237a_num1.png</td>\n",
       "      <td>0257a_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106038</th>\n",
       "      <td>0220a_num1.png</td>\n",
       "      <td>0225b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25136</th>\n",
       "      <td>0557b_num2.png</td>\n",
       "      <td>0557c_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16249</th>\n",
       "      <td>0365a_num4.png</td>\n",
       "      <td>0365c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135830</th>\n",
       "      <td>0279a_num1.png</td>\n",
       "      <td>0320c_num5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155050</th>\n",
       "      <td>0319a_num1.png</td>\n",
       "      <td>0325c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162442</th>\n",
       "      <td>0333a_num1.png</td>\n",
       "      <td>0383c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174632</th>\n",
       "      <td>0358a_num1.png</td>\n",
       "      <td>0378a_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157142</th>\n",
       "      <td>0323a_num1.png</td>\n",
       "      <td>0338a_num6.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165843</th>\n",
       "      <td>0340a_num1.png</td>\n",
       "      <td>0381a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29890</th>\n",
       "      <td>0664b_num5.png</td>\n",
       "      <td>0664c_num1.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126688</th>\n",
       "      <td>0261a_num1.png</td>\n",
       "      <td>0286b_num1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31809</th>\n",
       "      <td>0701a_num1.png</td>\n",
       "      <td>0701c_num2.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17047</th>\n",
       "      <td>0383a_num1.png</td>\n",
       "      <td>0383b_num4.png</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-1, -1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153234</th>\n",
       "      <td>0315a_num1.png</td>\n",
       "      <td>0341b_num3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133811</th>\n",
       "      <td>0275a_num1.png</td>\n",
       "      <td>0314a_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133658</th>\n",
       "      <td>0275a_num1.png</td>\n",
       "      <td>0297c_num2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129473</th>\n",
       "      <td>0266a_num2.png</td>\n",
       "      <td>0325b_num4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              img_id_A         img_id_B  target  \\\n",
       "144712  0298a_num1.png   0324a_num5.png       0   \n",
       "157376  0323a_num1.png   0366c_num2.png       0   \n",
       "50943   1097a_num6.png   1097b_num4.png       1   \n",
       "114373  0236a_num1.png   0283b_num4.png       0   \n",
       "136951  0281a_num1.png   0337a_num1.png       0   \n",
       "37377   0819c_num2.png   0819c_num3.png       1   \n",
       "122893  0253a_num1.png   0302c_num1.png       0   \n",
       "106257  0220a_num1.png   0253b_num3.png       0   \n",
       "27046   0599a_num2.png   0599a_num5.png       1   \n",
       "112393  0232a_num1.png   0280c_num2.png       0   \n",
       "136074  0280a_num3.png   0291c_num1.png       0   \n",
       "20467   0463a_num1.png   0463b_num2.png       1   \n",
       "44553   0959b_num4.png   0959c_num2.png       1   \n",
       "30199   0671a_num1.png  0671bb_num2.png       1   \n",
       "125134  0258a_num1.png   0276a_num1.png       0   \n",
       "169691  0348a_num1.png   0372a_num2.png       0   \n",
       "129386  0266a_num2.png   0314b_num2.png       0   \n",
       "39225   0853a_num4.png   0853b_num2.png       1   \n",
       "51556   1110a_num2.png   1110b_num3.png       1   \n",
       "161981  0332a_num1.png   0386a_num1.png       0   \n",
       "107131  0222a_num2.png   0238a_num1.png       0   \n",
       "128354  0264c_num1.png   0309b_num5.png       0   \n",
       "154678  0318b_num1.png   0338a_num2.png       0   \n",
       "108255  0224a_num1.png   0255c_num2.png       0   \n",
       "14068   0312c_num4.png   0312c_num5.png       1   \n",
       "28889   0641b_num3.png   0641c_num2.png       1   \n",
       "159545  0328a_num1.png   0334c_num1.png       0   \n",
       "176839  0364b_num1.png   0405a_num2.png       0   \n",
       "142015  0293a_num1.png   0297b_num5.png       0   \n",
       "107657  0223a_num1.png   0242b_num4.png       0   \n",
       "...                ...              ...     ...   \n",
       "162178  0333a_num1.png   0353a_num5.png       0   \n",
       "177764  0366a_num1.png   0397a_num4.png       0   \n",
       "158828  0326b_num1.png   0363b_num2.png       0   \n",
       "127097  0262a_num1.png   0276a_num1.png       0   \n",
       "10954   0230a_num3.png   0230b_num5.png       1   \n",
       "27012   0598b_num3.png   0598c_num3.png       1   \n",
       "113311  0234a_num1.png   0272c_num7.png       0   \n",
       "22943   0511b_num3.png   0511b_num4.png       1   \n",
       "112348  0232a_num1.png   0275b_num1.png       0   \n",
       "166687  0342a_num1.png   0366c_num2.png       0   \n",
       "38354   0836b_num3.png   0836c_num4.png       1   \n",
       "44325   0956b_num1.png   0956c_num5.png       1   \n",
       "114650  0237a_num1.png   0257a_num3.png       0   \n",
       "106038  0220a_num1.png   0225b_num4.png       0   \n",
       "25136   0557b_num2.png   0557c_num4.png       1   \n",
       "16249   0365a_num4.png   0365c_num2.png       1   \n",
       "135830  0279a_num1.png   0320c_num5.png       0   \n",
       "155050  0319a_num1.png   0325c_num2.png       0   \n",
       "162442  0333a_num1.png   0383c_num2.png       0   \n",
       "174632  0358a_num1.png   0378a_num1.png       0   \n",
       "157142  0323a_num1.png   0338a_num6.png       0   \n",
       "165843  0340a_num1.png   0381a_num4.png       0   \n",
       "29890   0664b_num5.png   0664c_num1.png       1   \n",
       "126688  0261a_num1.png   0286b_num1.png       0   \n",
       "31809   0701a_num1.png   0701c_num2.png       1   \n",
       "17047   0383a_num1.png   0383b_num4.png       1   \n",
       "153234  0315a_num1.png   0341b_num3.png       0   \n",
       "133811  0275a_num1.png   0314a_num4.png       0   \n",
       "133658  0275a_num1.png   0297c_num2.png       0   \n",
       "129473  0266a_num2.png   0325b_num4.png       0   \n",
       "\n",
       "                                             feature_id_A  \\\n",
       "144712  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157376  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "50943   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "114373  [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136951  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37377   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "122893  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106257  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "27046   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "112393  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136074  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20467   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44553   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30199   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125134  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "169691  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "129386  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "39225   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51556   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "161981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "154678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "108255  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14068   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28889   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "159545  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "176839  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "142015  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107657  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                   ...   \n",
       "162178  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "177764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "158828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "127097  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10954   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27012   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "113311  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22943   [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "112348  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "166687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38354   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44325   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "114650  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106038  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "25136   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16249   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "135830  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "155050  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "162442  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "174632  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157142  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "165843  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29890   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126688  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "31809   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17047   [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "153234  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133811  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "133658  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "129473  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                             feature_id_B  \\\n",
       "144712  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157376  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "50943   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "114373  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136951  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37377   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "122893  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106257  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27046   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "112393  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136074  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20467   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44553   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30199   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125134  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "169691  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "129386  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "39225   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "51556   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "161981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128354  [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "154678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "108255  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14068   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28889   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "159545  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "176839  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "142015  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "107657  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                   ...   \n",
       "162178  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...   \n",
       "177764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "158828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "127097  [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "10954   [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27012   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "113311  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "22943   [1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "112348  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "166687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38354   [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, ...   \n",
       "44325   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "114650  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106038  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "25136   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16249   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "135830  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "155050  [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "162442  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "174632  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "157142  [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, ...   \n",
       "165843  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29890   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126688  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "31809   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17047   [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "153234  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133811  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133658  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "129473  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "\n",
       "                                         feature_id_AconB  \\\n",
       "144712  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157376  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "50943   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "114373  [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136951  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "37377   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "122893  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106257  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "27046   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "112393  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "136074  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "20467   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44553   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "30199   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125134  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "169691  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "129386  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "39225   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "51556   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "161981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128354  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "154678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "108255  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "14068   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "28889   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "159545  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "176839  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "142015  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "107657  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                   ...   \n",
       "162178  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "177764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "158828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "127097  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10954   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "27012   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "113311  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "22943   [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "112348  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "166687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "38354   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44325   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "114650  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "106038  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...   \n",
       "25136   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "16249   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "135830  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "155050  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "162442  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "174632  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "157142  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...   \n",
       "165843  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "29890   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126688  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "31809   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "17047   [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "153234  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133811  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "133658  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "129473  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                         feature_id_AsubB  \n",
       "144712  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "157376  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "50943   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "114373  [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "136951  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "37377   [0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "122893  [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "106257  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...  \n",
       "27046   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "112393  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "136074  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "20467   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "44553   [-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "30199   [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "125134  [0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0,...  \n",
       "169691  [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "129386  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "39225   [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0...  \n",
       "51556   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -...  \n",
       "161981  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "107131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "128354  [-1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, ...  \n",
       "154678  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "108255  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14068   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "28889   [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "159545  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "176839  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "142015  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1,...  \n",
       "107657  [0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "...                                                   ...  \n",
       "162178  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0...  \n",
       "177764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "158828  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "127097  [0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0,...  \n",
       "10954   [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "27012   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1,...  \n",
       "113311  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0,...  \n",
       "22943   [-1, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, ...  \n",
       "112348  [0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -...  \n",
       "166687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "38354   [0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0,...  \n",
       "44325   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "114650  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "106038  [0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, ...  \n",
       "25136   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "16249   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "135830  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "155050  [-1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, ...  \n",
       "162442  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "174632  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1...  \n",
       "157142  [1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 1...  \n",
       "165843  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "29890   [0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "126688  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "31809   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17047   [-1, -1, 0, 1, 1, 0, 0, -1, 0, 0, -1, 0, 0, 0,...  \n",
       "153234  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "133811  [1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,...  \n",
       "133658  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...  \n",
       "129473  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1,...  \n",
       "\n",
       "[125000 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the features of images for GSC dataset(diff pairs)\n",
    "c = np.array(GSC_differentpairs1[['feature_id_A','feature_id_B']].values.tolist())\n",
    "GSC_differentpairs1['feature_id_AconB'] = np.hstack((c[:, 0], c[:, 1])).tolist()\n",
    "GSC_differentpairs1['feature_id_AconB']\n",
    "# Subtracting the features of images for GSC dataset(diff pairs)\n",
    "GSC_differentpairs1['feature_id_AsubB'] = np.subtract(c[:, 0], c[:, 1]).tolist()\n",
    "GSC_differentpairs1['feature_id_AsubB']\n",
    "GSC_df1=GSC_differentpairs1\n",
    "\n",
    "# Concatenating the features of images for GSC dataset(same pairs)\n",
    "d = np.array(GSC_samepairs[['feature_id_A','feature_id_B']].values.tolist())\n",
    "GSC_samepairs['feature_id_AconB'] = np.hstack((d[:, 0], d[:, 1])).tolist()\n",
    "GSC_samepairs['feature_id_AconB']\n",
    "# Subtracting the features of images for GSC dataset(same pairs)\n",
    "GSC_samepairs['feature_id_AsubB'] = np.subtract(d[:, 0], d[:, 1]).tolist()\n",
    "GSC_samepairs['feature_id_AsubB']\n",
    "\n",
    "# Combining the same and different pairs for the GSC dataset\n",
    "GSC_df = GSC_df1.append(GSC_samepairs)\n",
    "\n",
    "# Shuffling indexes to avoid biasing\n",
    "GSC_df = shuffle(GSC_df)\n",
    "GSC_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Tesing and Validation sets for Human are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Partitioning of Human Observed Dataset\n",
    "split_human_train = int(len(human_df)*0.8)\n",
    "\n",
    "# Training set\n",
    "human_train = human_df[:split_human_train]\n",
    "human_train\n",
    "# Testing Set\n",
    "human_test2 = human_df[:-split_human_train]\n",
    "split_human_test = int(len(human_test2)*0.5)\n",
    "human_test = human_test2[:split_human_test]\n",
    "# Validation Set\n",
    "human_val = human_test2[:-split_human_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Tesing and Validation sets for GSC are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Partitioning of GSC Dataset\n",
    "split_GSC_train = int(len(GSC_df)*0.8)\n",
    "\n",
    "# Training set\n",
    "GSC_train = GSC_df[:split_GSC_train]\n",
    "# Testing Set\n",
    "GSC_test1 = GSC_df[:-split_GSC_train]\n",
    "split_GSC_test = int(len(GSC_test1)*0.5)\n",
    "GSC_test = GSC_test1[:split_GSC_test]\n",
    "# Validation Set\n",
    "GSC_val = GSC_test1[:-split_GSC_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUMAN SUBTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding bias to training feature matrix\n",
    "p=np.ones((len(human_train['feature_id_AsubB']),1))\n",
    "p\n",
    "r=np.array(human_train['feature_id_AsubB'].values.tolist())\n",
    "r[:,0]\n",
    "X=np.hstack((p,r))\n",
    "X[1].shape\n",
    "\n",
    "# Extractig target values for training set\n",
    "y=np.array(human_train['target'])\n",
    "y[1].shape\n",
    "\n",
    "# Initializing Weights\n",
    "W_Now=np.zeros((X.shape[1]))\n",
    "\n",
    "# Adding bias to validation feature matrix\n",
    "p_val=np.ones((len(human_val['feature_id_AsubB']),1))\n",
    "r_val=np.array(human_val['feature_id_AsubB'].values.tolist())\n",
    "X_val=np.hstack((p_val,r_val))\n",
    "X_val[1].shape\n",
    "\n",
    "# Extractig target values for validation set\n",
    "y_val=np.array(human_val['target'])\n",
    "\n",
    "# Taking the range of Lambda (regularizer) and Learning Rate\n",
    "La_val = np.linspace(1, 10, num=19)\n",
    "La_val\n",
    "learningRate_val = np.linspace(0.01, 0.1, num=10)\n",
    "learningRate_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and initializing lists to store Accuracy values for validation dataset\n",
    "# This block of code also evaluates model performance varying learning rate from 0.01 to 0.1 and varying Lambda from 1 to 10.\n",
    "Accuracy = []\n",
    "X_lab = []\n",
    "X_lab1 = []\n",
    "\n",
    "for k in learningRate_val:\n",
    "    for j in La_val:\n",
    "        for i in range(0,len(y)):\n",
    "            learningRate = k   \n",
    "            La = j\n",
    "            z = np.dot(X[i], W_Now)\n",
    "            h = sigmoid(z)\n",
    "            gradient = np.dot(X[i], (h - y[i]))\n",
    "            La_Delta_E_W  = np.dot(La,W_Now)\n",
    "            Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "            Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "            W_T_Next      = W_Now + Delta_W\n",
    "            W_Now         = W_T_Next\n",
    "\n",
    "        z_val = np.dot(X_val, W_Now)\n",
    "        h_val_raw = sigmoid(z_val)\n",
    "        h_val=np.around(h_val_raw)\n",
    "\n",
    "        wrong_val  = 0\n",
    "        right_val   = 0\n",
    "\n",
    "        for i in range(0,len(y_val)):\n",
    "            if (y_val[i]==h_val[i]):\n",
    "                right_val = right_val + 1\n",
    "            else:\n",
    "                wrong_val = wrong_val + 1\n",
    "        Accuracy.append((right_val/(right_val+wrong_val)*100))\n",
    "        X_lab.append((str(np.round(k,2))+','+str(j)))\n",
    "    X_lab1.append((str(np.round(k,2))+','+str(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAE8CAYAAADNOraMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXmYHGW1/z+n91mTmUkmZE9IwpaQhGRMEBDIDXABJQTZwSj4UxSvCioXEb0X77244YaAF6NeUdkCqIDKIqsQEIEkQFhCmACTncyazPRMpme6+/39UVU9NT3dPdVLVU+m6/s8/cx0VXed962qfk+d5XuOKKVw4cKFCxelC0+xB+DChQsXLooLVxG4cOHCRYnDVQQuXLhwUeJwFYELFy5clDhcReDChQsXJQ5XEbhw4cJFicNVBGkgIjNERImIT3//iIh8yspnc5B1rYj8Op/xujhwISLfFpE7svj8b0XkejvHZHEcWY27GBCRaSISFhFvsccykjFqFYGI/E1E/jvF9jNF5INsF22l1GlKqd8VYFwnisiOpGN/Vyn1mXyPPYxMJSJX2yVjNEJELhGR54o9jgMRqe7zYkAptU0pVamUihX62LpC7tMVTbuIPC4ih2Xx/SYROanQ48oFo1YRAL8FVomIJG1fBdyplIo6P6Si4VNAu/7XUeRqJblwMRxEQ7HXsBuUUpXAZGAn8H9FHk9OKPZJtBMPALXAR4wNIlIDfAz4vf7+oyLyioh0ish2Efl2uoOJyN9F5DP6/14R+ZGItIrIe8BHkz57qYhsEpEuEXlPRD6nb68AHgEm6U8RYRGZlGxii8gKEXlTRPbqcg837WsSkatEZKOI7BORe0QklGHc5cA5wL8Bc0SkIWn/cSLyD13WdhG5RN9eJiI/FpGtupzn9G1DnvTMTzb6XP4gIneISCdwiYgsEZEXdBm7ReQWEQmYvj9Xf5pqF5E9uqvsIBHpEZE60+cWi0iLiPiT5E8Skf0iUmvadpR+ffwiMltEntHn0Soi96Q7X1aR7hrr+04UkR0icrWINOtzXikip4vIO/o8r006ZEi/ll0iskFEFiTNZYO+7x4gZNpXIyJ/1c9Lh/7/lAzjvkZE3tWP9ZaInGXad4l+nX+kH+t9ETnNtH+mfh67RORxYFyO5y6oy9imX+9fiEiZlfnov4fviMjzQA9wsL7tf0TkeX1sj4nIOP3zyS7etJ/V939Sv+fbROQ/xOJTu1JqP3AvsNB0rFki8pR+rFYRuVNExur7bgemAX8RbR24Wt9+tAz8Hl8TkRNzOcdZQyk1al/Ar4Bfm95/DnjV9P5E4Eg0hTgf2AOs1PfNABTg09//HfiM/v/ngbeBqWjK5umkz34UmAUIcALaDbvIJHNH0ji/Ddyh/38I0A2cDPiBq4EtQEDf3wS8BEzSZW8CPp/hHKwCdgNe4C/ATaZ904Au4EJdVh2wUN/3c33Ok/XvHgME04y/CTjJNJd+YKV+XsuAxcDRgE8/r5uAK/XPV+nj+xraAlcFLNX3PQxcbpLzU+DmNPN8Cvis6f0PgV/o/98NfFMfTwg4zuL9cwnwXJp9w13jKPCf+nn9LNAC3KXPby7QCxycdM7O0T9/FfC+/n8A2Ap8RX9/jv7Z6/Xv1gFnA+X6se8DHsgwp3P1e8cDnI92r000zbdfH68XuBzYBYi+/wXgJ/p9cLx+79yRRs6Q+8S070bgz2j3bxXaffk9K/NBuye36efQp5+TvwPvov12yvT338/wO0732SOAMHCcft5/pJ+Pk9LM47em61AB3A68Zto/G+13HATGA88CN6b63ejvJwNtwOn69TlZfz/e9rXSbgHFfOkXdB9Qpr9/HvhKhs/fCPw0ww1kKIKnMC2+wCnmz6Y47gPAFel+IAxWBP8B3Gva50EzOU803TyfMO2/AX3BSyP7CePmQ1vwWwC//v4bwP0pvuMB9gMLUuxLNf7EDa3P5dlhrsuVhlx9TK+k+dz5wPP6/17gA2BJms9+BnhK/1+A7cDx+vvfA78EpmR5/1xCGkVg4RrvB7z6+yr9/lhq+vx6Bh46vg38M+n870azZo/HtBjr+/+BvgClGMdCoCOLOb4KnGma7xbTvnJ93AehPTREgQrT/rvIUhHo16YbmGXa9mHgfSvzQfsd/nfSZ/4OfMv0/gvAo/r/Mxj6O0732f8E7k6afx+ZFUEvsBeIoynv+RnO9Urzvc5QRfB14Pak7/wN+FQ2920ur9HsGkIp9RzawnemiBwMfAjt5gVARJaKyNO6GboP7Unfirk7CW2hMbDVvFNEThORf+ougL1oGt6qGT3JfDylVFyXNdn0mQ9M//cAlakOJCJTgWXAnfqmB9GeiA1X1lS0p6NkjNM/l2qfFZjPDSJyiG7ifyCau+i7DJyPdGMwxnuEfu1OBvYppV5K89k/AB8WkUloi6cC1ur7rkZbgF4SzeX26RznZZ7TcNe4TQ0EKPfrf/eY9u9n8HVLnDP9mu9AuxcmATuVviroSNwfIlIuIqt1d0Yn2lPnWEmTJaO7Pl7VXQ97gXlJ407cW0qpHv3fSn0cHUqp7lTjyALj0RbY9aYxPKpvtzqf7UOOavE3McxnB/2u9fm3DTOfHymlxqIpnP3AocYOEakXkTUislOfyx1kXgemA+ca50U/N8cBE4cZQ94Y1YpAx++BT6K5SB5TSpl/jHehmahTlVJjgF+gLRjDYTfaAmZgmvGPiASBP6KZlRP0m+Rh03GHK/e6C+2GMI4nuqydFsaVjFVo1/gvIvIB8B7aAv9Jff92NPdGMlrRnnRS7etG+yEb4/Oi/4hNSJ7jrWiutDlKqWrgWgbOR7oxoJTqRfO7XqzP5fZUn9M/uxd4DDgPuAjtyU7p+z5QSn1WKTUJzT34vyIyO92xhoOFa5wLEveTaAHQKWj3wm5gsn4fGJhm+v9raIvPUv3cHm8cJsW4p6O5S78I1OnjfsPiuHcDNaLFuVKNwypa0RbMuUqpsfprjNICrlbnY1fJ5N1o510TqMUt6tJ/3DQgpbYBVwA/M+IdwPfQxjpfn8snyDyP7WgWwVjTq0Ip9f3cpmMdpaIITkLzeyanf1YB7UqpXhFZgraAWMG9wJdFZIpoAehrTPsCaD7BFiCqB9tOMe3fA9SJyJgMx/6oiCwXLSj6NSCC5g7IFp8E/gvNvDZeZ+vHr0OzFE4SkfNExCcidSKyUH8i/Q3wE9ECsV4R+bC+AL6DFtj8qD6+b+nzzYQqoBMIi5Zed7lp31+Bg0TkStGCiFUistS0//doLosVaE9UmXCXPuezGWz5nWsKOHag/QCtphOKiITML4a/xrlgsYh8XA9qXol2zf+J5pePot1vPhH5OLDE9L0qtIV1r2jB8usyyKhAm3uLPrFL0SyCYaGU2gqsA/5LRAIichxwxnDfS3HuFJoy+qmI1OufmSwi/5rDfAqNPwBniMgxoiUz/BdZKHel1ONoyvsyfVMVWsxhr4hMBv496St7gINN7+/Q5f+r/psLiZZ4kDb4XyiMekWglGpCW0Qr0J7+zfgC8N8i0oXmH7zX4mF/hea7ew3YAPzJJK8L+LJ+rA405fJn0/630YKX7+nm36Sk8W5Ge3K4Ge3p6QzgDKVUn8WxAVr2AZq5+nP9idh4/Rkt+Hyh/hRzOpqyaUfzFxvZKlcBrwMv6/t+AHiUUvvQztuv0ayUbjQ3RiZcpZ+HLrRzl8ja0c/Xyfo8PwAa0dxZxv7n0fyvG/RrmQl/BuYAe5RSr5m2fwh4UUTC+meuUEq9r5+nN0Xk4gzHPAZtYUp+pb3GOeJBtJhIB5r183GlVL9+3T+Opgw79M/8yfS9G9GCnq1oiuPRdAKUUm8BP0ZTLnvQEiWez2KMFwFL0e6H69Cz7zJgMkPP2yw0X/gW4J+6y+QJBlwqludTaCil3gS+BKxBsw66gGY0pWwVPwSu1h+a/gtYhBanfIjB1w00i+Fb+jpwlVJqO3AmmsXcgmYh/DsOrNNGNoALFyMWIvIUcJdSymVfu3AMIlKJFgieYzw4jFaMeovAxYENEfkQ2lNV3rn/LlwMBxE5Qw9YV6DFgF5Hy+4Z1XAVgYsRCxH5HZrb4ErdheTChd04E83PvwvNzXiBKgG3iesacuHChYsSh2sRuHDhwkWJw1UELly4cFHiOCAqQ44bN07NmDGj2MNw4cKFiwMK69evb1VKJRM+h+CAUAQzZsxg3bp1xR6GCxcuXBxQEBFLZUBc15ALFy5clDhcReDChQsXJQ5XEbhw4cJFieOAiBG4cOEC+vv72bFjB729vcUeiosRhlAoxJQpU/D7/cN/OAVcReDCxQGCHTt2UFVVxYwZM5AhrbhdlCqUUrS1tbFjxw5mzpyZ0zFGtWuoubOX81a/wFu79nHe6hdo7nKfpFwcuOjt7aWurs5VAi4GQUSoq6vLy1Ic1YrgpicbebmpnSvWvMrLTe3c9ERjsYfkwkVecJWAi1TI974YlYrg0G89woxrHuKOF7ehFDQ2h1EK7nhxGzOueYhDv/VIsYfowsUBhxNPPJG//e1vg7bdeOONfOELX8j4vcpKrfnYrl27OOecc9Ieeziu0I033khPT0/i/emnn87evXutDN0SFixYwIUXXliw4x1IGJWKYO3Vy1ixcBK+pNkFfR7OXDiJtV9flvqLLlyMMhju0UK4RS+88ELWrFkzaNuaNWssL56TJk3iD3/4Q87ykxXBww8/zNixY3M+nhmbNm0iHo/z7LPP0t3dPfwXckQ0GrXt2PlgVCqC+uoQVUEfMQUek8XUF4tTFfRRXxUq3uBcuHAQhnu0EG7Rc845h7/+9a9EIlrDrqamJnbt2sVxxx1HOBxm+fLlLFq0iCOPPJIHH3xwyPebmpqYN0/rjLl//34uuOAC5s+fz/nnn8/+/fsTn7v88stpaGhg7ty5XHed1qnypptuYteuXSxbtoxly7QHuRkzZtDa2grAT37yE+bNm8e8efO48cYbE/IOP/xwPvvZzzJ37lxOOeWUQXLMuOuuu1i1ahWnnHIKf/7zQLO5LVu2cNJJJ7FgwQIWLVrEu+++C8ANN9zAkUceyYIFC7jmGq1TrdmqaW1txSiL89vf/pZzzz2XM844g1NOOSXjufr973/P/PnzWbBgAatWraKrq4uZM2fS398PQGdnJzNmzEi8LxRGbdZQazjCxUun815LmA1bO/D7PJy5cDItbsDYxSjAf/3lTd7a1Zl2/0tN7ZgrzN/x4jbueHEbIrBkRm3K7xwxqZrrzpib9ph1dXUsWbKERx99lDPPPJM1a9Zw/vnnIyKEQiHuv/9+qquraW1t5eijj2bFihVpfde33nor5eXlbNy4kY0bN7Jo0aLEvu985zvU1tYSi8VYvnw5Gzdu5Mtf/jI/+clPePrppxk3btygY61fv57bbruNF198EaUUS5cu5YQTTqCmpobGxkbuvvtufvWrX3Heeefxxz/+kU984hNDxnPPPffw+OOPs3nzZm655ZaElXPxxRdzzTXXcNZZZ9Hb20s8HueRRx7hgQce4MUXX6S8vJz29va058zACy+8wMaNG6mtrSUajaY8V2+99Rbf+c53eP755xk3bhzt7e1UVVVx4okn8tBDD7Fy5UrWrFnD2WefnXOaaDqMSosAYPWqBq5fOY+7Pns0xx8ynoljQly/ch6rVzUUe2guXNiOhVPGUlcRSFjEHoG6igALp+TnSjG7h8xuIaUU1157LfPnz+ekk05i586d7NmzJ+1xnn322cSCPH/+fObPn5/Yd++997Jo0SKOOuoo3nzzTd56662MY3ruuec466yzqKiooLKyko9//OOsXbsWgJkzZ7Jw4UIAFi9eTFNT05Dvv/zyy4wfP57p06ezfPlyNmzYQEdHB11dXezcuZOzzjoL0HL1y8vLeeKJJ7j00kspLy8HoLY2tWI14+STT058Lt25euqppzjnnHMSis74/Gc+8xluu+02AG677TYuvfTSYeVli1FrEZhRVxlgw7aOYg/DhYuCIdOTu4Fv3v86d720jaDPQ18szmnzDuL6s47MS+7KlSv56le/yoYNG9i/f3/iSf7OO++kpaWF9evX4/f7mTFjxrDpjKmshffff58f/ehHvPzyy9TU1HDJJZcMe5xMzbWCwWDif6/Xm9I1dPfdd/P2228nXDmdnZ388Y9/5LzzzksrL9XYfT4f8XgcYMiYKyoqEv+nO1fpjnvsscfS1NTEM888QywWS7jXColRaxGYUVsRoKOnn3jc7cbmonRguEfv/8KxXLx0Oi3hSN7HrKys5MQTT+TTn/70oCDxvn37qK+vx+/38/TTT7N1a+ail8cffzx33nknAG+88QYbN24EtEW4oqKCMWPGsGfPHh55ZCDDr6qqiq6uoR1Ljz/+eB544AF6enro7u7m/vvv5yMf+Yil+cTjce677z42btxIU1MTTU1NPPjgg9x9991UV1czZcoUHnjgAQAikQg9PT2ccsop/OY3v0kErg3X0IwZM1i/fj1AxqB4unO1fPly7r33Xtra2gYdF+CTn/wkF154oS3WAJSMIggSiys6ewsbYHHhYiTDcI8eMam6oG7RCy+8kNdee40LLrggse3iiy9m3bp1NDQ0cOedd3LYYYdlPMbll19OOBxm/vz53HDDDSxZsgTQUjiPOuoo5s6dy6c//WmOPfbYxHcuu+wyTjvttESw2MCiRYu45JJLWLJkCUuXLuUzn/kMRx11lKW5PPvss0yePJnJkycnth1//PG89dZb7N69m9tvv52bbrqJ+fPnc8wxx/DBBx9w6qmnsmLFChoaGli4cCE/+tGPALjqqqu49dZbOeaYYxJB7FRId67mzp3LN7/5TU444QQWLFjAV7/61UHf6ejosC299YDoWdzQ0KDy6UfwwCs7ufKeV3nyaycwa3xlAUfmwoVz2LRpE4cffnixh+GiCPjDH/7Agw8+yO233572M6nuDxFZr5Qa9gmgJGIEtRUBANq7+5g1bK8eFy5cuBg5+NKXvsQjjzzCww8/bJuMklIEbeG+Io/EhQsXLrLDzTffbLuMEokRDFgELly4cOFiMEpKEXT0uIrAxYGNAyGm58J55HtflIQiCPm9VAS8rmvIxQGNUChEW1ubqwxcDILRjyAUyr10TknECABqKwO0d+efR+3CRbEwZcoUduzYQUtLS7GH4mKEwehQlitKRxFUBGlzYwQuDmD4/f6cO1C5cJEJJeEaAq3OihssduHChYuhKBlFUFsRoMNVBC5cuHAxBCWjCEI+D7v39dLc6ZahduHChQszbFMEInKoiLxqenWKyJX6vi+JyGYReVNEbrBrDGa8uasTBfz4sXecEOfChQsXBwxsCxYrpTYDCwFExAvsBO4XkWXAmcB8pVREROrtGgNo/Ysj0Xji/T3rtnPPuu0EfR42X3+anaJduHDh4oCAU66h5cC7SqmtwOXA95VSEQClVLOdgo3+xQGvNtWA1+1b7MKFCxdmOKUILgDu1v8/BPiIiLwoIs+IyIfsFGz0L+7XG0b0u32LXbhw4WIQbFcEIhIAVgD36Zt8QA1wNPDvwL2Soi2PiFwmIutEZF2+BBqjQcfYMj8Hj68oSIMOFy5cuBgtcIJQdhqwQSllNDDdAfxJaTz5l0QkDowDBq32SqlfAr8ErR9BPgMwGnK82xymNxpz+xa7cOHChQlOuIYuZMAtBPAA8C8AInIIEADSt/MpIGbXV7KlOezWanHhwoULE2xVBCJSDpwM/Mm0+TfAwSLyBrAG+JRyaGWeM6GSrt4ozV2ua8iFCxcuDNjqGlJK9QB1Sdv6gE/YKTcdZtdrbSob94SZUO0Gi124cOECSohZDAOK4Jv3v05z12CGcXNnL+etfmHI9lzR3NnLyp8/z1n/+3zBjnmgoNDn0oULF/aipBTB+MogAa+wtb2Hm55oHLTvpicbefn99iHbc8V3H97Eq9v38sq2vQU75oGC7z2yiZebCncuXbhwYS/kQAicNjQ0qHXr1uV1jGSG8XDIlXmcSc5oZzOnm/ton7cLFyMVIrJeKTVsmmTJWAQGw9jr0SgLIb/GMH74y8exYuEk/F5tu98reTGP1169jFOOmDBom0fg1LkTRj2bee3Vy1g8bWzivXGOR/u8Xbg40FEyisBgGMfjmgUUiWoM4yMmjdGYxzFte39M5cU8rq8OJXokG4grGFcZHPVs5vrqUMIi8HkkcY5H+7xduDjQUTKKADSG8bLDtBp3Jx8+IcEwbg1HOOygKgCm1JTlzTxu0dNTp9eWA1ovhFJhMxupuRctncbFS6eXzLxduDiQUVKKYPWqBq5fOQ+A4w8Zn2AYr17VwOETqwGtKF2+zOMfnrsAgHMbtB6inz/h4JJhM1eFtIzksoCX61fOK5l5u3BxIKOkFAHAxDEhKgJetjSHB203+hk3tXUTicbyktEf09wjNRUB/F4pmV7JfdE4TW09AIR7o0UejQsXLqyi5BSBiDC7vpLG5q5B29u7I4ho/vym1p68ZPTpfvKA10NtRYD2cGkogqa2bmJ6DCYccRWBCxcHCkpOEQDMrq9i8wddg0hP7eE+Dj9Icw/9253r8yJDGRZBwOehtiJIu24ROEEySyZzOUnuWr+1AwC/R0rGInDJcy5GA0pUEVTSGu5LkJ6UUrR199EwowaALS3deZGhjAwkv9dDXUWA9h5NEdz0ZKPtJLObnmwcROZKfm8n7n5xGwDVZX66SsQicPL8unBhF0qGUGbACWLZGzv38bGbn+NXn2zgL6/t4i+v7SLdWS4U2crqvOwgd5UikawU5+ziwINLKEuDtVcvY/lhA22SQ34PJx+hvV84dSw63ywvMlSf7hryeYXaigDlAS/HzKod9BlvgUlmBmHOIMYFvMKEqmBiv53kLkO20V7IK1Ae8I5qIlk6guJonrOL0YuSUwRmwpdBevJ6tNMwtsyHHuvMiwzVnxQs7u6LodTgJmyxApPMDMJc1ESMMyR6bSZ3GbKV0ljUMQWxuBrVRDJjzrEkguJonrOL0YuSUwQAnb39CLDyqElcvHQ6zZ1aoK+3P84xs7Sq2WfMn5QzGcocIzCUzrb2bgDmTdIC0hPHBAtOtmoNRxJxjoXTxtIZ6QdgyYwa28ldreEIY8v8LJlZy9xJ1YnMqdGM1nCEmeMqCPk8LnnOxQGNklQEq1c1UF8dxCPC9SvncdHS6QDccM4Cvn7qYQCcfuTEnMlQRtaQ3yvU6YrA7/Uwc1wFXzn5EABu/URDwclWq1c1cNq8iQAcPK6S42aPB2BPZ8R2cpdxTseWBTj9yIkoyJuPMdKxelUDC6aMIaaUS55zcUCjJBUBMCits71be5KrrQwwS+9Z8G5LOO13h0NfQhEMWARNbT3Mrq9MvDdkFhpxPfi/pbkrQZrb2t7jyKIciyu8HqEyqLGLuyOjWxEAROOK/phy25+6OKBRsoqgriKQYPy2dfcR8HqoCHipDPqYPLaMxj1dwxwhPcw8grrKgQJ0c+orqavQArjt3f15jD49orrPurE5zNb2HqbVlhOLq7xJclYQV+AxKYJS4BIYMQLDHejCxYGIklUENRWBAYsg3EdtRQDR015m1VfS2Jy7RdA/yCIYyNyZXV9JbaW9FoGxMPX0xYjFFacdeRDAECa1XbK9ApV6vaGuiD3KbiQhmlAEoz8m4mL0omQVQZ2p9MPuffvp3N+fYIfOqa9kS3MXK295LicWcH/UCBYLY8v8ieydcRVBKgJeAl6PbfWHjDLbBj40Q0tb/d7Dm2xnv8biCo9HqCqCRVAshm/MVQQubIRT93XJKoLaigBdkSiRaIxNu7vo6Y8l2KFz6iuJRBWv7tiXEwvYiBEEvB48HiHg007zw2/sRkRsrT8UTVIET7y1h8qgl517e21nv8aVwiuSsAicrDdULIavoQj6XEXgwgY4dV/7bD36CIYRtD30W48mtt3x4jbu0MskmGFst8oaNZ4Oj7vh6UFplGte3s6al7cjkHBLFRq3PL1l0Ps1L29P/J/tPLJFcrDYCUWQzPC1e47JcGMELuyA0/d1yVoERlrnCYeMT2wL+T3869wJHDe7btBns201aSiCv115vMY+TWIrL5lZa5tr6OKl0xBIyPQICdeU3ezXuNJcQ4kYgQOuobVXL2PZoYOvoZMM32hcu9b9JcCbcOEc1l69jONmj0u8t/u+HlYRiIjXFslFhmER9PZrKY4Gy3h8ZZDpdRWDPpttq0nj6XDSWL09JloNGoN9etCYkG0WQZnfi0dIyIwrEnWO7Ga/asFiZy2C+uoQAZ92i9rNoE4FN0bgwg7UV4cI6i5lv9f++9qKa2iLiPwBuE0p9ZYtoygCjLTObe1aWuUPzj6SV7bvo0UPykytKWNCdYh1WzuYUJUdC9hwB/k9HlrDES5eOp2Llkzjrpe20dLVy6SxZXTYpAiicQUiXLxkGhctmcbnbl/Hvv39dPZGuWjJNFvZr4ZryFBG3Q7FCPb2aHNaOrOWg8dXJq6hE4i6MQIXNqFDr1r8/Y/P55Xte229r60ogvnABcCvRcQD/AZYo5TqtG1UDsCc1glw0uEHcfbiqYO2rd/awdm3/oPvnzOfZYfWYxX9sTg+j+DxyCC2qdEm85anGhOB6qCvsAZXLK6o0NtEAqz9+r/wy2ff5bsPv803Tj888bRuB+IKPCKIbhU44RoC+OrJh3L+L//Jns5e7vrs0Y7INBB3YwQubMLnTpjF525fz6EHVXH24im2yhrWNaSU6lJK/UopdQxwNXAdsFtEficis20dnY0YU+ZHBHbv62V8VZAx5f4hnzHiCNlm+PTH4vi96U+toYQ6bCCVGU/lZhjKJtJvL9NXk639XxXyO5Y1ZLhnmtp6HK9x5PIIXNiFRKdDn/2hXEsxAhFZISL3Az8DfgwcDPwFeNjm8dkGr0eoKdcW+jl6WYlkDJC/slUEKlEOOuVxKzSl02YDqSymVKKaqgHD15hNH4ZcZXt0JVQZ9DnGI4jp5R1iccXWtm5HZCZkG4rADRa7KDDMLW/thhU/QSPwNPBDpdQ/TNv/ICLH2zMsZ1Crs4tnp1EEVUFfTs3n+2PxjFq8NlFmovBxglhs4KncQMivWQS9NlsEcT1YDBq72CmLwMydaGx8akPTAAAgAElEQVQOM2dClSNyzbL7465ryEVhYS5VYzesSJivlPp/SUoAAKXUl20Yk2Mw/OUHVQdT7jfIXx3dfQmG31u79g3L9BveNaRZGt/+85sFZwzGlMJXRIvAcEv5vcIr2zscYfrGTP757zxkP4N6kGzXInBhE/pGmCL4uYiMNd6ISI2I/MbGMTkGwzWzYdvetJ+prQjS1t2XYPhdsebVYZl+mmso/ak1Yg/v5dkbORW0Mg+DtwX99isCpZTemEZTBLv29tIdiTnC9DVbBDv37neUXeymj7qwC07GCCxlDSmlEiulUqpDRI6ycUy2I5m198SmZmZc81BK1t7bH3SyafdAgpRRjC4T068vFk8bIzDLVsMcJxfE4kMtgpDPfteQsSDe8vQWfvbkwELsBNP3i3dtGPTeSXaxW2LChV2IOBgjsCLBIyI1xhsRqeUAL02R3N83E2vv5MMnUB7wkqyUg7703+mPpncNGbINFJoxGIsrkpKGHLEIjIDtZccf7Hgv3+tWzAUoSL/pbJFgFrvpoy4KjJEWLP4x8A+dVAZwLvAd+4ZkPxL9feNqEOM3FWtv0tgy+mNxktfQvlj672QKFhuyQVu4Cs0YTGUROJE+qq+HVIf8dAb7He3lWxnU5leIftPZwnUNubALhmfBk/xkZwOGVQRKqd+LyHpgGVrZmo+PBoZxKsZvKtRWBOiPKabVlvFBZ4SqoI+27j7Ob5ialqU7XIygNRyhvipIfVWQhdNqCsoYjMbVkBsnpFsEvQ5YBF6PNr8jJ1fz+s5OLraZzQwQ1Z/GZ9SVs729hwuXTneMXezyCFzYhb5o3BFrACy6eJRSb4pICxACEJFpSqmhZToPIKRi/KaCkeHT0xfj2Fl1nDZvIlf/cSP/tmw2U2vLU34nU4zAkP2FO9ez+YOujLJzQVwpfEUglBlPxh7R2NS3PNXI6zs7uW7F3IxKsZCyl86so6mth/9eMdeRpyizbKeJbC5GP/qimdPQCwkrhLIVItIIvA88AzQBj1j43qEi8qrp1SkiV5r2XyUiSkTGZTpOsWFk+LSG+5gzocrUczg9B2C49FEY4DAUGqksAifSR41SC0ZswLiBnVggjafysoCm8JwM3LplqF3YhRGlCID/AY4G3lFKzQSWA88P9yWl1Gal1EKl1EJgMdAD3A8gIlOBk4ERb1UYCz/A7PGVltjG/bHhTbraiiB79w/40guFeDyFReAAoWzANaQrAq9ziiCuyy7XFYHdfAkzXNeQC7vQNwwxtZCwIqVfKdWGlj3kUUo9DSzMUs5y4F2l1Fb9/U/R6haN+Mcoc/P52RMqExZCJrZxf1Thy+AaAs3SUGqgwmChEI3HE+xeA05aBAaPwCgN7cTTuREjMBSBk24aN1jswi44GSOwImWviFQCzwJ3isjPgGxrB1wA3A2aqwnYqZR6LctjFAXmKqVjy/zUVAzffN6qawgoeDnqeJwUReecSx8thmvIWIyNUhpOuYaUUkXlERSrT7MLZxCJxhMPVHbDiiI4E82t8xXgUeBd4AyrAkQkAKwA7hORcuCbwH9a+N5lIrJORNa1tLRYFVdwjCkbqEr6m+fet1R/qM+Ca8iKZZELovH4EEUgInqarP3BYq8MVgROuGmGxAgcsgjMXr3+qPPGbbH6NLtwBk66hjJmDendyR5USp2E1vTqdznIOA3YoJTaIyJHAjOB10RbMKYAG0RkiVLqA/OXlFK/BH4J0NDQUBQXUrq+oZD5Sd6KRVBjIeicC2JqqEUAeoe0fjtdQ9pfwytVzBiBU4rAIJOBs66hYvdpduEM+qIxgiPBNaSUigE9IjImDxkXoruFlFKvK6XqlVIzlFIzgB3AomQlMFJgsIB9SSzZOfWVwwSLFX7f8DECKLxFEE/RjwC0gLGtFkGSa8hwRzkZIyjza881TikCc6DfSUWw9uplrFhgHzvdxchAXzQ+7DpSKFjhEfQCr4vI40Ci2LuVyqO6K+hk4HM5j7CIMFjAMTWYgTyhOjRMsDgLiyDLpjfDIRpXiYCtGSG/vRZBrIjpo7F4HJEB4lxfzN5y2wbMxe6cjBHUV4cS1o9XnO/T7MIZ9MXijPEObZhlB6wogof0V9ZQSvUAdRn2z8jluE4iFQO5tiLA9o6etN+xEiPwez1Uh3wZg865IFX6KGiksl4bLQLDPWMoIb+DrqGo3gfBybgEDC5/7TSPoKVLu29OnXcQNRUBR/s0u3AGTvIIrJSYyCUuMGqQioH87T+/mfFJ3kqMAKCuMuhIsBjsjxGktQgceDo32nMGHbRCYMAdBs73I/jhuQtY9D+PU27qT+1idKHPwayhYRWBiLxPinx/pdTBtozoAEBdRSBt8/lYXBFXWFIEtRWBgvMI4mmCxSG/19700WQegYMWQUy3ggJeZ4PFxYoRmOU5SZ5z4Sz6Y2pE1RpqMP0fQqs+WmvPcA4MGOzivT39TKgerAiMH6iVIE9tRYDt7eldTLkgk0VgJ7M4noZH4FT6qNcjJivEqayh4sQIYEDZ2ZkA4KK4iIykEhNKqTbTa6dS6kbgXxwY24hFrd70vi2FeyjRXs6KRVBe+HpDqQhlQCLYbRcGXEMD8sBBi8DrcTRADckxguJYBL02uvtcFBd90Vjid2Q3rBSdW2R6NYjI5wHnuoOPQBis4K/e++oQVqfhK7biGgr6PbR0RWjuLGQZ6qElJsBwDTkXLHby6dzIlHJaEQzmETgbLDbkFcMiKCajuZTY1COt1tCPTa/vAYuA8+wc1EiHUX9o8wddQ1idxg/UiiJ4c+c+FPDDxzYXbGyxOHhT1DnSXEN2WgTa32IUnYvF43qMQJPp1NN5XBXfIihGjKCYjOZSYlOPqH4ESimXpWLCcD2HEzGCDEXnkpmh963bwX3rdhSEGRorkkWQrsSEEwtkTHeHOZ0+Go0XL320rwiuoWIymkuNTR2NxYkrZxrXgzXX0HdFZKzpfY2IXG/vsEYuhmN1JmIEGS6gwVg2tH3AWzhmaCwds9jmGEHCNVQkQpnPK46ymWGA0Rzwepy3CIoQLE7u9Z2pb7ddso07e7Szqa2sI4WEFSmnKaX2Gm+UUh3A6fYNaWSjvjpEVUgzpCRFz+GoBdeQwVjuTzQ+LxwzNK0i8Hvt7UeQxCPweQQRBwllJteQ0+mjIX8RFIERI3DQIkj0+o4NdGVzitFsyFak/t2NNjjZuB6sKQKviCRqMYtIGRDM8PlRj9ZwhJl1FdRVBLh46fRBPXkHXEOZT63BWK4O+ZhVX1Gwvr6xFK0qAUK6RaCUPS6MWFKwWERbmCOOuIY0ZrHHI/g84jihrCzgdZxQNhAjcDZY3BqOMH+KVnrso/Mn2t6POlm2zyPMnVg95Hc32pBQBCOFWQzcATwpIrehucU/TW5VSEcNVq9q4H//voUbHt3M1089lKrQQD2QPgsxAuMYAO/s6SIeV4MYzPkglqJVJWgWgVI6ScWGQlbJrSpBu4mdtAiclAmD+yB0R5xdkI37zEmLALT79od/e5vXduzjc8fP4sgp+dSjzA63XryYg699GL/PM+rZ1BGHFYEVHsENwPXA4cBc4H/0bSWNOfVaBu2W5vCg7f1ZmnRz6itpbA4X7Ek9lrbWkDYeu+oNJQeLDZmOlKGOD3SEC/g8jscIyvzeksoaMpSPU8X9DHT3af2wSqEbnHEPO8UjsFJiYibwd6XUo/r7MhGZoZRqsntwIxmz6ysBaGwOc9S0msT2RPqoxQs4u76Sffv7aQlH8vZ3KqWVt0hVfdToWxzpj2v88AJjIFg8sC3gddIiGAi8F8MiKJYi6IvF08aF7EIkWhwlFI7oiqAITYCcxkiMEdyH1pTGQEzfVtKYWlNGwOfh3WSLwGKMwEDCstgTHuaTw8NYmDJZBHb5lJN5BODc07kWIzDJdJhQVhSLwLQYOtmjGUgkHTgttztSQhbBSHMNAT6lVKIOgv5/IMPnSwI+r4dpNWWseXk7b+3al2A7Wo0RGJgzQbMsvnH/63mzJY289pQxAsM1ZJNPOdGYRooRI4jjMywCnzMBahiwgrSsIWVbID4VzArW6YCxYQk4rQi6ejVFUIz+0E6jL8sHynxhRUqL3nAeABE5E2i1b0gHDvpjin37+7lizasJtqPxtGLVpKuvCuL3CFvbevJmSyYXfjPDaOxu16Kh1FAl5JQiMLtGnHQNGTEC49w6SSozPxU7XW/IuIecXpDDrkVgG6xkDX0euFNEbgEE2A580tZRjXAksxwbdfeQuaexFU1eaLZk1JJryCaLIEWwOOB1zjUU8hcva6gsoQicqw3TX0SLwFA8TlsE4V5DEZRQjGCkuIaUUu8qpY4GjgCOUEodA3TZPrIRjAFm8OAFN+jzcNRUjYRtJVhsHMdYt/NlSxopnCmDxXrfBLtIZcmEMtDdNKPZIjCCxYEBReAUzIuh00FbQ/E4/WTelQgWj36LIDICg8UGvMC5IvIEsMGm8RwQGGAGK8wP332mzmR+C1kcxnGMkjX5siUTFkGK+ITRz9euRSO5xARoVpGTrSrB2fTRZIvASVfJYNdQaVkEpRQjGBFlqPVU0fNF5EHgDeAnaJyCqU4MbiTDYAYffXAdE6o0ovWZCybR2dsPWA/ytIYjzJ+skXIuWjItL7akFYvALgJSImuoCDyCWJEJZWVFjhE4bxEUOX20BBRB/0iJEYjIncDxwGPALcBTwBal1N8dGdkIh5kJvH5rO2ff+gIrFk7iiOYw3334bcs8gtWrGvj9C01s3LmPK086hPFVuVfvMDJ3UsYI/Danj6biETiYPpoglBWFR6BXWnVwYRzkGiqxYHFcpa+pNVowkorOzQM6gE3A20qpGCl6F7uA2eM1LkDjnrCpH4H1m9RodJNvtzIjiyVV+mjIb69FEE8XLHbMIhhIH3W6VWXI73yMwHxenXYNRYrkGjLSR2H0WwUjhlCmlFqA1oCmGnhCRNYCVSJykCMjO4AwptzP+Kogjc3hxAX0e6xfQEMRtHXnV0QrnskisJ1QVtxaQ76iuIZ0QlmguDGCYgWLHY8RRAYUwWiPE4yorCGl1NtKqf9USh0KfAX4PfCSiPzDkdEdQJhTX8mW5jD9Ma1bVqqn8nQomEWQYjE2YDehLFWw2EnXkBEXsbvvghnRERMjKA2LIKzH32DA+h2tGEmuoUFQSq1TSn0NmA58w74hHZiYXV/JO3u6uG/ddmJxlRVLuFCKIFUFUAOG++J3LzTZ0u81NY/A6yCz2BwjcGZhTMUjcApaFVl7lXs69BY5RgCj3zU0ktNHAVAanrFjMAcy5tRX0tMXoyXch4KsWMI15bprKFwgiyBF1pCxUO7o2G9Lv9dYClazcxbBQJ/mgM/j2JN5LDlG4OATcl8sTrXeIMlJiyAWV4nzW8wYgdOynYbRr1hS/JbtgBVmsYthkMwQhuxYwn6vhzFl/rwtglR++lTjs6Pfa6rUVcNfr5Sy9YaOmS2CIgaLHY0RRONUBn20hvscjRGYF2DHi871RfF6RFdGJaAIHHILQQ4WgYuhWHv1Mk45YgLmpc4rcOrcCZZZwnUVAdp77FEETvR7TVV91IhL2P2EPqgxjddLLK4S58JOJFxDgeLECCqC2nOck1lDZllOFfczEO6NJqzn0V5moi8Wc1QRWOlHEATOBmaYP6+U+m/7hnVgob46xPiq4KDc2piCcZVByyzh2ooA7Xm6hlK5Z4zxGf1ewZ5+rwOtKge2JXoI21yDx2hVCQPBtb5oPLFA2wXDIjAUXtThGEHI78XvFUctgkiRLAKlFOFIlJnjKmgNR0rDInAoPgDWXEMPAvuA9cDobRKaJ1rDEabWlDF/ilZraOOOvVmxhGsqAmxr68lrDOksAmN8R04ewxs793HBh6YWvN9rXC+3IUmuIdAXDBu7XEfjalCMwJBptyKI62mrCZlO8ghicfxeIeTzOkooM8cjnFQEkWic/phKWASlkD46oiwCYIpS6lTbR3KAI9+ew3UVAV7ZtjevY2RSBKtXNfCX13bxpbtf4ZMfnsHhE6vzkjVEthrK9BykCGxEPIlHABCJxQB/hm/lj6jeH9p4cnPaNVQZ9BH0e2xrP5oK5gwlJxWBkTFkZNiN9sJzdlvRybAi6R8icqTtIylx1FYE6OjpSwRdc0GqFE4zjCY4jc35d0NLRtyUy28g4Rqy8UerlBrUqjLogEwDRpDan1AEzhLKAl4PwWJaBA7O1yg4V1NRIjGCEegaOg64RETeR3MNCVoW6XxbR1ZiqK0IEIsrOnv7GVueWwO4TBYBwMxxFXgEtuwpfBXxVLVfBlwm9j2xGnoz2SJwrleyJMqJOKoIogq/10PQ73E0fdSwCPxeKYpFUFOuWXmjPUYQicYt1ysrBKwogsLkF7rIiLrKAVKZXYog6PMyva7CFosgptQQSyThprFxwTD6BnuTFYFTxe48kvjBOrkw9se0hSLo8zpKKDOUTnXI7+h8DQ5BKcUIgg5aBFYa02wFxgJn6K+x+jYXBURthRZN/eJdG3Jm/g6nCACm1pTx9ObmgrOL47q/3Awnns6T5+yEO8os22uKEfzuH/awtlPBCBZrJTWcswgMN1RVyOfYYtzc2cu1f9oIDFh+dlkEzZ29nLf6Bd7atY+VP3+es/73+UE9ye2SlSxz7/4+3v6g07H7aVhFICJXAHcC9frrDhH5kt0DKzXU6k86m3Z35cz8zVRryEBbuI/e/jg3Pv5OTjLSIVWw2Al/vaEIiuEaiiVcQ5rM7TaxtlPBiBGE/M7VVoKB8hJVDloEP/zbZt7XM+qeeacFsE8R3PjEO7z8fjuX37GBV7fv5ZVte7n8jg2JnuSFxE1PNiZkJct8r6Wbzt6oY/eTFdfQ/wOWKqW6AUTkB8ALwM12DqyUYGb+KnJn/mZqXp/MLr7rpe3c9dL2grGLY/GhDXGccNMkWwR+By2CaFyxpzPCrGsfTmyzg7WdCv0xPUbg87I3TyJiNjAsguoyH7v32fu0moqx//RmTRH8+30bOeuoKbbJ2treM+T/Ql3bdLLMMo1guFP3kxUnlABm2zOmb8v8JZFDReRV06tTRK4UkR+KyNsislFE7heRsbkOfrRg7dXLWLFgUuJ9rszfTM3rB/osD9TtLyS7OB5XJLs0nViUk62ggfRRZyyCyWPLWLEw/2uXLfqjcV0ROGsRGLKqgn7bA7Zrr17GiYeMG7TNuLW/dsohBZdlZt+nQrBAvxlDlpVYcKFkDgcrFsFtwIsicr/+fiXwf8N9SSm1GVgIICJeYCdwP3Ao8A2lVFS3Lr4BfD2HsY8a1FeHqNILiHkkd+ZvplaViT7L+o+3v8Ds4kzBYidjBEGHs4aCPg9VwfyvXbboi8Xx+4SQ3+usa6jfcA35bD/H9dUhygODlygjS8yo71RIWWb2fSr0xQpzbQ1Zw+lRrxRO5nAYVhEopX4iIn9HSyMV4FKl1CtZylkOvKsHmc2B5n8C52R5rFGJ1nCEGXXldEei/Ou8ibTkECQasAhSP2q0hiOcPn8iD23czUfmjCsouzhjsNjGJ8dkK8jJGIEx59ZwhIqAl6Uz65hUU5bTtcsWAzwCj6O1hgylU13mdyRY3Nyl3aNHH1xLXUWQV7d3sHNvry3WSGs4QpnfS2XQy/7+WKKW096efiLROB87cmLBfjOt4QhzJ1Xzxq5ODqmvpEUvm2HIDPdGmT91DLPGVzlyP2XqWVytlOoUkVqgSX8Z+2qVUu1ZyLkAuDvF9k8D92RxnFGL1asa+PnTW/jh3zZzzWmHURnMvjBswiJIY3KuXtXA9vYeHtq4m48tmMR5DVPzGfIgpGQWe+1PHx3oweAZJNPJPgirVzVwyk+fIeDzcP3KebbLjcUVcYWJR+B8+mhl0Jco7mdn7+BVH57Ouq0dXL9yHrPrq+iLxjnkW4/YQij7xScWc+i3HuXji6fwjdMOT2x/Y+c+Pnbzc5w6byIfnT+xILJWr2rgG396nd37ennsqycU5Jj5IJOX6i7973pgnellvLcEEQkAK4D7krZ/E4iiZSSl+t5lIrJORNa1tLRYFXdAY3a9xvx9N8c8/+EsAiDhguo2NfkoBMyF3ww44aZJaxE4FCMwFsEyv/YU6QSMp2G/16PXGnKWUKZlK+mlt21WQkZpdiO92iDv2SE3HInSF4tTVzGYxzNrfCUisKXA/JuO7r5EyYxiI+1jp1LqY/rfmXnKOA3YoJTaY2wQkU8BHwOWK6VSqnal1C+BXwI0NDSMbj65jjn1AyUgFkzNPoaeqACaQb2bTc9CIq6KxSPQju0pErPYUEBlAS/7+5xZkPsSikD0WkPOWgRBn8ex4n7t3X14BMaWaYxiEY3JbYdryFA6NUmEzrKAl6k15TQ2F5aR3z6CFIEVHsGTVrZlwIWY3EIicipacHiFUiq/cpujDNNqy/F7hB8/tjknIklM/3Fksgj8eu552AaLINlDYCwWtz3/vm3EmHQWwf+tfc92Mk6xLAKjX6/fq3Vji8UVu/fud0R2JBon6PcmFfezD+3dfdSUBwY9ZGjztk8RGCx/M2bXV/L27s6CEsvauiMpZRUDaVcMEQnp8YFxIlIjIrX6awYwKd33ko5RDpwM/Mm0+RagCnhcTyv9Rc6jH2XweT2UB7X87FyIJIbbNF3ROQOVQR9dBVcEKXgEDhCt0jGLtzlA7hqkCAJeevoKe07TwewaWtekhepudIh41NuvWQROFfdr7+5LFJozYChAO2TBgBvKjDn1lbzX2l1QYtlIsggyRSQ/B1yJtuivZ4A70An83MrB9Sf+uqRts7Mf5uhHIdpJJgKn3uEVgR2uIXPQ0In2mDCYWeyUTAPRQRaBz7GaP8bie+39rye23bNuO/esKxxBMB0i0Tghv8cxF1xbisXS77WnHWmbYREkybPjvorFFXv39ycqChQbaS0CpdTP9PjAVUqpg5VSM/XXAqXULQ6OsSQwQDLRFpZciEmZmtebURny2eIaMisCYz4G7CJamQllTsk0oBWd035CZQGP48HixdPGJoKnThGPIv0xgj7vAFnQ5qB8e3ffkIU54BVb+hEMWASD5a29ehkfmTNAbCvEfdXR04dSQ2UVC1Z4BDeLyDzgCCBk2v57OwdWahggmWgLWy7EpEwlJsywyyIwu4aM+YC9RCuza8gs0+sAuctsEZQHfA66hrQ5lwd9iXhBn0NENi1G4JxFkMp94vfZFyMI+jyUJwW/66tDVOvZdkZr0HzPdYehdCptbN2XBaz0LL4OOBFNETyMlgX0HOAqggKjNRzhuDnjWNvYykdzIK8Yi8LwisDPzgIHF1Plk7eGI1QFfSyaXsPU2nJbiDHJc24NR6gIelk6w35yl9GYBjSma29/PCWxrtAwFsFwJMrxh4zjmXdaOe3IgwrefjQVIv1xQj6vY6zxjp6hFoFdMYK2sKZ0JIVF3blfU/JXLJ/DB52RvO+rdG6oYsEKa+kcYAHwilLqUhGZAPza3mGVJlavauCFd9tY29jKRUumcczsccN/yYRUDeRToSrkIxzpz3WYqWWn4BGsXtXAGTc/h9cjthGtDCvIcNGsXtXAaT9bi8dGmQZi8YG0VeMpsjcaG1IWodAw3DFXLJ+Dz+PhmXdaueSYmSyZWWurXNDmV1sRcIS4tzeN+8SuGEF7dyStq+aGc+fz4e89xbjKIF/8lzkFkJXaDVUsWCk6t18pFQeiIlINNAMH2zus0oWRTmY8MWSDWDyO1yMpn2jMsM01lOJuskOWGalKb1cFC6/oUsFsEZTpBCsnuASGfzzg9VAW0E66U/GJSH98EI/AzuJ+HT2p3ScBu3gEPf1pF+agT1f0BTrPI80isKII1ukVQn+Flj20AXjJ1lGVMIwbsT0nRTB8oBicCRabZRU6VXWwXIM7MSDbjvmlQjSJRwDQ44QiMHgEvgGGr1Nktkg0RsjvdYQ13hbWFUF5KteQPRZBuoU55C9suZR2fW7JqbHFgpVg8Rf0f38hIo8C1UqpjfYOq3RRUx5AJFdFELdU96Uy6KM/pnSWaGFYoTGVuuqp3U/nqeIilUEf77XYrwhiScxiKNwTYyaYeQSGG2p/vzOB6t4ki8BORZDOfeL3euiP2sAjCPel5BBA4etmtXdHqA75EtlXxUYmQtmi5BdQC/j0/13YAK9HGFvmz90isKAIjHpDhXTZxDNYBHa6hlK153TMIoipRLG7hGvIAUVgLjEx4JKyN3vHaKvY3dfP3ze30NWrKXe7FEFzZy/feXiT/m7wou/3FTZG0NzZy4qbn6O7L0bQl/r34/N68HmkYIp+175eItG4Y60oh0MmdfRj/fVz4EW0uj+/0v+/yf6hlS5qKgK2WwRAQRfLVMFiQ1Y4EiVNSan85arBJSZAs0K6bFQ+BjQSnfa/ESx2xjVkjhEYcu2d701PNvJyUztdvTGauyLc8U+tmrxdPIKbnmxkR4eW2Xb7Pwe3SC90jOCmJxvZuHMfABu27U37uUL2fnh9x14i0bhjrSiHQ6aic8sARGQNcJlS6nX9/TzgKmeGV5qoqwjQ1p19KmCqUtCpYBSeK+RimaronCFLc0PFC95MBNJYBEEfkWicvmg84cKwA1qMQDt+KOCcRWB2DRkWgV0uqVTtIgHuf2UXAP/xwBtcuGSarfLufmk7d5vaqvo8hYkRpJL14vvtzLjmoZSsYa0bXH7n2Wn2u1VY+ZUcZigBAKXUG+idx1zYg9qcLQJriqDKQYsg4YayyVWTMkZgU6ntZJhjBIZF4EzW0ECwOODTXBZ2WSIGWzt5oTCCxV9cVtiKMWuvXsbiaYMr73oFTp07IcHk1Qhl+VuYa69exilzJwza5kmSZYbWBCg/BeQ0+90qrCiCTSLyaxE5UUROEJFfAZuG/ZaLnFFbEcxdEVjMGoLCxgjSWSOGG8quRTmdRQD2KR8D0Vh8SNaQE4rAHCMwZNtliRhsbfPyZ7RQBG1RLrS85CU+pmBcZTDB5Jhh2sgAACAASURBVC1UGWozY9hAPEmWGYVwDdVXh/Dr94zPUxiWciFghflyKXA5cIX+/lngVttG5IK6igAdPf1Zs1SjFi0COxbKdGOttMENZUaqZjx2WyEGBmUNGemjDrqGjEyWsoDX1myllq4IHoGDxoSoDPqoqwwwa1wld7y4zZZg8Z5OTd5p87RuYBt37B3Emg4UMH10W5tWCf9DM2qorwoNkWVGwOcpSBOgbe2azOvOmMvmPV2OtKIcDlbSR3uBn+ovFw6gtiJALK7o7O1nbBbVCdNl7iTDsAgKmd+vNa9PL8uuRTmWor5ShUMWgdkKSqSPOhgs9psUgZ1B6m+cfjiPvbWHr5x0COea2pves267LcHig8aEmFxTxs8vTp2cWMgSE2ctmsJLTR385LyFTK0tz/jZoN9bkCZAZy6czMtNHSw/vJ5VH56e9/EKgUzpo/fqf18XkY3JL+eGWHrIlV1s7piVCVVBrdtTYdNHSWkR2CHLjIFmPClcQzZnDiU3pgGngsUDjWkM2Xa6pBr3aJ255kyoGrQ94PUU3CJQStG4pyvRrS8VNB5BYeQ27gkT8nuYPLZs2M+GCmQRbGkOUxHwMnFMcd1BZmSyCAxX0MecGIiLARit8tq7+5g13vr30mXuJCPk9+D1SEGJXmnTR+0OFseN9pym9FEbLJ50sg0F5PN6CHg9jqSPGotvIkYQsLc72pYWrVfv7KTFOeArvCJoCUfo7I1mVgQ+KZglsqUlzOz6Sku/m6DfS+f+/H8zW5o1mcOVgnESmdJHd+t/t6b7jAt7YDApDYq9VURj1iwCESl4DaDhgsV2LcrmxjQDMu21QkBzwylFIn0UNAXrFLPY7x2oKWW3RbBlT5iJenzADDsUwZY9htKpSvuZQsYItuzpslysT8sayv88NzZ3cdzsLJ7wHEAm11CXiHSmeHWJSKeTgyw1GK6h7z+yKSvmYXJPgEwo83v56+u7C8ZsTBcstoPFbEaqonMDVoiNpS0Scge22dmu0mD2vrVrH/et2040rhLXrryAMYLmzl5W/vx5zvrf53lr1z7OW/0CG3fspTsSHXKveD3CE5v25HwPmedkyHy2sQWA2gp/2u/5vR7iauAhIBeZzV29vN/aza59vUy04BYCLWsoF8VnnueKm59jT2eEg0aQWwgydyirUkpVp3hVKaWqnRxkqcGwCJraerJiHkbjCt8wbSoN9PbHaAv3FYzZqAWLh8oO+grvhjIjnsIiKPd7EbHZIkgEqQd+QuUBH/ttaldpMHuvWPMqLWGtPLNx7bReCIVRBDc92cir2/fyyra9XLHmVV5uamdLSzedvdEh90q4N0pbd+73kHlOhsy7X9wGwF3631QwYiO5WAWGzJueaOQHj74NwJu79ln6bq4WgXmeBoP5zZ3WZDoFy4XTRaSewR3K0l8pFzkjH+ZhLD68RWAXszEdmc0ON5QZqSwCj0eoDNhb9TSaQgGF/F72F9giSL5ejc3hxP/GtfMITKjO7wkzFcvWLMssLxnZ3kOZ5rRPv08yHdOIjfTFrLPVU933Bp59pzUtm9gMjVlsXflkmuff32mxJNMpDMsGEZEVItIIvA88AzQBj9g8rpKFwTw0lpdsmIdWmMUJpqhkf/xMiGdQQpVB+xZlTfkxJPBme7G7FIzmchuCtslMVDMMxu05i6bk7Rpae/Uyjp+TuRGS0Rf54S8fp92jOd5Dyfd4JlmpjmmUDckmcyj5vvcIWf/GsiWUWbl2xWYUG7BCC/wf4GjgHb2Z/XLgeVtHVcIwmJwK7UbNhnloRREkmKK6e7VQzMaYqQBbMqpsXJSjpgbyZhjF7uxC1OiDYHLFlfkLn89fXx0imObEGozb2spg3gqovjqU8d4x2MRVQR9HTBqj3aMq+3vUkGXc48PJSnXMAdeQ9RhB8n0fV2T9G8vWNWTuoZ2MZLZ0sWHFNdSvlGoTEY+IeJRST4vID2wfWQmjNRxh7qRq3t7dyQVLplnuRRtXqRfFVMc/ZlYd/3i3jTPmTypIr9t0PAKwd1GOp8tWsrkU9UBbUJMiCHhptaFv8PYOjYl66IRKdu7dT0XQR8P02gQLdnxViL5o3HKtqXTYpfexPvnwCWzcuZdwb5SAz8P4qqDGJh5flWDBtoYjTB4bIuT38uFZ47Jmx+7ep8maXltGW3ffAAmwN8r8qWMGyUpGrjGCXaY+3ROqg7R2RThm9jim11VYGn/I7yUaV0RjcXwW+wgYvcFn1JXr/bQHX7uRAiuKYK+IVKKVlrhTRJoBZ7pglChWr2rgTxt28NV7X+PSY2cOyd9Oh2hcEfIPvxCsXtXAa9v3cubPn+f0Iydy6ryD8h1y2mAxaItyRw61k6wgXcqs3RZBqrTVsgIGbc04e9EU/vleO7d+YjEHjx96L/zy2XcBjcyWnOKZDY6aVkNruI9ffaph2M+uXtXAv921gU27O3PqDf2FE2fz1NstXLdiLv9y2IThv2CCOUaQDT5/wiye3qxlJV11yqFc/ceNLJpWw1dOPsTS9xNd2bJQBJd95GD+vrmF61ceyXHDuN6KCSuzORPYD3wFeBR4FzjDzkG5gDl6HvWW5i7L38nkp0/GLF25ZHP8TMj0NGpvjCCON0WmlJ3uKEhd9dQO1xBopKeA18O0NCUQyvQuZfmmrjbqRCerKPN7cy6psUUPnM7JwBdIh0COFoFBjAN47K09KAVzJlifr6EIsqlAasjMRk4xkIlHcIuIHKOU6lZKxZRSUaXU75RSNyml2pwcZCliVn0FoFHgrcJqiQnQFufJY8sSP8h8YKRwplNCdscIMjXEsQsJi8A72DVkB8N3y54wB4+vSPsUmuhJkEeXMqVUgvFqFeUBb85F9hqbrZd2SEbCNZRlu8rGPVpph+l15TzzjmYZZKOIjAylbHoSNO4JUxXyUV+VugXmSEEmi6AR+LGINInID0TE7UHgIMoD2kKdnMKXCbEsq5XOqq/M6vhp5aYo/GaGnYtyOkukwsaUVTCnrQ78hMoC9jB8G5vDCQsuFcoL0BSnJRxh3/7+jKUdkpEPo7mxOcys8dZKOyTDb3LRZAND0c2pr6IvGscjMGNc5kJzZgSNBvZZWASNzV0jrpxEKmQilP1MKfVh4ASgHbhNRDaJyH+KiDWnmou8MK2ujCfe2pNgeJoZmMY2M6szloVFADCnvpItzV2svOW5tMe0glQ9AZLR0xfjg33acc3sznyRbs4etLIWe/YVvsRvc2cvX7nnVUAjdRnziMcV0bgaFJTMV86Km59jW3sPkzIwURMlsPNwDb38fjsA47N4ci0LaOmU2TJ8mzt7+ee7rUytyd4agIEYwTfvf93yPdTc2cuL77cxpaY84aYJeD3sy6J2UNCnW14WLYLmzl7WNXUwJcd5OolhYwRKqa1KqR8opY4CLgLOwm1M4wj29vTT0x9LMDzNDExjm5nVGbNYdM7AnPpKIlHFqzv2pT2mFRjtiNO5hjZs7QDgx49vBgazO/NFLK5Sxghe2a71nv3J4+/kLSMZNz3ZyBs6M/TdlnBiHuuatMX0p08URqa5l+7bu9PHcsoKYBHc9o8mAJ56u9nyd3Jtk/njx96hL6b4oDM3JW3ECDZ/0GX5HvrRY5vpjyl27t2fsHp6s+wZHMrSIvjh3zYTjSt2dhTmwcBOyHBNxUXED5wKXIDGIXgGuFsp9YD9w9PQ0NCg1q1b55S4oiNdn9hMCPo8HDQmxMKpY/nZBUcVRIZV1mM4EmXedX/j2tMP47LjZ2UlIxs5qfDlu1/h9Z37ePqqEzPKLASDM9vrkqvMTHJSHdPIAPu/TzWw/PDsMnDyOV+3v9DEfzz4Juu+dRLjKoe3JApxbbI9RiHvwee3tHLxr1/knsuOZunBdQUbo50QkfVKqWHTwDIFi08Wkd8AO4DLgIeBWUqp851UAqUIg5EYsFA3yMzAtNqq0pCx/LDUFRAzsTpTIZYmWJw8D79XBgXNCsFqNpjF6WQGspxLJqRjiga8wuSxZQmXRbbnL5Wc42cPXmgyMVHziRHkwzQPZdme05CVz3lae/Uylh06cN8ON95kmdq1CuU0XyNraDjFUoh5Oo1MScfXAncBVyml2h0ajwsGGIn9+iKXyQVrZmBmQyiqrw5x0JjUvstMrM5UiKeJESTmoadaRmMqQev3SGFYzdF4fBCJLllmfwF7wqZi+XoF+uOK8oA3kVLal6fM+uoQ3iQ5mZio2S7IybIMxm221yRbl1Tytcn2PjOOUV2mVSa10vN3yP0QV5QHNFazUTvI6hgGsoYyK4JCzNNpZOpHMHLVVwmgNRzh4qXTea8lTGs4QktXhP5YPMHA7Ojuoy+mOHfxlARDMVtmaWs4wtSaMuZPGcu6re20hyP0x+HCD1lnM0PmrKHWcISLlk7j3nXbmV1fyda2bgAOPaiKxdNr8+7XGosPlWvIvOfl7RxyUGVBGZzb9X6zhx1URVypBOP2sTc/4KKl01jz0jYOm1idt0wzyzfg82RkouYbI9ity/r0sTPpjcYtXxPDEsmGO2Ewkn0eDx85ZHxO19/of33pcTPZ3xcb9hit4QiTxoQI+r0cO3scj735ARcvnc5FS6Zx10vbLI9hgEcw/HxbwxEmjglSEfRz9MF1I6IvcSbkTkN0YStWr8rs1nv67WYu/e3LnP+hqSyerjXWyFYRJMt44JWdXHnPq1x67IwhbQkzIROPwJCxYdteJlQHmTy2nCc27WHX3l4e/vLcvNPqYvH4kNLbhsx1TR1MrC4b9lxmg7MXT+Gf77fzi08sZsa4isR2g1374vvtTBqbv8yFU2to67bG8k24hnJM5bx82Wye2tzCsXPGsezQesvfy8USWb2qgRN++DTzJo3JiZEM8MtVi5n9zUeoDPj45umHW5J53A+e4sjJmkyz3GzGYGQNWYk5rF7VwIe/92RC5kiHNZ60ixEHg/hjJpyl6xKW9TGz5BYMxyMAI1U1zLs603Lf/v6CPKlHM7CpZ0+oHMQmLQS2NIcJ+DxpG53Pqa/k3QJwM4z8cysI+bJ/Mh8ka4/B8s2O/VquM5r391tPW+3tj7GtvScr4loyfF4PZX6v5R4XPX1RdnTsz3p+yUhkDVlIH+3q7Wf3vt685ukkXEVwgGLy2DLK/N5Bi3Yslp8imDW+EpHs2Mxg4hFkeLqfU1/Jjo79bG3rZvH0GmCgLWE+yMSdmD2+km3tPQWt/9PYHObgcRVpz/Oc+kqa2rqzYp8mQymVVbkHj0fyapPZ2NxFmd/LpDQxo3QoS1gE1jOp3mvpzrq0QypkU1TwvRbNHZnvopzgEVhIH31Xl5mv8nEKtikCETlURF41vTpF5EoRqRWRx0WkUf9bY9cYRjM8HmFWfcVgRZCh8JsVlAW8TKkpozHL+kN6ReaMHAbjhx9XcJpe5K4QrOZoBnfYnAmVKEXCCikEGpu7MrrNZtVXElfwfmt3zjKauyJ09UazKn+QT50jg3GbLcs3oQiyUEDGvZXvolwV9CViBVZl5qt8gllYBI17CjNPp2CbIlBKbVZKLVRKLQQWAz3A/cA1wJNKqTnAk/p7FzlgTn0V73zQlWAb90VjPJRnH+JpteU8uanZMpsZzK6h9Mc1NyM/fGI1FUEvP396y6Dj5zLu3r4Ym3Z3pvyusZB++e5XcpaT3G92e/v+jCxfQ+YX79yQ9XwMWU9u2gPAeL13tRUEfV4eeSO7a2/0J/7ne205sXwTQeosGM2v6US/fKqkglZCpNuiRfDqNk2m4crKFYn0UQsWwWs79iIMKMuRDqdcQ8v5/+2deZhcVZn/P293p7vTW5YmHQhZEBKCwLBGHFCRyOLoyOIMijCPEnR01JkRUH8EB5UwqIOK4AK4zICiICDKGmSXQFhDCEkgIaEDWUlId9KdpJek1/f3xznVfbtSVV3brepUvZ/nqaerb917v+ecunXPPcv3vPCWqq7HrWZ6q99+K3BOjtJQcExvqOHdXXsG3Ma9/bBl556MHLstHd3s7unja3e+mpSbGeL7CIJMq68amDr64LLNVI4qpamta8j500n3Ozt2x4ynC4PryLzV3JG2Tqx4syu37Iq7/8ET3ADymuaOlPMT0breu6GfTMHl29ndy7YUY1BH4hP39Cmbd6bufk1nttLjvpL71YK3UtYLksr6VZFyvOmpNRlpigjlZSVJLTHx1KomFLgxQ81cMayzOCsizpi2RFVvEJEdqjo28FmrqibsHio2Z3EyZNMZnOz54p3/za1tnHH9M9xwwbF84qi9DVdhOHKHc29mWj7pHJ+uo3Q4rXTSmW7eUrlm+vqVQ/7rr1xy2gwuOS3x8mPZdtt+8feL2djSySOXnJwzTYC/m/co/3zcZOaddUTONDMhY2dxFhNSDpwF3J3icV8SkcUisri5uTmcxO3DRDssg6Tr2EzHzQzDDxZHOy2D8WKDlKQQxzXa5RvtEI18XhbjCk8mXmyiuLrxjk/XpRsvrcl8j+loLrxsNmccMXQpilTKPkJpiXtCTqZFsPCy2Zx51AED/2fqKq9NokWw8LLZfCKLmu4cieMWh6GZC3LRNfQxXGtgq/9/q4gcAOD/xmz/qupvVHWWqs6aMCH2Da+YaairZFKMtdyHi/ea6HxBN3Mios8/0DUU58DIuXv7lYqykoF4sdG796cQxzXo8i2N4TCNaPbp3jrJxItNFFc33vHpxoMeLJ/Bbcl+j0HNZOPvNtRVUl81dPwhlbIPUpXk0tsNdZUDixMm4wgejmRmDTnNwUhy2XCyV5SV0JWg4muoqxwSvS5bMcHDJheGsvOBOwL/PwBcCFzj/96fgzQUJNHO4GTivQ53vqTczMdNHuIB6Nfhp49Gzn3BCVP5tz+4br4p46tYvmkH1RVltHZ2U1ZSkpK3YIN3+X774+/lrW0de+U5mJ/lm3ZQKsLOPb1MrKtISmdbexc1FaVUjCqlq6cvqXiz6caDdo7b0bR0dDFlfNVeMYKHO9bFM97DOccemNQxW/zy3EdOqmNafXXaMXRTiUmwycdevu7TR7NoXWtGbtsaH29CVROaEjf5lT9/et4xvLi2JWOHb2RZikREVhv9xQXH8tya7SPeVQy4OcthvYAqYDswJrCtHjdbqNH/HT/ceY4//ng18s/fVm3VaXPn66K124dsX7K+RafNna9/e2Nr2uc+/zcv6Nk3PJvSMXct2qDT5s7Xddvak9r/uTXNOm3ufH2usTlpjSOvfESvvP/1lNK1dEOrTps7Xx9+bUtKx53y46f0K7ctTumYCNc/vlqnzZ2v3b19Se2/4p2dOm3ufP3r8s1p6UWY/eOn9N9vfyWpfX/2xJs6be587ejqyUhTVfXGpxp12tz5uru7N+F+1z22Wt9z+fD7JcvHf/aMfv63ixLuc+2jq/Tgbz2kXT3JfRdhAizWJO7VoXYNqWqnqtar6s7Atu2qeqqqzvB/bUG7fYTpE/Z2M8NgiyCT1SKme+expjB5obGpjYqyEiaPSy7KVH21W/l0e0d3Uvt39/bTtqeX8dXJT+OEwXjQqfgX9vT0sX57x0AZp0q9T2NrZ3J5a/FlkGreokklKltjUzuTx43OeBonuDECYFgvwZqmdqaOrxpYDiNTkmkRNG5tZ1p9FeWxBqhGKPtOSo28M+hmHmo4i0QMzMTVPKOhhvau3pSClTQ2tXPwhJqkdSM3vZYkK4LITTXVm2UkHnTEVJQMa7d10K8wPYU1noKM95Vca0dyyy5s73DdQPUpeBViMXpU8nGaG7e2Zc1pW1PpKoLhxgncUh3plWksKspKhzWUNTZlL5+5wioCI2lKSmTgyT1IMktMDEfkxxp97kSsaWpP6Qc3rsotX5xsi2B7u9uvPo2n5ukpxoOO5DvdG0iksorc4IdjsEWQWVD10eXJOZr7+pW3t3WktJhhIqp9qyJRXOrevn7WbuvIqrvXLeURv0XQ3dvPuu2dKbnCRwJWERgpMaOhhtUBN/PKzTu56sEVQOIlJoY9r7f/X3Hv60m5gNdv72BTa2KXbzRlpSWMrRpFS+BmmSh+cuRmOS7NimBNUxuf/tXzSbl9l2bouE21tdPS0U2JwFi/tn+6jB5Vyp6evmHjUC/d0Ep3bz8TU4iJnIhkWgRLNrTS06dMrMuOJriwrGua2uLmc8n6Fvr6dUgApn0BqwiMlDikoYamtq4hbuPV77oukEy6huqryykvFTa0dCblAr7m4VVAYpdvLMZXlw/pPkkUP7mlM/0WQSQe9MvrWpNy+z6+8l0Afv10eo7bdCqCcVXlGVXe4McIevqGjUP98785h+2iddkZEqytcBVYoorghojm2uwNQ67b3sHunvixjm/0jukX3t6eNc1ckBNncaaYs3hkkG03czrnzdS5ee4vn6esVHh1w45hz/O759Yy78GVvPLt06hPIiZvhFTSmC0nam9fP9OveJiLT53BpacndvkCfOW2V2hsaueJr384aY1YHPyth2JG0MvW9xWPtds6mH3tAq4/72g+eezkIZ+FoZmukz1fjuIII8ZZbBQOieIcA5x86IS0HJQDzuMYT6fRjtdBF67bN1Xn5vjqclo6uvdyDsc6T0tHNyIwtiq1FsHCy2YPrLA6XBqz5bgd7PZKcvyjozvjGUMAn5o1mbISietsjpRzaZzP0yXShRZrjCAMzYFzxrnuwspnrrCKwEiaRHGOAarLS9NyUA44a3VvV3O04zXoUk7WSRukvsZVBA11ldSUDzqHY51nu+8+SbXLq6GucuAmO1wc4Ia6yoE0ZOpEHV9dPtCdNRwtHd1pdXlFs19NBb39Ouim7onv8JYsxakGqPVjBG0xuoaiXeXZ0Bw4p8/ocE72fcVRHMFCVRopEe1m3t7eNbA0wo7dyU1djHfeoAu4alQpTe3djKks28vxuq29i9qKMo6aPIb3TKhJybk5vrqc1s4e+vuVjd7pCi42cLSO60dPbzB1W3sXE+sqGDu6nPe9J3Fs5ojj9iefPpqXM3Dc1leX09KefEWQjRZB9DLLJx5Sv/f31dZFaQn8499Nom70qKw4bSvKSigrkbizhra1dVEqcNbRk6iuzI7mtvYujps6liUbdnDerCl75bO5rYsSgX869kAqy8v2DUexxyoCIyWiY/He8uxa/nv+SgCuPPPwrJ0X4MT/eZITD6nnuk8fM2T7z88/lvd+5xGOnzaOr58xMyWd8dUV9PUrO3f3MOekg1jYuA2AU2Y2cMH7pw7Zd3tH94AJLVV+/dlZXHn/69yz5B2uPjtxbObT3juRpRt3ctp7J3L2MQempQcwrqqc9ds7h92vr19p7cxOi2B0lDns6CljmfsPhw3Z9t2zjuDRlVt5/8Hj+Zf3T8tYE9yS0InWG/r2Jw73mvV85oSpMfdJlV9/dha3v7SeJRt2cOnphzKxbujT/n99/L085jU/NWtKVjRzhXUNGRkRnKOdiY8g3rlj+Qoi5qtD0pgfXj8w3757YJ5/eWlJzKhsmT41T2+ooa2rl627Es/tb2xq58CxowfWdEqX+prypDwSOzq7Uc3cVQxDWwTlZSUxw5wOROtK0zUdj8h6Q7HIVlSyWJoQ29EcuZ72lahkQawiMDIi+EPLdCpiNJGKoD9qWspgsPXUTTvjA0sxNG5tZ2JdBTP3r41Z4bR2dDM+A+dtxCQ3XOjPxq3JxydOhOv26t6rvKIZcEynMBMqHqPL3S1EBD44fT/WxMjrgFkuS2ayCDUVZTHHCGDwGpk+IbuatQn8C9kKw5kPrCIwMmL/usqBp6RstwhmNNTS2d3HlqhlJ9Y0tVMig9HAUmHAgdvezZrmdmY01DIjRsujPwvdJ5FKMpFbuq9feas5NYd0PCLdXrv2JB6riTimx6c4GyoWo0e5737KuCqOPHAMG1o62RO15MSapnbqq8uz0gIJUpMgXOWapnYm1FYwJs0xnvia3r8Qo0WwpqmdA8ZUUluZXc1cYBWBkREiwtR6t+jbjiRnrCRL5Mnqot8uGuI2fmV9C6NKS4a94cUicjP63kMrWb5pB5PGVnJIQw1bdu7hrF88O+CWPvvGZ+lXGFWS/k+kvrqcutFl3LTgrZixn1du3smZv1hIV29/VtyvkUrrc7csoqltz0BM4mjNy/+yPGOtCJFwlbt299BQW0G/wtk3PjtEc9mmHXT39mcUSzsWo8qEZRt3xHSir9yyi66evqxrDkxb7eodEs/6079+geWbdtDR1Zt1zVxgFYGRMXv8WjO3vrAuq+eNPCU3bm0f4jZetLaFrt747s5ERCqCTa27UYV12zoHdJa/s3PALf3aO86x/NK69B2iIkJFaQnNUbGZI7GfL75zKSu3uO6Exetb09aJEMnba5t28vMnGgdiEkdrrvUDyve8uiljzSpfEezY3cMLb7myWv1u+xDNVVvaaOuKHVc6Eza37qGju28vJ7qqsvrdtrixrDMh2DUUjGf98roW1jR1hKKZC8xZbKRNmG7KVOIcJ6uXTlzmVDUy1Uq37HKZt0w1M70+MomvnSktHd0cd/XjOdXMBHMWG6ETcVNW+nXXs+mmHHQbx98n1djMkVi9wZGMEqChtjxmbGJILr5xPK1kY0BnohPUC7qZk1HNhuZZRw/Goh5OM51Y2nF145StAOMD4wLZdvhWV7gW0GH71xLvq81WPnOJVQRG2kTclF19/QMBO7Llphx0G+8dcxjSi83cUFfJhJqKIXGI+4Exo8tjxiaG5OIbJ0p/MjGgM9EJ6gXdzMm087OhWVs5GIs6kWa6sbTj6sYpW2Vw9lp5lq9JcPEIystKEHHlF00285lLzFBmZEQwFvEfF23Iqpsy2m0cmWefSWzmoDMaYPmmHezc3TPELb2trYv9aiuGjU+cSfrLy0oG4kFnqhPU27+ugrrR5TS37aG1s4ePzJzAii27QtUMxqLe1t7FRw6bOBBDu7yshAm1FSnFYE5FN1i229u7qCgrpb2rl/JS4d6vnsQdizZm3eFbW1FGFyOQVQAAFDtJREFU255epjdUs25bJwdPqKa5rSuUfOYKGyMwjAJi3gMruHvxRj41awp3L97I61d9NKGruZA495fPu9aAgqLc/eWTQtH58I+f4pgpY9nd3cf67Z08eunJoehkAxsjMIwi5JCGGjq6+1jY2MwhDTVFUwmAjwq3tS3r4SmjiTias7Ve00jAKgLDKCAiU2Hfas5uiMZ9gekNNbR29tDa2RNqzOCaCrfGUUuGzvORhFUEhlFABG+A+1rc3EwJLmGR7TWGgtT6xe62Z2kp75GAVQSGUUDU11QwZrQblJ6wj8XNzZRgCyjd5cOToaaijB2dPezc3WNdQ4ZhjEwqSt1c92febMpzSnLLpDGVeEsLdy7aGJpOTWUZW3buBrKzgutIwKaPGkaBEO24fWDZFh5Y9tCIcLiGTXTeb3tpA7e9tCGUvFdXlA1EZCuUisBaBIZRIEQctxUhOL1HOrnMe20gboRVBIZhjCgijtvuEJzeI51c5r0mUBGkG8FupGFdQ4ZRQITp9B7p5CrvNYF4A4XSIrCKwDAKiGDs5++dc2QeU5J7cpX3YIsgzNlJucS6hgzDMFIgEpNgbNUoykoL4xZaGLkwDMPIEZEWQaF0C4FVBIZhGClR41sE2Yj5PFKwisAwDCMFItNH32pu3yfjE8fCKgLDMIwUiMSVaO3s2SfjE8fCZg0ZhmEkSS4dzLnEWgSGYRhJUqjubasIDMMwkqRQ3duhdg2JyFjg/4AjcXGlPw/sBn4FVAK9wFdVdVGY6TAMw8gWhejeDjVmsYjcCixU1f8TkXKgCvgTcL2qPiwiHwcuU9VTEp3HYhYbhmGkTrIxi0NrEYhIHXAyMAdAVbuBbhFRoM7vNgbYHFYaDMMwjOEJs2voYKAZ+K2IHA28AlwMXAI8KiLX4sYoTop1sIh8CfgSwNSpU0NMpmEYRnET5mBxGXAc8EtVPRboAC4HvgJcqqpTgEuBm2MdrKq/UdVZqjprwoQJISbTMAyjuAmzItgEbFLVl/z/f8ZVDBcC9/htdwMnhJgGwzAMYxhCqwhU9V1go4jM9JtOBVbixgQ+7Ld9BCgMa55hGMY+Stizho7BTR8tB94GLgKOAH6G6zrag5s++sow52kG1oeW0OyzH7CtyLSLMc/51LY8m3YyTFPVYfvWQ60IihURWZzMlK1C0i7GPOdT2/Js2tnEnMWGYRhFjlUEhmEYRY5VBOHwmyLULsY851Pb8mzaWcPGCAzDMIocaxEYhmEUOVYRGIZhFDlWERiGYRQ5VhEYhmEUORazOAuIyEeBc4ADcQF4NgP3q+ojhajrtQ8Dzo7SfkBV3yhU7TyXt11jub3G8lXeY4B/iNJ9VFV3hKprs4YyQ0R+ChwK/B630B7AZOBzQKOqXlxIul57LnA+cGeU9meAO1X1mkLTznN52zU2qJ2Layxf5f054ErgMeCdgO7pwFWq+vswdMEqgowRkTdV9dAY2wV4U1VnFJJuRBs4QlV7oraXAysKUTvf5W3X2MD2nFxjeSrv1cD7o5/+RWQc8FKsNGULGyPInD0iEmsp7ffhFtUrNF2AfmBSjO0H+M8KUTuf5W3X2CC5uMbylW/BdQdF0+8/Cw0bI8icOcAvRaSWwWbkFGCX/6zQdMFFmXtSRBqBjX7bVGA68B8Fqj2H/JV3vrTzpQv5vcbmkJ98fx9YIiKPMTTPpwNXh6hrXUPZQkT2xw3wCC4gz7sFrluCCyo0oA28rKp9Ba6dl/LOp3YxXmNeP+f59t1AH2Vonh9V1dZQda0iMAzDKG5sjCBERGRJMel67fnFpp3n8rZrLLfa+SrvUBeesxaBkVVE5ABV3VJs2kbuKMbvWUSOHy6SY0bnt4ogO4jIRAImEFXdWsi6Af3xgIbdhzlStPNZ3naN5fYay3e+c4lVBBni4zL/ChjDUBPIDlw85lCakvnS9dpTgR8Bp3o9AeqAvwGXq+q6QtPOc3nbNZbbayxf5T0G+BbO0RyJM9wE3A9cE6q7WFXtlcELWIozgURv/3tgWaHpeo0XgPOA0sC2Upzr88VC1M5zeds1lqPvOc/l/SgwF9g/sG1/v+3xMPNsLYIMEZFGjeM0FJE1qjq9kHST0I772b6sPYLL266x3GqHWd6rVXVmqp9lAzOUZc7DIvIQbl2SiAlkCm5dkjAXqMqXLsArInITcGuU9oXAqwWqnc/ytmtsUDsX11i+8r1eRC4DblU/HuHHKeYE0hEK1iLIAiLyMQZXSYyYQB5Q1b8WqG458IUo7Y3Ag8DNqtpVoNp5Ke98ahfjNeb1c55vbya73Os2+M1bgQeAH6pqS2jaVhEYhmEUN2YoC5GwTSAjTddrf7fYtPNc3naN5VY7X+V9UajntxZBZvg5zjE/ws0wmFxIusMhIhtUdWqhaeezvO0aixIP+RobifkOO882WJw5zcB6GLJMrPr/G2IesW/rIiK74n0EjC5Q7byVdx61i/IaI0/5FpHl8T4CJoalC1YRZIO3gVNVdUP0ByIS5kh/vnTBGWvepzGclgWsnc/ytmsst9r5yvdE3Mqj0Q5qAZ4PUdfGCLLAT4FxcT77UQHqgptWNy3OZ38sUO18lrddY0MJ+xrLV77nAzWquj7qtQ5YEKKujREYhmEUO9YiCAkRmSUiBxaLrtc+QEQqikk7z+Vt11hutfOW77CxiiA8/hOYLyJ3FYkuwB+AVSJybRFp57O87RrLLXnJt4i84V+hhei0rqGQEZFaVW0rIl0BDlfVFUWmnZfyzqd2MV5jXj/n+RaR/XAL4T0UyvmtIggPETlMVVeFrDFKVXuitu2nqttC1i0BUNV+vxzAkcC6MG3wCdLyVVW9KceaNcChwNsa5vLADCy30KP+xyois4HjgJWq+nCIukeparwpjaEjbinqXaq6Q0QOAmYBq1T19Rzpz8KtMdQLNIb9W84n1jUULo+FdWIRmS0im4DNIvKY/6GEruu1zwG2AO+IyNnAQuBaYLmInBmy9tejXt8A/jvyf4i6NwXefxBYCfwEeE1EPh6WrudlYKzX/n/A93Fz6b8uIteEqPuqiKwRkatF5PAQdfZCRC4HngZeFJF/xS329jHgrjC/Z6/9YRFZDFwD3AL8G3CziCwQkSlhaidI02thnt98BBkiIj+P9xH+xxsSPwI+qqorRORc4HER+ayqvshQI0wYXAkcjbsZLcPN914tItOAv+AWBguLq4C/AisYzGcpUBuiJri16CNcDZyjqktE5GDgTz5NYVGqg9G5zgM+pKq7fSWwBLdQWRgsBz4LnA88ICIdwB3AnRpiYBjPZ4HDgSpgHXCwqjaLSDXwEnBdiNo/Bc7weu8BrlPVD4jI6cDNwBlhiIrIP8X7CBeXIDSsIsici4BvALFWQzw/RN3ySB+pqv5ZRN4A7vFPUqH396nquzBgfV/tt62PdBmFyBG4m0A1cJWqdorIhap6Vci6QerUR6lS1bdFpDRkvV0icqTvEtkGVAK7cb/fMMtbveYVwBUicgIuMMxCEdmoqieFqN3nK7tuXF63+wR1uCGCUClV1Wb/fgPez6Cqj4vIT0PUvQu4ndi/38oQda0iyAIvA6+r6l7OPxGZF6Juj4jsH7kh+5bBqThTyiEh6gJujEBV+4HPB7aVAuVh6nq357m+S+pxEbk+TL0Ah/klAAQ4SETGqWqrr/hGhaz9ZeB2EVmGC124WESeBo4CfhCi7pA7rqouAhb57riTQ9QFWCIif8RV+E8Ct4rII8BHcN1yYbJYRG72umfjzVwiUoVrfYbFcuDaWGMgInJaiLo2WJwp4hao2qOqnTnWPQ1oVtVlUdvHAv+uqt8PUft9wGuquidq+0HAB1X1trC0o/SqcF1F71fVUG9MvtsryBZV7fazOU5W1XtC1i/FdUkcinuA2wQ8GuZAtYhcoKphu3jjaZcBn8I9Hf8ZOAG4APeEfqOqdoSoPQr4Iq5rahlwi6r2ichooEFV14ek+yFgfZylLWap6uIwdMEqgqziKwUN9OcWtG6xaluec0uxaucSmzWUISIyVUTuFJFm3CDWyyLS5LcdVGi6xao9QvLclEvtYsxzvrUTpOkToQpomlHv7eVewAu4mRylgW2luEG1FwtNt1i1Lc/Fked8aydI01Vhnt+6hjJERBpVdUaqn+2rusWqbXnOnW6Rax/GYKxkBTbjYiW/EZYm2KyhbPCKOLPRrbjg2uDciBcCrxagbrFqW55zp1uU2iIyFzfl/E5gkd88GbhDRO5U1dDMg9YiyBBx9v8vMFiLC+7ieRC4WVVj+Qv2Wd1i1bY8F0ee86ktIm8CR+jeS8aUAytCbYlYRWAYhpF/RGQVbrWA9VHbpwGPqerMsLRt1lCIhD7SP8J0i1Xb8mzaWeIS4EkReVhEfuNfj+CMbReHqGsVQci8r8h0i1Xb8mzaGaOqj+AMg1cBj+IWj5wHzPSfhYZ1DWWBfI3050u3WLUtz8WR53xr5wNrEWSIH+m/EzegtAi39pDgRvrDWhUyb7rFqm15Lo4851s7X1iLIEPyNdKf1xkGRahtec6dbjFr5wtrEWROPzApxvYD/GeFplus2pbn3OkWs3ZeMENZ5kRG+hsZNJ9MBaYDoQWbzqNusWpbnnOnW8zaecG6hrKAuDXpT2DQfLIJeFlV+wpRt1i1Lc/Fked8a+cDqwgMwzCKHBsjMAzDKHKsIjAMwyhyrCIoMERkgYh8NGrbJX41xUTHtYebsri6d4jIchG5NGr7PBH5Zpa15ojIDUnsl3XtYfRipivZ9GYxHZNE5M9ZOtccEWkWkaUisir6+41zzCkiclI29I3UsIqg8LgDF0AjyGf89hGFiOwPnKSqR6lqroLQFzXiYgHHRFU3q+q5WZS7S1WPAT4AXCEiU4bZ/xTAKoI8YBVB4fFn4BMiUgEgLrTeJOBZEakRkSdFZImIvCYiZ0cf7J/K5gf+v0FE5vj3x4vI0yLyiog8KiIH+O1fE5GV/sn+zhjnrBSR33rNV0Vktv/oMaDBPzV+KJnMich9Xn+FiHwpsL1dRH7oP3tCRE7wraO3ReSswCmmiMgjIrJaRK4MHH+F3/YEMDOw/Ysi8rKILBORv4hIVYw0nSAiz/u8PS8iM/32OSJyj9drFJEfBY65SETeFJGncTfKpBGRM0TkBf893i0iNX77d31aXxe3YJn47QtE5Ade62IR+Z2I/Nyn9W0ROdfvd5CIvJ5E2r/g075ARP53uFaLqm4H1uDm4SMiZ4rIS768nhCRif46/TJwaeR6EJEJvsxf9q+UyslIgTDDn9krPy/gIeBs//5y4Mf+fRlQ59/vh/txRmaOtfu/pwDzA+e6AZgDjAKeByb47ecBt/j3m4EK/35sjPR8A/itf38YsAGoBA4CXo+Th3nAN2NsH+//jgZeB+r9/wp8zL+/F1fJjAKOBpb67XOALUB94PhZwPHAa0AVUOfL5Zv+mPqA9veA/4yRpjqgzL8/DfhLQO9tYIzP73pcgJMDfBlMAMqB54AbYpx3TvR2/709A1T7/+cC3w2WjX//B+BM/34BcFPgs98Bd+MeBA8H1vjtA99HgrRPAtYB4335Lhwu7bg5+EuBSv//OAavu38FfhLrOwf+CHwwcI438v3bKtSXGcoKk0j30P3+7+f9dgF+ICIn4xySBwITgXeTOOdM4Ejgcf+gWYq7qQIsB24XkfuA+2Ic+0HgFwCqukpE1uNWWdyVcs7gayLySf9+CjAD2A50A5EVGl8DulS1R0Rew93gIjyu7gkVEbnHpw3gXlXt9NsfCOx/pIh8DxgL1OBWhYxmDHCriMzAVUijAp89qao7/XlXAtNwN/MFqtrst9+FK49k+Hvczfs5/z2U42LsAswWkctwFdp4YAUumArAXVHnuU9V+4GVIjIxjla8tD+tqi1++90J0n6eb/3NBL6oqnv89snAXb5FWQ6sjXP8acDhPp8AdSJSq6ptcfY30sQqgsLkPuA6ETkOGK2qS/z2f8E9hR7vb5LrcE97QXoZ2mUY+Vxw66ycGEPvH4GTgbOA74jIEaraG/hcYhyTMiJyCu7mcKKqdorIgkD6etQ/OuIquS4AVe2Xof3i0cYZ9emLZ6j5HXCOqi7zXWSnxNjnauApVf2k7+JYEPgsGM2qj8HfXLoGHsFVZucP2ShSCdwEzFLVjSIyj6HfbUfUeYLpivf9xEp7Kt/lXar6HyJyIvCQiDysqu/iHgquU9UH/Hc6L87xJbjvencKmkYa2BhBAaKq7bib0S0MHSQeAzT5SmA27gkvmvW4p7AKERkDnOq3rwYm+B81IjJKRI4Q58CcoqpPAZcx+OQc5BlcJYSIHIpr5q9OI2tjgFZfCRyGezpOldNFZLyIjAbOwXXLPAN8UkRGi0gtcGZg/1pgi4iMiuQhTrre8e/nJJGGl4BTRKTen/dTKaT/ReADIjIdQESqfJlGbvrb/JhBNgd9gywCPiwi43wF+8/DHaCqL+C6qiLBVYLldWFg1zZceUd4jMCSDiJyTAbpNhJgFUHhcgeufzw4eHs7MEtEFuNuaquiD1LVjcCf8N09+GDdqtqNu7n8UESW4fp8T8J1Ed3mu2BeBa5X1R1Rp70JKPX73AXM0eTivn5bRDZFXriunzIRWY57Cn8xiXNE8yzuprQU15e/2LeY7opsw/V7R/gO7sb9ODHKy/Mj4H9E5DlceSREVbfgnoJfAJ4AliTYfU5UGVTgKps7fDm8CBzmy/x/cd1i9+GWTs46qvoO8ANcmTwBrAR2JnHoD4GLfEU7D7hbRBYC2wL7PIirkCOTB76Gu16X+66pL2cvJ0YQW2LCMIyUEJEaVW33LYJ7cZMG7s13uoz0sRaBYRipMk9EluJmXa0l9gQBYx/CWgSGYRhFjrUIDMMwihyrCAzDMIocqwgMwzCKHKsIDMMwihyrCAzDMIocqwgMwzCKnP8Pj0b9AEszRR8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of Accuracy with varying values of Lambda and Learning Rate\n",
    "plt.title('Validation Accuracy vs. Lambda and Learning Rate')\n",
    "plt.plot(X_lab,Accuracy,'*-', label='Validation Accuracy')\n",
    "ax = plt.gca() # grab the current axis\n",
    "ax.set_xticks(X_lab1) # choose which x locations to have ticks\n",
    "ax.set_xticklabels(X_lab1,rotation='vertical') # set the labels to display at those ticks\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.xlabel(\"Values of Lambda and Learning Rate\")\n",
    "l = plt.legend()\n",
    "plt.savefig('ValidationAccuracy_SGD_sub_human.pdf', bbox_inches='tight')\n",
    "plt.savefig('ValidationAccuracy_SGD_sub_human.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_lab</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.1,2.0</td>\n",
       "      <td>68.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.07,3.0</td>\n",
       "      <td>68.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.09,3.5</td>\n",
       "      <td>68.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.05,7.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.07,5.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.07,5.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.07,4.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.07,3.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.07,2.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.04,9.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.04,8.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.04,8.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.04,7.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.05,7.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.06,6.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.06,5.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.03,10.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.06,5.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.06,4.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.06,4.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.06,3.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.08,2.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.1,2.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.07,4.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.1,3.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.05,5.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.05,4.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.09,2.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.09,3.0</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.08,4.5</td>\n",
       "      <td>69.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.05,1.5</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.04,2.0</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01,3.5</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01,7.0</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.02,3.5</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.01,7.5</td>\n",
       "      <td>74.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.1,10.0</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.05,1.0</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.03,2.5</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.03,1.5</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.02,2.0</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01,6.0</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01,5.5</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01,4.0</td>\n",
       "      <td>74.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.04,1.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.03,2.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.03,1.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.02,3.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.02,2.5</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01,3.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.06,1.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01,5.0</td>\n",
       "      <td>75.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01,2.5</td>\n",
       "      <td>75.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.02,1.0</td>\n",
       "      <td>75.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.02,1.5</td>\n",
       "      <td>75.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.04,1.5</td>\n",
       "      <td>75.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01,4.5</td>\n",
       "      <td>75.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01,2.0</td>\n",
       "      <td>76.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01,1.5</td>\n",
       "      <td>76.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01,1.0</td>\n",
       "      <td>76.956522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_lab   Accuracy\n",
       "173    0.1,2.0  68.260870\n",
       "118   0.07,3.0  68.695652\n",
       "157   0.09,3.5  68.695652\n",
       "89    0.05,7.5  69.130435\n",
       "123   0.07,5.5  69.130435\n",
       "122   0.07,5.0  69.130435\n",
       "121   0.07,4.5  69.130435\n",
       "119   0.07,3.5  69.130435\n",
       "117   0.07,2.5  69.130435\n",
       "73    0.04,9.0  69.130435\n",
       "72    0.04,8.5  69.130435\n",
       "71    0.04,8.0  69.130435\n",
       "70    0.04,7.5  69.130435\n",
       "88    0.05,7.0  69.130435\n",
       "105   0.06,6.0  69.130435\n",
       "104   0.06,5.5  69.130435\n",
       "56   0.03,10.0  69.130435\n",
       "103   0.06,5.0  69.130435\n",
       "102   0.06,4.5  69.130435\n",
       "101   0.06,4.0  69.130435\n",
       "100   0.06,3.5  69.130435\n",
       "135   0.08,2.0  69.130435\n",
       "174    0.1,2.5  69.130435\n",
       "120   0.07,4.0  69.130435\n",
       "176    0.1,3.5  69.130435\n",
       "84    0.05,5.0  69.130435\n",
       "83    0.05,4.5  69.130435\n",
       "155   0.09,2.5  69.130435\n",
       "156   0.09,3.0  69.130435\n",
       "140   0.08,4.5  69.130435\n",
       "..         ...        ...\n",
       "77    0.05,1.5  74.347826\n",
       "59    0.04,2.0  74.347826\n",
       "5     0.01,3.5  74.347826\n",
       "12    0.01,7.0  74.347826\n",
       "24    0.02,3.5  74.347826\n",
       "13    0.01,7.5  74.347826\n",
       "189   0.1,10.0  74.782609\n",
       "76    0.05,1.0  74.782609\n",
       "41    0.03,2.5  74.782609\n",
       "39    0.03,1.5  74.782609\n",
       "21    0.02,2.0  74.782609\n",
       "10    0.01,6.0  74.782609\n",
       "9     0.01,5.5  74.782609\n",
       "6     0.01,4.0  74.782609\n",
       "57    0.04,1.0  75.217391\n",
       "40    0.03,2.0  75.217391\n",
       "38    0.03,1.0  75.217391\n",
       "23    0.02,3.0  75.217391\n",
       "22    0.02,2.5  75.217391\n",
       "4     0.01,3.0  75.217391\n",
       "95    0.06,1.0  75.217391\n",
       "8     0.01,5.0  75.217391\n",
       "3     0.01,2.5  75.652174\n",
       "19    0.02,1.0  75.652174\n",
       "20    0.02,1.5  75.652174\n",
       "58    0.04,1.5  75.652174\n",
       "7     0.01,4.5  75.652174\n",
       "2     0.01,2.0  76.086957\n",
       "1     0.01,1.5  76.956522\n",
       "0     0.01,1.0  76.956522\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized set of Lambda and Learning Rate\n",
    "d={\"X_lab\":X_lab,\"Accuracy\":Accuracy}\n",
    "g=pd.DataFrame(data=d)\n",
    "g.sort_values(by=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 53  Correct :176\n",
      "Testing Accuracy: 76.85589519650655\n"
     ]
    }
   ],
   "source": [
    "# Checking testing accuracy based on best values of hyperparameters\n",
    "for i in range(0,len(y)):\n",
    "    learningRate = 0.01   \n",
    "    La = 1\n",
    "    z = np.dot(X[i], W_Now)\n",
    "    h = sigmoid(z)\n",
    "    gradient = np.dot(X[i], (h - y[i]))\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "\n",
    "# Adding bias to testing feature matrix\n",
    "p_test=np.ones((len(human_test['feature_id_AsubB']),1))\n",
    "r_test=np.array(human_test['feature_id_AsubB'].values.tolist())\n",
    "X_test=np.hstack((p_test,r_test))\n",
    "X_test[1].shape\n",
    "\n",
    "# Extractig target values for testing set\n",
    "y_test=np.array(human_test['target'])\n",
    "# Taking results of Logistc Regression\n",
    "z_test = np.dot(X_test, W_Now)\n",
    "z_test.shape\n",
    "h_test_raw = sigmoid(z_test)\n",
    "h_test=np.around(h_test_raw)\n",
    "h_test.shape\n",
    "\n",
    "# Calculating Accuracy\n",
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    if (y_test[i]==h_test[i]):\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUMAN CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to training feature matrix\n",
    "p=np.ones((len(human_train['feature_id_AconB']),1))\n",
    "p.shape\n",
    "r=np.array(human_train['feature_id_AconB'].values.tolist())\n",
    "X=np.hstack((p,r))\n",
    "X[1].shape\n",
    "# Extractig target values for training set\n",
    "y=np.array(human_train['target'])\n",
    "# Initializing Weights\n",
    "W_Now=np.zeros((X.shape[1]))\n",
    "# Adding bias to validation feature matrix\n",
    "p_val=np.ones((len(human_val['feature_id_AconB']),1))\n",
    "r_val=np.array(human_val['feature_id_AconB'].values.tolist())\n",
    "X_val=np.hstack((p_val,r_val))\n",
    "# Extractig target values for validation set\n",
    "y_val=np.array(human_val['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and initializing lists to store Accuracy values for validation dataset\n",
    "# This block of code also evaluates model performance varying learning rate from 0.01 to 0.1 and varying Lambda from 1 to 10.\n",
    "Accuracy = []\n",
    "X_lab = []\n",
    "X_lab1 = []\n",
    "\n",
    "for k in learningRate_val:\n",
    "    for j in La_val:\n",
    "        for i in range(0,len(y)):\n",
    "            learningRate = k   \n",
    "            La = j\n",
    "            z = np.dot(X[i], W_Now)\n",
    "            h = sigmoid(z)\n",
    "            gradient = np.dot(X[i], (h - y[i]))\n",
    "            La_Delta_E_W  = np.dot(La,W_Now)\n",
    "            Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "            Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "            W_T_Next      = W_Now + Delta_W\n",
    "            W_Now         = W_T_Next\n",
    "\n",
    "        z_val = np.dot(X_val, W_Now)\n",
    "        h_val_raw = sigmoid(z_val)\n",
    "        h_val=np.around(h_val_raw)\n",
    "\n",
    "        wrong_val  = 0\n",
    "        right_val   = 0\n",
    "\n",
    "        for i in range(0,len(y_val)):\n",
    "            if (y_val[i]==h_val[i]):\n",
    "                right_val = right_val + 1\n",
    "            else:\n",
    "                wrong_val = wrong_val + 1\n",
    "        Accuracy.append((right_val/(right_val+wrong_val)*100))\n",
    "        X_lab.append((str(np.round(k,2))+','+str(j)))\n",
    "    X_lab1.append((str(np.round(k,2))+','+str(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block of code generates plot of Accuracy with varying values of Lambda and Learning Rate\n",
    "plt.title('Validation Accuracy vs. Lambda and Learning Rate')\n",
    "plt.plot(X_lab,Accuracy,'go-', label='Validation Accuracy')\n",
    "ax = plt.gca() # grab the current axis\n",
    "ax.set_xticks(X_lab1) # choose which x locations to have ticks\n",
    "ax.set_xticklabels(X_lab1,rotation='vertical') # set the labels to display at those ticks\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.xlabel(\"Values of Lambda and Learning Rate\")\n",
    "l = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_lab</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.05,10.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.07,4.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.07,5.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.07,5.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.07,6.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.07,6.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.07,7.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.07,7.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.07,8.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.07,8.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.07,9.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.07,9.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.07,10.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.08,1.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.08,1.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.08,2.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.08,2.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.08,3.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.08,3.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.08,4.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.07,4.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.07,3.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.07,3.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.07,2.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.06,2.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.06,2.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.06,3.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.06,3.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.06,4.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.06,4.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.05,8.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.05,8.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.05,9.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.05,4.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.03,5.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.04,8.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.04,7.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.03,6.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.03,7.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.03,7.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.03,8.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.03,8.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.03,9.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.03,9.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.03,10.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.04,1.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.04,1.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.04,2.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.04,2.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.04,3.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.04,3.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.04,4.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.04,4.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.04,5.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.04,5.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.04,6.0</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.04,6.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.04,7.5</td>\n",
       "      <td>61.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.02,1.0</td>\n",
       "      <td>61.739130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01,1.0</td>\n",
       "      <td>62.173913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_lab   Accuracy\n",
       "94   0.05,10.0  61.304348\n",
       "121   0.07,4.5  61.304348\n",
       "122   0.07,5.0  61.304348\n",
       "123   0.07,5.5  61.304348\n",
       "124   0.07,6.0  61.304348\n",
       "125   0.07,6.5  61.304348\n",
       "126   0.07,7.0  61.304348\n",
       "127   0.07,7.5  61.304348\n",
       "128   0.07,8.0  61.304348\n",
       "129   0.07,8.5  61.304348\n",
       "130   0.07,9.0  61.304348\n",
       "131   0.07,9.5  61.304348\n",
       "132  0.07,10.0  61.304348\n",
       "133   0.08,1.0  61.304348\n",
       "134   0.08,1.5  61.304348\n",
       "135   0.08,2.0  61.304348\n",
       "136   0.08,2.5  61.304348\n",
       "137   0.08,3.0  61.304348\n",
       "138   0.08,3.5  61.304348\n",
       "139   0.08,4.0  61.304348\n",
       "120   0.07,4.0  61.304348\n",
       "119   0.07,3.5  61.304348\n",
       "118   0.07,3.0  61.304348\n",
       "117   0.07,2.5  61.304348\n",
       "97    0.06,2.0  61.304348\n",
       "98    0.06,2.5  61.304348\n",
       "99    0.06,3.0  61.304348\n",
       "100   0.06,3.5  61.304348\n",
       "101   0.06,4.0  61.304348\n",
       "102   0.06,4.5  61.304348\n",
       "..         ...        ...\n",
       "90    0.05,8.0  61.304348\n",
       "91    0.05,8.5  61.304348\n",
       "92    0.05,9.0  61.304348\n",
       "83    0.05,4.5  61.304348\n",
       "47    0.03,5.5  61.304348\n",
       "71    0.04,8.0  61.304348\n",
       "69    0.04,7.0  61.304348\n",
       "49    0.03,6.5  61.304348\n",
       "50    0.03,7.0  61.304348\n",
       "51    0.03,7.5  61.304348\n",
       "52    0.03,8.0  61.304348\n",
       "53    0.03,8.5  61.304348\n",
       "54    0.03,9.0  61.304348\n",
       "55    0.03,9.5  61.304348\n",
       "56   0.03,10.0  61.304348\n",
       "57    0.04,1.0  61.304348\n",
       "58    0.04,1.5  61.304348\n",
       "59    0.04,2.0  61.304348\n",
       "60    0.04,2.5  61.304348\n",
       "61    0.04,3.0  61.304348\n",
       "62    0.04,3.5  61.304348\n",
       "63    0.04,4.0  61.304348\n",
       "64    0.04,4.5  61.304348\n",
       "65    0.04,5.0  61.304348\n",
       "66    0.04,5.5  61.304348\n",
       "67    0.04,6.0  61.304348\n",
       "68    0.04,6.5  61.304348\n",
       "70    0.04,7.5  61.304348\n",
       "19    0.02,1.0  61.739130\n",
       "0     0.01,1.0  62.173913\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized set of Lambda and Learning Rate\n",
    "d={\"X_lab\":X_lab,\"Accuracy\":Accuracy}\n",
    "g=pd.DataFrame(data=d)\n",
    "g.sort_values(by=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 89  Correct :140\n",
      "Testing Accuracy: 61.135371179039296\n"
     ]
    }
   ],
   "source": [
    "# This block of code checks the testing accuracy based on best values of hyperparameters\n",
    "for i in range(0,len(y)):\n",
    "    learningRate = 0.06   \n",
    "    La = 1\n",
    "    z = np.dot(X[i], W_Now)\n",
    "    h = sigmoid(z)\n",
    "    gradient = np.dot(X[i], (h - y[i]))\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "\n",
    "# Adding bias to testing feature matrix\n",
    "p_test=np.ones((len(human_test['feature_id_AconB']),1))\n",
    "r_test=np.array(human_test['feature_id_AconB'].values.tolist())\n",
    "X_test=np.hstack((p_test,r_test))\n",
    "X_test[1].shape\n",
    "\n",
    "# Extractig target values for testing set\n",
    "y_test=np.array(human_test['target'])\n",
    "\n",
    "# Taking results of Logistc Regression\n",
    "z_test = np.dot(X_test, W_Now)\n",
    "z_test.shape\n",
    "h_test_raw = sigmoid(z_test)\n",
    "h_test=np.around(h_test_raw)\n",
    "h_test.shape\n",
    "\n",
    "# Calculating Accuracy\n",
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    if (y_test[i]==h_test[i]):\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSC SUBTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to training feature matrix\n",
    "p=np.ones((len(GSC_train['feature_id_AsubB']),1))\n",
    "r=np.array(GSC_train['feature_id_AsubB'].values.tolist())\n",
    "X=np.hstack((p,r))\n",
    "\n",
    "# Extractig target values for training set\n",
    "y=np.array(GSC_train['target'])\n",
    "\n",
    "# Initializing Weights\n",
    "W_Now=np.zeros((X.shape[1]))\n",
    "\n",
    "# Adding bias to validation feature matrix\n",
    "p_val=np.ones((len(GSC_val['feature_id_AsubB']),1))\n",
    "r_val=np.array(GSC_val['feature_id_AsubB'].values.tolist())\n",
    "X_val=np.hstack((p_val,r_val))\n",
    "# Extractig target values for validation set\n",
    "y_val=np.array(GSC_val['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and initializing lists to store Accuracy values for validation dataset\n",
    "# This block of code also evaluates model performance varying learning rate from 0.01 to 0.1 and varying Lambda from 1 to 10.\n",
    "Accuracy = []\n",
    "X_lab = []\n",
    "X_lab1 = []\n",
    "\n",
    "for k in learningRate_val:\n",
    "    for j in La_val:\n",
    "        for i in range(0,len(y)):\n",
    "            learningRate = k   \n",
    "            La = j\n",
    "            z = np.dot(X[i], W_Now)\n",
    "            h = sigmoid(z)\n",
    "            gradient = np.dot(X[i], (h - y[i]))\n",
    "            La_Delta_E_W  = np.dot(La,W_Now)\n",
    "            Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "            Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "            W_T_Next      = W_Now + Delta_W\n",
    "            W_Now         = W_T_Next\n",
    "\n",
    "        z_val = np.dot(X_val, W_Now)\n",
    "        h_val_raw = sigmoid(z_val)\n",
    "        h_val=np.around(h_val_raw)\n",
    "\n",
    "        wrong_val  = 0\n",
    "        right_val   = 0\n",
    "\n",
    "        for i in range(0,len(y_val)):\n",
    "            if (y_val[i]==h_val[i]):\n",
    "                right_val = right_val + 1\n",
    "            else:\n",
    "                wrong_val = wrong_val + 1\n",
    "        Accuracy.append((right_val/(right_val+wrong_val)*100))\n",
    "        X_lab.append((str(np.round(k,2))+','+str(j)))\n",
    "    X_lab1.append((str(np.round(k,2))+','+str(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAE8CAYAAADNOraMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXl4VNXd+D/fWbLvCQHCFlREBdmM4FaUqrhVROuGGF/t29rXbnaxVKu/rti3r7XWon0t9W1tVdRarUsVrHtFqyAo4IIKStghJBCSkG2W8/vj3jtzM8xMJsncGxjO53nmSebemXvOvXPv+Z7vekQphUaj0WgOXTwD3QGNRqPRDCxaEGg0Gs0hjhYEGo1Gc4ijBYFGo9Ec4mhBoNFoNIc4WhBoNBrNIY4WBAkQkWoRUSLiM98vEZH/SOWzfWjrhyLyf/3pr+bgRUR+IiIP9uLzfxaR+U72KcV+9KrfA4GIjBSRVhHxDnRfDmQyVhCIyD9F5Gdxtl8gIjt6O2grpc5RSv0lDf06TUS2xBz7F0qpL/f32D20qURknlNtZCIicrWIvD7Q/TgYiXefDwRKqU1KqQKlVCjdxzYFcpcpaHaLyAsiclQvvl8nImeku199IWMFAfBnoFZEJGZ7LbBIKRV0v0sDxn8Au82/rtJXLUmj6QkxGOgx7DalVAEwDNgK/HGA+9MnBvoiOsmTQBnwOWuDiJQCXwDuN9+fJyLvikiziGwWkZ8kOpiIvCoiXzb/94rI7SLSICKfAefFfPYaEVkrIi0i8pmIfNXcng8sAarMWUSriFTFqtgiMktEPhCRJrPdo2376kTkBhFZIyJ7ReSvIpKTpN95wMXA14ExIlITs/8UEfm32dZmEbna3J4rIr8WkY1mO6+b2/ab6dlnNua5PCYiD4pIM3C1iEwVkTfNNraLyN0ikmX7/jhzNrVbRHaaprIhItImIuW2zx0nIrtExB/TfpWItItImW3bZPP38YvIESLyL/M8GkTkr4muV6ok+o3NfaeJyBYRmSci9eY5zxaRc0XkE/M8fxhzyBzzt2wRkXdEZGLMubxj7vsrkGPbVyoiz5jXZY/5//Ak/b5RRD41j/WhiFxo23e1+Tvfbh5rg4icY9s/2ryOLSLyAlDRx2uXbbaxyfy9fy8iuamcj/k83CoibwBtwGHmtp+LyBtm354XkQrz87Em3oSfNfdfZd7zjSLy/yTFWbtSqh14FJhkO9bhIvKyeawGEVkkIiXmvgeAkcA/xBgH5pnbT5Do87haRE7ryzXuNUqpjH0B9wL/Z3v/VWCV7f1pwLEYAnECsBOYbe6rBhTgM9+/CnzZ/P+/gI+AERjC5pWYz54HHA4IcCrGDTvF1uaWmH7+BHjQ/P9IYB9wJuAH5gHrgSxzfx2wHKgy214L/FeSa1ALbAe8wD+ABbZ9I4EWYI7ZVjkwydz3O/Och5nfPQnITtD/OuAM27kEgNnmdc0FjgNOAHzmdV0LfNv8fKHZv+9hDHCFwDRz32LgOls7vwHuSnCeLwNfsb3/FfB78/+HgZvN/uQAp6R4/1wNvJ5gX0+/cRD4kXldvwLsAh4yz28c0AEcFnPNLjY/fwOwwfw/C9gIfMd8f7H52fnmd8uBLwJ55rH/BjyZ5JwuMe8dD3AZxr021Ha+AbO/XuA6YBsg5v43gTvM+2C6ee88mKCd/e4T2747gacx7t9CjPvyv1M5H4x7cpN5DX3mNXkV+BTj2ck13/8yyXOc6LPHAK3AKeZ1v928HmckOI8/236HfOABYLVt/xEYz3E2MAh4Dbgz3nNjvh8GNALnmr/Pmeb7QY6PlU43MJAv8wfdC+Sa798AvpPk83cCv0lyA1mC4GVsgy8w0/7ZOMd9Erg+0QNCd0Hw/4BHbfs8GCrnabab50rb/tswB7wEbb9o3XwYA/4uwG++vwl4Is53PEA7MDHOvnj9j9zQ5rm81sPv8m2rXbNP7yb43GXAG+b/XmAHMDXBZ78MvGz+L8BmYLr5/n7gD8DwXt4/V5NAEKTwG7cDXvN9oXl/TLN9fiXRScdPgLdirv92DG12OrbB2Nz/b8wBKE4/JgF7enGOq4ALbOe73rYvz+z3EIxJQxDIt+1/iF4KAvO32Qccbtt2IrAhlfPBeA5/FvOZV4FbbO+/Bjxn/l/N/s9xos/+CHg45vy7SC4IOoAmIIwhvCckudaz7fc6+wuCHwAPxHznn8B/9Oa+7csrk01DKKVexxj4LhCRw4DjMW5eAERkmoi8YqqhezFm+qmou1UYA43FRvtOETlHRN4yTQBNGBI+VTW6yn48pVTYbGuY7TM7bP+3AQXxDiQiI4AZwCJz01MYM2LLlDUCY3YUS4X5uXj7UsF+bRCRI00Vf4cY5qJfEL0eifpg9fcY87c7E9irlFqe4LOPASeKSBXG4KmApea+eRgD0HIxTG5f6uN52c+pp9+4UUUdlO3m3522/e10/90i18z8zbdg3AtVwFZljgomkftDRPJEZKFpzmjGmHWWSIIoGdP0sco0PTQB42P6Hbm3lFJt5r8FZj/2KKX2xetHLxiEMcCutPXhOXN7quezeb+jpvhM9PDZbs+1ef6NPZzP7UqpEgyB0w6MtXaISKWIPCIiW81zeZDk48Ao4BLrupjX5hRgaA996DcZLQhM7geuwjCRPK+Usj+MD2GoqCOUUsXA7zEGjJ7YjjGAWYy0/hGRbOBxDLVysHmTLLYdt6dyr9swbgjreGK2tTWFfsVSi/Eb/0NEdgCfYQzwV5n7N2OYN2JpwJjpxNu3D+NBtvrnxXyIbcSe4z0YprQxSqki4IdEr0eiPqCU6sCwu841z+WBeJ8zP9sEPA9cClyBMbNT5r4dSqmvKKWqMMyD/ysiRyQ6Vk+k8Bv3hcj9JIYDdDjGvbAdGGbeBxYjbf9/D2PwmWZe2+nWYeL0exSGufQbQLnZ7/dT7Pd2oFQMP1e8fqRKA8aAOU4pVWK+ipXhcE31fJwqmbwd47obDRp+i/LEH7d1SKlNwPXAby1/B/DfGH2dYJ7LlSQ/j80YGkGJ7ZWvlPpl304ndQ4VQXAGht0zNvyzENitlOoQkakYA0gqPAp8S0SGi+GAvtG2LwvDJrgLCJrOtpm2/TuBchEpTnLs80TkdDGcot8DOjHMAb3lKuCnGOq19fqiefxyDE3hDBG5VER8IlIuIpPMGemfgDvEcMR6ReREcwD8BMOxeZ7Zv1vM801GIdAMtIoRXnedbd8zwBAR+bYYTsRCEZlm238/hsliFsaMKhkPmef8RbprfpfYHI57MB7AVMMJRURy7C96/o37wnEicpHp1Pw2xm/+FoZdPohxv/lE5CJgqu17hRgDa5MYzvIfJ2kjH+Pcd5kndg2GRtAjSqmNwArgpyKSJSKnAOf39L04105hCKPfiEil+ZlhInJWH84n3TwGnC8iJ4kRzPBTeiHclVIvYAjva81NhRg+hyYRGQZ8P+YrO4HDbO8fNNs/y3zmcsQIPEjo/E8XGS8IlFJ1GINoPsbs387XgJ+JSAuGffDRFA97L4btbjXwDvB3W3stwLfMY+3BEC5P2/Z/hOG8/MxU/6pi+vsxxszhLozZ0/nA+UqprhT7BhjRBxjq6u/MGbH1ehrD+TzHnMWciyFsdmPYi61olRuA94C3zX3/A3iUUnsxrtv/YWgp+zDMGMm4wbwOLRjXLhK1Y16vM83z3AGswzBnWfvfwLC/vmP+lsl4GhgD7FRKrbZtPx5YJiKt5meuV0ptMK/TByIyN8kxT8IYmGJfCX/jPvIUhk9kD4b2c5FSKmD+7hdhCMM95mf+bvvenRhOzwYMwfFcogaUUh8Cv8YQLjsxAiXe6EUfrwCmYdwPP8aMvkvCMPa/bodj2MLXA2+ZJpMXiZpUUj6fdKOU+gD4JvAIhnbQAtRjCOVU+RUwz5w0/RSYguGnfJbuvxsYGsMt5jhwg1JqM3ABhsa8C0ND+D4ujNNWNIBGc8AiIi8DDymldPa1xjVEpADDETzGmjhkKhmvEWgObkTkeIxZVb9j/zWanhCR802HdT6GD+g9jOiejEYLAs0Bi4j8BcNs8G3ThKTROM0FGHb+bRhmxsvVIWA20aYhjUajOcTRGoFGo9Ec4mhBoNFoNIc4B0VlyIqKClVdXT3Q3dBoNJqDipUrVzYopWITPvfjoBAE1dXVrFixYqC7odFoNAcVIpJSGRBtGtJoNJpDHC0INBqN5hBHCwKNRqM5xDkofAQajQYCgQBbtmyho6NjoLuiOcDIyclh+PDh+P3+nj8cBy0INJqDhC1btlBYWEh1dTWy31LcmkMVpRSNjY1s2bKF0aNH9+kYGW0aqm/u4NKFb1LfomdQmoOfjo4OysvLtRDQdENEKC8v75emmNGCYMFL63i7bjcLXlw30F3RaNKCFgKaePT3vshIQTD2liVU3/gsDy7bhFLw4LJNVN/4LGNvWTLQXdNoDlpOO+00/vnPf3bbduedd/K1r30t6fcKCozFx7Zt28bFF1+c8Ng95QrdeeedtLW1Rd6fe+65NDU1pdL1lJg4cSJz5sxJ2/EOJjJSECydN4NZk6rwegwpmeP3cMGkKpb+YEYP39RoMot0mkfnzJnDI4880m3bI488kvLgWVVVxWOPPdbn9mMFweLFiykpKenz8eysXbuWcDjMa6+9xr59+3r+Qh8JBoOOHbs/ZKQgqCzKoTDbRyhsVFbtDIYpzPZRWZgzwD3TaNwlnebRiy++mGeeeYbOTmPBrrq6OrZt28Ypp5xCa2srp59+OlOmTOHYY4/lqaee2u/7dXV1jB9vrIzZ3t7O5ZdfzoQJE7jssstob2+PfO66666jpqaGcePG8eMfGytVLliwgG3btjFjxgxmzDAmdNXV1TQ0NABwxx13MH78eMaPH8+dd94Zae/oo4/mK1/5CuPGjWPmzJnd2rHz0EMPUVtby8yZM3n66ehic+vXr+eMM85g4sSJTJkyhU8//RSA2267jWOPPZaJEydy443GSrV2raahoQGrLM6f//xnLrnkEs4//3xmzpyZ9Frdf//9TJgwgYkTJ1JbW0tLSwujR48mEAgA0NzcTHV1deR9usjYqKGG1k4mjShm1ea9zDl+BLtae7PanEZzYPPTf3zAh9uaE+5fXrcbe4X5B5dt4sFlmxCBqdVlcb9zTFURPz5/XMJjlpeXM3XqVJ577jkuuOACHnnkES677DJEhJycHJ544gmKiopoaGjghBNOYNasWQlt1/fccw95eXmsWbOGNWvWMGXKlMi+W2+9lbKyMkKhEKeffjpr1qzhW9/6FnfccQevvPIKFRUV3Y61cuVK7rvvPpYtW4ZSimnTpnHqqadSWlrKunXrePjhh7n33nu59NJLefzxx7nyyiv3689f//pXXnjhBT7++GPuvvvuiJYzd+5cbrzxRi688EI6OjoIh8MsWbKEJ598kmXLlpGXl8fu3bsTXjOLN998kzVr1lBWVkYwGIx7rT788ENuvfVW3njjDSoqKti9ezeFhYWcdtppPPvss8yePZtHHnmEL37xi30OE01ERmoEAAtra5g1cRgA884+ioW1NQPcI43GPSYNL6E8PwvTOopHoDw/i0nD+2dKsZuH7GYhpRQ//OEPmTBhAmeccQZbt25l586dCY/z2muvRQbkCRMmMGHChMi+Rx99lClTpjB58mQ++OADPvzww6R9ev3117nwwgvJz8+noKCAiy66iKVLlwIwevRoJk2aBMBxxx1HXV3dft9/++23GTRoEKNGjeL000/nnXfeYc+ePbS0tLB161YuvPBCwIjVz8vL48UXX+Saa64hLy8PgLKy+ILVzplnnhn5XKJr9fLLL3PxxRdHBJ31+S9/+cvcd999ANx3331cc801PbbXWzJWIwDIzfIC0NYVoiRvgDuj0aSRZDN3i5ufeI+Hlm8i2+ehKxTmnPFDmH/hsf1qd/bs2Xz3u9/lnXfeob29PTKTX7RoEbt27WLlypX4/X6qq6t7DGeMpy1s2LCB22+/nbfffpvS0lKuvvrqHo+TbHGt7OzsyP9erzeuaejhhx/mo48+iphympubefzxx7n00ksTthev7z6fj3A4DLBfn/Pz8yP/J7pWiY578sknU1dXx7/+9S9CoVDEvJZOMlYjAMgzBUF7IDTAPdFo3KehtZO500bxxNdOZu60UWkxjxYUFHDaaafxpS99qZuTeO/evVRWVuL3+3nllVfYuDF50cvp06ezaNEiAN5//33WrFkDGINwfn4+xcXF7Ny5kyVLopF+hYWFtLTsv2Lp9OnTefLJJ2lra2Pfvn088cQTfO5zn0vpfMLhMH/7299Ys2YNdXV11NXV8dRTT/Hwww9TVFTE8OHDefLJJwHo7Oykra2NmTNn8qc//SniuLZMQ9XV1axcuRIgqVM80bU6/fTTefTRR2lsbOx2XICrrrqKOXPmOKINQIYLgly/KQi6tCDQHHosrK1h/uzxHFNVxPzZ49NmHp0zZw6rV6/m8ssvj2ybO3cuK1asoKamhkWLFnHUUUclPcZ1111Ha2srEyZM4LbbbmPq1KmAEcI5efJkxo0bx5e+9CVOPvnkyHeuvfZazjnnnIiz2GLKlClcffXVTJ06lWnTpvHlL3+ZyZMnp3Qur732GsOGDWPYsGGRbdOnT+fDDz9k+/btPPDAAyxYsIAJEyZw0kknsWPHDs4++2xmzZpFTU0NkyZN4vbbbwfghhtu4J577uGkk06KOLHjkehajRs3jptvvplTTz2ViRMn8t3vfrfbd/bs2eNYeOtBsWZxTU2N6st6BK+va+DKPy7j0a+eyNTRPdvxNJoDmbVr13L00UcPdDc0A8Bjjz3GU089xQMPPJDwM/HuDxFZqZTqcQaQ4T4CQ+Fp6zowY3c1Go2mJ775zW+yZMkSFi9e7FgbmS0I/MbpdWgfgUajOUi56667HG8jo30EebaoIY1Go9HERwsCjeYg4mDw6Wncp7/3haOCQETqROQ9EVklIivMbT8Rka3mtlUicq5T7eeYgkCbhjSZQE5ODo2NjVoYaLphrUeQk9P3Ejpu+AhmKKViY6l+o5S63emG8/xaI9BkDsOHD2fLli3s2rVroLuiOcCwVijrKxntLPZ5PWR5PVoQaDICv9/f5xWoNJpkOO0jUMDzIrJSRK61bf+GiKwRkT+JSKmTHcjxe7RpSKPRaJLgtCA4WSk1BTgH+LqITAfuAQ4HJgHbgV/H+6KIXCsiK0RkRX9U4bwsn84j0Gg0miQ4KgiUUtvMv/XAE8BUpdROpVRIKRUG7gWmJvjuH5RSNUqpmkGDBvW5D3lZXm0a0mg0miQ4JghEJF9ECq3/gZnA+yIy1PaxC4H3neoDGBVItWlIo9FoEuOks3gw8IRZVtUHPKSUek5EHhCRSRj+gzrgqw72gVy/1gg0Go0mGY4JAqXUZ8DEONtrnWozHrlZXlo6tI9Ao9FoEpHRmcVg+Ah0GWqNRqNJTMYLgly/Vy9Mo9FoNEnIfEGQ5dM+Ao1Go0lCxgsCwzSkfQQajUaTiENDEARCulCXRqPRJCDjBUGO30tYQWcwPNBd0Wg0mgOSjBcE1poEOnJIo9Fo4nPoCAIdOaTRaDRxyXhBkKPXJNBoNJqkZLwgyMsykqe1aUij0WjicwgIAksj0CGkGo1GE4+MFwS52keg0Wg0Scl8QeDXUUMajUaTjIwXBFHTkBYEGo1GE4+MFwSWaeiul9dR39IxwL3RaDSaA4/MFwSmaWhjYxsLXlw3wL3RaDSaAw8nVygbcMbesiRSWkIBDy7bxIPLNpHt8/Dx/HMGtnMajUZzgJDRGsHSeTOYNakq8j7H7+GCSVUs/cGMAeyVRqPRHFhktCCoLMqhMNtQerxiFJ4rzPZRWZgzwD3TaDSaA4eMNg0BNLR2UpDtpaa6jOGleezSDmONRqPpRsYLgoW1NXzhrqV4RZg/e/xAd0ej0WgOODLaNGRRkO2jpVOXmNBoNJp4HCKCwE9rhxYEGo1GE49DQhAU5vho1RqBRqPRxOWQEAQF2VoQaDQaTSIODUGQ49OmIY1Go0nAoSEIsn10hcJ0BnXhOY1Go4nlkBAEhTlGlKybWkF9cweXLnxTF7rTaDQHPIeEICgws4vd9BMseGkdb9ft1oXuNBrNAU+PCWUi4lVKHdQ2FUsQtLigEdgL3YEudKfRaA58UtEI1ovIr0TkGMd74xAFOe5pBEvnzWDWRF3oTqPRHDykIggmAJ8A/ycib4nItSJS5HC/0kphth9wx0dQWZQT0UA8utCdRqM5COhRECilWpRS9yqlTgLmAT8GtovIX0TkCMd7mAbc1AgAdrUaDuLjq0uZO20Uu1o7XWlXo9Fo+kKPgkBEvCIyS0SeAH4L/Bo4DPgHsLiH79aJyHsiskpEVsTsu0FElIhU9KP/KRHxEbgkCH57+WQAwgrmzx7PwtoaV9rVaDSavpBK9dF1wCvAr5RS/7Ztf0xEpqfw/RlKqQb7BhEZAZwJbEq5p/3A7fDRYFgBsLc94Ep7Go1G0x9SEQQTlFKt8XYopb7Vx3Z/g2FmeqqP3+8V2T4PPo/Q2unOwBwMaUGg0WgOHlJxFv9OREqsNyJSKiJ/SvH4CnheRFaKyLXm92cBW5VSq3vf3b4hIq6WmQiGjfBRLQg0Gs3BQKoaQZP1Rim1R0Qmp3j8k5VS20SkEnhBRD4CbgZm9vRFU3BcCzBy5MgUm0uMm2sShEzTUEcgTEcgRI7f60q7Go1G0xdS0Qg8IlJqvRGRMlJc2Uwptc38Ww88AZwKjAZWi0gdMBx4R0SGxPnuH5RSNUqpmkGDBqXSXFIKsl3UCEzTEECz1go0Gs0BTioD+q+Bf4vIY+b7S4Bbe/qSiOQDHqVUi/n/TOBnSqlK22fqgJpYZ7ITuLkmgaURgGEeqizSOQQajebApUdBoJS6X0RWAjMAAS5SSn2YwrEHA0+IiNXOQ0qp5/rT2f5QkO2jcV+XK20FYwSBRqPRHMikauL5QER2ATkAIjJSKZU09FMp9RkwsYfPVKfYz35TkONnY2ObK23FagQajUZzIJNKQtksEVkHbAD+BdQBSxzuV9px01lsRQ2BFgQajebAJxVn8c+BE4BPlFKjgdOBNxztlQMUuhg+qjUCjUZzMJGKIAgopRoxooc8SqlXgEkO98sR2gMhtje1O96O3UfQ1KYFgUajObBJRRA0iUgB8BqwSER+Cxx0CwCv3LgHgDte+MTxtuzho1oj0Gg0BzqpOIsvANqB7wBzgWLgZ052Kp3ELhTzt5Vb+NvKLY4uFGP3Eeg8Ao1Gc6CTVCMQES/wlFIqrJQKKqX+opRaYJqKDgqWzpvBrElVZHmNU83yOb9QzED6CPRayRqNprckFQTmEpVtIlLsUn/STmVRDoXZPgIhY5YecGGhGMtHkOv3ui4I9FrJGo2mt6RiGuoA3hORF4B91sZ+VB51nYbWTi6tGc5fV2xh8sgSxxeKCZk+gvKCLJpcEgR6rWSNRtNXUhEEz5qvgxZrYZjF7+/g2GHF/PSC8Y62Z2kE5flZbNvrjolm6bwZzF+8liXvbScQUmT7PJw9fgg3n3e0K+1rNJqDl1RKTPzFjY64wbCSXLY2OT8wWz6C/GwvDS2d1Ld0OL5msWUCsyKWuvRayRqNJkVSySzeICKfxb7c6Fy6GVqcw/a9buQRGCaa7Xs7UcAdzzsfsgqGCWzKKKNQ7BlHD9ZrJWs0mpRIxTRkX3A3B6P6aJkz3XGWqpJcVm1u6vmD/eR7jxpr7mxoMFwqj7y9mUfe3uy4vX5hbQ0L//UpKzfu4bLjR3DGMYMda0uj0WQOPWoESqlG22urUupO4PMu9C3tVJXksqctQFuXs/lwN55zFAA+jwDGUplOh6xaWP6JfQ6fo0ajyRx61AhEZIrtrQdDQyh0rEcOMqwkF4BtTR0cUVngWDt5WcZltXwFbtrrLR+BW2svaDSag59UF6axCGJUIb3Ume44y9BiYyDevrfdUUEQMn0EM46q5OWP6pl+ZIVr9nqr7X1aEGg0mhRJJWrIeXuGS1RFNAJnHcaWeeaGmWN5+aN6zh4/lDlT+7/ucioEwpZGEHKlPY1Gc/CTStTQL0SkxPa+VETmO9stZxhiagS/fWmdoyUYLJNQRWEW4G4F0mBIawQajaZ3pFJ99BylVCTURim1BzjXuS45h9/rIdfvZVtTh6MlGCyNoCDbR5bPQ1ObO0tk2tvWgkCj0aRKKj4Cr4hkK6U6AUQkF8h2tlvpx80SDJZG4PN4KM3zs8dNQaCdxRqNppekohE8CLwkIv8pIl8CXgAOumxjqwqp14joJMfvXEinNRj7PEJpXhZ73DQNaY1Ao9H0klScxbeJyBrgDECAnyul/ul4z9KMVYLBWjOm08GQzlA4jAh4PEJJnt9d01DER6CdxRqNJjVSySMYDbyqlHrOfJ8rItVKqTqnO5duGlo7mTa6jGUbdjN7UpVjIZ3BsIokk5XmZbG+vtWRdhK1Ddo0pNFoUicV09DfgLDtfcjcdtCxsLaGb35+DACX1oyMVCVNN6GwwmsKgpI8/8CYhnRmsUajSZFUBIFPKRWxbZj/ZznXJWcZUWbkEmze0+ZYG4ZGYFzakrwsmtq6UEr18K00ta3DRzUaTS9JRRDsEpFZ1hsRuQBocK5LzlJVkotHYMtuBwVBKBzRCErz/ATDyjVTjTYNaTSa3pJK+Oh/AYtE5G4MZ/Fm4CpHe+Ugfq+HocW5bN7jXHax3UdQkhdNKivM8TvWZqRtUyPoCIQJhsL4vKnIeo1GcyiTStTQp8AJIlIAiFKqRUQO6vrGI8py2eygRtDNR5BrDP5NbQFGuFC829IIAPZ1hSjO1YJAo9EkpzejhBe4REReBN5xqD+uMKI0z3Efgd+ciZfmGxqBW0llVg4DuO8nqG/u4NKFbzpavkOj0aSfpILADBW9TESeAt4H7gDmAyPc6JxTjCjLY2dzJxff829HBi27RlCaZ2gErgmCcDTAy21BsOCldbxdt9vR8h0ajSb9JDQNicgiYDrwPHA38DKwXin1qjtdcw4rcmjlxj0seHEd8y88Nq3Hj+cj2NvuTgip3TTklsPYzfIdGo0m/STzEYwH9gBrgY+UUiERcScG0kHsg5a8NPRQAAAgAElEQVTCmUErFI5GDRWbPoI9+1wSBCFFfpaXfV0h17KLl86bwfzFa/nHqm0ojPIdZ40bws3nHe1K+xqNpn8kNA0ppSZiLEBTBLwoIkuBQhEZ4lbnnGDpvBmcMz56CiJw1rjBaa05FAxFTUN+r4f8LC8PLd/oiu08EApHtBC3NAKrfIfCCCtzsnyHRqNJP0l9BEqpj5RSP1JKjQW+A9wPLBeRf7vSOweoLMqhLD8Ls/YcSsFnu/alddAKhRU+q7odgMDO5k5XbOehsIpoIW76CBpaOynM8TGoMJu500a5tiKbRqPpP6nkEQCglFoBrBCRGzB8Bz0iInVAC0ZZiqBSqkZEfg5cgFG2oh64Wim1rbcd7w8PL9+E3ca1rr6V6hufTZt5KBhWeD2eAbGdB8OKigJTELhYZmJhbQ3Tb3uFPW1dzJ893rV2NRpN/+l1kLky+FcvvjJDKTVJKWUV9vmVUmqCUmoS8Azwo972ob+8ddPpzJpUFZm1Z/vSW5I6GA7j80ik9LXHhdLX9rZLzEglt7OLg6EwLR3ByHoMGo3m4MD1bCOlVLPtbT7g+qgRKUltxtx3pdmmbfkIrHbCLpS+tredn+3DI9Dmcilqa73kZpcipDQaTXpwWhAo4HkRWSki11obReRWEdkMzGUANAIwbNqX1AwHYOKI4rTatEO28NGG1k6OqSrE5xHmTh3puO3cSGYT8rN9rmsEAbO8RZMWBBrNQUUq6xFkA18Equ2fV0r9LIXjn6yU2iYilcALIvKRUuo1pdTNwM0ichPwDeDHcdq9FrgWYOTIkamcS6+wSlC/sb6REWX53DVnctqOHQwrck1BsLC2hntf+4xbF6/l+2cfFXHkOkUwFMbnMdZmfmbNNr4243DXonesrGa3ciY0Gk16SEUjeArDuRsE9tlePWI5gZVS9cATwNSYjzyEIWTiffcPSqkapVTNoEGDUmmuTxw9tIi125t7/mAvCNlKTABUFBrhnI0uRNIEQ0bEUkcgRENrl6tZvhGNwMUV2TQaTf9JJWpouFLq7N4eWETyAY9ZpC4fmAn8TETGKKWs0WkW8FFvj51OjhlayMsf7aQjECLH703LMYO2EhMAFQXZADS0dnGYczINgJbOIPe9URd572aWryUIBkIjqG/u4BsPv8vdV0zW+QsaTS9JRSP4t4j0pQbDYOB1EVkNLAeeNZe7/KWIvG+ugzwTuL4Px04bRw8tIqzgi2msOxQyo4YsyvMNQeCGRuDzwJjKAlcjlQDCYRVxig+EINB1jjSavpOKRnAKcLWIbAA6MZJHlVJqQrIvKaU+AybG2R7XFDRQHD20CIAPtzWnre7QfhqBaRpqcEEQhJQRDhtW7mb5BmzF7va6uDSnrnOk0fSfVARBxj5NTtUdskcNAZTlZSFimIacJBRWKAXtgTBjKguob+7g/EnD2OVKaYtoFLCbUUNL583ghr+t5rV1xqJ5btc50iYpTSbQo2lIKbURKAHON18l5raDHqcSvow8guil9Xk9lOZlOa4RWDb6i6YM48xjBrOvK8TPLxgXiZByEmtlNHDXNFRZlBPx7Xg94nqdI22S0mQCqYSPXg98Bfi7uelBEfmDUuouR3vmApFiaVbCVyA9g0gwxkcAUFHgvCCwMnp9HqEo11grua0rRH52ypVE+oxdI3DbR2Ct9XDqkYOoKsl1RQM6lE1SWgvKPFJxFv8nMM0sPvcj4AQMwZARNLR2MmtiFQAF2V62pGHlslBY4fV2FwTl+dk0OmwasuL4fV5PJF+hucOdQTkQGhgfAcA3Pj8GMEpqzJ893hUNyNImvWL8zm455Q8EtBaUeaQyVRSMonEWIXNbRrCwtgalFC9+tJOWzhDDS/P6fcxgjI8AoKIwm/e2NPX72MnbNQZjn0cigmBve4ChxbmOtgvdl8h0WyMImDPzrXvaXWszUqbEVCcHovS22zPzQ1kLynRS0QjuA5aJyE9E5CfAW8AfHe2Vi4y9ZQmjb1ocWcTlwWWbqL7xWcbesqTPxwyFukcNAZTnZzmvEVimIa9NELg0O7eihnL8Hpra3U0o6zK1ke1727tpJk7T0NrJqHJj4nD+hKGul952e2ZuaUF+h4o1agaOVJzFdwDXALsxViy7Ril1p9Mdc4vYmzsrDTd3PI1gUGE2LZ1BOgLOFYKzBkGfRyjKiWoEbmC1XVGQ7b5GYLYdVrC9yXn/gMXC2hqOMcOPLzt+pCsmKTAmL9U3PsuDyzahVHomL6lgaUFBh4o1pkJ9cweXLnzTlUWeDqS2nSahIBCRIvNvGVAHPAg8AGw0t2UEsTd3IA03t7EwTfdLW55v5BJc/gfnbqSos9juI3Cn8Jx1/SoKsukIhB0VeLF02cwV6fDx9AZLCO1sdm9wWDpvBudPGBp576Z/oqG1k8kjSwCoqS7NeC3oQGnbaZL5CB4CvgCspHupaDHfH+Zgv1ylobWTK6aN5K9vb+bIIQX9vrnjRw0Z2cWrN+9NW+JaLIFQHNOQ6xqBIfCa2wNpK9nRc9vR23OLi34CgC6z7Z3N7g2IlUU55GUZj66Iu/6JhbU1/O6V9byzqYkpo0q56Rx38jUG0j9xKPhGEgoCpdQXzL+j3evOwGCp9Cvq9lBVnNsvFd8qtWD3ETiVuBaLXSMoyDF+WrcEgeWfsATef/5lBX+8usaVwcnuF3BbIwgOgEYA0Sz1ysJszjxmiCshsxbW9XbTDLd03gzmL17LM6u3EVbuJg4unTeDGx9fw8sf7wLcT1p0gx59BCLyUirbMoEjKgtYX9/ar2NYUSR2jWDpvBmcNW5w5L1TqnzER+AVvB6hMMfn2iIxVuSOJQje37rXNRXaOu+CbC+Llm1y1YZrte223fhXlxjVWxpau/jJ+ce45p+AqBlwW5P7UVpul06x2rZMvb4BSFp0g2Q+ghzTF1AhIqUiUma+qoEqtzroJocPymfT7jY6g323b1uzcntmcWVRTmSA9DioygfD3YVQca7fPUFgtn33K+uBqObjhhPTihrK9nlp3Od26W33TUNGu8Y5h8KK7XvdFULWfeZ2uw2tnRRm+yjI8TF32ihX/RNW0uKcqSNdb9sNkmkEX8XwDxxl/rVeTwG/c75r7nN4ZQFhBXUNbX2OEIgdjC0aWjupLMjmqCGFjt1IISuPwJy9FOX43TMNmQPTSYeXR7a55cT89fOfANC4z3hY3RJAMDDOYnu7AJsHyBy2o7nD1fWpF9bWUJLvp6UjyC3nHe2qFvT1GUcAxkTOraRFN0koCJRSvzX9AzcopQ5TSo02XxOVUne72EfXOKKyAID19a19jhCwHpLYPIKFtTXMHD+YLXvaHav/Y81O/TaNwG1ncYnppHZS84nlqhNGIUSFr5tRNBHTUHMnSrk3KHZzkO9210FuTXZCYcWuFpc1oaDR9g6XtRHrejtdOHKg6DGzWCl1l4iMB44Bcmzb73eyYwPBYRWGIPj6Q+9EtvXWsWtP6orlyMGFNHcE2dncyZDi9A+OUbNUVBB81tA/n0eqBGzLVI4qz6O9K8TMce44MbN8HjwSPf901YxKhUhMfShMU1uAUjNM2Pl2B04jsGsj2/a2O3IvJ8LKnt++t4PqinzX2rXO2Y1S8gNBKs7iHwN3ma8ZwG0YK4tlHLlZXnL93S9Jb2eXsYOxnSMHFwLw8c6WfvY0PlFnsXEObmoE1gN664XHMmfqSOpbOvnemUe6okJ3BsOICGeNHwLAqWMHuWbD7QqFyTHvmZ0uOoy77IJgt9umoag24mbkEEQnHDua3dWCDnlBAFwMnA7sUEpdg7HYTLajvRoArGzN9kD3EgUdvZxdJvIRQFQQrHNIEESKzpltF+X63DMNBaOa0LHDigF4f9ted9oOhSnJ8/PLi4zcjFOOqHDNhhsMKYaVGLWcrn94lWvRQ/bB+MW1O92NlAobzwTA/zy3dkCitHbsdXdAtpIWM9U0lIogaFdKhYGgmW1cTwYlk1lYpSZyfNFLku0TvnjcsF7NLkOh/aOGLMrysyjLy+L3//rUkYcn1ixVnOunIxBm8559jqfGW7WG/F4P46sMQXDT399zZZAIhML4vR5K8rIozvWzsdG9GXIgFGaYWajwk50trkUsWRpYXpaX1s6Qq5FSwZCivCALrwc27W53vW2AHXvd1giipk97JnumkIogWCEiJcC9GFFD72CsQZxRWHHKnaEw2T6PGaus+N6ZY3s1uwxGBsT4BVqzfEJDqzMhjtHqo1HTEMCdLzifGm/XRorz/GZJb3cGiUBI4TfNYdXledQ17nO8TYvGfV289omRaORmyOzlf3gLgLau9BVLTJVn1myjrrENyzrlapSWzUfgJna/SOO+zDMPpeIs/pr57+9F5DmgSCm1xtluDQwNrZ3MnTaKK6aO5K6X17Hk/R2s3LiHqpLUyzgn8hG4kaYeitEIfvqPDwF4/J2tjrVpYT0oJ/7y5W4zJjfS8btC4YjgHVmez+rNzpb7tpPj8zCsNJdPdxnCx62s0zsuncg3H16F1yOEwopsn4ezx7uT7XrKERW8u7mJfZ1BV7N8raVYwQhddZNugqC1y5XS7m6SLKFsSuwLKAN85v8Zx8LaGubPHs8xVUUsmDOZbJ8w/9kPe2XeSOQjcKOEbzR81PhZf3XJhG77/R6hPD+LJ75+UtrajG37he9Md71UcSAY7qYRbNnT5pr6HgyriLPYzbo/1qpzlvB3sxKo1yP4PR6sFAK3ztk+GLsdPmp3zmdaMhkkNw392nz9DlgG/AHDPLQMWOB81wYWv9dDcW4WO5s7e2XeiJdZDO6U8LUSyrxeYewtS/jOX1d32x8IKxr3dfHQW5vS1qaFFc5YVZJrnKcVyunCIBEIhckyfTujyvMJK7j49/923D+hlCIYVnQEwowqy6OqOMe1rFNL8E4bbRQCPs3FSKlgWBFSYY4ZWojPK8ydOtKlczbusfwsL/UtnVziwm8caTsYdc43uJw74QbJEspmKKVmABuBKUqpGqXUccBkYL1bHRwIrAiievMH740NNFnUUENrJ3OmjsTngWOqitL+8NgTypbOm8HZtvpGdpyw6QZs522Z2CaPLCEvy8Oz72131lEd4yMAeG+L87WOrOt90ZThnD1+CLtau/jZLGeSBWOxJhTXTjfiNs49dqirkVJjBxdx0ZThBEOK7591lKvnPKLM+I1X1O1xzVHdFYqWncnEyKFUlqo8Sin1nvVGKfW+iExysE8DjlXp8Ln3dtAVCpPlFc45dmhKNtBEmcUQrXK6anMTZflZaX947G2XF2RTXpCNiCEYumzhhk7YdAOmnV5EIuf117c38YPH36O9K+xY6W3A/I08rlV5tbBmqH6vMLw0l65gmF2tnQwucq/iqtWWVQvHDYJhwxQ3qNCIIq9v6aA4z+94u9Y5f7TDCL924zeOtq3INcuqZ2IuQSpRQ2tF5P9E5DQROVVE7gXWOt2xgcQy41gRCl0hlbJ5I5lGYDGuqogPtzWnvSRBNHzU+FmtmfmTXz+FMWb5DHDGXBMMhSPRSmBoVT943Jg/OB1NEwiF8fs8hh9mYrQeotOlJqKRUp7IWtdulcG2BsXiXD9ZXk+kzpI7bRsLL1n3T71LphJL6xxdEV1X3K1yIl1BY6JTXpBF4yEqCK4BPgCuB74NfGhuy2isQXTa6FJye2HeSJZZbDGuqojGfV2cf9frXPi/b6TNbGIJAstRa3d+HzYon5pRpQBcNLl3uRGpYAwO3Utvz5pUhbXFyQe2K2hobZVFORSa6zC4UevIciD6fR5GlBlRJG4tjGOZpbJ8Hsrys9jjoiCwFl6qLIpqBK60a15vaxlWN0tCW36o4lw/L39Un3HLVaayZnGHUuo3SqkLzddvlFKZdRXiYA2iX5hQRXuXUUcmFXtkslpDFuMimbfNvLupKW12zmRCaGFtDTedexQAZ40bknazVMA0z1hYWpXC+frxVkIZGAJ8aHE2R1Q6V+XVwsrbyPIKw0osjcAdQRDNGRFK87PYvc+9daKDIWNN7krTNORW4TlLC7KesdoTR7nonDfusaa2AM0dwYxbrjKhj0BEHlVKXSoi79F9qUoAlFIT4nwto0hmc146bwbfePhd7r5icrfBLRK5EyezOPaYFumyc0Zs1gnaPmZoMV6PsGbLXmaOG9LnduIRjNEIwBiUjxxcwPa9HVwwaZhjBejszuKFtTXMe2w1r368i/mzxzvSXqTdYNQ0lJvlpaIgyzXTkBUe6/N6KMv3s9vFJCdrUCzI9pHr91Lv0loMlhb0jRlHcP0jq8j2efnx+Ue50vZjK7dEwmUh85arTKYRXG/+/QJwfpxXxhNr3hCB046s4JiqIv7nuY9YvmE3X1jwejc1MbbeT7xjzowTzVOa5+93fH8wpBABT4K2c7O8jK7I5/4369Ku2gbC3X0EYAzKFx83nJaOIN8/q3cZ2r2hy5ZHADC0OJddrZ2O5xLYTUMAw0rzXNQITNOQ10NZfjZ72lzUCMKG0BcxzENu+QisZ8vv9VCc515BRYAZR1VSkO0bkFLnbpAsfHS7+XdjvJd7XRw4LPOGhVLw6icNvLupKZKtW9/SydRbX4o4QWOze+Mdc1DB/jX79rQF+h3fHwyrhNpABKUcUW0DIRWJ5bcz0gz1c7JCpmG/jV7vqpIclHJ+sZhIORFzcKgoyGL5ht2u2I8te7nPK5Tl+dntpo8gpCJCv7Iw2zV7eZftnI3Kuu6ds1cEn1eipc4zbLnKZJnFLSLSHOfVIiLNbnZyIHl4+ab97WJx6AyGGXvLkpSihhpaOxlRmkvsR/obWRMMhRMKICs3Yr1ZCiHdUTxG1ND+bY9wSRDEagTgfD0ayzRktb29qZ3OYJjfumA/7rJpnqX5WextD3TLvHUSK1QYYFChmxpBtLBhSa6fJhe1oEAojAorThljrMB33rFDMyrDOKGPQClV6GZHDlTeuul05i9ey5I12yPha7F4BM6fWMXN5x3Nv9c3Aol9BBDNJ6hv7mD+4rU8s3pbWmq2BMMqYbRSJDfi/R2RKJtUcyNSwQopjMUSBJscFQSqmyCoKjFmadsdrlBpzVCvW7Sy24phi5ZtYpHD9uOgLW+j3FwMp6ktEIntdxLLNARQWZjD0k8aHG/TahdMQZDnd7XwXFcozJjBhXx9xhiWrmvk8uNHcsqYCtfad5pUwkcBEJFKERlpvZzs1IFEpDSEUvvN4C3CCt781BAAqWgEscdOV80WK9EnWVvWrDHQi9yIVAiYiXexFOX4Kc3zOyoIuhJoBNscXjTFmqHeedkkZk2qipx/lhv1lWznbK2K5lZSWcCWMzKoMJuWziAX3+N8uYeAzTRU5LZGEDQmGxWmWTfTkspSWaFsloisAzYA/wLqAOfrzR5AWDkFJxxWzpGDCzjx8DJGlOYyojSXy44fDhi+ggUvrkuaWZzo2CceZqibsyZU9UvdDIYSawT28yjJ9XNYRX5aVdtgOBxXIwDDT+CUIFBKdas1BEZBtqIcn+MagaUFRIWs8T7gSn0lFZlslOUZgqDRpdIHobCKmIasENKVG50v92AvqliSm+Wqs7jLTFoclKGCIJUSEz8HTgBeVEpNFpEZwJxUDi4idUALEAKCSqkaEfkVRtRRF/ApcI1Syr26wX0gUbTL2FuW8Ne3t0TeP7gs6uxNRSOwjr1qcxOzf/cG500Y2q+wTsNZnLhd6zzW17fQFQynNYrHPjDFUlGYzevrGqhv6Uj74Bg0SxPHaiNVJbmOawQBWyx/Q2snFx83nL+t3MJxo0odtx/bNYKyAnc1gqBpBnS7pIfdQV6S56e1M7iff8gpLI23KNeH3ysZV28olSsYUEo1Ah4R8SilXgF6U2tohlJqklLKGnVeAMabeQifADf1rssHDlZJA3v2rLVMY6oaAcARZvmHdfX9W2g+GArjTZLIZm9vfX1rWktcJHsgdzR1OOZEDdgciHbK8v38+9MGZ4vdBaNtL6yt4baLJ5Dl83DcqFLHi7AFbX4RSyNwK3IoYGYWL503g9PGDopsdzqkMmDzEViLLjW7tRSreX8bPpnsjNMIUhEETSJSALwGLBKR3wLBvjaolHpeKWV9/y1geF+PNdBYJQ2s4bQzGDVRxMbUJ6Mg20dVcQ7r+ysIUgkfBQ4fVEBzRzC9pqFQ1FxgYUUqfbDdCDJb5ETV05jIHYv6li7aupxdwjEQ6t62iDC0OMcVJ2YgHI0QKzEFwT2vOrMEqh1rcRifx0NlUQ5lpn/CjXIPUcFraAQATa4JgqjgrSjMOiQFwQVAO/Ad4DkMc06qCWUKeF5EVorItXH2f4kE/gYRuVZEVojIil27dqXYnPs0tHZy8hGGjf+ccUNo6TBuzFRm5naOGFzIuvr+LWofL7s3blumBvJpffqWdAyE9vcRWAl5VumJLG/6Z4yxSV2RMFlTqDpZ7C7esqSDi3JcWT0rEFLR6+rz4PcKW5ucXxrU7rAFaO0w5nSX1oxwraSHz6YRuOUw7rJN8ioKDiGNQETuFpGTlFL7lFIhpVRQKfUXpdQC01SUCicrpaYA5wBfF5HptuPfjKFZLIr3RaXUH8w1EGoGDRoU7yMHBAtra7j9kokAHFddxuzJw4DUfQQWYyoLWLezhUv7sdhGMBxOGrZqYQmCG/++Jq0F72I1gtgqroFQ+meM1sBk+QjcWAnOoiu4v1lqaHGOK6tnWTkjluCztBOn1w+OLWz4h6tqKMvPwuOB+bPHO2oSi2pgEtGC3DYNgSkIWg4dH8E64NciUici/9OXNQiUUtvMv/XAE8BUABH5D4zSFXNVumsxDwBDi3MZXprL0k92RbKDe+MjADhycAGdQcXb/VhsI95gHI8hRTn4PLCxsS1tM8hEPoKG1k7mTh1Jjs/DkUMKHKh62n0wdmMlOAt7XLvFEFMQOH1bW9c7UgbF/NmdL71tOcij51yen+VKxJK9llZEI3Apu9geHl1RkE3jvk7Hf2M3SZZQ9lvgtyIyCrgcuE9EcoCHgUeUUp8kO7CI5AMepVSL+f9M4GcicjbwA+BUpZQ7FbpcYGp1Gc++tz0SRdEbjSBd0Rc9hY/GtkU/2orXdjy/iDVDfHdzE4MKsx2pegrdB+OG1k5mTx7GE+9uZeroMsfMFfaFaSyGFOXQFQqze18X5XFKiaSvbXNNAKvKq0vrB9tn5RblBe7YzCN1vLxCyQCYhqIaQRaBkKK5PejKgjxukEoZ6o1Kqf9RSk0GrgAuJLWFaQYDr4vIamA58KxS6jngbqAQeEFEVonI7/ve/QODsbcs4e/vbu02wI6+aXHK6vnSeTM479juYaPV5Xm9ntUFw+EencXWDNKb5hmkvexAPEZX5LOhIX0+CYuuOM5iy1zn84ijETz2CqAWQ4uNAdhpP0EgFI6ECje0djq61oQdu53eorwg2x2NIBwV+kWmIHArlyAQUhE/lJVUlkklJlJJKPOLyPkisgjDsfsJ8MWevqeU+kwpNdF8jVNK3WpuP0IpNcIMKZ2klPqvfp/FALN03gzOOLqy27beDK6VRTkseX9Ht211jW1MvfUljrx5ccr9SMVZbM0gQ2meQQaS1DkCOKwin82729JeEdSalWfHFLzzeoSqklxHq4HaK4BaDDGzmp32E9jDRxfW1vD9s8YC8IUJVY7a6eNV1x3kkvPUHiHm9QiFOT5XNAKlVLfs9UzMLk7mLD5TRP4EbAGuBRYDhyulLlNKPelWBw8GKoty9luntreD6/QxFVSX53UbVMCoYZQqyWoN2bFnM18wsX/ZzJG2Y+r9xDJ6UD5hlf6aQ4nyCACGl+Y6uj6APZzRwtIIHC94F+4ueI8aWgTAh9udrQcZ73qX52fR3BF0vOx3MBxGJOp/K3GpFHVU4Js+gkLDUX3LE+9lzEplyTSCHwJvAkcrpc5XSi1SSqVft88QGlo7mVptqOdC79eu/fOXpnHyERWRcEiLx9/ZmnIUSLJaQ3YW1tbwrdPHAHBJzYi0zCADPbQ9usKIVPpsV/9yJWLpijMYWxiCwDmNwEpwsgvfioJsPAL/+8p6Z5PZYlaEK871M6wkl7UOC4J4ZdYtX4jTCW2xxQXdKjMRK/wsjeDTXfsyZqWyZOsRzFBK3auU2u1mhw5WFtbWsLC2BsFw+FqLmfeGhtZOLpoyjNOOHIR9WEvVX5CKs9hiZHl6q4ImKzEBMLo8H4CfP/NhWgfI2DwCO8NL86hv6aQjEEpbe3aswVgket5ej5Dj97Jtb4ejg0Q8M+DRQ4t4b0sTly580zEhFIiYhuw+AmOG7LSpxO4XAcj1e1m+odH5Ync2k9TYW5ZQM/9FIBrY4WS4rls4X6TjEGHsLUuY/PMXIlnGfblBFtbWcMelk3ht3a5uayBY/oKejpVq+CgY0S1+r6RFECilzEJkiW+n4jw/2T4Pm/ekN+kpsoh7AtMQwLYmZ7SCQLC7ecaK6W/rMgSPk4NEbMVVgGOGFrJxdztv1+12TAjZ10q2qHBJEARjkhZ3NHfQ2uls9jh0n2zErlqYKSuVpVJ0TpMCVr3/59/fQUcw3K+1BaaPqaCusY1tTe10hRQCzJpU1eOxgqHUEsrAmLkOK8lNiyCIF1Jox6mQVaPtZD6C6ILyhw0q6Fc78QjGCD/rHli8ZjvBsCLb5+Hs8X1fXyJp2zFmkm4hyMq5AnCBUBzTUL5hKnE6cihgXm8n76e47dqSFiPhuhgm4ExZqUxrBGnCukE6Q2GyfZ5+3SCWvyAQVhFTUyrH6qn6aCwjyvLSsnJYtOxA/NspErLqwHqv8WL5LSyN4EdPve+I+SB2Vh6NyIouZ2itU5FuAjErwi2dN4PTj4pGrjk1Uw3GEbwVZinqxn3OawR+r+yXPZ7l9XDWuMEcU1XkyO8cEQSm+bGhtZOjhxbi9QpXTB2ZEWGkWhCkEave/xNfO7nfdVesY335c6MBeHr1Nj7ctjep/TfVWkMWydYJqG/uSNnWHAztH8tvJ79INrkAACAASURBVLIAj7XeayB9s6h4ZR4sBhflIBimtV8u/ijttvNgnNwJ63c75YhyPETXqUg39rh2MK5xZZExIHsdLAAXb+Gl/Cwv2T6P46WZA+b9HckeN/vSFQrzr092sWpTkyPXOvYeW1hbw1UnVhMMKf7r1MMdrzTrBto0lEbsN8T82ePTcqzte9u5d+kGmjuCXP/IKtbvamXBi+uYf+Gx+30n1VpDFiPL8mhqCzDrrtfxeoWFtcdFBo4FL63j7brd/HLxR2xpaufuKyYnHFQCcYqvxdLQ2skFk6p4ctU2jq9OX7ZvxEcQ4yyONR/8/d2tAJzwi5f47L/PS1vbsQJoYW2NK6YLI3mw+/Xeva+L4lwfYwcXcuSQInY5ODu2a38i4kohNsNZHJ2Vz502ioeWbSSsoCNg9MuJa90VRws6rMIIfvh0V2tkOdaDGa0RHMCMvWUJJ/73y5H36+pbI/bfeE7I3jiLwRAEAGu27uVdczZlOTwfXLYJpYwBdPmG3Zzwi5cSHicyOPSwTvNvLptEeX4WI8ry0jaLSuQjsMwHsYQVaXPgdiVIols6bwZfmDA08t4JM00guL+zeGFtDScfUUF9SyfzZ4/n5xeMd0ALiu8PKsr18dLaekcjeGKT6P62YnNkmVc7HiG91zpOQMLhVgXfXZkRUa8FwQFMtIxz94dOgLPGDd7vZu9N+OjYW5Zw3aJ3um17cNkmOoNh4h0h2QAajONAjIeIMHlkCSvqGtM2QMXaby0s80Es6RyUgzGx/Pa2raJoHjF8BV4RvvHQu2kbKANhFdcnc8SgAjbtbqMjEIpodek0l0Sjhrq3vbctwN72gCMmOIvY7PVYgWtx4eRhjlS4jU2iK8rxpT0vZqDQguAAJlrGWWEf3xXwyY6W/QaWQCjEkvd3pPQQxiuLYRFPq0g2gEajKnq+nSaNKGHj7naWb9jNFxa83u8BoyuJs7ihtZMrTxjFueOjdZzSaTuPZxqytz2qLI/KwhzmThvF23W70zoo26th2jm8soCwgqP+33MRrS6dYayxEWKWBrnNzKS2NMipt76UdmEQK/xiBS7AoIIsWjv7vG5W/HbjJC2KCCPL83h69baMyC7WguAAx7KFxmrAGxrbWL5hN9NufSkyA+sKKnakmMgUryyGRZf5sNuHmWQDaDBOtmk8xt6yhNufjxatrW/pZOqtLzH6xmf7/DBFHHkJKp/Onz2ekFIcN7IEMJZ1TFfZiWT1lRbW1vDF44azo7mDB9/ayOY97WkblCOrhMURQoebYbJHDo6GyybSIPtCbNG5RCY4IKXcl161HUf4WcL+6W+cAkBRjj/tzttESYv7OkO0dAQzIrtYC4IDHGswW3bT6XEfOAWRGVhvk9kaWjsZUZrbLeww9tgWsyclrmoZqcKZQuXTeJYrheHE7U2kkoUVRulJYhJbWFvDXVdMAaBxX1efsr4TtZ0siW7skEKAiBCC9JimYlcJs2MJgk92Rk0WCvhs176IELeuc09RaPGILTqXyARn0RkMM/aWJX36bWMxfuv9/SLzZ49n/LBiKgqyOX50WZ+Pn7jd7j4CSwuyqulmQnaxFgQHCfYHricvQKqDzcLaGpb+4PP88erjuchcWc3CI0YF1Ue/eiIA72zaw88TREJFinL5eq58OnvSsLj7wgqm/uIllm/onfkkmXnGYuwtSzjpl1Gnu/Xg9kcTibad+JyPHmIUgrMPyukwTSUzxeVmecnP8u63fV19a2SwsnwH1z+yqtfmqnjanzUr//zY7isJWvfQ0h/MSIu/ItBDeHRlYTa7WtIfuRTrI4hdgtXnMXwGT3z9pLS37RZaEBxEWA/cOTabdyxZfUxm29cVZExlAYLxAIeVkcR2fHUpBdleNiaJxY+3alVP7STDGqhTKcFtLBiSXADFJiBZKOjXwBTsQSMYXppLrt9DS2cwIrzPGjek36Gz8UpBQ3Smuq9r/9pKfq9EHNeW76CnKLT4be//W1uzcr/PE7mHwLiHnlm9jam3vpQWf0UwHN85bzGoMJt6RwVBdy0oEDYCK4JhQ9O0Vic8GNGC4CDCbvO+8oRRcc0sXWbUT28Hm4W1NRw2KJ+5J4zimW9+jitPGMVDyzcx+qbFtHYaA4vlCLT7JSBqQ00lmc1qJ1H/7aRSgjsQCu8XMRRL5MEN7R9r2J+BqasHbcTjEfKyDC3uGLNM9Adb90Y0q/rmDmb/7g0u/N83em0Og/1t1pbAswSE/ecIhBSzJw1jVpxr2pt1nZOVE7F+2zlTRyLA+KoiPjemottCSH6P9Hn2HAj2TiNIhzkKiOSE2O+zhtZOPEg38+nBbCLSguAgxBII1hoG1oBqqeJv/fD0PjnMrOMeU1XE/NnjeasHv4SVW9BTZnFP/U80y0ulBHdPdnoLq7LriYd1tyGLwNl9dKTGyyy2sGbnjWZp5g/M8tD2onsLXlrHqs1NkRyOVAcuq/x1rIPcXuIiy+chpODIygLOHT8Er8DLH9ezY69RgM/e665Q6hpkvBXK7CysreEXFx3LiLI8RlXk8+cvTeu2EFIgrPo8ezbWYEiuETS0dkYy2NMVPhvPFLewtoY3b/o8syY6W4AuXcKsJ7QgOIixahIpjNlKqjWJUsXul4g3e7dyC/7zL28DqQsCi2hNpXDk+FYlSzAG6Z6iXVLxEUC0smts8TmljPry9muW8oCcRAhZs/PYldMgOnN8cNmmbttS9ZFY4YzxZsdWlNmTXzuZK08YxehB+cyePIyQMmL9V27aAxiDpnWtL5kyPGUNMpDALBVLdUU+daYzdWPj/lFafZk9B0MqqWmosjCbYFhx1I+e65YU2d+ZeiBBGZPKohwKc3wRrcBukk3XAO5ELkg8tCA4yLE/+P2tb5To+FeeMIqzx+3vl7BmQPNnG+UuehocEh1/7rSoOSrHH3V0KtU9XyLew9XVw1rJ8dobUZrLeccOZViJMfg/+972bseM9/DFa9tYgyF5faUuW8hjqpenp4ErGI4/MMH+Wt2rH+/i2gdWAoYmZ617VN/SGakNdMnxqS9OlKr2N7o8j7qGfSilOHmMsRqeXRvry+w5GFNoL5ZBpjC/75rju+XIeKV/ztyIOSyOUG9o7eTkw43zO3f80Mjz198BPDbD32mzk641dJCTzvpGyY7/1QdWcOUJo9jd2slic33ljkAYnwj52cbg3VuNwH58MPp/2E3Pdtu/obGNDY1tnPCLl7hi6sjIw2XVWgoEw2T59o+SSaW9xe9t52uL3qGpLcCCF9fx6Iot3VaIs+rWCHDRlGH7tx0KJ42UsoTcFVNHcv0j77KuPvUs1GRlEnoq+23HKo39j1Xbutmzs30eph9ZwQsf1rNuZyvHV6cWdhm7XGQiqivy2dcVYldrJ48s3wwQETx9Ld/cFYqfTW0xyKyCGg4rVm1uimwPqagzN16Nrp7bTZy0uLC2hs92tfL5X/+Lz42p4MdPf0D1jdF7uK+1j5bOm8F3H13N6+sbAPpV1j4VtCDQpIRdIJwzfghLTGGwvG43E0YYcfItHf1fNvCtm05n/uK1PL1qW7ftYXNWBN0frhMPL4+bYdsT3er3QzczjUWO30NnIIzC8FfEtp2X5U0q/OxC57BB+Rw7vJj65s7Iww0wuDCbsFLsiqncmaxMQrI1GGKJmPfEUP/Dypghd4XCVBbmkOv3sq6+pcfjRNtWcZP3Yqk2i7JNvTVao8oShAqYc/yIXmuvRtRQcmcxQO0fl8fd39dBOXK9E5x3dXk+RTk+Vm/Zy9J5M7j5ifd4YW090PcBvLIoh86gEaSR5e1fWftU0KYhTa949eNdESEAhvPzx09/AMCDb23s9/G75UvYnnn745/lFUpz/YwZXMC+zmCfNBHLhp9sYtthCoFYLJNGMIlpKBbLRzGqPA8Rw6cjAmceM5iZptnN6ovfK0nLJEQXh0mtbUszOeGwco4cXMDUw8qYO20UDa2dHFFZwPpeaCrBJNnUdqylSe0Dd47fE1nX+7LjR/Y6oCGYokZQYEtws/++fXXm9pS06PEIE4aXsHLjbr7x8LvsaTMmRP1duGbn3g58HuGJr5/kiNnXjtYINL3CMjUseW/7fuGYT67axpOrtvW7BLDll7CboewtdYUUXe0B9mwNUJ6XRVc4TH1LR68etsgaCfFGehMrn8KOSNQh3xUK4+8hiS4Wu7nooeWbIqWirzzB2HbT39ewestefnXJxITHSLYYTzySDbjXLVrJix/uTPn6BcPJ16a2GF6aixAtV2It1lRVkgvs4b2te5k0oiTpMWJJVO3VIj/bR67fExGiVpsWfR2UUwlImDC8mHtebQCBPNPPlev3ctGUxBn5PVFVmkt5YTbjqoqZP7u4T8dIFS0INL0iWUw+pKcEsN0M5RWJrPgVj8Y2w6TSl3UGrJDS3a1dvPrJrm77srzSreaS1YNLjotG2Njr46dKTz6d780cy1V/Ws6lv3+T+/9zavzaTr0M103G9qYOAiHFr/75Mb+6OLHwibQd7jlcN3Y9BjAGYY9AeyBEaZ6f97fs7XVfgylcb8t3MbQ4mz/+x1S++sAK2rpCNO7r4pzxfUvm6ylpsdv5/v/2zjxMqurc1++vG5pmamRoZhAVgYhTtIPHMeIYNU6JOWrykGBicvJkMDlJLnqiSfQ6exPjMcZ7TY45MYNITMQJUZAEJU6IyDyIR0XmQUFAoMfv/rHX7t5dVHVXddWugqr1Pk9B9dq192+vVbv2t/Za6/s+o9mhb3d9Iz84ZzR9ulek3Lct3tq0i3OOGNChfTPFDw15Mia8gYarJaLkMgRwuFZ7/Ojq5qGhVD/HjuQZCIdrhriUltGObl3E0IX5aQEuOHowD0yoobHJaLLc3IyjHON6ySs27ky54qQl/0PmcyMh4aqUcFL10Xlr02q/dEKdp0pNGvq3jBrQk6cyjNrZXnuH9QmdHzd8VMv5985h885aHv/2yQCceGjfDvnX1LXjtDhn0ng+M7b1DbtX16CPvfqDjuUr2Lqrlg8/rmPUgJ4d2j9TvCHwZEx4Aw0nBMP7wqj+PXIeArh/VWUwnKDgUT/Vs0E2zjzhUNTT3z2Vzx03hIG9ujSv/w+9bv/+w08D8OPHFrN5596Id2/Hb8aJjL5hOsfcNKP571RLBjOZLE5FeLOu7BwcQ6S3xDKdYZKWYTdLmr97194Gdtc38suZb7V5nNa6bXuvJ/ptRK+Hob270qd7Bfc8v6pD6/rrG9oObdG/qpK+Pbq06qR0cSvZUqWCbY+3NgYT+N4QePZ7ojfQ0Hkpjvyt0VzQw1z8nguOGtScYS2ModPRSbno2vu7//VYzhwzgLrGJrp0Kmv2uj2kuge9unZi3fY93PHMCr70X68BqVeSdIRUPel9EhA1ZT80FN6saxuCiVAjvXg5DU3pTRYny98d9tpDL+vJc9ekH+Ooqe0ls1G/jUTjI4nuFeV88HFdh9b11zc2JfUhiDJ57vutOilhzKMf/GVhxnoAb6z+EIA+PTp3aP9M8XMEng4Ttw9DMp05157R/P7f/jiP00ZV7zPxmi2JE7oPv7a61fLSMP8xpD9hmw7NPWl306utT27cMp0sTkUYL6chMiPe3hLLYKVUejGlQm6JxFa65ZnlPLdkI7Vu3P38owaltbSyIY2noGQT8bnIH53OU1C47HnG0o3srW8KnrQMzvxEx8b4n1i4AaDDvg+Z4g2B54AlLkOUeNxrzhiZ1LcB4ManlnH79BU5S5S+dVctV4wbxtT56xjYqwvTFm/gmrMOb2UM6nM0WfzAhJrmm/O0hetptPbXvacb2ykZ0V57cCxL+ymuJbBhen4bicbn2cUbm728z0vT+ES10x0Oq21oeSKp7t4l46GoXBiujuCHhjyedmgrF8Qnhx2U0yBjD0yo4fbPHc2ZRwxgw0e1bN9Tv89wRi7mCEJaAtUFf7c3xNbQ1HYE0PYIe+2De1VycJ9uaa/iaV4pleEEeTRkNASLADIdQkyVFjSRxOEwCRas2Z6RMUjMwxxHILtkeEPg8aRBqlwQ3SrKc+7tOfqG6UxbtIHahqakcWYa2pk4zZStu2o5ya0Au+jowW3enJNlCcuEcD5mzKAqulaUNz+VtBegLZsls+ENemT/Hgzo2SXjJaTpPgUlxnka0KuS+kbj7hnpT4r3r6psSb9artg9ikO8IfB40iCaC+LKTw2j0k0eZtrjS4c5k8Zz/lEtBiexV1jX3DvOzc/3gQk1XHPm4QBcVjO0zQn/hnaysqXLYdXdeXfrxzQ1Gb98/q12A7TVN3Xc+IXf3cmH9WVnbQO/dmlL0+XjvY2s3LQz7e85nBRf5HwlHnl9TdqJloDmqK1/+Oq42D2KQ7wh8Hgy4IEJNdz++aM51z0Z7K5rzHmI4P5VlfTu1uKElNgrbIhh6epQ50uxbtueNj/X0JTdE0HIodU9qG1o4tAfP8PkuWvajbCZi+Gw4w7uze66Ri75dWaJgNZs251RkvrEpbkh6SRaAhjapxuHVXfnxMP6ccslR8ayEi+RWA2BpPckLZa0QNI8V/YFSUslNUmKv4YeT44ZfcN0nnATx2HAulyHCN66q5bxLgdwVWVn1m5rWY/enDc4h0tXB1ZVUl4m1rZrCLKbIwg51PmghPGBoO3x8Fx4Ux83PIhztHT9jrRu6onJhdL9nsN5ib31rb2r00m0tHH7Hl5YuZmjhsQbUiKRfDwRjDezY80svOkvAT4HvJgHbY8n58yZNJ6LjmlJXh7HhN4DE2r4zZdr6NKpjI/21DO0d7fmbdEx5FzRqbyMgVWVrNvejiFIc/loexzm8laHqSXb8wVpz6GsPUbfMJ1T7/oHkL7xnjNpPGdH8hpk8j2H3vefHlWd0f43T1tOoxFL7uW2yPvyUTNbDiDl7iL2ePJJmJmqvmlf56Vc0dYywqtPPYROZcr5b2jIQV1Zt20Pm3fs5TuT3+S+L34yqQ9DulFP26Jv9wo6l6t5KezAqkrO+MSAlL4g9VnOi4TBEp9euJ6mNJbJhrz0Px8AmYeCDodzrp+6uLks1f6bd+zlhNtnEQ2p9fL/fMCI66bFvmw0JO4nAgNmSHpD0jdi1vJ48kYyz9lcEo4zh08dFeUtvcl003NmytDeXVm7bXeb2bUamrKfLB59w3QO+Y9nWgUuXP/RXh6dtybleHhDlk504XBNeLNN5awX5e6Zb7G7rpEeXcp5/Nsd+56jK7IuTLEi695ZqzCD7hUtCZbytWw0JO4ngpPNbL2k/sBMSSvMLK0hIWc4vgEwfPjwOM/R48mYuL2q913/3nLj+mhPHXUNjRmH3m6PJxaup7HJkiYACnulDVkuH4V9Q5mHTwZ/uvqElPvUh/MiWRjArbtqufSTQ3jszXUcN/yglDf1xKexXbWNnH/vnA71zh+YUMOitdu56L6XOP+oQXwmsvx41PXTW2XEC6OWQpAL46mF6/nPKz6ZkV5HifWJwMzWu/83A1OBcRns+xszqzGzmurq6vZ38HiKjPCpY0TfbgzuVdl845q/ejuNRs5XK1177uhWf5clyfVb35j9ZHFo5BqagqB04UTw9t2pM9zV52Be5IEJNdx9+bGMGdgTSWzbXd+8eijqyzBn0niGuVVUkH3v/LDqYD7k7YRMcFHHsSidy8WIvt04bVT+7nuxGQJJ3SX1DN8D5xBMFHs8njQI17+fNLIfO2sbmL1yCyOum9ac8jHXq5WOGNx6pUqT7RuIrqEp8xwMyYgOrX2hZigA725Nnilt84693PnsCiA3K6VOH92fN1Zvax7+2rxjL5/91T95/b0POfG2WYy7bRZrIqunwt55R5++unfpxJCDujZ/b+FqpGjMqpByBcNvp4zsx++vSrvfnDVxDg0NAKa6Ca1OwMNm9qykS4FfAdXANEkLzOzcGM/D4zmgOX54bx5+7X3GDOzJxo/2sn1P0HPOZULzZMlkQqJDRD26dMrJ8tHo0Npdlx3DzKWb+O2cd7kkST6Le2etak6nWZGl70RiEpmwbiGNCZ+v6CQG9+raHHK9oxw+oAerNgV1mDNpPD95YgnPLd0EBDf/3t0rqKrszIBeXTisumfOAiimS2yGwMzeAfZJeWRmUwmGiTweTxocf3Cw/n3FxtZDC7kcR54zaTw3P72MaYs30GSt03RGDc5Zv3ghlonqTuViy85a7n1+VXO0zWTG6ay7X8xqJU1Yz6cWbWj3sxXlZdQ3NnHKyH5ZRwA9vH8PXn57K5fc909UJra65aECmoDPjB2YlyijqfDRRz2e/Zi2euoj+nbLuqca0r+qkqqunTGS5PqNrLBJN2dxurS1THbOpPH8eOpinl++uXn70N6VPPatkzusF9azLbp0KqNTmfjLN09k8tw1OemdH96/J3WNxoKEFJ3m/nl4bn7CTafCh5jwePZjEpeRQtBbl8j5OHJiAqABVYHX74mH9W3J09zQxLTFG3IWXymsX2hcKju1npiNGgGAtdv2Mu7WWVnNi2zdVcuJhwZLOsMlm1HbVtvQxPlHDXJJ47MP8TD6hulM+tuilNvDNJ6FxBsCj2c/JrqMNLxZnTd2YCy+C9HomXOuPYNXrjuT3t0607t7RfMKm/omY8NHe3O2Yqk5DHaYjMc5XWFwwb1zAOhU1pKfuUxkvb7+gQk1/OcVxwLBks1+PSp4+runcnEkFlAYViIXzJk0njPH9N+nPDTo+Ygu2h5+aMjj2c9JlnkrzoxwIWVl4oRD+jJ7xWY+rmtk3K2zmrflMmHK1l21fPGE4Uydv47Bvbvy8NzWE7jByFFgKIzc3Dj7V1XSp3tnPvy4nkOre3Dp/S+1GqL6+4rNOfPs7V9VycBe+57veWMH0rtHl7xPDCfDGwKPZz8nXylBE2lrfiKXK5bC+m3eWcvMZZtSfu5PV4/j2SWbsr5xJtZr7rtBfuAyBQ5rdQ1NOa0fBMZuWO+uHD30IAAWrd1Og1lev8+28IbA4/EkJdlkLUBFTPGVThnZj5nLNtGprHUeZYDPHzeEU0ZWc8rI7J2sQs/maH7hc8cOpLxMTH1zXSzxo/IRSjobvCHweDxJ6V9Vyd9XbN6nvK4hmK/I9RzFySP7AS1htsMlrKP692BXbUPOdJLlF+7ZpRNbkgzBlQreEHg8npScdng/3vtgN+u376WuMTAAFx4zmOsv+ESs0VaBZn+GQ6q757xHnWzepVBDcPsDMrP2P1VgampqbN68eYU+DY+nJLl+6mIenvs+FeVl1DU28aVxw3O+5n3zjr3c8sxynluykdqEMfpCr6g5kJH0RiQXTEr8E4HH42mTZL3nXBMO19Q1xpfjwZMabwg8Hk+b5GvIJB8Gx5McPzTk8Xg8RUq6Q0Pes9jj8XhKHG8IPB6Pp8TxhsDj8XhKHG8IPB6Pp8TxhsDj8XhKnANi1ZCkLcDqQp9HBvQDtpaYdinWuZDavs5eOx0ONrN2AzQdEIbgQEPSvHSWbBWTdinWuZDavs5eO5f4oSGPx+Mpcbwh8Hg8nhLHG4J4+E0JapdinQup7evstXOGnyPweDyeEsc/EXg8Hk+J4w2Bx+PxlDjeEHg8Hk+J4w2Bx+PxlDg+MU0OkHQucAkwBDBgPfCEmT1bjLpOewxwcYL2k2a2vFi1C9ze/hrL7zVWqPbuBXwmQfc5M9seq65fNZQdku4BRgF/ANa64qHAl4FVZva9YtJ12tcCVwKPJGhfATxiZncUm3aB29tfYy3a+bjGCtXeXwZ+BswA1kV0zwZuMrM/xKEL3hBkjaS3zGxUknIBb5nZ4cWkG2oDY82sPqG8AlhajNqFbm9/jTWX5+UaK1B7rwROSOz9S+oNvJbsnHKFnyPInr2SxiUp/xQQZ9LVQukCNAGDk5QPctuKUbuQ7e2vsRbycY0Vqt4iGA5KpMltiw0/R5A9E4H/K6knLY+Rw4Adblux6QJ8H5glaRWwxpUNB0YC3ylS7YkUrr0LpV0oXSjsNTaRwtT7VmC+pBm0rvPZwM0x6vqhoVwhaSDBBI+AtWa2sch1y4BxUW3gdTNrLHLtgrR3IbVL8Rpz+nmvtxsGOpfWdX7OzLbFqusNgcfj8ZQ2fo4gRiTNLyVdp/10qWkXuL39NZZf7UK1d6yB5/wTgSenSBpkZhtKTduTP0rxe5Z0vJm9EdvxvSHIDZIGEHECMbNNxawb0e8DWNxjmPuLdiHb219j+b3GCl3vfOINQZZIOhb4f0AvWjuBbAe+ZWaxPEoWStdpDwfuAs50egKqgL8D15nZe8WmXeD29tdYfq+xQrV3L+A/CDyawzzDm4EngDti9S42M//K4gUsIHACSSz/F2Bhsek6jVeAy4HySFk5gdfnq8WoXeD29tdYnr7nArf3c8C1wMBI2UBXNjPOOvsngiyRtMpSeBpKetvMRhaTbhraKbcdyNr7cXv7ayy/2nG290ozG53ptlzgHcqyZ7qkaQRxSUInkGEEcUniDFBVKF2ANyTdDzyUoP0V4M0i1S5ke/trrEU7H9dYoeq9WtIk4CFz8xFunmJi5DxiwT8R5ABJ59ESJTF0AnnSzJ4pUt0K4GsJ2muAp4AHzay2SLUL0t6F1C7Fa8zp573ezpnsOqfb3xVvAp4E7jSzD2PT9obA4/F4ShvvUBYjcTuB7G+6TvunpaZd4Pb211h+tQvV3lfFenz/RJAdbo1z0k0EKwyGFpNue0h638yGF5t2IdvbX2MJ4jFfY/tjveOus58szp4twGpoFSbW3N/9k+5xYOsiaUeqTUDXItUuWHsXULskrzEKVG9Ji1JtAgbEpQveEOSCd4Azzez9xA2S4pzpL5QuBI41n7IknpZFrF3I9vbXWH61C1XvAQSRRxM9qAW8HKOunyPIAfcAvVNsu6sIdSFYVndwim0PF6l2IdvbX2OtifsaK1S9nwZ6mNnqhNd7wOwYdf0cgcfj8ZQ6/okgJiTVSBpSKrpOe5CkLqWkXeD29tdYfrULVu+48YYgPr4LPC1pSonoYRH8gwAADDlJREFUAvwRWCHp5yWkXcj29tdYfilIvSUtd6/YUnT6oaGYkdTTzHaWkK6AI8xsaYlpF6S9C6lditeY0897vSX1IwiENy2W43tDEB+SxpjZipg1OptZfUJZPzPbGrNuGYCZNblwAEcC78XpBt/GuXzLzO7Ps2YPYBTwjsUZHpjmcAv15n6sksYDxwHLzGx6jLpHm1mqJY2xoyAU9Q4z2y5pBFADrDCzJXnSryGIMdQArIr7t1xI/NBQvMyI68CSxktaC6yXNMP9UGLXddqXABuAdZIuBuYAPwcWSbowZu0fJLx+CPzv8O8Yde+PvD8FWAb8Algs6fy4dB2vAwc57f8F3Eqwlv4Hku6IUfdNSW9LulnSETHq7IOk64AXgFclXU0Q7O08YEqc37PT/rSkecAdwO+AfwMelDRb0rA4tds4p8VxHt/7EWSJpHtTbcL9eGPiLuBcM1sq6TJgpqQJZvYqrR1h4uBnwDEEN6OFBOu9V0o6GPgbQWCwuLgJeAZYSks9y4GeMWpCEIs+5GbgEjObL+lQ4C/unOKi3Fqyc10OnGpme5wRmE8QqCwOFgETgCuBJyV9DEwGHrEYE8M4JgBHAN2A94BDzWyLpO7Aa8DdMWrfA5zj9A4B7jazkyWdDTwInBOHqKTPpdpEkJcgNrwhyJ6rgB8CyaIhXhmjbkU4Rmpmf5W0HHjM9aRiH+8zs43Q7Pq+0pWtDoeMYmQswU2gO3CTme2W9BUzuylm3ShV5rJUmdk7kspj1tsh6Ug3JLIVqAT2EPx+42xvc5rXA9dLGkeQGGaOpDVmdlKM2o3O2NUR1PUDd0IfB1MEsVJuZlvc+/dx/gxmNlPSPTHqTgH+TPLfb2WMut4Q5IDXgSVmto/nn6QbY9StlzQwvCG7J4MzCZxSDotRFwjmCMysCfhqpKwcqIhT13l7XuaGpGZK+mWcehHGuBAAAkZI6m1m25zh6xyz9jeBP0taSJC6cJ6kF4Cjgdti1G11xzWzucBcNxx3Woy6APMlPUxg8GcBD0l6FjiDYFguTuZJetDpXoxz5pLUjeDpMy4WAT9PNgci6awYdf1kcbYoCFC118x251n3LGCLmS1MKD8I+LaZ3Rqj9qeAxWa2N6F8BHCKmf0pLu0EvW4EQ0UnmFmsNyY37BVlg5nVudUcp5nZYzHrlxMMSYwi6MCtBZ6Lc6Ja0hfNLG4v3lTanYAvEPSO/wqMA75I0EP/tZl9HKN2Z+DrBENTC4HfmVmjpK5AfzNbHZPuqcDqFKEtasxsXhy64A1BTnFGwSLjuUWtW6ravs75pVS184lfNZQlkoZLekTSFoJJrNclbXZlI4pNt1S195M6b86ndinWudDabZzTZ2MVsA5mvfev4AW8QrCSozxSVk4wqfZqsemWqravc2nUudDabZzTTXEe3w8NZYmkVWZ2eKbbDlTdUtX2dc6fbolrj6ElV7IB6wlyJS+PSxP8qqFc8IYCZ6OHCJJrQ+CN+BXgzSLULVVtX+f86ZaktqRrCZacPwLMdcVDgcmSHjGz2JwH/RNBlihw//8aLVZcBBfPU8CDZpbMv+CA1S1VbV/n0qhzIbUlvQWMtX1DxlQAS2N9EvGGwOPxeAqPpBUE0QJWJ5QfDMwws9FxaftVQzES+0z/fqZbqtq+zl47R3wfmCVpuqTfuNezBI5t34tR1xuCmPlUiemWqravs9fOGjN7lsBh8CbgOYLgkTcCo9222PBDQzmgUDP9hdItVW1f59Koc6G1C4F/IsgSN9P/CMGE0lyC2EMimOmPKypkwXRLVdvXuTTqXGjtQuGfCLKkUDP9BV1hUILavs750y1l7ULhnwiypwkYnKR8kNtWbLqlqu3rnD/dUtYuCN6hLHvCmf5VtDifDAdGArElmy6gbqlq+zrnT7eUtQuCHxrKAQpi0o+jxflkLfC6mTUWo26pavs6l0adC61dCLwh8Hg8nhLHzxF4PB5PieMNgcfj8ZQ43hAUGZJmSzo3oez7LppiW/vtivfMUupOlrRI0r8nlN8o6Uc51poo6b40Ppdz7Xb0kp5Xuuebw/MYLOmvOTrWRElbJC2QtCLx+02xz+mSTsqFviczvCEoPiYTJNCIcoUr36+QNBA4ycyONrN8JaEvaRTkAk6Kma03s8tyKDfFzI4FTgaulzSsnc+fDnhDUAC8ISg+/gp8VlIXAAWp9QYD/5TUQ9IsSfMlLZZ0ceLOrlf2dOTv+yRNdO+Pl/SCpDckPSdpkCu/RtIy17N/JMkxKyX9t9N8U9J4t2kG0N/1Gk9Np3KSHnf6SyV9I1K+S9Kdbtvzksa5p6N3JF0UOcQwSc9KWinpZ5H9r3dlzwOjI+Vfl/S6pIWS/iapW5JzGifpZVe3lyWNduUTJT3m9FZJuiuyz1WS3pL0AsGNMm0knSPpFfc9Piqphyv/qTvXJQoClsmVz5Z0m9P6nqTfS7rXnes7ki5znxshaUka5/41d+6zJf22vacWM/sAeJtgHT6SLpT0mmuv5yUNcNfpN4F/D68HSdWuzV93r4zayZMBcaY/86/CvIBpwMXu/XXA/3HvOwFV7n0/gh9nuHJsl/v/dODpyLHuAyYCnYGXgWpXfjnwO/d+PdDFvT8oyfn8EPhv934M8D5QCYwAlqSow43Aj5KU93H/dwWWAH3d3wac595PJTAynYFjgAWufCKwAegb2b8GOB5YDHQDqly7/Mjt0zeifQvw3STnVAV0cu/PAv4W0XsH6OXqu5ogwckg1wbVQAXwEnBfkuNOTCx339uLQHf397XAT6Nt497/EbjQvZ8N3B/Z9nvgUYKO4BHA2668+fto49wHA+8BfVz7zmnv3AnW4C8AKt3fvWm57q4GfpHsOwceBk6JHGN5oX9bxfryDmXFSTg89IT7/6uuXMBtkk4j8JAcAgwANqZxzNHAkcBM19EsJ7ipAiwC/izpceDxJPueAvwKwMxWSFpNEGVxR8Y1g2skXereDwMOBz4A6oAwQuNioNbM6iUtJrjBhcy0oIeKpMfcuQFMNbPdrvzJyOePlHQLcBDQgyAqZCK9gIckHU5gkDpHts0ys4/ccZcBBxPczGeb2RZXPoWgPdLhXwhu3i+576GCIMcuwHhJkwgMWh9gKUEyFYApCcd53MyagGWSBqTQSnXuL5jZh6780TbO/XL39Dca+LqZ7XXlQ4Ep7omyAng3xf5nAUe4egJUSeppZjtTfN7TQbwhKE4eB+6WdBzQ1czmu/IvEfRCj3c3yfcIentRGmg9ZBhuF0GclROT6F0AnAZcBPxE0lgza4hsV5J9MkbS6QQ3hxPNbLek2ZHzqzfXdSQwcrUAZtak1uPiiY4z5s4vlUPN74FLzGyhGyI7Pclnbgb+YWaXuiGO2ZFt0WxWjbT85jrqwCMCY3Zlq0KpErgfqDGzNZJupPV3+3HCcaLnler7SXbumXyXU8zsO5JOBKZJmm5mGwk6BXeb2ZPuO70xxf5lBN/1ngw0PR3AzxEUIWa2i+Bm9DtaTxL3AjY7IzCeoIeXyGqCXlgXSb2AM135SqDa/aiR1FnSWAUemMPM7B/AJFp6zlFeJDBCSBpF8Ji/sgNV6wVsc0ZgDEHvOFPOltRHUlfgEoJhmReBSyV1ldQTuDDy+Z7ABkmdwzqkOK917v3ENM7hNeB0SX3dcb+Qwfm/CpwsaSSApG6uTcOb/lY3Z5DLSd8oc4FPS+rtDOzn29vBzF4hGKoKk6tE2+srkY/uJGjvkBlEQjpIOjaL8/a0gTcExctkgvHx6OTtn4EaSfMIbmorEncyszXAX3DDPbhk3WZWR3BzuVPSQoIx35MIhoj+5IZg3gR+aWbbEw57P1DuPjMFmGjp5X29QdLa8EUw9NNJ0iKCXviraRwjkX8S3JQWEIzlz3NPTFPCMoJx75CfENy4Z5KkvRx3AbdLeomgPdrEzDYQ9IJfAZ4H5rfx8YkJbdCFwNhMdu3wKjDGtflvCYbFHicInZxzzGwdcBtBmzwPLAM+SmPXO4GrnKG9EXhU0hxga+QzTxEY5HDxwDUE1+siNzT1zdzVxBPFh5jweDwZIamHme1yTwRTCRYNTC30eXk6jn8i8Hg8mXKjpAUEq67eJfkCAc8BhH8i8Hg8nhLHPxF4PB5PieMNgcfj8ZQ43hB4PB5PieMNgcfj8ZQ43hB4PB5PieMNgcfj8ZQ4/x820e1NWLLn6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block of code generates plot of Accuracy with varying values of Lambda and Learning Rate\n",
    "plt.title('Validation Accuracy vs. Lambda and Learning Rate')\n",
    "plt.plot(X_lab,Accuracy,'*-', label='Validation Accuracy')\n",
    "ax = plt.gca() # grab the current axis\n",
    "ax.set_xticks(X_lab1) # choose which x locations to have ticks\n",
    "ax.set_xticklabels(X_lab1,rotation='vertical') # set the labels to display at those ticks\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.xlabel(\"Values of Lambda and Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('ValidationAccuracy_SGD_sub_GSC.pdf', bbox_inches='tight')\n",
    "#plt.savefig('ValidationAccuracy_SGD_sub_GSC.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_lab</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.08,8.0</td>\n",
       "      <td>50.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.07,9.5</td>\n",
       "      <td>50.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.08,8.5</td>\n",
       "      <td>50.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.07,9.0</td>\n",
       "      <td>50.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.09,7.0</td>\n",
       "      <td>50.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.08,9.0</td>\n",
       "      <td>50.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.09,8.5</td>\n",
       "      <td>50.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.07,10.0</td>\n",
       "      <td>50.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.08,10.0</td>\n",
       "      <td>50.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.1,6.5</td>\n",
       "      <td>50.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.1,6.0</td>\n",
       "      <td>50.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.08,7.5</td>\n",
       "      <td>50.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.1,7.0</td>\n",
       "      <td>50.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.06,9.0</td>\n",
       "      <td>50.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.08,9.5</td>\n",
       "      <td>50.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.06,9.5</td>\n",
       "      <td>50.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.09,7.5</td>\n",
       "      <td>50.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.09,8.0</td>\n",
       "      <td>50.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.07,7.5</td>\n",
       "      <td>50.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.09,6.5</td>\n",
       "      <td>50.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.07,8.5</td>\n",
       "      <td>50.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.09,9.0</td>\n",
       "      <td>50.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.06,10.0</td>\n",
       "      <td>50.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.1,7.5</td>\n",
       "      <td>50.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.1,8.5</td>\n",
       "      <td>50.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.1,8.0</td>\n",
       "      <td>50.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.09,9.5</td>\n",
       "      <td>50.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.07,8.0</td>\n",
       "      <td>50.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.08,6.5</td>\n",
       "      <td>50.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.1,9.0</td>\n",
       "      <td>51.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.04,2.0</td>\n",
       "      <td>52.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.02,3.0</td>\n",
       "      <td>52.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.07,1.5</td>\n",
       "      <td>52.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.08,1.5</td>\n",
       "      <td>52.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.06,1.5</td>\n",
       "      <td>52.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01,5.0</td>\n",
       "      <td>52.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01,5.5</td>\n",
       "      <td>52.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.05,1.5</td>\n",
       "      <td>52.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.04,1.5</td>\n",
       "      <td>52.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.02,2.5</td>\n",
       "      <td>52.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.09,1.0</td>\n",
       "      <td>52.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.1,1.0</td>\n",
       "      <td>52.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.07,1.0</td>\n",
       "      <td>52.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.01,4.5</td>\n",
       "      <td>52.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.08,1.0</td>\n",
       "      <td>52.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.06,1.0</td>\n",
       "      <td>52.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.01,4.0</td>\n",
       "      <td>52.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.05,1.0</td>\n",
       "      <td>52.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.03,1.5</td>\n",
       "      <td>52.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.02,2.0</td>\n",
       "      <td>53.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01,3.5</td>\n",
       "      <td>53.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01,3.0</td>\n",
       "      <td>53.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.04,1.0</td>\n",
       "      <td>53.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01,2.5</td>\n",
       "      <td>53.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.02,1.5</td>\n",
       "      <td>53.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.03,1.0</td>\n",
       "      <td>53.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01,2.0</td>\n",
       "      <td>53.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.02,1.0</td>\n",
       "      <td>54.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01,1.5</td>\n",
       "      <td>54.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01,1.0</td>\n",
       "      <td>55.376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_lab  Accuracy\n",
       "147   0.08,8.0    50.696\n",
       "131   0.07,9.5    50.720\n",
       "148   0.08,8.5    50.752\n",
       "130   0.07,9.0    50.752\n",
       "164   0.09,7.0    50.768\n",
       "149   0.08,9.0    50.776\n",
       "167   0.09,8.5    50.784\n",
       "132  0.07,10.0    50.784\n",
       "151  0.08,10.0    50.824\n",
       "182    0.1,6.5    50.824\n",
       "181    0.1,6.0    50.824\n",
       "146   0.08,7.5    50.840\n",
       "183    0.1,7.0    50.840\n",
       "111   0.06,9.0    50.856\n",
       "150   0.08,9.5    50.864\n",
       "112   0.06,9.5    50.880\n",
       "165   0.09,7.5    50.880\n",
       "166   0.09,8.0    50.896\n",
       "127   0.07,7.5    50.896\n",
       "163   0.09,6.5    50.904\n",
       "129   0.07,8.5    50.904\n",
       "168   0.09,9.0    50.912\n",
       "113  0.06,10.0    50.928\n",
       "184    0.1,7.5    50.928\n",
       "186    0.1,8.5    50.960\n",
       "185    0.1,8.0    50.968\n",
       "169   0.09,9.5    50.976\n",
       "128   0.07,8.0    50.976\n",
       "144   0.08,6.5    50.992\n",
       "187    0.1,9.0    51.024\n",
       "..         ...       ...\n",
       "59    0.04,2.0    52.416\n",
       "23    0.02,3.0    52.424\n",
       "115   0.07,1.5    52.432\n",
       "134   0.08,1.5    52.440\n",
       "96    0.06,1.5    52.440\n",
       "8     0.01,5.0    52.496\n",
       "9     0.01,5.5    52.520\n",
       "77    0.05,1.5    52.584\n",
       "58    0.04,1.5    52.608\n",
       "22    0.02,2.5    52.648\n",
       "152   0.09,1.0    52.664\n",
       "171    0.1,1.0    52.664\n",
       "114   0.07,1.0    52.728\n",
       "7     0.01,4.5    52.736\n",
       "133   0.08,1.0    52.776\n",
       "95    0.06,1.0    52.816\n",
       "6     0.01,4.0    52.872\n",
       "76    0.05,1.0    52.888\n",
       "39    0.03,1.5    52.944\n",
       "21    0.02,2.0    53.080\n",
       "5     0.01,3.5    53.080\n",
       "4     0.01,3.0    53.144\n",
       "57    0.04,1.0    53.168\n",
       "3     0.01,2.5    53.440\n",
       "20    0.02,1.5    53.600\n",
       "38    0.03,1.0    53.696\n",
       "2     0.01,2.0    53.768\n",
       "19    0.02,1.0    54.168\n",
       "1     0.01,1.5    54.464\n",
       "0     0.01,1.0    55.376\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting Accuracy to get the optimized set of Lambda and Learning Rate\n",
    "d={\"X_lab\":X_lab,\"Accuracy\":Accuracy}\n",
    "g=pd.DataFrame(data=d)\n",
    "g.sort_values(by=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 5578  Correct :6922\n",
      "Testing Accuracy: 55.376000000000005\n"
     ]
    }
   ],
   "source": [
    "# This block of code checks the testing accuracy based on best values of hyperparameters\n",
    "for i in range(0,len(y)):\n",
    "    learningRate = 0.01   \n",
    "    La = 1\n",
    "    z = np.dot(X[i], W_Now)\n",
    "    h = sigmoid(z)\n",
    "    gradient = np.dot(X[i], (h - y[i]))\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "\n",
    "# Adding bias to testing feature matrix\n",
    "p_test=np.ones((len(GSC_test['feature_id_AsubB']),1))\n",
    "r_test=np.array(GSC_test['feature_id_AsubB'].values.tolist())\n",
    "X_test=np.hstack((p_test,r_test))\n",
    "X_test[1].shape\n",
    "\n",
    "# Extractig target values for testing set\n",
    "y_test=np.array(GSC_test['target'])\n",
    "\n",
    "# Taking results of Logistic Regression\n",
    "z_test = np.dot(X_test, W_Now)\n",
    "z_test.shape\n",
    "h_test_raw = sigmoid(z_test)\n",
    "h_test=np.around(h_test_raw)\n",
    "\n",
    "# Calculating Accuracy\n",
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    if (y_test[i]==h_test[i]):\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSC CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias to training feature matrix\n",
    "p=np.ones((len(GSC_train['feature_id_AconB']),1))\n",
    "r=np.array(GSC_train['feature_id_AconB'].values.tolist())\n",
    "r[:,0]\n",
    "X=np.hstack((p,r))\n",
    "X[1].shape\n",
    "\n",
    "# Extractig target values for training set\n",
    "y=np.array(GSC_train['target'])\n",
    "\n",
    "# Initializing Weights\n",
    "W_Now=np.zeros((X.shape[1]))\n",
    "\n",
    "# Adding bias to validation feature matrix\n",
    "p_val=np.ones((len(GSC_val['feature_id_AconB']),1))\n",
    "r_val=np.array(GSC_val['feature_id_AconB'].values.tolist())\n",
    "X_val=np.hstack((p_val,r_val))\n",
    "\n",
    "# Extractig target values for validation set\n",
    "y_val=np.array(GSC_val['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and initializing lists to store Accuracy values for validation dataset\n",
    "# This block of code also evaluates model performance varying learning rate from 0.01 to 0.1 and varying Lambda from 1 to 10.\n",
    "Accuracy = []\n",
    "X_lab = []\n",
    "X_lab1 = []\n",
    "\n",
    "for k in learningRate_val:\n",
    "    for j in La_val:\n",
    "        for i in range(0,len(y)):\n",
    "            learningRate = k   \n",
    "            La = j\n",
    "            z = np.dot(X[i], W_Now)\n",
    "            h = sigmoid(z)\n",
    "            gradient = np.dot(X[i], (h - y[i]))\n",
    "            La_Delta_E_W  = np.dot(La,W_Now)\n",
    "            Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "            Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "            W_T_Next      = W_Now + Delta_W\n",
    "            W_Now         = W_T_Next\n",
    "\n",
    "        z_val = np.dot(X_val, W_Now)\n",
    "        h_val_raw = sigmoid(z_val)\n",
    "        h_val=np.around(h_val_raw)\n",
    "\n",
    "        wrong_val  = 0\n",
    "        right_val   = 0\n",
    "\n",
    "        for i in range(0,len(y_val)):\n",
    "            if (y_val[i]==h_val[i]):\n",
    "                right_val = right_val + 1\n",
    "            else:\n",
    "                wrong_val = wrong_val + 1\n",
    "        Accuracy.append((right_val/(right_val+wrong_val)*100))\n",
    "        X_lab.append((str(np.round(k,2))+','+str(j)))\n",
    "    X_lab1.append((str(np.round(k,2))+','+str(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This block of code generates plot of Accuracy with varying values of Lambda and Learning Rate\n",
    "plt.title('Validation Accuracy vs. Lambda and Learning Rate')\n",
    "plt.plot(X_lab,Accuracy,'*-', label='Validation Accuracy')\n",
    "ax = plt.gca() # grab the current axis\n",
    "ax.set_xticks(X_lab1) # choose which x locations to have ticks\n",
    "ax.set_xticklabels(X_lab1,rotation='vertical') # set the labels to display at those ticks\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.xlabel(\"Values of Lambda and Learning Rate\")\n",
    "l = plt.legend()\n",
    "#plt.savefig('ValidationAccuracy_SGD_con_GSC.pdf', bbox_inches='tight')\n",
    "#plt.savefig('ValidationAccuracy_SGD_con_GSC.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_lab</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.1,1.0</td>\n",
       "      <td>61.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.09,1.0</td>\n",
       "      <td>62.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.08,1.0</td>\n",
       "      <td>62.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.1,1.5</td>\n",
       "      <td>62.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.07,1.0</td>\n",
       "      <td>62.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.09,1.5</td>\n",
       "      <td>63.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.08,1.5</td>\n",
       "      <td>63.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.06,1.0</td>\n",
       "      <td>63.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.07,1.5</td>\n",
       "      <td>63.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.05,1.0</td>\n",
       "      <td>63.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.1,2.0</td>\n",
       "      <td>63.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.09,2.0</td>\n",
       "      <td>63.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.06,1.5</td>\n",
       "      <td>63.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.08,2.0</td>\n",
       "      <td>63.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.04,1.0</td>\n",
       "      <td>63.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.07,2.0</td>\n",
       "      <td>63.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.05,1.5</td>\n",
       "      <td>64.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.1,2.5</td>\n",
       "      <td>64.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.09,2.5</td>\n",
       "      <td>64.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.06,2.0</td>\n",
       "      <td>64.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.08,2.5</td>\n",
       "      <td>64.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.07,2.5</td>\n",
       "      <td>64.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.03,1.0</td>\n",
       "      <td>64.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.05,2.0</td>\n",
       "      <td>64.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.04,1.5</td>\n",
       "      <td>64.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.06,2.5</td>\n",
       "      <td>64.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.08,3.0</td>\n",
       "      <td>64.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.09,3.0</td>\n",
       "      <td>64.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.1,3.0</td>\n",
       "      <td>64.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.07,3.0</td>\n",
       "      <td>64.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.03,10.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.04,3.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01,6.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.01,5.5</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.01,5.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.04,5.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.03,9.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.04,5.5</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.04,6.5</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.04,7.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.04,7.5</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.04,8.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.04,8.5</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.04,9.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.04,6.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01,7.0</td>\n",
       "      <td>64.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.1,4.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.06,4.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.08,4.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.04,3.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.04,4.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.04,4.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.07,5.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.07,4.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.05,4.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.05,4.5</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.05,5.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.06,4.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.06,5.0</td>\n",
       "      <td>64.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01,1.0</td>\n",
       "      <td>64.664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_lab  Accuracy\n",
       "171    0.1,1.0    61.880\n",
       "152   0.09,1.0    62.176\n",
       "133   0.08,1.0    62.504\n",
       "172    0.1,1.5    62.952\n",
       "114   0.07,1.0    62.984\n",
       "153   0.09,1.5    63.064\n",
       "134   0.08,1.5    63.216\n",
       "95    0.06,1.0    63.296\n",
       "115   0.07,1.5    63.456\n",
       "76    0.05,1.0    63.552\n",
       "173    0.1,2.0    63.688\n",
       "154   0.09,2.0    63.760\n",
       "96    0.06,1.5    63.768\n",
       "135   0.08,2.0    63.864\n",
       "57    0.04,1.0    63.928\n",
       "116   0.07,2.0    63.952\n",
       "77    0.05,1.5    64.016\n",
       "174    0.1,2.5    64.104\n",
       "155   0.09,2.5    64.104\n",
       "97    0.06,2.0    64.128\n",
       "136   0.08,2.5    64.152\n",
       "117   0.07,2.5    64.224\n",
       "38    0.03,1.0    64.240\n",
       "78    0.05,2.0    64.280\n",
       "58    0.04,1.5    64.280\n",
       "98    0.06,2.5    64.296\n",
       "137   0.08,3.0    64.328\n",
       "156   0.09,3.0    64.336\n",
       "175    0.1,3.0    64.344\n",
       "118   0.07,3.0    64.360\n",
       "..         ...       ...\n",
       "56   0.03,10.0    64.464\n",
       "61    0.04,3.0    64.464\n",
       "10    0.01,6.0    64.464\n",
       "9     0.01,5.5    64.464\n",
       "8     0.01,5.0    64.464\n",
       "65    0.04,5.0    64.464\n",
       "54    0.03,9.0    64.464\n",
       "66    0.04,5.5    64.464\n",
       "68    0.04,6.5    64.464\n",
       "69    0.04,7.0    64.464\n",
       "70    0.04,7.5    64.464\n",
       "71    0.04,8.0    64.464\n",
       "72    0.04,8.5    64.464\n",
       "73    0.04,9.0    64.464\n",
       "67    0.04,6.0    64.464\n",
       "12    0.01,7.0    64.464\n",
       "177    0.1,4.0    64.472\n",
       "102   0.06,4.5    64.472\n",
       "140   0.08,4.5    64.472\n",
       "62    0.04,3.5    64.472\n",
       "63    0.04,4.0    64.472\n",
       "64    0.04,4.5    64.472\n",
       "122   0.07,5.0    64.472\n",
       "121   0.07,4.5    64.472\n",
       "82    0.05,4.0    64.472\n",
       "83    0.05,4.5    64.472\n",
       "84    0.05,5.0    64.472\n",
       "101   0.06,4.0    64.472\n",
       "103   0.06,5.0    64.472\n",
       "0     0.01,1.0    64.664\n",
       "\n",
       "[190 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting Accuracy to get the optimized set of Lambda and Learning Rate\n",
    "d={\"X_lab\":X_lab,\"Accuracy\":Accuracy}\n",
    "g=pd.DataFrame(data=d)\n",
    "g.sort_values(by=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 4442  Correct :8058\n",
      "Testing Accuracy: 64.464\n"
     ]
    }
   ],
   "source": [
    "# This block of code checks the testing accuracy based on best values of hyperparameters\n",
    "for i in range(0,len(y)):\n",
    "    learningRate = 0.04   \n",
    "    La = 3\n",
    "    z = np.dot(X[i], W_Now)\n",
    "    h = sigmoid(z)\n",
    "    gradient = np.dot(X[i], (h - y[i]))\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(gradient,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "# Adding bias to testing feature matrix\n",
    "p_test=np.ones((len(GSC_test['feature_id_AconB']),1))\n",
    "r_test=np.array(GSC_test['feature_id_AconB'].values.tolist())\n",
    "X_test=np.hstack((p_test,r_test))\n",
    "X_test[1].shape\n",
    "\n",
    "# Extractig target values for testing set\n",
    "y_test=np.array(GSC_test['target'])\n",
    "\n",
    "# Taking results of Logistic Regression\n",
    "z_test = np.dot(X_test, W_Now)\n",
    "z_test.shape\n",
    "h_test_raw = sigmoid(z_test)\n",
    "h_test=np.around(h_test_raw)\n",
    "\n",
    "# Calculating Accuracy\n",
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    if (y_test[i]==h_test[i]):\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
